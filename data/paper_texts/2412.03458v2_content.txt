SECTION: Understanding the Impact of Evaluation Metrics in Kinetic Models for Consensus-based Segmentation

In this article we extend a recently introduced kinetic model for consensus-based segmentation of images. In particular, we will interpret the set of pixels of a 2D image as an interacting particle system which evolves in time in view of a consensus-type process obtained by interactions between pixels and external noise. Thanks to a kinetic formulation of the introduced model we derive the large time solution of the model. We will show that the choice of parameters defining the segmentation task can be chosen from a plurality of loss functions characterising the evaluation metrics.

SECTION: 1Introduction

The primary objective of image segmentation is to partition an image into distinct pixel regions that exhibit homogeneous characteristics, including spatial proximity, intensity values, color variations, texture patterns, brightness levels, and contrast differences, thereby enabling more effective analysis and interpretation of the visual data. The application of image segmentation methods plays an important role in clinical research by facilitating the study of anatomical structures, highlighting regions of interest, and measuring tissue volume[1,5,15,35,38,50]. In this context, the accurate recognition of areas affected by pathologies can have great impact for more precise early diagnosis and monitoring in a great variety of disease that range from brain tumors to skin lesions.

Over the past decades, a variety of computational strategies and mathematical approaches have been developed to address image segmentation challenges. Among these, deep learning techniques and neural networks have emerged as one of the most widely used methods in contemporary image segmentation tasks[26,27,32,31,48,55,56,33,34]. Leveraging a set of examples, these techniques are capable of approximating the complex nonlinear relationship between inputs and desired outputs. A different approach is based on clustering methods[14,23,29,46,45,30]. Recently, a kinetic approach for clustering problems for image segmentation has been introduced in[9,25]. In these works, microscopic consensus-type models have been connected to image segmentation tasks by considering the pixels of an image as a interacting system where each particle is characterised by its space position and a feature determining the gray level. A virtual interaction between particles will then determine the asymptotic formation a finite number of clusters. Hence, a segmentation mask is generated by assigning the mean of their gray levels to each cluster of particles and by applying a binary threshold. Among the various nonlinear compromise terms that have been proposed in the literature, we will consider the Hegselmann-Krause model described in[24,47]where it is supposed that each agent may only interact with other agent that are sufficiently close. This type of interaction is classically known as bounded confidence interaction function. As a result, two pixel will interact based on their distance in space and their gray level. The approach developed in[9]is based on the methods of kinetic theory for consensus formation. In the last decades, after the first model developed in[17,16,22,51], several approaches have been designed to investigate the emergence of patterns and collective structures for large systems of agents/particles[8,12,37]. To this end, the flexibility of kinetic-type equations have been of paramount importance to link the microscopic scale and the macroscopic observable scale[2,10,20,21,42,41,53].

In order to construct a data-oriented pipeline, we calibrate the resulting model by exploiting a family of existing evaluation metrics to obtain the relevant information from a ground truth image[4,13,18,28,36,52]. In particular, we will concentrate on the Standard Volumetric Dice Similarity Coefficient (Volumetric Dice), a volumetric measure based on the quotient between the intersection of the obtained segmented images and their total volume, the Surface Dice Similarity Coefficient, which is analogous to the Volumetric Dice but exploits the surface of the segmented images[7]. Furthermore, we test the Jaccard index, which is an alternative option to evaluate the volumetric similarity between two segmentations, and the-measure, which is a performance metrics which allows a balance between precision and sensitivity.

In more detail, the manuscript is organized as follows: In Section2we introduce an extension of the Hegselmann-Krause model in 2D and we present the structure of the emerging steady states for different values of the model parameters. Next, we present a description of the model based on a kinetic-type approach. Furthermore, we show how can this model be extended and apply for the image segmentation problem. In Section3we present a Direct Simulation Monte Carlo (DSMC) Method to approximate the evolution of the system and we introduce possible optimization methods to produce segmentation masks for given images. To this end, we introduce the definition of the principal optimization metrics used in the context of bio-medical images and their principal characteristics. In Section4we show the results for a simple case of segmenting a geometrical image with a blurry background and compare the results obtained for different choices of the diffusion function. Finally, we present the results obtained for various Brain Tumor Images and discuss how the choice of different metrics may affect the final result. We show that themeasure does not produce consistent results for different values of. We reproduce the expected relationship between the Volumetric Dice Coefficient and Jaccard Index and show that both metrics plus the Surface Dice Coefficient yield similar results. Nevertheless, we argue that for this type of images the Surface Dice Coefficient produces more accurate loss values and its definition is more representative compare to the Volumetric Dice Coefficient and Jaccard Index.

SECTION: 2Consensus modelling and applications to image segmentation

In recent years, there has been growing interest in exploring consensus formation within opinion models to gain a deeper understanding of how social forces affect nonlinear aggregation processes in multiagent systems. To this end, various models have been proposed considering different scenarios and hypothesis on how the pairwise interactions may lead to the emergence of a position. For a finite number of particles, the dynamics is usually defined in terms of first order differential equations having the general form

where,, characterise the position of the agentat time, andtunes the interaction between the agents, see e.g.[3,12,24,37,39].

In addition to microscopic agent-based models, in the limit of an infinite number of agents, it is possible to derive the evolution of distribution functions characterising the collective behavior of interacting systems. These approaches, typically grounded in kinetic-type partial differential equations (PDEs), are capable of bridging the gap between microscopic forces and the emerging properties of the system, see[41].

SECTION: 2.1The 2D bounded confidence model

We now consider the bidimensional case,, and we specify the interaction function based on the so-called bounded confidence model. In more detail, we consideragents and define their opinion variable through a vector, characterised by initial states. Agents will modify their opinion as a result of the interaction with other agents only if, whereis a given confidence level. Hence, we can write (1) as follows

where, beingthe characteristic function of the set. We can easily observe that the mean position of the ensemble of agents is conserved in time, indeed

thanks to the symmetry of the considered bounded-confidence interaction function. The bounded confidence model converges to a steady configuration, meaning that the systems reaches consensus in finite time. The structure of the steady state depends on the value of, see[42].

Furthermore, to account for random fluctuations given by external factors in the opinion of agents we may consider a diffusion component as follows

whereis a set of independent Wiener processes. The impact of the diffusion is weighted by the variable. To visualise the interplay between consensus forces and diffusion we depict in Figure1the steady configuration of the model (4) for different combinations of the model parameters. For, the system forms a finite number of clusters depending on the value of, as illustrated in Figure1(a). For values of the diffusion coefficient, the number of clusters of the system varies as depicted in Figure1(b). The right panel of Figure1(b) shows the scenario in which the diffusion effect becomes comparable to the tendency of agents to cluster. Finally, in Figure1(c), for, the diffusion effect dominates the grouping tendency, resulting in a homogeneous steady-state distribution.

SECTION: 2.2Kinetic models for consensus dynamics

In the limitit can be shown that the empirical density

of the system of particles (4) converges to a continuous densitysolution to the following mean-field equation

whereis defined as follows

see e.g.[11].

We can derive (5) using a kinetic approach by writingandfor a generic pairof interacting agents/particles and we approximate the
time derivative in (4) in a time step, through an Euler-Maryuama approach, in the same spirit as[10,44]. Hence, we recover the binary interaction rule

where,andare two independent 2D centered Gaussian distribution random variable such that

wheredenotes the integration with respect to the distribution.
Furthermore, in (7) we shall consider. We can remark that, if, sinceandwe get

since the interaction functionis symmetric, consistently with (3). This shows that the mean position is conserved at every interaction. Finally, we have

and the mean energy is dissipated at each interaction, since. Hence, we consider the distribution function, such thatrepresents the fraction of agents/particles inat time. The evolution ofas a result of binary-interaction scheme (7) is obtained by a Boltzmann-type equation which reads in weak form

beinga test function. As observed in[53], whenwe can observe that the binary scheme (7) becomes quasi-invariant and we can introduce the following expansion

beinga reminder term andthe Hessian matrix. Hence, scalingand the distribution, we may plug (12) in (11) to get

Following[9], see also[41], we can prove that

as. Hence, integrating back by parts the first two terms we obtain (5). In more detail,
we can prove thatconverges, up to extraction of a subsequence to a probability densitythat is weak solution to the nonlocal Fokker-Planck equation (5).

SECTION: 2.3Application to Image Segmentation

An application of the Hegselman-Krause model for data-clustering problems has been proposed in[25]. The idea is to extend the 2D model by characterising each particle with an internal featurethat represents the gray color of theth pixel. Therefore, we interpret each pixel in the image as a particle characterized by a position vector and the static featureas shown in Figure2.

To address the segmentation task, we can define a dynamic feature for the system of pixels through an interaction function that accounts for alignment processes among pixels with sufficiently similar features. In particular, let us consider the following

Therefore, the time-continuous evolution for the system of pixels is given by

In this case we introduced two confidence bounds,taking into account the position and the gray level of the pixels, respectively. In this way, the interactions between the pixels will form clusters where the pixels in each cluster share a similar gray level blue and are in close proximity within the image. This dynamics is represented in Figure3.

Biomedical images are often subject to ambiguities arising from various sources of uncertainty related to clinical factors and potential bottlenecks in data acquisition processes[5,31]. These uncertainties can be broadly categorized into aleatoric uncertainty, stemming from inherent stochastic variations in the data collection process, and epistemic uncertainty, relates to uncertainties in model parameters and can lead to deviations in the results. Aleatoric uncertainties poses significant challenges in image segmentation, as image processing models must contend with limitations in the raw acquisition data. Addressing these uncertainties is critical, and the study of uncertainty quantification in image segmentation is an expanding field aimed at developing robust segmentation algorithms capable of mitigating erroneous outcomes. To this end, in[9], it has been proposed an extension of (14) to consider segmentation of biomedical images. In particular, the particle model (15) has been integrated a nonconstant stochastic part to take into account aleatoric uncertainties arising from the data acquisition process. These uncertainties may include factors such as motion artifacts or field inhomogeneities in magnetic resonance imaging (MRI). They modified equation14as follows:

whereis set of independent Wiener processes,is the interaction function defined in (13), andquantifies the impact of diffusion related to the value of the feature. Since the aleatoric uncertainties are expected to appear far away from the static feature’s boundaries, only diffusion functions that are maximal at the center and satisfyare considered. Similarly to (7), we may introduce the following binary interaction scheme by writinga random couple of pixels having features. We get

whereand. At the statistical level, as in[9], we may follows the approach described in Section2.2. Hence, we introduce the distribution function, such thatrepresents the fraction of agents/particles incharacterized by a featureat time. The evolution ofwhose interaction follow the binary scheme (16) is given by the following Boltzmann-type equation

Hence, since the feature is not evolving in time, we can proceed as in Section2.2to derive in the quasi-invariant limit forthe corresponding Fokker-Planck-type PDE

where

SECTION: 3Evaluation metrics and parameters estimation

In this section we present classical Direct Simulation Monte Carlo (DSMC) methods to numerically approximate the evolution of (17) as quasi-invariant approximation of the Fokker-Planck equation (18). The resulting numerical algorithm is fundamental to estimate consistent parameters from MRI images. To this end, we present several loss metrics with the aim to compare the result of our model-based approach with existing methods for biomedical image segmentation. In this work we focus exclusively on binary metrics. For evaluation of segmentation with multiple labels we point the reader to[52]for a detailed presentation of various metrics.

SECTION: 3.1DSMC algorithm for image segmentation

The numerical approximation of Boltzmann-type equations has been deeply investigated in the recent decades, see e.g.[19,40]. The approximation of this class of equations is particularly challenging due to the curse of dimensionality brought up by the multidimensional integral of the collision operator, and the presence of multiple scales. Furthermore, the preservation of relevant physical quantities are essential for a correct description of the underlying physical problem[43].

In view of its computational efficiency, in the following we will adopt a DSMC approach. Indeed the computational cost of this method iswhereis the number of particles. Next, we describe the DSMC method based on a Nanbu-Bavosky scheme[40]. We begin by randomly selectingpairs of particles and making them evolve following the binary scheme presented in7. We consider a time intervalwhich we divide inintervals of size. We introduce the stochastic rounding of a positive real number x as:

whereis the integer part of. The random variableis sampled from a 2D Gaussian Distribution centered at zero and a diagonal covariance matrix.

SECTION: 3.2Generation of a model-oriented segmentation masks

In this section we present the procedure to estimate the Segmentation Mask of Brain Tumor Images. The procedure described in this section closely follows the methodology presented in[9]. In particular, for this work we used thebrain tumor datasetthat consists of 3D in multi-parametric MRI of patients affected by glioblastoma or lower-grade glioma, publicly available in the context of the Brain Tumor Image Segmentation Challengehttp://medicaldecathlon.com/. The acquisition sequences include-weighted, post-Gadolinium contrast-weighted,-weighted andFluid-Attenuated Inversion Recovery volumes. Three intra-tumoral structures were manually annotated by experienced radiologists, namely ”tumor core”, ”enhancing tumor” and ”whole tumor”. We evaluate the performances of the DSMC algorithm for two different segmentation tasks: the ”tumor core” and the ”whole tumor” annotations. For the first task we use a single slice in the axial plane of the post-Gadolinium contrast-weighted scans while for the second task we use a single slice in the axial plane of the-weighted scans. The procedure to generate the segmentation masks is as follows:

We begin by associating each pixel with a position vectorand with static feature. We scale the vector position to a domainand the static feature to.

We utilize the DSMC Algorithm1to numerically compute the long-term solution of the Boltzmann-type model described in (11), settingand. This approach enables pixels to aggregate into clusters based on their Euclidean distance and gray color level.

The segmentation masks are generated by assigning to the original position of each pixel the mean value of the clusters they belong to. In this way we generate a multi-level mask composed of a number of homogenous regions.

Finally we obtain the binary mask by defining a thresholdsuch that:

Following this procedure, we apply two morphological refinement steps to remove small regions that have been misclassified as foreground parts and to fill small regions that have been incorrectly categorized as background pixels. We begin by labeling all the connected pixels in the foreground and reassigning them to the background those whose number of pixels is less than a certain threshold. Then we repeat the same procedure but for the pixels in the background. To this end, we use the scikit-image python library that detects distinct objects of a binary image[54]. This allows us to obtain more precise segmentation masks by reducing small imperfections. This entire process is illustrated in Figure4.

In this section, we outline the procedure for optimizing the parameters,andto generate segmentation masks that best approximate the ground truth. The goal is to identify the parameter configuration that minimizes the discrepancy between the computed and ground truth masks, measured through a predefined loss metric.
To achieve this, we solve the following minimization problem:

whereis the Ground Truth segmentation mask andis the segmentation mask computed by the model. The differentmetrics quantify the discrepancy between the masks, with lower values indicating greater similarity. Accordingly, thefunction, detailed in Section3.3, measures the similarity between the two masks, with higher values indicating better agreement. The relationshipis satisfied when theis defined to take a value of 1 for perfect agreement and 0 for complete mismatch.

To solve the optimization problem (23), we used theHyperoptpackage[6]. This optimization method randomly samples the parameter configurations from predefined distributions and selects the configuration that minimizes themetric. This sampling process is repeated for a predefined number of iterations. In this work, we sample the values of our parameters from the following distributions:

whererepresents the distance between the initial positions of the pixels at. We perform 300 iterations of the optimization process. To ensure reproducibility and correctly compare the different results obtained, the random seed for parameter sampling is fixed.

SECTION: 3.3Segmentation Metrics

Next, we introduce the principal optimization metrics used for evaluating a binary segmentation mask. We definewhereandrepresent the set of pixels that belong to the background and foreground of the ground truth segmentation mask respectively. Same applies forbut for the binary mask we want to evaluate. One could also wish to asses the validity of a segmentation mask with multiple labels, we refer to[52]for an introduction to the subject. Figure5presents a summary of the key terms used in the definitions of metrics.

TheVolumetric Diceindex, also known as the Standard Volumetric Dice Similarity Coefficient, first introduced in[18], is the most used metric when evaluating volumetric segmentations. It is defined as follows:

where,indicates the total number of pixels of the considered region.
This metric is equal to one if there is a perfect overlap between the two segmentation masks and null if both segmentations are completely disjoint. Since the Volumetric Dice coefficient is the most commonly used metric for segmentations, especially in the biomedical field, the results are highly interpretable and can be compared with those obtained in other studies. However, when assessing surface segmentation masks, the Volumetric Dice coefficient can yield suboptimal results. This limitation arises because the Volumetric Dice coefficient evaluates the similarity between segmentation masks based on pixel overlap without considering the spatial accuracy of the boundaries. Specifically, it treats all pixel displacements equally, without considering how far a segmentation error might be from the true boundary of the object. This means that segmentations with minor errors spread across multiple areas and those with a major error in a single area might receive similar scores. To address this limitation, the Surface Dice Similarity Coefficient was presented in[38]as a metric that can assess the accuracy of segmentation masks by considering the similarity of their boundaries. We defineas a parameterization of, the boundary of the segmentation mask. The border region, which is a region around the boundarywith tolerance, is defined as:

where,is a positive real number that defines the maximum allowable distance from the boundaryfor a pointto be considered part of the border region.
The Surface Dice Similarity Coefficient betweenandwith toleranceis defined as:

, ranges from 0 to 1. A score of 1 indicates a perfect overlap between the two surfaces, while a score of 0 indicates no overlap. A larger value ofresults in a wider border region, making the metric more tolerant to small deviations in the boundary.

The Jaccard Index (JAC)[28], similar to the Volumetric Dice coefficient, measures the similarity between two segmentations by quantifying the overlap between the computed mask and the ground truth. It is defined as the ratio between the intersection and the union of the foreground’s segmentation masks

The JAC Index and the Volumetric Dice coefficient are closely related since we have

From (29) we get the relationship between the JAC index and the Volumetric Dice coefficient. While both are widely used for measuring segmentation similarity, they can produce slightly different results. To understand the implications of these differences, we can analyze how their absolute and relative errors are related.

A similarity S is absolutely approximated bywith errorif the following holds for all y and:

A similarity S is relatively approximated bywith errorif the following holds for all y and:

The following result holds

JAC and Volumetric Dice approximate each other with a relative error of 1 and an absolute error of.

We point the reader to[7]for a deeper comparison between the Jaccard and Volumetric Dice Index.

Themeasure is commonly used as an information retrieval metric[49,13]. To define this metric, we first introduce two terms: Positive Predicted Value (PPV) and True Positive Rate (TPR), which are also known as Precision and Sensitivity, respectively. ThePrecisionmetric quantifies the proportion of correctly predicted foreground pixels (true positives, TP) out of all pixels predicted as foreground (TP + false positives, FP). TheSensitivitymeasures the proportion of actual foreground pixels (TP) correctly identified by the model out of all actual foreground pixels (TP + false negatives, FN). These two metrics can be expressed as follows

ThePrecisionmetric indicates how many of the predicted foreground pixels are actually correct.
TheSensitivity, on the other hand, measures how many of the actual foreground pixels were correctly predicted by the model.

We can define themeasure as a combination of Precision and Sensitivity, with a parameterthat controls the trade-off between these two metrics. Specifically, themeasure is given by

We may observe that ifwe obtain the Volumetric Dice metric.

To understand the impact ofin themeasure, we can substitute the definitions of PPV and TPR into (31), which results in the following

Ifthemeasure emphasizes minimizing False Negatives (maximizing Sensitivity), which can lead to more False Positives (lower Precision).
Ifthemeasure focuses on minimizing False Positives (maximizing Precision), potentially increasing the number of False Negatives (lower Sensitivity).

Furthermore, it can be noticed that

since forwe neglect the contribution of the False Positives by considering only the contribution of the False Negatives where we re-obtain the TPR metrics defined in (30).

In summary, thanks to theparameter, the-measure offers a flexible way to evaluate segmentation models by allowing for a tunable balance between Precision and Sensitivity. It provides a useful metric when dealing with class imbalances, especially in the field of medical imaging, where the relative importance of false positives and false negatives can vary according to each segmentation task.

SECTION: 4Numerical Results

SECTION: 4.1Impact of different diffusion functions

In this section we study the impact of choosing different diffusion functions in images consisting of a blurry background and a geometric shape in the center, as shown in Figure7. The objective is to detect the shape of the Geometric Figure and to compare how the choice of different diffusion functions affect the value of the parameters. To this end, we chose the following diffusion functions:

We point the reader to Figure6for a summary of the various introduced diffusion functions in (34).

The resulting binary mask was the same for all choices of diffusion functions, obtaining the same loss function value. The results are shown in Figure7.
For both the square and circle images, the Surface Dice Coefficient was used to optimize the parameters with a tolerance equal to the length of 1 pixel. Both images have a shape ofpixels. The final time was set towith. In the case of the square Figure7(a) we can see from Table1that forandthe values ofdo not differ greatly for this two diffusion functions. In the case ofwe obtain a slightly smaller value for. Lastly, forwe obtain a higher value of. If we look at Figure6we notice thatexcept atwhere both functions are equivalent. So a bigger value of the diffusion functions is counter by a smaller value ofso as to obtain a similar diffusion effect. This holds also forandfor the circle Figure7(b). Furthermore, comparingandfor the square image we can see that the resulting parameters are smaller for. This is consistent because again we can see from Figure6that. If we now compareandfor the circle image we can see that the value ofis similar in this case. Nevertheless, in this case the difference is given by the values ofand, which are both smaller for.
This indicates that, for different diffusion functions, the optimal parameters adjust to yield similar results. A very straight way is to obtain similar values ofandand a lower value offor the diffusion function that has a higher value as in the case of the square image. However, the example of the circle image shows that us that we can also obtain different combinations of parameters so as to counter the effect of a bigger diffusion function.

From Table2we can see the parameters obtained by minimizing three different optimization metrics using as a diffusion functionfor the square image. For all the cases the resulting Surface Dice was equal to one indicating a perfect overlap between the computed and the ground truth segmentation masks.
The resulting binary mask obtained were the same for the three examples and are equivalent to the ones shown in Figure7.
For the Volumetric and Surface Dice Coefficient we can see that the parameters obtained where identical. Nevertheless, for the Jaccard Index, the resulting parameters differed, being smaller in this case. The loss is null in both cases, coherently with the relationship (29).

SECTION: 4.2Optimization Metrics for Tumor Image Analysis

In this section we study the impact that the different optimization metrics have on the resulting binary mask for the Core and Whole Tumor. We also analyze the parameters obtained for the different optimization metrics. Both brain tumor images consist ofpixels. For the optimization procedure we determineand. For each segmentation mask generated we evaluateddifferent combinations of parameters. Figure8show the Segmentation Masks obtained for both the Whole and Core Tumor by optimizing the Jaccard Index and the Volumetric Dice Coefficient. In Table3the resulting parameters and the loss obtained for both optimization metrics are presented, in this case the loss is equal to 1 for a perfect overlap and 0 if the images are totally disjoint. First, we can observe that the loss obtained with both metrics satisfy (29) as expected. It can be noticed that for both segmentation masks the loss obtained is greater for the Volumetric Dice Coefficient. Furthermore, the parameterobtained with both optimization metrics is similar for both the Core and Whole Tumor. Nevertheless, we can see that for the Whole Tumor theparameter obtained with the Jaccard Index is bigger than the one obtained with the Volumetric Dice Coefficient. For the case of the Core Tumor instead theparameter is bigger for the Volumetric Dice Coefficient. If we compare this to the values obtained forin both cases for both metrics we can see that a bigger diffusion value is countered by a smaller value ofso as to obtain similar Segmentation Masks as seen from Figure8.

For the Surface Dice Coefficient, the tolerancewas set to the length of 1 pixel, both when used as the optimization loss and when used as the evaluation metric. In Figure9shows the resulting binary mask obtained with the Surface Dice Coefficient and the Volumetric Dice Coefficient for the Core and Whole Tumor. In the case of the Whole Tumor the loss obtained with Surface Dice Coefficient is smaller than the one obtained with the Jaccard Index and the Volumetric Dice Coefficient. For the Core Tumor the loss obtained with the Surface Dice Coefficient is similar to the one reported by the Jaccard Index and both are smaller than the obtained with the Volumetric Dice Coefficient. For the Whole Tumor we can see that the resulting parameters are similar for all the optimization metrics. Nevertheless, for the Core Tumor we can notice that the parameters obtained with the Surface Dice Coefficient differ compared to the ones obtained with the Jaccard Index and the Volumetric Dice Coefficient. In particular, we obtained a smaller value forand slightly bigger value for. This indicates that a smaller value for the diffusion of the particles is compensated by allowing the particles to aggregate with others that are slightly more separated than in the case of the Volumetric Dice and Jaccard Index. Given that both the Volumetric Dice Coefficient and the Jaccard Index are a measure of the superposition between two volumes (in this case two surfaces) they do not represent the proximity between two surfaces making the Surface Dice Coefficient more suitable to use as a loss metric when comparing two different surfaces.

For themeasure it can be noticed that the loss reported decreases as the value ofincreases as seen from11. This means that the Segmentation Mask obtained should be more accurate for smaller values of. Nevertheless, if we look at Figure10for the case offor the Core Tumor, where the loss reported is maximum, it can be seen that the resulting binary mask shows areas of missing pixels in the tumor area. If we recall (31) we can see that forthe False Negatives weigh less than the False Positive in the loss function. Thus, for low values ofthe loss will try to minimize the number of False Positives and produce more False Negatives without having a significant impact on the loss value. This suggests that the Fβ-measure may not be the most representative metric for these types of segmentation masks and for this segmentation method.

SECTION: Conclusions

In this paper we presented a consensus-based kinetic method and show how can this model can be applied for the problem of image segmentation. The pixel in a 2D image is interpreted as a particle that interacts with the rest through a consensus-type process, which allows us to identify different clusters and generate an image segmentation. We developed a procedure that allows us to approximate the Ground Truth Segmentation Mask of different Brain Tumor Images. Furthermore, we presented and evaluated different optimization metrics and study the impact on the results obtained. In particular we found that the Jaccard Index and the Volumetric and Surface Dice Coefficient are appropriate metric to optimize our model. Nevertheless, given that the Surface Dice Coefficient is measure of discrepancy between the boundaries of two surfaces it is a better representation compared to the Jaccard Index and the Volumetric Dice Coefficient as they account only for absolute differences and do not attain to point wise differences. Furthermore, we assessed the use of the-Loss as a potential optimization metric. We found that both the loss values and the corresponding results were difficult to interpret, as low loss values often corresponded to low accuracy, making this metric challenging to apply effectively for optimization in this context. Future researches will focus on the case of multidimensional features and potential training methods for the introduced model.

SECTION: Acknowledgments

M.Z. is member of GNFM (Gruppo Nazionale di Fisica Matematica) of INdAM, Italy and acknowledges support of PRIN2022PNRR project No.P2022Z7ZAJ, European Union - NextGeneration EU. M.Z. acknowledges partial support by ICSC – Centro Nazionale di Ricerca in High Performance Computing, Big Data and Quantum Computing, funded by
European Union – NextGenerationEU

SECTION: References