SECTION: GenView: Enhancing View Quality with Pretrained Generative Model for Self-Supervised Learning

Self-supervised learning has achieved remarkable success in acquiring high-quality representations from unlabeled data. The widely adopted contrastive learning framework aims to learn invariant representations by minimizing the distance between positive views originating from the same image. However, existing techniques to construct positive views highly rely on manual transformations, resulting in limited diversity and potentially false positive pairs. To tackle these challenges, we present GenView, a controllable framework that augments the diversity of positive views leveraging the power of pretrained generative models while preserving semantics. We develop an adaptive view generation method that dynamically adjusts the noise level in sampling to ensure the preservation of essential semantic meaning while introducing variability. Additionally, we introduce a quality-driven contrastive loss, which assesses the quality of positive pairs by considering both foreground similarity and background diversity. This loss prioritizes the high-quality positive pairs we construct while reducing the influence of low-quality pairs, thereby mitigating potential semantic inconsistencies introduced by generative models and aggressive data augmentation. Thanks to the improved positive view quality and the quality-driven contrastive loss, GenView significantly improves self-supervised learning across various tasks. For instance, GenView improves MoCov2 performance by 2.5%/2.2% on ImageNet linear/semi-supervised classification. Moreover, GenView even performs much better than naively augmenting the ImageNet dataset with Laion400M or ImageNet21K.

SECTION: 1Introduction

Self-supervised learning (SSL) has demonstrated remarkable capability in acquiring robust and generalized visual representations from abundant unlabeled data sources[63,27,94,64,33,16,30,106,39,32,50,2,26,90,97,15,104], which can be transferred or leveraged in downstream tasks. Among the various approaches within SSL, Contrastive Learning (CL)[16,17,12,13,19,106,39,52]has emerged as a prominent method, showcasing its effectiveness in numerous downstream tasks (e.g., classification[35,51,98], detection[72,53,34,9,87], and segmentation[28,59,92,48,49]). CL aims to learn invariant representations that remain consistent across various conditions or environments by maximizing the similarity of representations obtained from different distorted versions of a sample, referred to as positive views. Consequently, the construction of high-quality positive views is crucial for CL. A high-quality positive view should retain the semantics of the original images while introducing as much semantic-irrelevant attribute diversity and environmental variations as possible, such that the learned representations can be more generalizable for downstream tasks.

Current CL methods[16,30,12,17,13]often employ predefined image augmentations (e.g., random cropping, color distortions, and Gaussian blur) on the same instance to obtain positive views. However, they face two limitations:(1) Limited Diversity: Standard augmentations only modify surface-level visual characteristics and fail to introduce new content to capture high-level variations, such as different object viewpoints, textures, or variations within a semantic category. This limitation hinders performance in domains with high intra-category diversity.(2) False Positive Risk: Aggressive augmentations are not always precise, potentially leading to false positive pairs. As depicted inFig.1(a), random cropping of distant patches may miss the entire object, which could mislead the representation learning by minimizing the distance between the object and background in the embedding space. Additionally, as shown inFig.1(b), cropping nearby patches may fail to introduce sufficient object variations, causing limited diversity in another way.
Advanced methods, such as employing stronger augmentations while preserving task-relevant information[84], saliency-guided sampling[79], and center-suppressed sampling[66], have been developed to create informative positive pairs. Some methods expand the diversity of positive pairs by utilizing information from the entire training dataset[12,23]. However, these methods primarily concentrate on optimizing positive views within an instance without introducing new content or incorporating additional information beyond the existing dataset. Consequently, they still have limited ability to capture extensive high-level variations.

Generative models, such as Stable Diffusion[74]and DALL-E2[69], have been very successful in generating high-quality diversified images conditioned on an image or embedding. These off-the-shelf pretrained models could help enrich view contents given an image due to their abundant prior knowledge learned from large-scale datasets[77,14]. Albeit they have been leveraged for image classification to address data scarcity[10,76,107,22,85,8,105,100], integrating pretrained generative models to pair the images for self-supervised learning is NOT a trivial problem. Despite the strong generative ability, these models may be pretrained on the datasets from different distributions, and the sampling process is not determinant. As a result, they will still inevitably face the risk of generating images with different semantics from the conditional images, resulting in false positive pairs. This presents a key challenge: how to appropriately control the randomness of generation while maintaining semantic consistency to help SSL in a controllable way.

To address these challenges, we introduceGenView, a controllable framework that enhances view quality for SSL using the powerful pretrained generative model, and guide contrastive learning via quality assessment. In our framework, as shown inFig.1, given an image as the source view, we construct its positive view using the synthetic image sampled from a pretrained generative model conditioned on this image. To optimally balance the trade-off between diversity and semantic fidelity, we develop an adaptive view generation method, which dynamically adjusts the noise level of the generative model to control the extent of perturbation applied to the conditional image embedding. We calculate the proportion of the foreground area within an input image. If the subject is not prominent with a low foreground proportion, it reduces the perturbation strength to ensure the correct semantic content of the synthetic image. If the subject is clear and distinguishable with a high foreground proportion, it increases the perturbation strength to create more variations for more diverse content and environments. As depicted inFig.1(c), the view constructed by our method has a different pose and environment compared to the traditional way.

Even with our adaptive view generation, false positive pairs are still inevitable because both the sampling of the generative model and cropping are not determinant. To further mitigate the effect of potential false positive pairs that could mislead contrastive learning, we introduce a quality-driven contrastive loss to guide the contrastive learning with pair quality. Concretely, we assess the quality of positive pairs considering both foreground similarity and background diversity. It prioritizes the positive pairs with high foreground similarity to ensure semantic coherence, while also favoring the pairs with low background similarity to promote diverse environments for learning invariant representations. We then recalibrate the contrastive loss function by reweighting each pair with its pair quality, which enhances the contributions of high-quality positive pairs, and simultaneously reduces the influence of low-quality and even false pairs. As illustrated inFig.1(c) and (d), our quality-driven contrastive loss assigns a higher score to the high-quality positive pair and a lower score to the pair with a relatively lower quality. In summary, the contributions of this paper include:

We introduce GenView framework, which enhances the view quality for SSL leveraging the power of pretrained generative model in a controllable way. An adaptive view generation method is developed to construct positive views, balancing the trade-off between diversity and semantic fidelity.

We propose a quality-driven contrastive loss that prioritizes high-quality positive pairs to guide the contrastive learning with pair quality, further mitigating the impact of low-quality and false pairs.

In experiments, GenView significantly enhances the performance of popular contrastive learning algorithms including MoCov2[17], SimSiam[18], BYOL[30], and MoCov3[19]on various downstream tasks such as linear/semi-supervised classification, semantic segmentation, and object detection. Particularly, GenView also performs better than naively augmenting the ImageNet1K dataset with Laion400M or ImageNet21K.

SECTION: 2Related Work

Self-supervised learning is a promising paradigm for representation learning, relying on unlabeled data and pretext tasks such as auto-encoders[65,86], image pixel generation[29,44], rotation prediction[27], jigsaw puzzles[63], and mask image modeling[4,32]. In recent years, contrastive learning (CL) methods[94,64,83,33,16,17,106,40]have significantly improved SSL by reducing the distance between representations of positive pairs and increasing the distance between representations of negative pairs in the latent feature space simultaneously. Complementing CL approaches, various non-CL methods have emerged, seeking alternatives to negative samples and strategies to prevent network output collapse[11,1,47,12,18,30,102,24,13].

The construction of a pair of views is crucial in contrastive learning[83,16,12], and traditional SSL generates positive views through hand-designed augmentations, which may face limited diversity and induce semantically irrelevant pairs. Later studies introduce stronger augmentations preserving task-relevant information[84], unsupervised saliency maps for cropping constraints[79], and center-suppressed sampling for increased diversity[66]. Clustering-based methods[11,12]and neighborhood-based methods[23]expand the diversity of positive pairs by leveraging information from the training dataset. However, the diversity introduced is ultimately confined to the scope of the training dataset, limiting the ability to capture extensive diversity for learning more generalizable representation. In our method, we break free from this limitation by utilizing the pretrained image-conditioned generative model for high-quality view generation.

Various generative models, including VAEs[44,71], GANs[29,42,7,54], autoregressive models[70], and diffusion models[37,38,74,69,6,91](DMs), have demonstrated the ability to create highly realistic images. Particularly, DMs such as Imagen[75], GLIDE[62], Stable Diffusion[74], and DALL-E2[69], trained on extensive large-scale datasets such as LAION-5B[77]and CC12M[14], have excelled in generating photorealism images.
Recent research has explored generative models for data augmentation in various tasks, including classification[36,55,80,22,100,61,76], segmentation[56,96,88,92,48], and test-time optimization[25]. In representation learning, GANs[81], instance-conditioned GANs[3,99], neural transformation networks[43], and DMs[101]have been employed to introduce more variations. However, the diversity introduced is still constrained by the training dataset used for SSL.

Instead of training generative models from scratch, some methods use pretrained generative models to augment representation learning, leveraging the prior knowledge learned from large-scale datasets[77,14]to enhance the high-level diversity of the generated views[41,36,82,22,80,85,103]. However, these models rely onconstant[36,22,82,103]orrandom[80,85,107]hyperparameters to determine the extent of deviation in the generated images. This can lead to uncontrolled data generation characterized by inconsistent semantics with the conditional image, reducing the quality of positive pairs. In contrast, our approach employs adaptive view generation that controls the noise level when sampling images to keep a balance between semantic fidelity and diversity based on individual image characteristics. We also propose a quality-driven contrastive loss to enhance the contributions of high-quality positive pairs while diminishing the impact of low-quality and false pairs.

SECTION: 3Method

In this section, we first provide a review of self-supervised learning inSec.3.1. We introduce our framework inSec.3.2. Then, we develop adaptive view generation and quality-driven contrastive loss inSec.3.3and3.4.

SECTION: 3.1Preliminaries on Self-Supervised Learning

Current SSL frameworks often create positive pairs () for each instancein a batch ofimages. These pairs are generated by applying random predefined augmentations to the same instance:

where the augmentations,and, can either be from the same () or from different distributions (,).
The encoder networkis then applied toto extract the representation, resulting in. These representations are projected into an embedding space using a two-layer non-linear projection head, denoted as. Additionally,can be encoded using the same encoder and projection head as[18,12], or their momentum-updated versions[33,30].

Various SSL frameworks, including SimCLR[16]and MoCo[33], use the noise contrastive estimation objectiveto distinguish between instances:

withas the temperature parameter. Additionally, methods like BYOL[30]and SimSiam[18]introduce a non-linear predictor headto mapto, minimizing negative cosine similarityas:

SwAV[12]employs a linear mapping of positive embeddingsandto learned prototypes to obtain “codes”and. The targets are transformed with a Sinkhorn-Knopp () step. Then the Kullback-Leibler divergence lossis computed as:

In experiments, we will integrate GenView on all these popular SSL methods to test its generalizability.

SECTION: 3.2Our Framework

The framework of our method is depicted inFig.2. Traditional methods face the challenge of limited view diversity by generating positive pairs by applying augmentation twice to the same instance, as illustrated inEq.1. To this end, we employ an image-conditioned pretrained generative model to enhance the view quality. Specifically, we utilize the Stable unCLIP model, an extension of Stable Diffusion[74]with unCLIP[69], fine-tuned to accept CLIP[68]ViT-H/14 image embeddings in addition to text encodings. To improve the diversity of positive views, we inject Gaussian noise perturbations to the conditional image embedding through a diffusion process, which addssteps of Gaussian noise to the conditional image embedding. The degree of variation in the final images is controlled by the perturbation strength, with a higher value leading to an increased diversity.

The generation stage starts with a random normal distribution, whererepresents the denoising steps of the generation process. The pretrained diffusion model, conditioned on the noisy image embeddings, iteratively denoises the latent features. The synthetic positive view can be defined as:

whererefers to the pretrained parameters of the generative model, andrepresents the conditional image embedding obtained from the CLIP image encoder as.

We then design a pair construction mechanism by leveraging the original image as one view and pairing it with another view generated by the generative model for contrastive learning.
Specifically, hand-designed data augmentations (for the original image andorfor the synthetic image) are applied to create an enhanced pair of positive views:

Through this mechanism, we significantly increase view diversity by leveraging the capabilities of the generative model, as illustrated inFig.1. Meanwhile, unlike most generative model-based augmentation methods[81,43,99], which generate positive pairs from two synthetic images derived from the same original image, GenView integrates the original image itself as one of the views. This approach effectively controls potential feature drift caused by domain differences between the dataset used to train the generative model and the current pre-training dataset. Furthermore, when the synthetic image contains noise, such as artifacts or semantic discrepancies, the presence of the original real image prevents excessive deviation in feature learning. Thus, while enhancing the view diversity, our framework maintains stability and fidelity when combining the traditional augmentation with the strength of the generative model.

SECTION: 3.3Adaptive View Generation

To address the concerns related to inappropriate noise levels during image generation, we develop an adaptive view generation method, which dynamically adjusts the noise level based on the proportion of the foreground content. This introduces diverse positive pairs while ensuring coherent subject semantics. Given a conditional image, we employ a pretrained CLIP image encoderto extract latent features, where,, andrepresent the height, width, and the dimension of features, respectively. To separate the image’s main object from the background, we perform Principal Component Analysis (PCA) among features for all images and obtain the first component. Then, we apply min-max normalization to generate attention maps, where higher values indicate a higher probability of being foreground content. The proportion of foreground content, denoted as, is calculated as follows:

whererepresents a binary thresholding function withas the threshold. To map the proportion to the noise level, we introduce a function. The range of the ratiois evenly divided into 5 intervals, and values are mapped to discrete scales:. To reduce the risk of excessive distortion from higher noise levels, we limit the maximum at 400, even though noise levels during training could reach up to 1000. The adaptive noise levelis calculated as follows:

whererounds down to the nearest integer. Our approach adapts noise levels to the characteristics of images, and thus effectively balances the trade-off between semantic fidelity and diversity in generated images. As illustrated inFig.3, the selected noise level (in blue) is low for the images with a lower foreground proportion to better preserve their semantic contents, while for those with a higher proportion, a high noise level is adopted (in green) to introduce more diversity because the key subjects are less likely to be changed or disappeared in their generated images. The adaptive generated positive view is defined as:

This process works in an offline manner before SSL training, so does not increase the burden on training time. Besides, the offline view generation is once-for-all and the generation result can be re-used multiple times for various baselines.

SECTION: 3.4Quality-driven Contrastive Loss

In this section, we introduce a quality-driven contrastive loss that guides contrastive learning by assessing the quality of positive pairs. It prioritizes the pairs with high foreground similarity and low background similarity to facilitate the learning of invariant representations.

Given a pair of positive views, we employ a frozen encoder that is pretrained by CLIP (without accessing the dataset for SSL), denoted as, to extract feature maps. PCA is performed on feature maps, and min-max normalization is applied to the first component of PCA features, generating foreground attention maps. The background activation map for the-th sample is defined as. Subsequently, we use these maps to aggregate feature maps into foreground and background representations, yielding, which can be computed as follows:

where the operationrepresents spatial aggregation defined as. We calculate the foreground-foreground similarityand background-background similarityas follows:

wheredenotes the cosine similarity of the input representations. Next, we introduce a quality score for each positive pair:

We then propose a re-weighting factor denoted as, based on the computed pair qualities of a batch of images, to adjust the contribution of each pair to the overall loss during contrastive training:

The re-weighting factoris used to balance the influence of different pairs, allowing us to prioritize the pairs with higher foreground similarity and lower background similarity, and also mitigate the potential influence of those low-quality or wrong positive pairs. The final contrastive loss is defined as:

wherecan be any contrastive loss in Eqs. (2)-(4).

SECTION: 4Experiments

We compare GenView with state-of-the-art SSL methods, including MoCov2[33], BYOL[30], SwAV[12], SimSiam[18], and MoCov3[19]. We experiment with various network architectures, such as ResNet-18[35], ResNet-50[35], ViT-S[21], and ViT-B[21]. By default, ResNet-50 serves as the backbone. ViT-S and ViT-B are adopted for comparison with MoCov3. For details on adaptive view generation and quality-driven contrastive loss implementations for different pretraining datasets, please refer to theAppendices0.Aand0.C.

SECTION: 4.1Main Results

Linear classification.

GenView is framework-agnostic, allowing flexibility with SSL frameworks and associated training components like backbone networks, loss functions, and optimizers. To ensure fair comparisons, we maintain consistent pretraining settings as baseline methods on ImageNet-1K[20](IN-1K). To evaluate our method, we follow a standard linear classification protocol, as described in previous works[16,17,30]. The linear classifier is trained on top of the frozen representation for 90 epochs with a batch size of 1,024, an initial learning rate of 0.4, an SGD optimizer with 0.9 momentum and no weight decay, and the cosine-annealed learning rate schedule[60]. For ViT-based models, the initial learning rate is set to 12.LABEL:tab:exp_linearpresents the results of top-1 accuracy on the validation set of IN-1K.
GenView consistently improves SSL performance across various frameworks, including ResNet-50 and Transformer architectures like ViT-S and ViT-B. Its effectiveness is maintained across different pretraining epochs, outperforming the MoCov3 baselines pretrained for 100 or 300 epochs. GenView outperforms C-Crop[66]that also constructs better views, highlighting our advantage in utilizing pretrained generative models’ prior knowledge to create diverse views in a controlled manner. GenView can complement both contrastive (e.g.MoCov2 and MoCov3) and non-contrastive methods (e.g.BYOL, SimSiam, and SwAV), addressing their limitations of positive pair quality. These results demonstrate GenView’s consistent ability in enhancing the linear classification performance of various SSL models. It’s noted that when GenView is integrated with MoCov3 utilizing a ResNet-50 backbone and pretrained over 300 epochs, it achieves competitive performance (74.8% with 1.28 million images) compared to CLIP (74.3% on WebImageText with 400 million pairs), highlighting GenView’s efficiency.

We evaluate the fine-tuning performance of the pretraind models for semi-supervised classification with 1% and 10% of labeled IN-1K samples, selected by SimCLR[16]. We fine-tune the models for 20 epochs with the classifier learning rate 1.0 (0.2) and backbone learning rate 0.00001 (0.02) for 1% (10%) subset with a cosine-annealed scheduler.Tab.3presents the results of top-1 and top-5 accuracy on the validation set of IN-1K. Our method consistently outperforms the baseline approaches across different training durations. With 1% labels, GenView pretrained for 200 epochs with MoCov2 achieves an improvement of +8.5% in top-1 accuracy, and the one pretrained for 300 epochs with MoCov3 still improves top-1 accuracy by +1.9%.

We evaluate the transfer learning performance of the pretrained models on MS-COCO object detection and instance segmentation benchmarks[58]. The models are pretrained on IN-1K for 200 epochs, followed by fine-tuning on thesplit and evaluation on thesplit. We use a batch size of 16 and follow Detetron2’sschedule[93], consisting of 90k training iterations with learning rate decay at the 60k-th and 80k-th iterations by a factor of 10. Both tasks utilize Mask R-CNN[34]with ResNet-50-FPN[57]backbone.Tab.3presents the results of bounding box AP and instance mask AP. We observe that GenView is also able to enhance the downstream performances. When integrated on SimSiam, MoCov2, and BYOL, GenView excels in all metrics for detection and instance segmentation, highlighting its capacity to improve representation learning for complex localization and pixel-level tasks. Additionally, FreeATM also generates the same number of images as GenView using augmented prompts[103]. We notice that GenView surpasses FreeATM on object detection even without relying on text prompts, emphasizing our approach’s effectiveness.

We evaluate our method by comparing it to traditional data augmentation techniques. We extend IN-1K by incorporating 0.3 million images from Laion400M[78]and 0.3 million from ImageNet-21K[73](IN-21K). All experiments utilize MoCov3 with ResNet-50, which is pretrained for 50 epochs on these extended datasets.Tab.5presents the results of linear evaluation on IN-1K. Expanding IN-1K with Laion400M (2nd row) or synthetic images (4-th row) yields a slight improvement in top-1 accuracy, suggesting a limited contribution when directly incorporating images with domain gap. Extending IN-1K with IN-21K improves more than Laion400M, indicating the benefits from more training data in a similar domain. The most impressive results are obtained when using our framework with only 0.15 million generated images, leading to a remarkable 3.2% improvement in top-1 accuracy, demonstrating that the effectiveness of our framework mainly stems from better pair construction, instead of introducing more training data.

To evaluate GenView’s effectiveness in enhancing SSL models compared to existing positive view construction methods, we conduct pretraining and evaluation on CIFAR-10[45](CF10), CIFAR-100[45](CF100), and Tiny ImageNet[46](TinyIN) datasets. We train ResNet-18[35]for 500/500/200 epochs on CF10/CF100/TinyIN. For linear evaluation on validation sets of these datasets, the classifier is trained for 100 epochs using the SGD optimizer with a cosine-annealed learning rate of 0.2, no weight decay, and momentum of 0.9. As shown inTab.5,
the methods are categorized based on the source of variance they use in data augmentation: within instance, within the pretraining datasets, and beyond the pretraining datasets. GenView, when combined with MoCov2, consistently outperforms the other data augmentation methods in SSL, demonstrating its effectiveness in borrowing rich knowledge from large-scale datasets to construct high-quality positive views.

SECTION: 4.2Ablations

We evaluate the contributions of individual components as well as their combinations. ResNet-18 models are pretrained on IN-100 for 100 epochs using MoCov3 as the baseline, with a batch size of 512. IN-100 is a subset of IN-1K selected by[83]. For conditioning the generation of positive views with GenView, we employ 50,000 randomly selected class-balanced images from IN-100. We use a cosine decay learning rate schedule and employ the LARS optimizer with a learning rate of 1.2, weight decay of 1e-6, and momentum of 0.9. Linear evaluation settings are consistent with those detailed inLABEL:tab:exp_linear, with a training duration of 50 epochs.Tab.8offers valuable insights:
(1) Utilizing our framework but without our adaptive view generation significantly enhances accuracy, achieving a top-1 accuracy improvement of 5.98% compared to the baseline.
(2) The incorporation of adaptive view generation further elevates model performance, resulting in an improvement of 8.44% (from 65.52% to 73.96%).
(3) The quality-driven contrastive loss also plays a pivotal role in our framework. It can further improve the performance of adaptive view generation. Applying the quality-driven contrastive loss to the baseline method leads to a modest gain of 1.45% (from 65.52% to 66.97%). However, when combined with our framework, a more substantial performance improvement of 3.38% (from 71.50% to 74.88%) is observed. This highlights the effectiveness of our framework and also the importance of the proposed modules in enhancing contrastive learning by improving the quality of positive pairs.

We examine the impact of different noise level selection strategies on SSL performance inTab.8. Three strategies are compared: Constant Selection (CS), Random Selection (RS), and Adaptive Selection (AS). CS applies a uniform noise levelto all samples, with experiments conducted at various fixed levels (CS(0), CS(100), CS(200), CS(300), CS(400)). RS introduces variability by randomly selecting noise levels from the set. AS dynamically adjusts noise levels based on the input image’s foreground proportion, as guided by Eq. (8).
We use the same pretraining and linear evaluation settings asTab.8. The results indicate that AS achieves the highest accuracy at 73.96%, demonstrating the advantage of dynamically adjusting noise levels according to input characteristics. CS and RS yield lower performance, because static or random noise levels may result in overly similar or false positive pairs.

The impact of different probabilities () for applying GenView augmentation is shown inTab.8. An increase of the probability () of applying GenView results in improved model performance, with top-1 accuracy consistently increasing from 62.39% atto 70.55% at. This highlights the significance of a higher GenView application probability in enhancing the model’s ability to learn meaningful representations. By default, we setfor all the experiments in our main results.

SECTION: 4.3Qualitative Evaluation

A qualitative illustration of the positive views constructed by GenView is shown inFig.4. The top rows display original images, and the bottom rows show images generated by GenView. This visualization demonstrates GenView’s capacity to introduce variations in background, pose, and view angle while preserving the main semantics, which is crucial for learning invariant representations. More visual examples are provided in theAppendix0.B.

SECTION: 5Conclusion

In this paper, we aim to address the challenge of creating diverse and semantically coherent positive views for SSL. We introduce GenView, a framework that leverages the ability of pretrained generative model in a controllable way to enhance the view quality. It employs an adaptive view generation method that dynamically adjusts noise levels for controlled variability. The quality-driven contrastive loss prioritizes high-quality positive pairs with greater foreground similarity and background diversity while diminishing the impact of low-quality or even false pairs.
Experiments demonstrate that GenView consistently improves the SSL performance in various tasks, and outperforms other view augmentation methods. Ablation studies analyze the efficacy of each component, and qualitative evaluation shows its effectiveness in constructing views with background, pose, and view angle variations.

SECTION: Acknowledgements

This work was supported in part by the National Natural Science Foundation of China under Grant 62376069, in part by Young Elite Scientists Sponsorship Program by CAST under Grant 2023QNRC001, and in part by Guangdong Basic and Applied Basic Research Foundation under Grant 2024A1515012027. The work was also supported by funding from KAUST Center of Excellence on GenAI, under award number 5940.

SECTION: References

SECTION: Appendix 0.AImplementation Details

We use the pretrained Stable unCLIP v2-1††https://huggingface.co/stabilityai/stable-diffusion-2-1-unclipmodel to generate image variations based on CLIP image embeddings. An empty string serves as the text prompt to avoid any reference to image contexts or object names. The noised image embedding with perturbation strengthis defined as:, where, andis the cumulative product ofvalues forranging from 0 to. Eachis defined as, withrepresenting the noise variance introduced at step, following the default linear schedule forfrom DDPM[37]. Higher values of the perturbation strengthresult in increased diversity in the generated images.

We use the pretrained CLIP ViT-H/14 backbone††https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K, which serves as the conditional image encoder in Stable UnCLIP v2-1, for the encoderused in determining the proportion of foreground content before image generation. This backbone generates 256 tokens with a dimension of 1280 from ainput resolution. For calculating PCA features, 10,000 images are randomly sampled from the original dataset. The thresholdinEq.7is selected to ensure that foreground tokens account for approximately 40% of the total tokens, providing a clear separation between foreground and background as depicted in theSec.3.3.

We employ the pretrained CLIP ConvNext-Base (with wide embedding dimension) backbone††https://huggingface.co/laion/CLIP-convnext_base_w-laion2B-s13B-b82K-augregas the encoderto extract feature maps from augmented positive views. These feature maps have a resolution ofbased on ainput resolution. We compute foreground and background attention maps using the PCA vector computation method described in the previous paragraph.

We generate one augmented image for each image in the training set of IN-1K/CF10/CF100/TinyIN, with(the number of denoising steps) set to 20 for efficiency. The classifier-free guidance scale[38]is set to 10 to ensure image quality. The diversity of generated images is controlled by the level of noise perturbations applied to the image embeddings. To match the original dataset sizes of IN-1K/CF10/CF100/TinyIN, we resize the generated images from their original resolution ofto///, respectively.

The baseline results of MoCov3 inLABEL:tab:exp_linearare from the public codebase††https://github.com/facebookresearch/moco-v3. Hyper-parameters for comparison with other SSL methods pretrained on IN-1K are listed inTab.9.

To expand IN-1K with additional training data without introducing new classes, we employ a retrieval-based technique[5]for expanding IN-1K with Laion400M. We query the entire Laion400M dataset with 0.3 million randomly sampled IN-1K images and select the most similar image for each query image. For expanding IN-1K with IN-21K, we randomly sample 0.3 million non-repeating images with labels matching those in the IN-1K dataset. For GenView, positive views are generated for 0.15 million randomly sampled IN-1K images. For the experiments, the ResNet-50 models are pretrained on the expanded dataset using a batch size of 512. We apply a cosine decay learning rate schedule and use the LARS optimizer with a learning rate of 1.2, weight decay of 1e-6, and momentum of 0.9. Linear evaluation settings align with those inLABEL:tab:exp_linear.

We compare our approach with several baseline methods, including ContrastiveCrop[66](C-Crop), ViewMaker[81], Neural Transform Network[43](NTN), Local Manifold Augmentation method[99](LMA), Diffusion-based augmentation from scratch[101](DiffAug), and-perturb[31]. For our experiments, we use three datasets: CF10, CF100, and TinyIN. We employ SGD as the optimizer with a learning rate of 0.5, weight decay of 5e-4, and momentum of 0.9. The learning rate follows a linear warm-up for 10 epochs and then switches to the cosine decay scheduler. Batch sizes are set to 512 for CF10 and CF100 and 256 for TinyIN. The momentum coefficient for the momentum-updated encoder and memory buffer size is set to 0.99/0.99/0.996 and 4096/4096/16384 for CF10/CF100/TinyIN, respectively. We use theloss with a temperature of 0.2 and train for 500 epochs on CF10 and CF100 and 200 epochs on TinyIN. The backbone architecture used is ResNet18 with an embedding dimension of 512 and a projection dimension of 128. We replace the first 7x7 Conv of stride 2 with 3x3 Conv of stride 1 and remove the first max-pooling operation. For data augmentations, we use random resized crops (the lower bound of random crop ratio is set to 0.2), color distortion (strength=0.4) with a probability of 0.8, and Gaussian blur with a probability of 0.5. The images from the CF10/CF100 and TinyIN datasets are resized to 32x32 and 64x64 resolution.

SECTION: Appendix 0.BAdditional Illustration

Further visualization and limitation analysis.

Fig.5displays positive pairs generated by GenView, showing its ability to introduce variations while maintaining semantic consistency. Notable observations include:

Capacity constraints in the CLIP conditional encoder or generator can challenge the accurate representation of long-tailed categories or complex scenes, resulting in less realistic generations, such as the generated airships (2nd row of the 9th column) and lobsters (2nd row of the 10th column).

The granularity of conditional images is crucial, as lower-resolution images can lead to a loss of detail and misclassification of the generated images. For instance, conditioning on a camel image in the 5th row of the 9th column with a resolution ofproduces a generated image resembling a horse, losing the camel’s distinctive features.

Partially visible objects, like the head of the king penguin in the 3rd row of the 9th column, may result in generation errors, yielding images resembling ducks (4th row of the 9th column) or birds (4th row of the 10th column).

Despite these limitations, GenView’s adaptive view generation method ensures that the synthesized samples maintain attributes similar to the conditional images, providing valuable information for SSL training. Additionally, our quality-driven contrastive loss mechanism addresses semantic inconsistencies, mitigating their impact on contrastive representation learning. Future work will focus on refining diffusion models to enhance generative augmentation and address the highlighted failure cases.

We calculate the average cosine similarity between the original images and their associated positive views for different methods. We randomly sample 50,000 images from IN-1K and compute the cosine similarity of CLIP image embeddings for each pair. The results are presented inTab.10, which demonstrates that our method produces positive views with significantly higher semantic similarity to the original images compared to the RS and Retrieval Laion400M methods.

SECTION: Appendix 0.CAlgorithm

The GenView algorithm is detailed in Algorithm1.