SECTION: PP-SSL: Priority-Perception Self-Supervised Learningfor Fine-Grained Visual Recognition

Self-supervised learning is emerging in fine-grained visual recognition with promising results. However, existing self-supervised learning methods are often susceptible to irrelevant patterns in self-supervised tasks and lack the capability to represent the subtle differences inherent in fine-grained visual recognition (FGVR), resulting in generally poorer performance. To address this, we propose a novel Priority-Perception Self-Supervised Learning framework, denoted as PP-SSL, which can effectively filter out irrelevant feature interference and extract more subtle discriminative features throughout the training process. Specifically, it composes of two main parts: the Anti-Interference Strategy (AIS) and the Image-Aided Distinction Module (IADM). In AIS, a fine-grained textual description corpus is established, and a knowledge distillation strategy is devised to guide the model in eliminating irrelevant features while enhancing the learning of more discriminative and high-quality features. IADM reveals that extracting GradCAM from the original image effectively reveals subtle differences between fine-grained categories. Compared to features extracted from intermediate or output layers, the original image retains more detail, allowing for a deeper exploration of the subtle distinctions among fine-grained classes. Extensive experimental results indicate that the PP-SSL significantly outperforms existing methods across various datasets, highlighting its effectiveness in fine-grained recognition tasks. Our code will be made publicly available upon publication.

SECTION: 1Introduction

Self-supervised learning (SSL)[47,13,1]have demonstrated impressive performance in various visual tasks like image classification[25], object detection[45], semantic segmentation[5]and image retrieval[13], enabling models to capture general feature representations without labeled data. Recently, an increasing number of self-supervised methods have been proposed, which can be roughly categorized into two groups: clustering-based methods[3,17,49,6]and contrastive learning-based methods[8,20,16,4].

Clustering-based methods learn the structure of data by grouping it into different clusters or groups. However, it can not effectively optimize inter-class distances through positive and negative sample pairs[8,20]. In contrast, contrastive learning-based methods demonstrate superior feature learning capabilities by learning data representations through comparisons between positive and negative samples. Owing to its notable performance, several researchers employ it in FGVR tasks[50,31,51], and have achieved impressive performance. Different from the research on large-scale general image datasets[37,42,12], FGVR tasks require to differentiate subtle visual patterns, and primarily focuses on identifying subcategories within visual data, such as different bird species[2,43,44], aircraft models[32], and vehicle types[28]. Therefore, existing contrastive learning-based methods may suffer from “granularity gap” (i.e., the disparity between coarse-grained and fine-grained features)[11]. Moreover, recent studies show that existing methods are usually distracted by irrelevant features (i.e., the background noise)[27,41,40], resulting in feature entanglement in FGVR tasks and suboptimal intra-class boundaries (see Fig.LABEL:fig1).

To address these challenges, we propose a novel priority perception self-supervised learning framework, which effectively solves the issues of irrelevant feature interference and mitigating granularity bias. Specifically, the proposed Anti-Interference Strategy (AIS) leverages the unique decoupled modality property of CLIP[36]by embedding fine-grained text representations. In the fine-grained text corpus, we define both relevant and irrelevant items to the current task, which are stored as shared embeddings. This process guides the image encoder to filter out interference from irrelevant features, allowing it to extract meaningful visual representations rather than relying solely on image-level features. By implementing this strategy, we eliminate interference from irrelevant features without depending on labeled data, thereby facilitating seamless integration into the self-supervised learning training process. Furthermore, we assert that the original image retains the most comprehensive details. Our findings indicate that leveraging information from the original image can assist the network in learning subtle distinctions between categories. Consequently, we designed the Image-Aided Distinction Module (IADM), which focuses on capturing crucial details to mitigate the impact of subtle inter-class differences and large intra-class variations, which generates GradCAM[38]by taking gradients of the original image with respect to the contrastive learning loss[18], allowing us to identify important regions within the original image. This guides the network’s attention to focus on these regions, facilitating the exploration of more nuanced discriminative representations. During the inference phase, we eliminate redundant modules to maintain a streamlined and lightweight process, relying solely on the image encoder for predictions and generating features for downstream tasks. Extensive experimental results demonstrate that our proposed method significantly enhances the performance of self-supervised learning in fine-grained recognition tasks.

Our main contributions are summarized as follows:

We propose a self-supervised learning framework tailored for fine-grained recognition, with experimental results demonstrating its effectiveness on benchmark datasets and significant performance improvements in both retrieval and classification tasks.

We propose an Anti-Interference Strategy (AIS) that leverages a fine-grained text corpus to mitigate the interference of irrelevant features, thereby facilitating the model’s learning of high-quality visual representations that are crucial for the task.

We design the Image-Aided Distinction Module (IADM) to extract fine-grained cues from the original images. By leveraging this information, the network learns subtle category distinctions, mitigating the impact of inter-class differences and intra-class variations. This approach guides the network to focus on more discriminative regions, offering a novel perspective for fine-grained tasks.

SECTION: 2Related Works

Self-Supervised Learning (SSL)has made significant progress in the field of computer vision by designing pretext tasks to learn useful feature representations from unlabeled data. Early methods, such as Jigsaw[15]and Jigsaw++[34], learn feature representations by shuffling and restoring image patches, effectively improving image feature learning. In recent years, contrastive learning has become an important direction in self-supervised learning. The MoCo[20]achieves efficient feature learning by building a dynamic dictionary and contrastive learning. The SimCLR[8]learns image features through data augmentation and a contrastive loss function. The BYOL[16], which conducts contrastive learning in a self-guided manner without the need for negative samples. Additionally, SwAV[4]implements self-supervised learning by swapping cluster assignments across different views. MAE (Masked Autoencoders)[21]learn feature representations effectively by masking parts of the input data and predicting the masked parts.

Self-Supervised Learning for Fine-Grained Visual Recognition.Despite the impressive transferability and generalization demonstrated by SSL methods in many tasks, recent studies[27,11]pointed out that it is hard to capture critical features for fine-grained visual recognition. To overcome this, researchers have proposed several improvements.
On the one hand, some methods focus on improving data augmentation techniques. For instance, DiLo[52]generates images with different backgrounds by combining images with new backgrounds, thereby enhancing the model’s ability to localize foreground objects. ContrastiveCrop[35]introduces an optimized cropping method to generate better views of the image. OLDFS[46]enhances the discriminative capability of the encoder by perturbing feature vectors to generate realistic synthetic images.
On the other hand, Researchers aim to enhance the encoder’s focus on salient regions by linking auxiliary neural networks to its convolutional layers. For example, CAST[39]aligns Grad-CAM attention with key regions from saliency detectors to improve feature learning. CVSA[14]generates new views by cropping and swapping salient regions and employs cross-view saliency alignment loss to focus on foreground features. Nonetheless, they typically depend on pre-trained saliency detectors. LCR[41]and SAM[40]eliminate the dependence on pre-trained saliency detectors by guiding the network to match Grad-CAM outputs, with Grad-CAM serving as a benchmark for aligning the encoder’s attention maps.

Despite significant advances in self-supervised learning for fine-grained visual recognition, several challenges remain. Irrelevant factors, such as background clutter, often obscure subtle feature differences, making it difficult to discern fine-grained distinctions. Additionally, small inter-class variations, coupled with large intra-class discrepancies, further complicate accurate recognition. These issues highlight the critical need for methods that can both minimize interference and effectively extract nuanced features. Addressing these challenges is essential for achieving more precise and robust fine-grained visual recognition.

SECTION: 3Method

As illustrated in Fig.1, we propose a Priority-Perception Self-Supervised Learning framework, which mainly consists of two key components: the Anti-Interference Strategy (AIS) and the Image-Aided Distinction Module (IADM).

SECTION: 3.1Preliminary

Given an imagefrom a batch of samples, two different data augmentation operations are applied to introduce perturbations, resulting in imagesand. These augmented images are then processed through the image encoderand momentum encoderto obtain feature embeddingsand.and. Theand, derived from the same image, serve as positive pairs. Conversely, embeddings, obtained from different views of other images, serve as negative pairs and are stored in a queue as a negative sample pool. Consequently, we can compute the contrastive learning loss[10]for the first stage:

whereis the temperature parameter.is the number of negative samples in the queue.

SECTION: 3.2Anti-Interference Strategy (AIS)

In our implementation, we regard the image encoder within the contrastive learning framework as the student model and the CLIP image encoder as the teacher model, with the CLIP text encoder serving as the bridge between the two. Given the nature of our task, our objective is to enable the network to distinguish irrelevant feature interference during the self-supervised learning process.

To achieve this, as shown in Fig.1, we have pre-designed a fine-grained textual corpus that includes attribute descriptions for several common categories, along with broader category descriptions relevant to the fine-grained datasets employed in this paper. It aims to enable the student image encoder to recognize these attributes, thereby filtering out irrelevant feature interference. We have designed eight fine-grained attribute descriptions, denoted as, where. These descriptions include examples such as “an animal characterized by feathers, wings, and the ability to fly or perch.” Among these, seven descriptions are unrelated to the current image, while one is relevant, encouraging the model to learn the ability to filter out irrelevant features and achieve high-quality feature extraction. We input the text corpus into the CLIP text encoder to obtain text embeddings, which are thennormalized. These text embeddingsserve as shared feature representations between the student image encoder and the teacher CLIP image encoder. Based on this strategy, we only need to train the student image encoder. By inputting the imagesfrom the unlabeled training datasetinto the pre-trained teacher CLIP image encoder, we obtain the normalized image embedding.

After obtaining the visual embeddingfrom the student image encoder, we first extract the feature map using aconvolution kernel, followed by further processing to generate the visual features required for distillation:

wheredenotes a 11 convolution kernel,is defined as, andrepresents the ReLU activation function. whilerepresents the max-out operation, which selects the maximum value across each channel. The symbolindicates the Hadamard product. Finally, a learnable, is applied to ensure efficient and precise alignment withwhile maintaining low computational overhead, yielding the student image embedding. The student image embeddingis obtained by performing matrix multiplication with the text embeddingto obtain the logits. Similarly, the image embeddingfrom the CLIP image encoder is also multiplied by the text embeddingto generate the logits. By optimizing our image encoder, we aim to produce image embeddings with semantic understanding capabilities on the unlabeled dataset, thereby reducing the influence of irrelevant feature interference.

The distillation process of AIS is illustrated in Fig.2. Knowledge distillation, first introduced by Hinton[22], uses Kullback-Leibler (KL) divergence[29]to align outputs, optimizing the following objective:

whereanddenote the predictable logits of teacher model and student model.denotes the softmax function,is the temperature parameter, which control the smoothness of the distributions.

SECTION: 3.3Image-Aided Distinction Module (IADM)

Our IADM method is designed based on the GradCAM technique, which computes gradients with respect to the original image.

By substituting the cross-entropy loss in the standard GradCAM computation with the contrastive loss derived from Eq.1, we can compute the GradCAM as follows:

Eq.5computes the importance of each region in the image through gradients derived from the contrastive loss in the self-supervised learning framework, which is used to calculate the GradCAM weights. In Eq.6, the GradCAM weights are multiplied with the original image to obtain the final GradCAM visualization. This serves as a pseudo-label for the regions of interest, guiding the network to focus on subtle details in the original image. Subsequently, we apply the same series of operations on the original imageas in the AIS framework, as described below:

where remaining operations follow a similar procedure to those in AIS.
Our optimization objective is as follows:

where symboldenotes multiplication operation.

SECTION: 3.4Total Loss and Inference

Overall, the loss function during training can be defined as:

where, anddenote the hyperparameters that control the weight of the loss function.

During inference, the additional computations required during the training phase are no longer needed. We use the image encoderto generate the image embedding.
The final featuresapplied to the downstream task are obtained through the following operations:

wheredenotes the result derived from Eq.2, withrepresenting the average pooling operation andindicating the L2 normalization operation. As shown in Fig.1(b),is used for downstream tasks.

SECTION: 4Experiments

SECTION: 4.1Experimental Setup

Datasets.We evaluate our proposed method on 7 public fine-grained image classification datasets, including CUB-200-2011 (200 bird species), Stanford Cars (196 car categories), FGVC-Aircraft (100 aircraft categories), NABirds (555 bird species), Flowers102 (102 flower species), Butterfly200 (200 butterfly species), and Stanford Dogs (120 dog breeds). Specifically, CUB-200-2011[44]: 11,788 images, 200 bird species, with 5,994 training and 5,794 testing images.
Stanford Cars[28]: 16,185 images, 196 car categories, with 8,144 training and 8,041 testing images.
FGVC-Aircraft[32]: 10,000 images, 100 aircraft categories, with 6,667 training and 3,333 testing images.
NABirds[43]: 48,562 images, 555 bird species, with 23,929 training and 24,633 testing images.
Flowers102[33]: 7,169 images, 102 flower species, with 1,020 training and 6,149 testing images.
Butterfly200[7]: 25,279 images, 200 butterfly species, with 10,270 training and 15,009 testing images.
Stanford Dogs[26]: 20,580 images, 120 dog breeds, with 12,000 training and 8,580 testing images.

Implementation Details.We employ the ResNet50[19]as the backbone of our network, initialized with ImageNet-trained weights. Following MoCo v2[10], the momentum factor of our MoCo contrastive module is set to 0.999. The projection headconsists of two fully connected layers with ReLU activation and a linear layer with batch normalization (BN)[24]. We set the batch size to 128, and use the SGD optimizer with a learning rate of 0.03, momentum of 0.9, and weight decay of 0.0001. The CLIP image encoder (i.e., teacher model) and CLIP text encoder employ the viT-B/32 architecture. The retrieval phase is conducted over 100 epochs. During training, images in the FGVR dataset were resized to 224×224 pixels. In the testing phase, images are resized to 256 pixels and then center-cropped to obtain a final size of 224×224 pixels.

SECTION: 4.2Evaluation Protocols

We evaluate our method in two settings: image retrieval and linear probing. First, we use image retrieval to assess the learned features by identifying images that match the query’s category. This approach is crucial in unsupervised learning, as it relies on high-quality features without requiring extensive labeled data. Specifically, it effectively measures the features’ ability in similarity retrieval, emphasizing its practicality as it requires no manual annotations or human intervention. We use rank-1 accuracy, rank-5 accuracy, and mean Average Precision (mAP) to provide a comprehensive assessment of feature quality. Secondly, linear probing is a common evaluation protocol for assessing the quality of features learned by SSL algorithms. In this setting, the SSL-trained feature extractor is fixed, and a linear classifier is trained on the extracted features. The classifier’s performance reflects the quality and utility of the learned features for classification tasks.

SECTION: 4.3Experimental Results

Effectiveness of the Proposed Method.To evaluate the performance improvements of our method, we first compared it with two recent advanced methods, i.e., LCR and OLDFS, for retrieval and classification tasks. As illustrated in Tab.1, our method significantly outperforms other methods. On the CUB-200-2011 dataset, our method achieved the best performance in various label proportions for classification tasks. Besides, on the Stanford Cars and FGVC Aircraft datasets, our method achieved the highest performance in terms of rank-1 and rank-5 for retrieval tasks. Specifically, our method improved the rank-1 accuracy by 8.78%, 4.79%, and 3.48% over two advanced methods on the three datasets, with a particularly notable improvement of 8.78% on CUB-200-2011. Our method’s superior performance in the retrieval task is attributed to the integration of AIS and IADM, which effectively mitigate the interference of irrelevant features and harness discriminative cues from the original image, thus driving improvements in fine-grained retrieval tasks. In terms of classification metrics, our method also demonstrates performance gains. However, in certain datasets and label proportion settings, the OLDFS method does not show a significant gap compared to our method. This may be due to OLDFS’s ability to learn task-irrelevant features, which could contribute to enhancing performance in downstream visual recognition tasks[9,30].

Comparison with Other SSL Methods.Furthermore, we compared our method with other self-supervised learning approaches to evaluate its performance in fine-grained recognition tasks. We report the rank-1 accuracy for image retrieval and top-1 accuracy for classification, with all experiments conducted using a batch size of 128, as shown in Tab.2. Our method consistently achieves the highest rank-1 and top-1 accuracies on the CUB-200-2011, Stanford Cars, and FGVC Aircraft datasets. It demonstrates sustained competitiveness in both retrieval and classification tasks compared to other SSL methods. Compared to the latest self-supervised approaches, our method continues to exhibit outstanding performance.

We further conducted experiments on four public FGVR datasets. As shown in Tab.3, our method achieves the best performance across these datasets as well. Fig.3presents visualizations of the attention regions for our method and others. By visualizing the regions the model attends to, our method shows an enhanced ability to focus on more discriminative cues while diminishing the impact of irrelevant features, leading to superior performance.

SECTION: 4.4Ablation Study of PP-SSL Architecture

The ablation experiments are conducted on the CUB-200-2011 dataset, with results for other datasets provided in the supplementary material.

Anti-Interference Strategy (AIS).As shown in the first and last columns of Tab.4, applying AIS on top of layer0(i.e., IADM, guided by the original image information) significantly improves the retrieval rank-1 and rank-5 accuracy on the CUB-200-2011 fine-grained dataset, with increases of 6.28% and 7.21%, respectively. This substantial improvement highlights the effectiveness of the proposed AIS in mitigating interference from irrelevant features. Additionally, we conducted ablation studies on two other datasets, exploring various combination strategies, with the results provided in the appendix.

Ablation of the Number of AIS Fine-Grained Descriptions.Tab.5shows the performance differences between using coarse class text and fine-grained description text. The fine-grained descriptions are more effective in mitigating interference from irrelevant features.

Image-Aided Distinction Module (IADM).The GradCAM visualizations with and without IADM are shown in Fig.4. It can be observed that interference from irrelevant regions is significantly reduced, enhancing the model’s ability to capture fine-grained discriminative features within key areas and focus on more detailed distinguishing patterns, thereby demonstrating the effectiveness of the proposed IADM. Furthermore, as shown in Tab.4, the GradCAM obtained by computing gradients with respect to the original image (i.e., IADM) achieves the best performance compared to using other layers for guidance (from columns 2 to the last in Tab.4). Notably, the combination of deep-layer features and original image guidance is less effective than using original image guidance alone. The combination of AIS and IADM achieves the best performance.

SECTION: 4.5Further Analysis

Analysis of Hyperparameters.In this section, we conduct sensitivity analysis of two hyperparameters, i.e.,andused in Eq.9, on the CUB-200-2011 dataset, which determine the strength of the weights ofand, respectively. The analysis results are demonstrated in Tab.6. It is observed thatandachieve the best performance. Therefore, we adopt it for our experiments.

Analysis of the Preset Text Library.Here, we further analyze the effect of the number of text prompts () in the preset text library. As shown in Fig.5, storing 8 text prompts achieves the highest rank-1 accuracy. Therefore, we setin this paper by default. For the text prompt configuration within the library, we employed a more refined prompt, like “an animal characterized by feathers, wings, and the ability to fly or perch.” Future experiments will explore alternative designs for these prompts.

SECTION: 5Conclusion

This paper presents PP-SSL, a novel self-supervised framework for fine-grained visual recognition, addressing the issues of irrelevant feature interference and mitigating granularity bias. Specifically, the proposed anti-interference strategy enables the model to acquire semantic understanding of categories, allowing it to focus on key regions of the target while reducing the impact of irrelevant feature interference in fine-grained visual recognition tasks. Additionally, the proposed image-aided distinction module extracts crucial fine-grained cues, enhancing the model’s ability to distinguish subtle differences. Extensive experiments on 7 benchmarks show that our PP-SSL outperforms recent state-of-the-art methods in both classification and retrieval tasks.

SECTION: References