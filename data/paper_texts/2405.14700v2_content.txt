SECTION: Sparse-Tuning: Adapting Vision Transformers with Efficient Fine-tuning and Inference
Parameter-efficient fine-tuning (PEFT) has emerged as a popular solution for adapting pre-trained Vision Transformer (ViT) models to downstream applications.
While current PEFT methods have achieved parameter efficiency, they overlook the efficiency of computation and GPU memory during both fine-tuning and inference, falling short of practical requirements.
In this paper, we propose, a novel PEFT method that accounts for the information redundancy in images and videos to boost the above efficiency.
By sparsely preserving the semantic-relevant tokens and merging irrelevant ones, Sparse-Tuning minimizes the quantity of tokens processed at each layer, leading to a quadratic reduction in computational and memory overhead.
To align our token sparsification strategy suitably with fine-tuning purposes, we further design Dense Adapters that establish dense connections from shallow layers to deeper layers.
These Dense Adapters integrate multi-level local features to enrich the current tokens, improving both token preservation and model adaptation.
Empirical results on VTAB-1K, three image datasets, and two video datasets show that our Sparse-Tuning reduces GFLOPs toof the original ViT-B while achieving state-of-the-art performance. Source code is available at.

SECTION: Introduction
Large-scale Vision Transformer (ViT) modelshave demonstrated strong generalization capabilities across a wide range of downstream vision tasks. The prevalent approach to adapt these models for specific tasks follows theparadigm, where models are initially pre-trained on large-scale datasetsand then fine-tuned for each downstream task. However, as these pre-trained ViT models continue to scale up, fully fine-tuning them becomes more computationally intensive. Additionally, there are risks of catastrophic forgetting and overfitting when fine-tuning on limited downstream datasets.

Recently, various parameter-efficient fine-tuning (PEFT) methodshave been proposed to address the high computational costs and risks associated with fully fine-tuning large models. By updating additional parameters inserted into the modelor appended to the input data, PEFT methods can achieve similar or even better performance compared to full fine-tuning.
However, we have discovered that the parameter efficiency, pursued by these methods, does not directly correlate with the efficiency that is truly of concern during deployment, including computational and memory efficiency.
As shown in Figure, despite the significant disparity in the number of trainable parameters, the gap of GFLOPs and memory between PEFT methods and full fine-tuning is not pronounced.
The reason lies in the fact that the reduction of trainable parameters achieved by existing PEFT methods does not address the actual bottleneck of ViTs.
The computational and memory costs of pre-trained ViTs comes from not only its own size, but also the length of the input due to the quadratic complexity of the attention operation, which is the core component of the Transformer.
Thus, we suggest that PEFT methods should consider managing the number of tokens for efficient and effective ViT adaptation.

According to the above analysis, we propose, a novel tuning paradigm that achieves efficient fine-tuning and inference for ViT adaptation. Inspired by the fact that heavy redundancy exhibits spatially in images and spatio-temporally in videos, we introduce a parameter-free Token Sparsification method that effectively compresses the extracted visual tokens during fine-tuning.
Specifically, it progressively preserves semantic-relevant tokens while condensing irrelevant ones into a representative token, thereby reducing the number of tokens in each layer.
While reducing computational overhead, our ablation study (shown in Table) indicates that the token reduction may lead to information loss and decreased accuracy.
Since the remaining visual tokens in deeper layers can be viewed as low-resolution representations, which contain coarse and holistic characteristics of visual cues, our tuning method further supplements high-resolution local details with well-designed Dense Adapters.
Different from conventional intra-layer adapters, our Dense Adapters establish dense connections between different layers, integrating multi-level features from shallower layers to enhance the current tokens.
Additionally, the full utilization of local features also contributes to the precise identification of semantic-relevant tokens within the subsequent layers.
With these non-trivial designs, Sparse-Tuning improves performance while significantly reducing computational cost and GPU memory consumption for efficient ViT fine-tuning and inference, as shown in Figure.

To fully evaluate the generalization, we conduct extensive experiments on the common PEFT benchmark VTAB-1K, three complete image datasets: CIFAR-100, SVHN, and Food-101, as well as two complete video datasets: Kinetics-400 (K400)and Something-Something V2 (SSv2). Empirical results on VTAB-1K demonstrate that with only, approximatelyof the computational cost of the original ViT-B, Sparse-Tuning outperforms all state-of-the-art methods in performance and efficiency. Moreover, Sparse-Tuning achieves superior performance in both image and video recognition on complete datasets while significantly improving both fine-tuning and inference efficiency.

SECTION: Related Work
SECTION: Parameter-efficient Fine-tuning
With the trend of scaling up Vision Transformers (ViT)for enhanced performance and generalization, adapting the entire model to downstream tasks becomes increasingly computationally expensive. To mitigate this, parameter-efficient fine-tuning (PEFT)emerges as a strategic approach. PEFT methods updates only a small subset of additional parameters while keeping the majority of the pre-trained model frozen, thereby mitigating the risks of catastrophic forgetting and overfitting. Most PEFT methods methods designed for Transformercan be classified into three types: (1), that only updates a small subset of inherent parameters while freezing most original parameters. (2), that integrates a fixed length of learnable tokens (i.e., prompts) appended with the input data. It only updates the prompts during fine-tuning. (3), that only updates additional parameters in the module inserted into the model (i.e., Adapter) during fine-tuning.

While most PEFT methods improve parameter efficiency during fine-tuning, they often introduce new parameters that compromise inference efficiency.methods, such as LoRAand FacT, introduce learnable parameters that can be integrated into the original model during inference, thus maintaining the original modelâ€™s inference efficiency. However, current PEFT methods still fall short in enhancing inference efficiency, which is crucial for adapting large-scale ViTs (e.g., ViT-L) to practical applications. In this paper, we aim to improve both the fine-tuning and inference efficiency of pre-trained ViT.

SECTION: Token Compression for ViT
Recent works have explored to accelerate the inference efficiency of ViT, with most of them aiming to reduce the token redundancy to decrease computational complexity. For instance, DynamicViTefficiently sparsifies ViT by pruning less informative tokens identified through prediction modules. DVTenhances computational efficiency by automatically adapting the token count for each input image. SuperViThandles diverse patch sizes with a single model, adaptively adjusting token retention during inference.

Existing token compression methods for ViT generally require either fine-tuning all pre-trained parameters, or training models from scratch. Consequently, these approaches necessitate substantial training or fine-tuning time to adapt ViT to downstream vision tasks. Recently, Dynamic Tuning (DyT)keeps the pre-trained ViT parameters frozen, updating only the adapters and token dispatchers to enhance parameter efficiency and reduce redundant computation during inference. Unlike DyT, which directly skips the uninformative tokens, our method consolidates these tokens into a single representative token to retain visual features beneficial for classification. Additionally, Different from DyT that necessitates computing all tokens to update the parameters of the proposed token dispatchers, we do not introduce any additional modules for token sparsification. Our method efficiently fine-tunes the pre-trained ViT by selectively adapting the tokens, thereby improving efficiency during both the fine-tuning and inference stages.

SECTION: Method
In this section, we present our proposed Sparse-Tuning in detail. First, we briefly review the preliminaries of Vision Transformer and Adapter Tuning. in Section. Next, we provide a general introduction to the overall framework of Sparse-Tuning in Section. Following this, we elaborate on the core techniques of Sparse-Tuning: Token Sparsification and Dense Adapter.

SECTION: Preliminaries
(ViTs)basically consist of a patch embedding layer and a stack of transformer encoder layers. The patch embedding layer first splits and flattens an input imageinto a sequence of patches, wheredenotes the size of the input image,represents the size of each image patch,is the number of channels, andis the number of image tokens. The patches, prepended with a learnabletoken, are fed into a stack of transformer encoder layers, each of which includes a Multi-Head Attention (MHA) block and a Feed-Forward Network (FFN). In MHA, the tokens are linearly projected and packed into three vectors, namely,, and. The self-attention operation can be written as:

is the attention map, whereindicates the attention from thetoken to all tokens and reflects the importance of each token. Subsequently, the output tokens are sent to a Layer Normalization (LayerNorm)and a FFN, which consists of two fully-connected layers with a GELU activation functionin between. After processing the tokens by a stack of encoder layers, thetoken is extracted and utilized for classification.

is a prevalent strategy for efficient fine-tuning of ViT, typically involving the insertion of an MLP in parallel with the FFN. The adapters consist of a down-projection layer, ReLU non-linear activation, and an up-projection layerin sequence. Given the input feature, the function of a standard adapter can be formally expressed as:

wheredenotes the scaling factor. Unlike the standard adapter, in this paper, we introduce the Dense Adapter, which receives multiple adapted features from different encoder layers to establish connections across the encoder layers in ViT.

SECTION: Sparse-Tuning for Efficient ViT adaptation
Existing workshave demonstrated that the final prediction in ViT largely depends on a subset of the most informative tokens. Dynamic Tuning (DyT)keeps the pre-trained parameters frozen and updates the adapters with the proposed token dispatcher to distinguish and discard uninformative tokens. This design can improve the inference speed of ViT but suffer from: (1), as DyT requires gradients to backpropagate through all tokens to update the parameters of the proposed token dispatcher, thus leading to low efficiency in terms of GPU memory consumption and fine-tuning speed. (2), as the token dispatcher directly removes those inactivated tokens, which can lead to a direct loss of information, thereby deteriorating the classification accuracy.

Motivated by the above analysis, we introduce Sparse-Tuning with Dense Adapters, efficiently fine-tuning pre-trained ViT by selectively adapting tokens to focus on informative regions, enhancing efficiency during both fine-tuning and inference stages. As shown in Figure, the overall framework includes two parts: (1) a pre-trained ViT-B/16that consists of a patch embedding layer and 12 transformer encoder layers with our carefully designed Token Sparsification process, and (2) our Dense Adapters. During fine-tuning, we freeze the pre-trained ViT and only update a series of Dense Adapters to facilitate efficient adaptation to downstream tasks. In the 4th, 7th, and 10th encoder layers (we conduct relevant analysis on Table), we implement Token Sparsification (see Figure) to enable ViT focus more on the informative tokens and reduce redundant computation cost.

The main idea of Sparse-Tuning is to decrease the computation load on uninformative tokens, which in turn reduces the computational cost for both ViT and Dense Adapters during fine-tuning and inference, thereby improving overall efficiency and speed. An intriguing question arises:Previous workshave demonstrated the strong relationship between thetoken and the class-specific tokens. In other words, the attention scores between thetoken and other tokens reflect the contribution of the current token to the classification. Consequently, tokens that exhibit higher/lower attention scores with thetoken contain more/less semantic information for classification, and thus can be viewed as the attentive/inattentive tokens. Though the inattentive tokens show lower attention scores, they may still influence the classification results in some cases, such as the prediction of large objects which cover large regions of the image. To this end, unlike DyT, our Token Sparsification progressively preserves the attentive tokens and merges the inattentive ones into one token during fine-tuning and inference to reduce the computation cost. Specifically, as shown in Figure, we calculate the average attention scores of all heads in MHA, and preserve thelargest (i.e.,) elements corresponding to tokens (attentive tokens), and fuse the rest tokens (inattentive tokens) by weighted average into a representative token to supplement the attentive ones. With this design, Sparse-Tuning allows a pre-trained ViT to concentrate on the most informative regions while discarding the uninformative ones, consequently lowering redundant computational costs during both fine-tuning and inference. Furthermore, by integrating the inattentive tokens, Sparse-Tuning mitigates information loss resulting from Token Sparsification.

To further alleviate the information loss caused by Token Sparsification and efficiently adapt the pre-trained ViT for downstream tasks, we consider utilizing the adpter-tuning method. Most current adapter-tuning methods for ViTfollow the basicapproach from ResNet, which can only establish connections between two adjacent ViT encoder layers, greatly limiting the propagation of adapted features during fine-tuning. The transition from local features to global features across encoder layers in ViT affects the effectiveness of Token Sparsification. Given that Token Sparsification occurs across encoder layers in ViT, we introduce the Dense Adapter (DA), inspired by DenseNet, to establishacross multiple encoder layers. As shown in Figure(right), unlike the standard adapter, DA takes multiple features from different encoder layers as inputs to establish interactions between multiple Token Sparsification steps, thereby compensating for the information loss caused by Token Sparsification.

According to the position, DA consists of one to three down-projection layers (i.e.,,,), ReLU non-linear activation, and an up-projection layer. Specifically, we donate the N-th DA as. The output ofcan be formulated as:

whereandrepresent the outputs of theand MHA at the N-th encoder layer, respectively. It is noteworthy that when, theandmay be sparsified by the corresponding Token Sparsification step to ensure consistency in token length for,, and. Dense Adapters facilitate multiple interactions between the lower and higher layers of the ViT encoder, thereby enhancing the representational capacity and quality of Token Sparsification.

SECTION: Experiments
SECTION: Experimental Setup
We compare our Sparse-Tuning with other state-of-the-art methods on the common PEFT benchmark VTAB-1Kto evaluate the adaptation performance when the training data is limited. For each downstream classification task, the training data in VTAB-1Kis extremely scarce, comprising only 1,000 training samples. Thus, following, we conduct experiments on three complete image datasets: CIFAR-100, SVHN, and Food-101, as well as two complete video datasets: Kinetics-400 (K400)and Something-Something V2 (SSv2), to further evaluate the adaptation performance and efficiency of our Sparse-Tuning.

We utilize the ViT-Base (ViT-B/16) modelas our backbone, which is pre-trained on the ImageNet21K datasetunder full supervision. The bottleneck dimensionof our Dense Adapter is set to 32 by default, and we reduceto 8 on VTAB-1K, following most existing works. The scaling factoris set to 1. We set the keeping rateof attentive tokens to 0.7 by default, unless otherwise specified. We adhere to the same training schedule as reported in. For all the downstream tasks, we employ top-1 accuracy as the primary evaluation metric. We conduct all experiments on a A800 GPU. More details are provided in the Appendix.

SECTION: Main Results
The comparison results with state-of-the-art (SOTA) PEFT methods on VTAB-1Kare presented in Table, from which we can observe that:Sparse-Tuning outperforms all SOTA PEFT methods. Sparse-Tuning achieves aimprovement in terms of the average accuracy across the three subgroups, compared with the previous best model DyT.Sparse-Tuning largely improves inference efficiency. With only 11.65 GFLOPs, about 66% of the computational cost of the original ViT-B, Sparse-Tuning with keeping ratehas outperformed all state-of-the-art methods in terms of both performance and inference efficiency.Sparse-Tuning continues to exhibit better performance when the keeping rateincreases. While even the Sparse-Tuning withcan outperform recent strong methods, such as Res-Tuningand FacT, which validates the effectiveness and efficiency of our Sparse-Tuning.

We conduct experiments on comprehensive image and video datasets to evaluate the adaptation performance with abundant training data. The results on complete image and video datasets are shown in Table, from which we find that:Sparse-Tuning outperforms all baseline methods on both image and video datasets, demonstrating its strong transferability on complete datasets.Sparse-Tuning demonstrates exceptional inference efficiency on both image and video datasets. Particularly on video datasets, Sparse-Tuning reduces the computational complexity of the original ViT-B by around 30%, highlighting its strong efficiency in video applications. With only 1.10M updated parameters, our Sparse-Tuning achieves superior performance in image and video recognition, while significantly improving inference efficiency.

SECTION: Ablation Studies
In this subsection, we first analyze the effectiveness of Token Sparsification and Dense Adapters. We then provide an in-depth analysis of the feature inputs and their fusion methods in our Dense Adapters. Subsequently, we investigate the impact of different positions of Token Sparsification to achieve optimal performance. Finally, we verify the effectiveness of Sparse-Tuning when the pre-trained ViT is scaled up. We conduct all ablation studies on three complete image datasets.

In Table, we report the performance of using different components of Sparse-Tuning to investigate the effectiveness of Token Sparsification and Dense Adapters. We can observe the following:Token Sparsification can reduce the computational complexity, but it leads to a significant performance degradation, resulting in a 7% decrease in average accuracy (Table(a,b)).Dense Adapters can significantly improve the performance across three datasets (Table(a,c)), which demonstrates their effectiveness in ViT adaptation.Sparse-Tuning incorporates Token Sparsification and Dense Adapters into the pre-trained ViT, achieving the best trade-off between performance and fine-tuning and inference efficiency (Table(a,b,c,d)). Compared to using only Dense Adapters for efficient ViT adaptation (Table(b)), Sparse-Tuning sacrifices only 0.48% average accuracy while significantly reducing the computational cost from 17.89 GFLOPs to 11.70 GFLOPs, highlighting its strong adaptation performance and efficiency.

To investigate the effectiveness of dense connections, we compare different inputs to our Dense Adapters. As shown in Table, when feeding multiple features from different encoder layers into the Dense Adapters, the performance increases. This suggests that our Dense Adapters effectively facilitate dense interactions between the lower and higher layers of the ViT to enhance the representational capability, thereby improving performance compared to standard adapter-tuning (Table(a)). It is worth noting that while our Sparse-Tuning introduces more feature interactions requiring computation, the GFLOPs are still reduced compared to adapter-tuning, demonstrating that Token Sparsification also alleviates the computation cost in Dense Adapters.

Since Dense Adapters take multiple features as inputs, we consider three variants of Dense Adapters that can fuse these multi-level features, as shown in Figure. We report the performance of different feature fusion methods in Table. Fusing the multi-level features before feeding them into the Dense Adapters (Figure(a)) requires fewer trainable parameters but deteriorates performance. This occurs because this fusion method leads to information loss; features from different layers may contain complementary information, and simple addition may not effectively integrate this information. Fusing the features after feeding them into the Dense Adapters (Figure(c)) also deteriorates performance. This is due to the fact that multi-level features are mapped into different spaces, and directly fusing them may obscure important information, thereby reducing classification performance. Our Dense Adapters first project multi-level features into the same space, then fuse them, and finally up-project the fused features back into their original shape (Figure(b)). This ensures that the dense interaction process occurs within the same feature space, which leads to better performance.

Since Token Sparsification occurs across different encoder layers in the ViT, we investigate its effects at various positions to achieve the best trade-off between performance and computational cost. As shown in Table, the shallower the position of the first Token Sparsification, the fewer encoder layers with full tokens need to be processed, hence the lower the computational cost. However, in the early stages, ViT cannot reliably identify important tokens, so merging tokens based on unreliable attention maps may result in the loss of important information, leading to decreased performance (Table(a, b)). In contrast, as shown in Table(d, e), shallower-layer tokens tokens processed later by Dense Adapters may have lost local features, resulting in better overall performance compared to Table(a, b) but still not optimal. We find that adopting Token Sparsification in the 4th, 7th, and 10th encoder layers yields the best performance. This suggests that performing multiple dense interactions in the relatively middle encoder layers of ViT balances local and global features more effectively during Token Sparsification. Therefore, we select the 4th, 7th, and 10th encoder layers in ViT for Token Sparsification to achieve the best trade-off between performance and computational cost.

We apply Sparse-Tuning to ViT-Lto evaluate its performance and efficiency when scaling up the pre-trained model. As shown in Table, Sparse-Tuning reduces tunable parameters byand decreases GFLOPs bycompared to full fine-tuning, while also surpassing its performance. Additionally, Sparse-Tuning outperforms DyTin both performance and efficiency, demonstrating its effectiveness for larger pre-trained models.

SECTION: Conclusion
In this work, we aim to enhance efficiency during both fine-tuning and inference stages when adapting the pre-trained ViT. To this end, we propose a novel tuning method called Sparse-Tuning, which selectively adapts tokens to enable the pre-trained ViT to focus more on the foreground and less on background regions during the fine-tuning stage. By gradually preserving informative tokens and merging uninformative ones into one representative token, our Sparse-Tuning significantly reduces redundant computational costs, achieving both fine-tuning and inference efficiency for ViT adaptation. We conduct empirical experiments on the VTAB-1K benchmark, three complete image datasets, and two complete video datasets to ensure the generalizability of our Sparse-Tuning for efficient ViT adaptation. Extensive experimental results demonstrate that our Sparse-Tuning can enhance performance as well as significantly improve fine-tuning and inference efficiency.In this paper, as we mainly focus on classification tasks in our experiments, extending our Sparse-Tuning to other vision tasks, such as segmentation and detection, will be our future direction.

SECTION: References
SECTION: Additional Experiments
SECTION: Performance and Efficiency on CIFAR-100
We present the numbers of updated parameters during fine-tuning, GPU memory usage during both fine-tuning and inference, time for fine-tuning and inference, GFLOPs, and accuracy of our Sparse-Tuning method compared to other mainstream PEFT methods on the CIFAR-100 dataset. Evidently, our Sparse-Tuning achieves state-of-the-art performance while significantly enhancing efficiency during both the fine-tuning and inference stages.

SECTION: Effects of Different Bottleneck Dimensions of Dense Adapter
We explore the impact of the bottleneck dimensionof our Dense Adapter in Sparse-Tuning to achieve the best trade-off between performance, updated parameters, and computational cost. As reported in Table, a higher bottleneck dimensionintroduces more parameters and higher GFLOPs. However, with a smaller, the down-projection may lose significant information about the original features, leading to performance degradation. We observe that performance peaks at a bottleneck dimension of 32 and declines thereafter. Therefore, considering the trade-off between trainable parameters, GFLOPs, and performance, we select a bottleneck dimension of 32.

SECTION: More Visualizations of Token Sparsification
We present more visualization results of Token Sparsification in Figure. The results demonstrate that given various images, the Token Sparsification in our Sparse-Tuning can effectively maintain the tokens from the foreground regions.

SECTION: Implementation Details for Each Task
Following previous works, we fine-tune the model for 100 epochs on each dataset in VTAB-1K. Wedo notuse any data augmentation strategy in these experiments. We adopt the AdamWoptimizer. The base learning rate is set to 0.01 and gradually decays to 0 based on a cosine schedule.

We use the settings in Tableto fine-tune the ViT with the proposed Sparse-Tuning. Experiments on other parameter-efficient methods such as AdaptFormer, LoRA, and VPTalso follow the settingsin Table.

We use two video datasets, Kinetics-400 (K400)and Something-Something V2 (SSv2), to evaluate performance as the token count scales up. The experimental settings are shown in Table. The number of input frames is set to 8. During testing, we use multi-view, a common practice in video action recognition. Experiments on others PEFT methods also follow these experimental settings.

SECTION: Pseudocode of Sparse-Tuning
We present the PyTorch-like pseudocode of our Sparse-Tuning into help to better understand the whole process.