SECTION: Knowledge-Enhanced Conversational Recommendation via Transformer-based Sequential Modelling
In, conversations usually involve a set of items and item-related entities or attributes, e.g., director is a related entity of a movie. These items and item-related entities are often mentioned along the development of a dialog, leading to potential sequential dependencies among them. However, most of existingneglect these potential sequential dependencies. In this paper, we first propose a Transformer-based sequential conversational recommendation method, named TSCR, to model the sequential dependencies in the conversations to improve. In TSCR, we represent conversations by items and the item-related entities, and construct user sequences to discover user preferences by considering both the mentioned items and item-related entities. Based on the constructed sequences, we deploy a Cloze task to predict the recommended items along a sequence. Meanwhile, in certain domains, knowledge graphs formed by the items and their related entities are readily available, which provide various different kinds of associations among them. Given that TSCR does not benefit from such knowledge graphs, we then propose a knowledge graph enhanced version of TSCR, called TSCRKG. In specific, we leverage the knowledge graph to offline initialize our model TSCRKG, and augment the user sequence of conversations (i.e., sequence of the mentioned items and item-related entities in the conversation) with multi-hop paths in the knowledge graph. Experimental results demonstrate that our TSCR model significantly outperforms state-of-the-art baselines, and the enhanced version TSCRKG further improves recommendation performance on top of TSCR.

SECTION: Introduction
In general, a recommender system learns user preference from historical user-item interactions and then recommends items based on the learned user preference. The recommended items can be delivered to users through various interfaces, e.g., a list of products on e-commerce websites. Thanks to the rapid development of chatbots,is now becoming a promising interface to deliver recommended items to users directly through dialogs.

An example dialog foris shown in Fig.. The task of this example dialog is to recommend movies (i.e., the items) to users. Observe that there are two important properties demonstrated in this dialog: (a) first, it is common to mention entities that are closely related to the items to be recommended, e.g., director and genre of a movie in our example; (b) second, the mentioned items (and also item-related entities) naturally form a sequence, following the development of the conversation.
That is, there is an order impact among items in a conversation and potential sequential dependencies exist within the conversation. For instance, when people talk about a movie (e.g., Fast & Furious 1), it is natural and reasonable to recommend a sequel or prequel of that movie (e.g., Fast & Furious 4). Motivated by this observation, we argue that the modeling of such sequential dependency can well capture the context of users’ activities, and has great potential to improve the quality of recommendations.

Generally, aintegrates two modules: a recommender module, and a dialog module.
The latter generates natural language conversations to interact with users. The recommender module focuses on recommending desirable items to users by utilizing the information from the conversation, as well as related information from external sources like historical user-item interactions and knowledge bases. In this work, we focus on the recommender module only.

Conversational systems have shown great potential in a wide range of areas, such as information-seeking systems, conversational product search, dialog systems, and. As for, a number of solutions have been proposed, given it is a rapidly growing research topic in recent years. Early efforts in the development of CRS can be traced back to interactive recommendationsand question-based recommendations. More recently, two categories ofincluding anchor basedand dialog basedhave been proposed. Anchor basedis based on the “system asks – user responds” mode and simulates conversations by using some “anchor” text, e.g., item aspects, facets, entities, topics, and attributes. They usually utilize a belief tracker to infer the anchor-based preferences to improve. Another mainstream approach, dialog based, is based on human-generated dialogs. For instance, at the early stage,proposed adataset, ReDial, and presented a benchmark model for item recommendation. ReDial soon became the most widely used dataset for. Subsequently, the dataset TG-ReDial is proposed forin the Chinese scenario, which is constructed in a similar way with ReDial. Given that entities within utterances of ReDial and TG-ReDial are linked to a knowledge base, most subsequent work uses knowledge graphs to improve.
Although the aforementioned work demonstrates success to some extent, they neglect theamong items or entities discussed earlier. Hence, these existing solutions do not model the potential sequential dependency within the conversations.

Inspired by the success of Transformer methods like BERT, in our research, we explicitly model the bidirectional sequential dependency in conversations by using Transformer. Recent studies have shown that a carefully designed task-specific input format to BERT could lead to state-of-the-art performance. Our solution is along this line.
We first propose a simple and effective() model for. Specifically,extracts both the mentioned items and item-related entities in conversations as contextual information to construct a user sequence.
Based on this user sequence,randomly masks some items and deploys a Cloze taskto predict the masked items by leveraging the bidirectional contextual information in the input sequence. The bidirectional representation for the user sequence is modeled by the deep bidirectional self-attention architecture.

Given that the use of knowledge graphs has shown to be highly beneficial for recommender systems,
we further propose a Transformer-based Sequential Conversational Recommender enhanced by knowledge graphs (TSCRKG), as an extension of. Knowledge graphs provide additional information such as closely related items/item-related entities in a structured format. We aim to capture the spatio-temporal relationships of items and item-related entities by combining the modeling of sequential dependency (temporal relationships) and knowledge graphs (spatio relationships). We argue that leveraging knowledge graphs to make use of such additional information properly helps in making a better recommendation. Specifically, we extend
TSCR by leveraging knowledge graphs in the following ways: (a) we utilize knowledge graphs to train representations of items and item-related entities offline
and use them as initialized representations in the model;
and (b) given the rich structural information of knowledge graphs, we augment the user sequence of conversations with the multi-hop paths between two nodes (i.e., items or item-related entities) in knowledge graphs and then train the model based on the augmented sequences.

To sum up, our main contributions are as follows:

We propose a method, TSCR, which models the sequential dependencies in the conversations to improve.
To the best of our knowledge, this is the first effort to explicitly model the bidirectional sequential dependency in natural language conversations for.

We propose an extension model, TSCRKG, with a novel method to incorporate knowledge graphs in offline representation learning and knowledge graph-enhanced sequence modeling.

The extensive experiments on the twodatasets demonstrate the effectiveness of our TSCR model, and our extension model TSCRKG significantly improves the recommendation performance compared to various state-of-the-art baselines and TSCR.

Among these contributions, (a) and part of (c) (namely the experiments for TSCR on the ReDial dataset) were covered in our prior work, (b) and part of (c) (namely the experiments for TSCRKG, the experiments for both models on the TG-ReDial dataset, and the case studies) are new contributions in this extended version.
Specifically, the new contributions are in the following aspects.
(1) First, we propose a new extension model, TSCRKG (See Sectionsand).
(2) Second, we conduct the experiments on an additional dataset, TG-ReDial, which demonstrates the effectiveness of TSCR and TSCRKG in the Chinese conversational recommendation scenario, besides the English scenario. The experiments on the additional TG-ReDial dataset yield some different insights and conclusions from the original ReDial dataset (See Section.). (3) Third, we form a research question and conduct an analysis to explore the impact of knowledge graph-enhanced representation learning and knowledge graph-enhanced sequence modeling (See Section.). (4) Forth, we add a case study to illustrate the ability of TSCR and TSCRKG to generate recommendations (See Section.).

The rest of this paper is organized as follows. In Section, we discuss the related work. In Section, we introduce our models TSCR and TSCRKG in detail. Sectiondescribes the conducted experiments and the corresponding analysis of experimental results. Sectionconcludes the paper and summarizes some future work.

SECTION: Related Work
In this section, we review the related work from the following two categories: conversational recommender system and knowledge graph learning. A large number of studies have been conducted on these topics. In this paper, we will review only the work that is most closely related to our research.

SECTION: Conversational Recommender System
Thanks to the great power of collecting users’ explicit feedback, conversations have shown great potential in a wide range of areas, such as information-seeking systems, conversational product search, dialog systems, and CRS. In information-seeking systems, early exploration investigated mixed-initiative systems by interacting with users via script-based conversation during a search session. Recently, researchers investigated conversational information-seeking systems by asking clarifying questions, to enhance the ability to understand the users’ underlying information needs and retrieve the right information. In conversational product search, existing studies usually involve learning to ask strategiesto ask informative clarifying questions to form conversations, in order to locate relevant products a user is willing to purchase. Dialog systems usually structure conversations in multiple turns, including task-oriented systems, open-domain dialogs, and question-answering dialog systems.is an emerging field that utilizes human-like natural language to deliver personalized and engaging recommendations through conversational interfaces like chatbots and intelligence assistants. It is gaining considerable attention in recent years, driven by the rapid development of dialog systems. When considering CRS primarily as dialog systems, CRS can be regarded as either (1) system-driven (e.g., critiquing-based systems), (2) user-driven, or (3) mixed-initiative systems (most of CRS approaches), based on who takes the initiative in the dialog. The dialog systems could generate flexible and contextual responses, however, they may suffer from meaningless responses and fail to drive the conversation toward more attractive states for making recommendations or suggestions. One type of solution tends to enable conversational systems the ability for proactive conversationsto alleviate this problem. Various aspects have been explored in this field, including proposing new intents and conversational slots through ontology expansion and actively analyzing failures in novel situations (e.g.,), learning to ask questions to move the conversation forward (e.g.,), leveraging task information and domain knowledge to guide purposeful topic shifts and exploration (e.g.,), controlling the quality of response generation (e.g.,), and improving evaluation (e.g.,) etc.

Early efforts in the development of CRS can be traced back to the work of, and. More recently, various feedback mechanisms have been explored.investigated interactive collaborative filtering, proposing methods to enhance probabilistic matrix factorizationby using linear bandits to select items for user feedback.explored set-based feedback, whilefocused on choice-based feedback to learn latent factors and perform online interactive preference elicitation. We refer the reader toandfor a comprehensive review of interactive recommendations.

Recently, the research onhas been classified into two categories generally, including anchor basedand dialog based.

Anchor-based basedare in the form of “system asks – user response” mode. That is, systems could ask questions, then users provide feedback, and thus systems would use this information to refine their recommendations. For building effective anchor-based, recent studies have applied various ways for generating anchors to characterize items, including intent slots (e.g., item aspects and facets), entities, topics, and attributes. These anchors are usually collected to build a predefined question pool. Based on the constructed question pool, a line of work adopt multi-armed bandit, reinforcement learning, or greedy strategiesto select appropriate questions to simulate multi-turn interactions to interact with users. By doing so, user feedback through conversational interactions is leveraged to optimize recommendation policies. Other than selecting appropriate questions, deep reinforcement learning is also applied to train a policy network to determine whether to recommend items or ask a question in a turn. This category of studies usually relies on predefined dialog templates to construct conversations and focuses on how to offer recommendations within the shortest number of conversation turns. In other words, they do not focus on modeling natural language conversations.

The other category is called dialog based, which is based on human-generated dialogs. Unlike anchor-basedsimulating conversations on the basis of extracted anchor text, dialog basedfocus on generating human-like responses while making accurate recommendations. At the early stage, as there are no publicly available datasets for,propose adataset in the movie domain, named ReDial. Subsequently,propose the dataset TG-ReDial forin the Chinese language scenario. For datasets proposed inlike ReDial and TG-ReDial, a human-like conversation usually contains items and entities, which can be linked to a knowledge base (e.g., DBpedia). Therefore, many existing studies incorporate external knowledge graphs for enriching the conversation information and capturing user preference accurately. These studies typically employ a graph neural network to encode the knowledge graph and user preferences, along with a dialog management module to guide the conversation. For example,introduce the entity-oriented knowledge graph, i.e., DBpedia, to understand the user’s intention. They adopt Relational Graph Convolutional Networks (R-GCNs) to learn entity representations in knowledge graphs for item recommendations and apply the Transformer framework to generate natural language responses. Based on the work of, which mainly focuses on the entity-oriented knowledge graph,expand upon this by introducing an additional knowledge graph, i.e., the word-oriented knowledge graph – ConceptNet. They propose a novel knowledge graph-based semantic fusion model forby utilizing the two knowledge graphs. Based on this,propose a contrastive learning approach from the coarse-to-fine perspective to improve the fusion of data semantics. In order to reduce the amount of redundant information in knowledge graphs,introduce different subgraphs to improve the performance of the recommendation for. To make use of multiple reasoning paths in knowledge graphs,andproposemodels to perform multi-hop reasoning in knowledge graphs to track users’ interest shifts. To alleviate the limitation of incomplete knowledge graphs,present a variational reasoning approachwith dynamic knowledge reasoning over incomplete knowledge graphs. In addition to knowledge graphs, pre-trained language models are also applied to enhance.finetune the largescale pre-trained language models and propose a unified pre-trained language model-based framework to address the low-resource challenge in. Similarly,apply a pre-trained language model and propose amodel to unify the recommendation and conversation subtasks into the prompt learning paradigm. Another promising direction in conversational recommendation is the use of side information. For instance,incorporate the review information of items to improve the recommendation performance in.leverage multi-aspect user information to learn multi-aspect user preferences, whileintroduce item meta information to improve item representations.

This paper follows the line of the second category and focuses on the recommendation module only. Similar to the existing work using knowledge graphs, we also leverage the knowledge graph into our model but use it in both offline representation learning (i.e., we use the presentations of items and non-item entities learned based on the knowledge graph as the initialized embeddings in the recommendation module) and knowledge graph-enhanced sequence modeling (i.e., we augment the sequences of items and non-item entities in the conversations with paths from the knowledge graph and train the recommendation module based on both the original sequences and the augmented sequences). Moreover, different from the aforementioned studies, we explicitly model the bidirectional sequential dependency in natural language conversations for.

SECTION: Knowledge Graph Learning
There has been a growing interest of research on knowledge graph learning. Knowledge graph-based representation learning learns the representations of entities in a knowledge graph in the form of high-dimensional vectors, while preserving the intrinsic properties of the knowledge graph. This learned representation can then be conveniently applied to various downstream tasks, such as question answering, and recommender systems.present a survey on knowledge graph learning.
For knowledge representation learning, various models have been proposed, such as TransE, TransH, TransRTransD, HolE, and R-GCN. In this paper, we use R-GCNto learn representations of the entities (items and other non-item entities) in knowledge graphs.

Moreover, several previous studieshave explored the integration of external text into knowledge graph embeddings. For example,employ Graph Convolutional Networks (GCN)to extract subgraphs that combine the semantics of both graphs and text sentences for question-answering tasks.propose a transfer learning approach to enhance graph representations from pre-trained language models.create augmented graphs and learn representations based on the original and augmented graphs.

In this paper, we apply knowledge graphs to the specific downstream task of conversational recommendation.
We utilize knowledge graphs for learning offline embeddings of items and non-item entities to initialize our model, and take advantage of the graph structure to enhance the user sequences in conversations.

SECTION: Sequential Conversational Recommender System
We suppose there is a user set=, item set=, and conversations. We extract entities=from conversationsbased on DBpedia. The entity setconsists of all the items and other item-related entities (i.e.,). For a user, we have his/her entity mention (i.e., items or other item-related entities) history, extracted from his/her conversation, denoted as a sequence=(), we aim to accurately predict the next itemthat userlikes, along the development of the conversation.
The main notations used throughout the paper are summarized in Table.

In the following, we first describe the TSCR model, in particular the base model adopted, i.e., Transformer(Section), and how we train our model and perform the item recommendation (Section). Then, we express in detail how we incorporate the knowledge graph for our extension model TSCRKG, in particular, (a) knowledge graph enhanced representation learning (Section) and (b) knowledge graph enhanced sequence modeling (Section).

The overview of our model is shown in Figure. The TSCR model extracts the items and item-related entities to form an input sequence, and then uses the bidirectional Transformer as the base model (Section) to generate item recommendations by applying a Cloze task (Section). The TSCRKG model extends the TSCR model by leveraging the knowledge graph for offline representation learning (Section) and sequence augmentation (Section).

SECTION: Base Model
Inspired by,
we adopt Transformeras our base model, which consists of the embedding layer, self-attention layer, and prediction layer.

Given a sequence, we denote the embedding for the element at positionin the input sequence as.
For the representation of, we inject a learnable position embedding,, into the embedding of each element of the input sequence,:

All elements together form a trainable embedding matrix. Based on this initially trainable embedding matrix, we interactively calculateat each Transformer layer.

A self-attention layer consists of two sub-layers: a multi-head self-attention sub-layer and a Position-wise Feed-Forward Network (PFFN). More details can be found in.

We construct the PFFN by the Feed-Forward Network (FFN) with GELU activationat each position separately:

In addition, we deploy a residual connection around each of the two sub-layers, followed by a dropout and layer normalization, i.e., the output of each sub-layer is
actually:

where sub-layer is MultiHead or PFFN in Eq..

After N layers of Transformer, we get the final outputfor the input sequence. Assuming we maskat the input sequence, we then utilizeto predict the masked item. Specifically, we apply a softmax function through a two-layer FFN with GELU activation in between to produce an output distribution over items. To ensure recommendations are all items, we set the score of non-item entities in the softmax function to.

In this work, we adopt the Transformer architecture, which utilizes attention mechanisms to capture temporal relations while processing input tokens of a sequence in parallel. The Transformer architecture has been demonstrated as a powerful framework for supporting large-scale training datasets with enough parameters. Notably, the Transformer architecture makes few prior assumptions about the structural information of data but does not make any assumptions about how the data is structured. This makes Transformer a universal and flexible architecture that is well-suited for capturing dependencies across various ranges. However, this also poses a challenge: making Transformer difficult to train on small-scale datasets. The methods to alleviate this issue are to conduct pre-training on extensive unlabeled datasets and introduce structural bias or regularization into the model. Another key challenge of applying Transformer lies in its inefficiency when processing long sequences, primarily due to the computation and memory complexity of the self-attention module. The self-attention’s complexity concerning sequence length can significantly constrain the performance of downstream tasks. To overcome this challenge, one can apply improvement methods including the incorporation of lightweight attention (e.g., sparse attention variants) and the application of divide-and-conquer strategies (decomposing an input sequence into finer segments, facilitating more efficient processing by Transformer or Transformer modules), such as recurrent and hierarchical Transformers.

SECTION: Masked Item Prediction
We apply a Cloze taskon the sequence from the conversational history of a user (i.e., the sequence of mentioned items and item-related entities) to train our model. Given a sequence, we randomly mask a proportion of items
in the input sequence by replacing them with the special token “[mask]”, and then predict the original IDs of the masked items (in our implementation, items are represented by their corresponding item IDs).
Following BERT, we leverage the bidirectional contextual information in the input sequence for predicting the masked item. We use the negative log-likelihood of the masked targets as the loss:

whereis the masked version for user historical sequence,is the set of masked items in, andis one of the masked items.

For testing, it is not practical to use bidirectional information to predict as the testing item is always in the future given the current context. To this end, we construct a contextual sequence for each testing item, and then add a “[mask]” token to the end of the sequence to predict a testing item. For example, if there are three items in a sequence, we mask the first item and predict it with possible entities that are already mentioned in the dialog till this prediction. Then we mask and predict the second item based on the first item and other item-related entities mentioned so far in the dialog till this prediction, then the third item. To better match the last item prediction during testing, we also mask the last item for each training sequence to generate a training sample during training.

Contrary to bidirectional Transformer models like BERT, which is a pre-training model for sentence representation, our model is an end-to-end model trained for sequential conversational recommendation. Also, we removed the next sentence loss and next sentence prediction since there is only one sequence of user’s historical mentions of items and other item-related entities in. Different from those studies using bidirectional Transformer or pre-trained BERT for recommender systems, we trained our model in an end-to-end style and incorporated the conversational information (e.g., entities), aiming to improve.

SECTION: Offline Representation Learning
We introduce an offline representation learning technique that utilizes an external knowledge graph to initialize our model TSCRKG. Given it is difficult to comprehensively understand user preferences based solely on conversational context, the inclusion of external knowledge is necessary to encode user preferences. For dialogs in, item mentions and item-related entities can be extracted to construct external knowledge graphs. Inspired by the previous studies, we introduce a
knowledge graph sourced from DBpediato encode structural and relational information in the knowledge graph. Specifically, we perform entity linkingto map the item mentions and item-related entities in the dataset to DBpedia. With the help of the external knowledge graph, it enables us to model user preferences more accurately.

Given a knowledge graph(i.e., DBpedia), it consists of an entity setand a relation set. The knowledge graphstores semantic facts in the form of a triple, whereare items or item-related entities from the entity setandis the relation betweenandfrom the relation set.

In this paper, we employ R-GCNto encode entity representations in the knowledge graph. Specifically, we pre-train the representationby using R-GCN to initialize the offline embeddings of items or other item-related entities. Formally, in R-GCN, the representation of nodeat-th layer is calculated as:

wheredenotes the representation of node (i.e., entity)at the-th layer, andis the set of neighboring nodes forunder the relation.is a learnable transformation matrix for transforming the representations of nodes at the-th layer, whileis another learnable relation-specific matrix for transforming the embedding of neighboring nodes with the relation.is a normalization factor.

After aggregating the structural and relational information of the knowledge graph, we obtain all the node representations on the top R-GCN layer. These knowledge-enhanced node representations are then used to initializein Equation, i.e., the embeddings of items or other item-related entities in the dataset, for the Transformer model.

SECTION: Knowledge Graph Enhanced Sequence Modelling
In TSCRKG, we utilize the knowledge graph to enhance the user sequence of conversations (i.e., the sequence of mentioned items and item-related entities in the conversation). Specifically, we introduce an approach to augment the user sequence with the multi-hop paths in the knowledge graph.

Given a user historical sequence, we aim to generate an augmented sequence, which is able to maintain the useful structural information of the knowledge graph but also inject the global collaborative context into the augmented sequence to facilitate the sequence modeling. The algorithm for sequence augmentation by the knowledge graph is shown in Algorithm. Let=, we first pair each of two neighboring entities in the user sequence. For each pair, we find the shortest path in the knowledge graph (if any) between the two entities, and then augment the user sequence by sequentially adding all the entities in the shortest path following the last entity of the user sequence. Take the pairin the sequence as an example, if there are paths existing in the knowledge graph betweenand, we augment the user sequence by filling the entities in the shortest path betweenand. If there is a shortest path, thenin the original user sequencewill replaced byin. If there is no path existing in the knowledge graph betweenand, thenin the user sequence will still keep the same. In this way, (a) we complete the sequence in which some entities in the path are missing. This enables us to alleviate the problem of incomplete user sequences; (b) this helps us to have a complete path for knowledge graph reasoning, which improves the explainability of the recommendation; (c) this data augmentation allows us to increase the number of training data. For extracting the shortest path between two entities in the knowledge graph, we apply(A-Star) algorithm, which is one of the best-known graph searching algorithms to find the shortest paths from the initial node to final node.
It utilizes heuristic functions to guide its search, which is conducted by visiting nodes in the tree. The algorithm uses a best-first search and creates a path with the least cost through an evaluation function:
the algorithm selects the node with the lowest total cost by an evaluation function:

whererepresents the distance so far to reach node(entity in this paper),is a heuristic function to calculate the estimated distance fromto target.is the addition ofand, representing the estimated total cost of the path throughto the target. The lowest value ofis determined for the shortest path. After the enhanced user sequence is obtained, we feed the enhanced sequence into the bidirectional Transformer (see Section) and apply a Cloze task to perform the masked item prediction (see Section), same with the TSCR model.

SECTION: Experiments and Analysis
In this section, we evaluate our proposed models: TSCR and TSCRKG. We first introduce our experimental settings, including the used datasets, evaluation metrics, parameter settings, and baselines. Then we report and analyze our experimental results. Through experiments, we aim to answer the following research questions:

How effective is our proposed simple model compared to current state-of-the-art baselines?

What are the impacts of different knowledge graph components?

What are the contributions of items and entities in a sequence?

What is the effect of item position?

How do the parameters of our proposed model affect its efficacy?

RQ1, and RQ3 – RQ5 were also investigated by. In this paper, we extendon answering RQ1 and RQ3 – RQ5 by adding (1) more state-of-the-art baselines, (2) an additional dataset TG-ReDial for validating the effectiveness of TSCR in the Chinesescenario, and (3) additional experiments for the extension model TSCRKG. Also, we extendby adding RQ2 to explore the impacts of knowledge graph representation learning and knowledge graph enhanced sequences. In addition, we add a case study in this paper to illustrate the ability of TSCR and TSCRKG to generate recommendations.

SECTION: Experimental Setting
In this work, we conduct the experiments on two existing conversational recommendation benchmark datasets, ReDial and TG-ReDial, as done inand. The statistics of our datasets are shown in Table.

. Same as, and, we use theto evaluate our model. ReDial is a English conversational recommendation dataset, including a set of annotated dialogs in which a seeker requests movie suggestions from the recommender. It contains 956 users, 51,699 movies, 10,006 conversations, and 182,150 utterances.

. The TG-ReDial datasetis a Chinese conversational recommendation dataset which is also
used inand. The dialogs are generated between a user and a recommender in the movie domain via a topic-guided way. It contains 1,482 users, 33,834 movies, 10,000 dialogs, and 129,392 utterances.

The datasets are split into training, validation, and test sets by 8:1:1 ratio. Besides movies (i.e., the items), we extract the relevant entities, such as director and genre, from DBpedia, as suggested by, and.

Following, and, we use Recall@(= 1, 10, and 50) as our evaluation metrics for the recommendation task in. Recall@evaluates whether the target item provided by human recommenders appears in the top-items produced by the recommender system.
Moreover, we use the Mean Reciprocal Rank (MRR) to indicate the mean of the reciprocal of the rank of the target item in the ranked list predicted by the model.
For each conversation, we start from the first item (movie) to recommend in the recommender’s responses. This means, each item in the recommender’s responses is regarded as ground truth and we evaluate them one by one throughout the conversation following the previous work. For each testing instance, we rank all possible items within the dataset.

We train our model using Adamand TensorFlow with a learning rate of 1e-4. We set the batch size = 256, the number of Transformer layers N = 2, head number = 2, the maximum sequence length K = 100, L2 regularization strength = 0.01, and the global norm clip of gradients = 5 for stable training. The number of R-GCN layers and the normalization factorof R-GCN are set to 1.
We study the effect of the hidden dimensionality and mask proportion in Section. The hidden dimensionality ranges within [32, 64, 128, 256] and the mask proportion is tuned within the range of [0.2, 0.4, 0.6, 0.8]. For the parameter settings of all baselines, we use the results of each baseline under its optimal hyperparameter settings.

In this work, we consider two classical baselines and several strong baselines used against ReDial and TG-ReDial:

is a classical baseline sorting the items according to historical recommendation frequency.

is a classical CNN-based recommendation model learning embeddings from contextual utterances.

is the benchmark model of ReDial applying an autoencoder recommender for conversational recommendation.

utilizes the knowledge graph of DBpedia to introduce knowledge-grounded information to improve conversational recommendation.

incorporates a knowledge graph enhanced recommender by utilizing both entity-oriented and word-oriented knowledge graphs for the conversational recommendation.

is a knowledge-enriched conversational recommendation model based on a constructed knowledge graph in the movie domain called TMDKG.

is a review-augmented conversational recommender by incorporating reviews to enrich item information.

is one of the state-of-the-art conversational recommendation models based on coarse-to-fine contrastive learning framework and semantic fusion of multi-type data. For a fair comparison, we removed the review information from the-CRS.

is one of the state-of-the-art conversational recommendation models, which is based on a variational reasoning approach over incomplete knowledge graphs.

Among these baselines, Popularity, and TextCNN are classical recommendation methods, while ReDial, KBRD, KGSF, KECRS, RevCore,-CRS, and VRICR are conversational recommendation methods. For parameters used in baselines, we utilize the optimal parameters reported in the corresponding paper.

SECTION: Overall Performance (RQ1)
In this section, we study how effective is our proposed method compared to prior solutions. We compare our recommendation performances with baselines on ReDial and TG-ReDial, as shown in Tableand Table, respectively. The evaluation metrics are reported as the average performance for the max number of conversational turns.

For the recommendation performance on the ReDial dataset from Table, we observe that ReDial outperforms the classical recommendation models, Popularity and TextCNN, by using mentioned items in the dialog to make recommendations. However, for the recommendation performance on the TG-ReDial dataset from Table, we observe that the ReDial model performs worse than Popularity and TextCNN. That is, ReDial performs much worse on the TG-ReDial dataset than the ReDial dataset. The possible reason is that the ReDial model relies heavily on the mentioned items in conversations, but the mentioned items in the TG-ReDial dataset are much sparser than those in the ReDial dataset. Furthermore, KBRD, KGSF, KECRS, RevCore,-CRS, and VRICR outperform ReDial in both Tableand Table, which might be because they introduce external knowledge graphs and entities to understand the user’s intentions.
Also, we see that our proposed model, TSCR, significantly outperforms all the baselines on all three metrics on the ReDial dataset. Take Recall@50 as an example, TSCR outperforms Popularity, TextCNN, ReDial, KBRD, KGSF, KECRS, RevCore,-CRS, and VRICR by 150%, 134%, 56%, 32%, 18%, 31%, 13%, 13%, and 7%, respectively.
As for the TG-ReDial dataset, TSCR outperforms Popularity, TextCNN, ReDial, KBRD, KGSF, KECRS, RevCore, and-CRS on all three metrics. TSCR achieves a similar performance with VRICR, which might be because that VRICR benefits from the leverage of the knowledge graph although it neglects the modeling of the sequential dependencies. The results indicate that our TSCR model is effective and incorporating the sequential occurrence of items and entities is highly beneficial for improving recommender performance in. But note that this does not mean KBRD, KGSF, KECRS, RevCore,-CRS, and VRICR are worse than our TSCR model, as they focus more on the natural language response generation part and need to balance the recommender part and natural language response generation part by jointly modeling them in.
In addition, different from most prior solutions which use knowledge graphs to reduce candidate item space, TSCR does not make use of the structure of knowledge graphs, although a sequence in this work can be mapped to a path in knowledge graphs. In other words, TSCR only uses the knowledge base as a dictionary to extract the relevant entities for the items mentioned in a dialog.

Moreover, we find that our proposed extension model, TSCRKG, significantly outperforms all the baselines on all metrics on both ReDial and TG-ReDial datasets. As for Recall@50 on the ReDial dataset, TSCRKG outperforms ReDial, KBRD, KGSF, KECRS, RevCore,-CRS, and VRICR by 62%, 38%, 23%, 37%, 17%, 18%, and 12% respectively. As for Recall@50 on the TG-ReDial dataset, TSCRKG outperforms ReDial, KBRD, KGSF, KECRS, RevCore,-CRS, and VRICR by 592%, 17%, 22%, 30%, 20%, 17%, and 11% respectively. While TSCR achieves similar performance with the state-of-the-art baseline VRICR on the TG-ReDial dataset, TSCRKG achieves a significant improvement over VRICR on the TG-ReDial dataset. This indicates that TSCRKG enhanced by the knowledge graph is more powerful than TSCR. Furthermore, TSCRKG achieves a significant improvement over TSCR on both ReDial and TG-ReDial datasets. TSCRKG outperforms TSCR by 21% on Recall@1, 4% on Recall@10, and 4% on Recall@50, on the ReDial dataset, while outperforming TSCR by 40% on Recall@1, 16% on Recall@10, and 13% on Recall@50, on the TG-ReDial dataset. This again demonstrates that TSCRKG powered by the structure of the knowledge graph is able to provide more accurate recommendations than TSCR. Learning offline representations from knowledge graphs and enhancing sequences by multi-hop paths of knowledge graphs are highly beneficial for item recommendations in.

SECTION: Impact of Knowledge Graph Representation Learning and Knowledge Graph Enhanced Sequences (RQ2)
Next, we answer RQ2. To understand what are the contributions of different knowledge graph components,
we conduct an ablation study comparing our model with its ablation variants (TSCRKG removing the offline representation learning on knowledge graphs, represented by “-w/o offline”, TSCRKG removing the part of knowledge graph enhanced sequences, represented by “-w/o KGseq”, and TSCRKG removing both offline representation learning and knowledge graph enhanced sequences, represented by “-w/o both”).
The results on the ReDial dataset are shown in Tableand the results on the TG-ReDial dataset are shown in Table. From what we observed in Tableand Table, either removing the offline representation learning or removing path-enhanced sequences over knowledge graphs lowers the recommendation performance on all evaluation metrics.
Specifically, on the ReDial dataset, the performance of TSCRKG -w/o KGseq, TSCRKG -w/o offline, and TSCRKG -w/o both drops by 3% and 4% and 4% on Recall@50, respectively. On the TG-ReDial dataset, the performance of TSCRKG -w/o KGseq, TSCRKG -w/o offline, and TSCRKG -w/o both drops by 7% and 4% and 11% on Recall@50, respectively.
This demonstrates that both the two knowledge components play an essential role in item recommendations. Moreover, offline representation learning is more helpful for item recommendations on the ReDial dataset because the performance drops more without offline representation learning than without knowledge graph enhanced sequences on the ReDial dataset. In contrast, the knowledge graph enhanced sequences are more helpful on the TG-ReDial dataset since TSCRKG -w/o KGseq achieves lower performance than TSCRKG -w/o offline on the TG-ReDial dataset. This indicates that offline representation learning is the more important component on the ReDial dataset while the knowledge graph enhanced sequence is the more important component on the TG-ReDial dataset. This discrepancy might be because that item mentions are much more sparse and the user sequences in conversations are shorter in the TG-ReDial dataset, leading to more need to enhance user sequences by the knowledge graph. This suggests that, when the data suffers from the lack of contextual information in conversations, future research onmodels can incorporate the structure of external knowledge graphs to increase the number of effective entities, instead of simply mapping mentioned entities to knowledge graphs.

SECTION: Impact of Entity and Item in Sequence (RQ3)
To understand what are the contributions of items and entities in a sequence, we conduct an ablation study comparing our model TSCR with its ablation variants (TSCR removing the non-item entities “-w/o entity” and TSCR removing the item mentions “-w/o item”).
The results on the ReDial dataset and TG-ReDial dataset are shown in Tableand Table, respectively. Observe from Tableand Table, both the item mentions and item-related entities in the conversation contribute to the final performance of TSCR. After removing item mentions or non-item entities from the context, the recommendation performance on all three metrics on the two datasets drops, which indicates the importance of the two components. Also, we observe that item mentions contribute more than non-item entities on the ReDial dataset.
This might be because that item mentions are more reflective of user true preferences on the ReDial dataset and non-item entities contain more noise than item mentions. This suggests that sentiment analysis for entity mentions to distill the sequence of entity mentions might be beneficial. As for the TG-ReDial dataset, non-item entities are more useful than item mentions. One reason might be that the TG-ReDial dataset contains very few item mentions but much more non-item entities. The other reason is that the sequential dependency in item mentions in the TG-ReDial dataset is not as strong as in the ReDial dataset, given the TG-ReDial dataset contains items with multiple and diverse topics in a conversation.

In addition, we conduct an ablation study comparing our model TSCRKG with its ablation variants (TSCRKG removing the non-item entities represented by “-w/o entity” and TSCRKG removing the item mentions represented by “-w/o item”). The results on the ReDial dataset and TG-ReDial dataset are shown in Tableand Table, respectively. Observing from Tableand Table, we see a similar trend of TSCRKG with TSCR. That is, both the item mentions and non-item entities in TSCRKG are helpful because the performance on all evaluation metrics drops without any of them. Also,
It is confirmed again that item mentions contribute more than non-item entities on the ReDial dataset while non-item entities contribute more than item mentions on the TG-ReDial dataset.

SECTION: Effect of Item Position (RQ4)
In this section, we explore whether the item position in the conversation affects the recommender performance. We first compare the recommender performance for each position of item predictions, from 1-st item prediction to 5+ item prediction in the conversation on the ReDial dataset. From Fig., we observe that,
most of dialogs (74.1%) on the ReDial dataset contain only 1–3 item recommendations.
This is in line with that users expect the system can perform high-quality recommendations with fewer rounds in real applications. Overall, Fig.shows that the performance of our TSCR model and TSCRKG improve as the item position increases. We attribute this to the fact that the models collect more contextual information about the user as the item position increases. A higher item position means a longer sequence length of item mentions and item-related entities. This indicates that both the performance of TSCR and TSCRKG improve when the sequence length gets longer, given the longer sequence in the conversation contains more useful information.
Specifically, when the recommender suggests the first item (i.e., the item position is equal to 1, corresponding to the classical problem
“cold start”), the TSCR and TSCRKG recommender can still achieve high performance based on the contextual information. In addition, TSCRKG outperforms TSCR in almost every item position, suggesting the robustness and effectiveness of TSCRKG.

Interestingly, compared to the performance of TSCR and TSCRKG for each position of item predictions from the ReDial dataset (Fig.), we observe a very different trend from the TG-ReDial dataset, as shown in Fig.. Unlike the ReDial dataset including a varying number of predicting items in each conversation, the TG-ReDial dataset contains three predicting items in each conversation. The performances of our TSCR model and TSCRKG drop from the 1-st item prediction to the 2-nd or 3-rd item prediction. This means that a longer sequence in the conversation of the TG-ReDial dataset contains more contextual information, yet, it does not always ensure better performance. This might be because that the TG-ReDial dataset in Chinese involves many more non-item entities than the ReDial dataset, leading to more noise. Also, given the TG-ReDial dataset contains simulated items with multiple and diverse topics in a conversation, the sequential dependency in item mentions in the TG-ReDial dataset is not as strong as in the ReDial dataset. Therefore, more item mentions in the user sequence do not guarantee better modeling of user true preferences.

SECTION: Parameter Sensitivity (RQ5)
In this section, we explore how our proposed model is affected by its main parameters, including the hidden dimensionality and the mask proportion. For simplicity, we use TSCR model and ReDial dataset for the experiments.

We now explore how the hidden dimensionality affects the model performance.
As shown in Fig., we observe the recommendation performance of our TSCR model decreases with the embedding size increases. This is probably because of over-fitting. The TSCR model achieves the best performance when the hidden dimensionality is equal to 32.

Fig.shows how the mask proportion affects the model performance. It can be seen that from Fig., our TSCR model performs stably with the change of the mask proportion.
The best-performing mask proportions are 0.4–0.6.

SECTION: Case Study
To illustrate the ability of TSCR and TSCRKG to generate reasonable recommendations, we present use cases of them on the ReDial dataset, as shown in Table. Observe from “Case1” in Table, both TSCR and TSCRKG can accurately recommend the movie, i.e., “The Shining (1980)”, which is consistent with the ground truth. This is because both TSCR and TSCRKG are able to model the sequential dependency of contextual information. The two scary and classic movies mentioned in the context, i.e., “Rosemary’s Baby (1968)” and “The Exorcist (1973)”, are usually talked together and followed by another scary and classic movie “The Shining (1980)”. This sequential dependency can be modeled and trained in similar conversations in the dataset.

Observing from “Case2” in Table, we find that TSCRKG can generate the accurate recommendation with the ground truth, i.e., “Talladega Nights: The Ballad of Ricky Bobby (2006)”, while TSCR is not able to generate the accurate recommendation. This is because modeling the sequential dependency is not sufficient for generating accurate recommendations in this case. There is no sequential dependency learned between the movies “Blades of Glory (2007)”
and “Talladega Nights: The Ballad of Ricky Bobby (2006)”, leading to a failure of TSCR. As a comparison, TSCRKG can generate the recommendation accurately, with the help of the knowledge graph. As shown in the row of “extracted knowledge graph” in Table, TSCRKG can learn from the knowledge graph by connecting the movies “Blades of Glory (2007)” and “Talladega Nights: The Ballad of Ricky Bobby (2006)” through the actor “Will Ferrell”. This complete path in the knowledge graph leads to improved explainability for the recommended results. It confirms the effectiveness of TSCRKG and the benefit of incorporating the knowledge graph and sequential modeling. Furthermore, the contextual information learned from the knowledge graph can be used to enrich the conversation, i.e., the generated responses. For instance, one can enrich the response “Well, other good movies like that include Talladega Nights: The Ballad of Ricky Bobby (2006).” to the response “Well, other good movies with Will Ferrel include Talladega Nights: The Ballad of Ricky Bobby (2006).”, by using the actor “Will Ferrell” from the knowledge graph. This suggests a potential research direction, i.e., enhancing the dialog generation model with extracted knowledge.

SECTION: Conclusion and Future Work
In this paper, we have proposed the() for. TSCR deploys a Cloze task and models the sequential dependency of both items and entities in conversations by the deep bidirectional self-attention architecture. TSCR uses the knowledge base as a dictionary to get related entities, but does not use the structure of the knowledge base for any reasoning, making it simple and straightforward. Experimental results on the two CRS datasets show that our TSCR model, despite simple is highly effective, constituting a very strong baseline for future researchers to use. To take advantage of knowledge graphs, we have proposed an extension model TSCRKG, which fully uses the structure of knowledge graphs for offline representation learning and user sequence augmentation. The extensive experiments on the CRS datasets demonstrate the effectiveness of TSCRKG and the benefit of knowledge graphs.

One limitation of this work is that we only focus on the recommender module. As for future work, we plan to incorporate the natural language response generation part as well.
Moreover, in this work, we do not model the sentiment of mentioned items or entities and treat them as the same in the conversation. It is worth exploring the CRS by incorporating and modeling the sentiment (e.g., positive or negative) from users’ feedback in future work. Also, although we use entities from the DBpedia knowledge graph following previouswork, the extracted entities may not be 100% accurate. It is thus worth exploring thesystem by incorporating and modeling uncertainty and noise.

In this work, we use the knowledge graph to initialize the offline representations of the augmented sequences, which are constructed by using the knowledge graph and conversation history. Then we learn the representations of the augmented sequences by bidirectional Transformer. In this way, the semantic representations from the knowledge graph are aligned with the conversation. However, one can also use other techniques, e.g., contrastive learning, to bridge the semantic gap between the knowledge graph and conversation history. Moreover, we model the sequential dependencies in all conversations. However, this is just the first step toward sequential modeling and there is still room for improvement. For example, conversations may involve complicated scenarios (e.g., multi-topics in case users shift focus from topic to topic) and do not form strict sequential dependencies in the entire sequence. A potential research direction is to model the entire sequence into several subsequences (e.g., detecting topic threads in multi-topic conversationsand modeling each topic as a subsequence) so that more strict sequential dependencies remain in subsequences. It is also worth exploring the CRS model to detect sequential consistency in conversations, and thus the CRS model can predict the necessity to incorporate sequential modeling or model the noise in the sequence to relax the strict sequential dependencies. Last, exploring proactive CRSand integrating other forms of conversational interactions are also promising directions.

SECTION: Acknowledgement
This study is supported under the RIE2020 Industry Alignment Fund – Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from Singapore Telecommunications Limited (Singtel), through Singtel Cognitive and Artificial Intelligence Lab for Enterprises (SCALE@NTU). This research was also supported by the Natural Science Foundation of China (62402093),
the NWO Smart Culture - Big Data / Digital Humanities (314-99-301), the NWO Innovational Research Incentives Scheme Vidi (016.Vidi.189.039), and the H2020-EU.3.4. - SOCIETAL CHALLENGES - Smart, Green And Integrated Transport (814961).
All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.

SECTION: References