SECTION: Why Machine Learning Models Systematically Underestimate Extreme Values
A persistent challenge in astronomical machine learning is a systematic bias where predictions compress the dynamic range of true values—high values are consistently predicted too low while low values are predicted too high. Understanding this bias has important consequences for astronomical measurements and our understanding of physical processes in astronomical inference. Through analytical examination of linear regression, we show that this bias arises naturally from measurement uncertainties in input features and persists regardless of training sample size, label accuracy, or parameter distribution. In the univariate case, we demonstrate that attenuation becomes important when the ratio of intrinsic signal range to measurement uncertainty () is below—a regime common in astronomy. We further extend the theoretical framework to multivariate linear regression and demonstrate its implications using stellar spectroscopy as a case study. Even under optimal conditions—high-resolution APOGEE-like spectra () with high signal-to-noise ratios (SNR=100) and multiple correlated features—we find percent-level bias. The effect becomes even more severe for modern-day low-resolution surveys like LAMOST and DESI due to the lower SNR and resolution. These findings have broad implications, providing a theoretical framework for understanding and addressing this limitation in astronomical data analysis with machine learning.

SECTION: Introduction
The advent of modern astronomical surveys has revolutionized our understanding of the universe. Large-scale observational programs have amassed unprecedented volumes of high-quality data, enabling detailed studies across multiple astronomical domains. This wealth of information has motivated the development of data-driven models and machine learning techniques to extract meaningful insights from complex astronomical phenomena.

Given that many physical simulated models continue to suffer from systematic errors, data-driven approaches have become increasingly popular for astronomical inference problems. These approaches leverage both observational data and more reliable target properties from other sources to establish relationships between them. For example, in stellar spectroscopy, data-driven models have emerged as powerful tools for inferring fundamental properties such as stellar ages and distances, where ‘golden’ age measurements come from asteroseismology and precise distances from parallax measurements of nearby stars.

Even for more routine applications, such as determining basic stellar properties, data-driven methods have proven valuable. This is particularly true for low-resolution spectra, where theoretical spectral models often fall short. A common approach is to transfer stellar parameters and elemental abundances measured from high-resolution surveys like APOGEE to lower-resolution surveys such as RAVE, LAMOST, DESI, or Gaia. These low-resolution observations, especially of distant stars or those obtained as part of larger cosmological surveys, typically achieve lower spectral resolution and signal-to-noise ratios (SNR).

In astronomical data analysis, mapping between observables (e.g., spectra) and physical parameters (e.g., stellar properties) presents no inherent distinction between independent and dependent variables. Two main approaches exist in this field. The first approach uses generative models (e.g., The Payne, The Cannon, Cycle-StarNet) that map labels to observables and useminimization or MCMC sampling to obtain labels. The second approach employs discriminative models (e.g., AstroNN, StarNet) that directly map observables to labels.

A persistent challenge in data-driven models is systematic bias in the predictions compared to ground truth. This bias typically manifests as predictions being too low at high values and too high at low values (e.g., see the right panel of Figure). While this bias has anecdotally been attributed to either inadequate model training or sample imbalance leading to regression to the mean, this study demonstrates that both explanations are incorrect. We show that this bias persists even with uniformly distributed training data in both feature and label spaces with perfect ground truth labels. Furthermore, the bias depends only on the input measurement uncertainties, which systematically attenuate the regression coefficients and, consequently, the inferred label.

In linear regression models, this systematic effect, known as attenuation bias or regression dilution, has been studied analytically. While these seminal works established rigorous mathematical foundations, particularly in epidemiological applications, their scope was often focused on specific use cases of their time. The emergence of modern machine learning methods and high-dimensional data analysis presents new challenges and contexts that warrant revisiting these fundamental insights.

In this study, we aim to develop a more general framework, with particular emphasis on multivariate scenarios with correlated features and large feature spaces - regimes that are increasingly relevant to modern astronomical data analysis. While previous works provided thorough mathematical treatments, our framework specifically examines how feature correlations and dimensionality interact with attenuation bias, offering insights particularly pertinent to contemporary astronomical applications where high-dimensional, correlated measurements are common. To our knowledge, this phenomenon and its implications have not been systematically examined in an astronomical context. This study therefore aims to bridge this gap by providing both theoretical understanding and practical insights specifically tailored to the astronomical community, while building upon and extending these classical statistical results.

Our focus on linear regression stems from several considerations. First, many practical astronomical applications effectively employ linear models—for example, The Cannon, a widely-used tool in stellar spectroscopy, is fundamentally a linear regression model with polynomial features. Second, the mathematical simplicity of linear models enables us to derive concrete, analytical solutions that clearly demonstrate these effects. As many regression tasks, including Gaussian Process and neural networks, can be reframed as generalized linear regression, this framework provides insights into the broader challenges of using machine learning for direct label inference through data-driven models.

The paper is organized as follows: Sectionprovides a theoretical framework and mathematical derivations of attenuation bias, including both univariate and multivariate cases. Sectionexplores our empirical analysis using spectra and the implications for spectra-to-label models. Sectionbroadens the discussion to the impact of attenuation bias, providing key considerations to mitigate and calibrate such bias, and its effects on fundamental relations, while Sectionsummarizes our findings.

SECTION: Attenuation Bias
In astronomical analyses involving mappings between observables and physical parameters, we adopt the convention of treating observables (e.g., spectra) as input featuresand physical parameters as target labels. We focus on direct inference using error-ignorant discriminative models—where measurement uncertainties in input featuresare ignored during model training and inference. While this simplifies the full statistical problem, it reflects standard practice in machine learning applications due to the practical challenges of error quantification and incorporation in model architectures, especially in modern-day machine learning.

Attenuation bias, also known as regression dilution or errors-in-variables bias, systematically underestimates regression coefficients when independent variables contain measurement errors. While classical regression assumes error-free inputs with all uncertainty contained in the dependent variable, real astronomical observations invariably include measurement uncertainties in both dependent and independent variables. These systematic and statistical uncertainties can arise from multiple sources, including instrumental noise from thermal effects and detector imperfections, photon statistics, calibration errors in wavelength and flux measurements, atmospheric distortions in ground-based observations, and uncertainties in derived stellar parameters used as training labels.

To develop an understanding of this effect, we first examine the mathematical framework in the univariate case that describes how measurement errors propagate through regression analyses. This framework will provide the foundation for understanding both the causes and consequences of attenuation bias in more the complex multivariate scenarios.

SECTION: Theoretical Framework – Univariate Linear Regression
Consider a linear relationship between a single dependent variableand independent variable. By extending the feature vector with a constant term, we can absorb the intercept into the coefficient and write:

whereis the slope andrepresents both the measurement uncertainty and intrinsic scatter inwith:

The observed inputdiffers from the true valueby unbiased measurement error:

whereis assumed to be independent of both the ground truthand the uncertainty in(), with:

This independence assumption is reasonable for astronomical applications where observed quantities in bothandare typically measured independently.

Recall that for linear regression, the ordinary least squares (OLS) estimator forwhen regressingonhas an analytic solution for its expected value:

To understand the bias ofcompared to the true, we first examine the numerator – the covariance betweenand:

Given our assumptions about independence between measurement uncertainties and true values, this simplifies to:

whererepresents the signal variance (i.e., the “true spread”) of.

As for the denominator, the variance of observedis:

again drawing from the assumption thatis independent of. Combining these results yields:

If we define the attenuation factor:

our expression for the expected value ofbecomes:

This result reveals that the regression coefficient is systematically biased wheneverhas measurement uncertainty, with the attenuation factordetermined by the ratio of measurement uncertainty to the true spread of the variable (). Sinceis always less than unity, the estimated coefficient is inevitably attenuated toward zero, with the effect becoming more severe as measurement uncertainties increase relative to the true spread of values.

The mechanism underlying this attenuation bias can be understood geometrically: measurement uncertainties in the independent variablesystematically increase the horizontal dispersion of data points. When these uncertainties are ignored in the regression analysis, the increased scatter inleads to a systematic underestimation of the true slope coefficient. This effect becomes particularly pronounced when the measurement uncertainty is comparable to the range ofvalues in the dataset.

An insight from the mathematical framework is that measurement uncertainties in the dependent variabledo not contribute to the attenuation bias. While uncertainties insystematically bias the slope estimate toward zero through the attenuation factor, uncertainties indo not affect the expected value of. This asymmetry stems from the fundamental formulation of ordinary least squares regression, whereandplay mathematically distinct roles in the estimation process. Further, this bias is completely independent of training sample size. No amount of additional data can resolve the bias. The attenuation factordepends solely on the ratio of signal variances and the measurement uncertainty of, making it immune to increases in the training sample size.

It’s worth noting that, although in the derivation above, we assume the simple case of OLS where we ignore the errors in the dependent variable. However, even adopting Generalized Least Squares (GLS) to account for known heteroscedasticity in the dependent variable would not alter the conclusions of this paper (see Appendixfor details).

Recall that, in machine learning applications, our goal is to train a model on a training dataset and then use it to make predictions on new, unseen data. In astronomy, this typically involves learning relationships from a training set with known physical parameters and applying the trained model to make predictions for new observations. In the context of linear regression presented above, the linear coefficient plays the role of the “machine” (like weights in a neuron in a neural network). However, since we have established that the coefficient is biased, even after successfully training a model to minimize empirical error on the training set, predictions on new data will systematically deviate from the true values for.

To see this mathematically, consider that for a trained linear model where, and, we can examine the expected predictions on new observations. For any new observed value, the expected value of the prediction is:

The separation of expectations in line 2 follows from the independence between(determined from the training set) and(from new observations), while the transition to line 3 uses our assumption of unbiased measurement errors for. This demonstrates that the attenuation factordirectly translates to a bias in the predicted values for the label, which we can denote as. At least in this univariate case, the predictions are systematically biased by the same factor, and the two lambda factors can therefore be used interchangeably.

To understand the practical implications for astronomical applications, consider the case of inferring stellar properties from spectral features. In this univariate case (representing a single spectral feature), we have established that the bias depends solely on the ratio between the standard deviation of the true signal and the measurement error,.

For a concrete example, consider a high-resolution spectroscopic survey with resolution, whereis the wavelength andis the wavelength resolution. In normalized spectra, the intrinsic variation in flux typically spans at most 10% or(see also in Section 3). Even with a high signal-to-noise ratio of SNR = 100 (), yielding, Equationpredicts, corresponding to a 1% bias. For more typical observations, especially of distant targets in surveys like DESI and LAMOST where SNR = 10 (), the bias becomes severe with, indicating a 50% underestimation. Even at moderate SNR = 30, we still find, implying a 10% bias. These calculations demonstrate that attenuation bias is non-negligible in spectroscopic machine learning applications even under optimistic conditions, motivating a deeper investigation of this effect.

Finally, for completeness, while measurement uncertainties indo not affect the attenuation bias in, they do influence its variance. If bothandare centered (mean substracted), we can show (see Appendix) that the variance of the OLS estimator follows:

This shows that, while the bias depends solely on the ratio between measurement uncertainty and signal variance in, the precision of our estimate is affected by measurement uncertainties in both variablesandand scales with sample size. Thus, while increasing the training set size cannot mitigate the fundamental attenuation bias, it can reduce the statistical uncertainty in our biased estimates.

SECTION: Numerical Validations – Univariate Linear Regression
To validate and illustrate our theoretical analysis of attenuation bias in the univariate case, we conducted numerical simulations examining its relationship with measurement uncertainties. We generated synthetic data using a true slopeand sampleduniformly from. We then added Gaussian measurement errorsto create. The choice of a uniform distribution forwas deliberate to demonstrate that attenuation bias persists even with a balanced, unbiased sample, with no uncertainty in the training labeland a straightforward one-dimensional relationship. This confirms that the bias stems solely from measurement uncertainties in the independent variable rather than from sample imbalance or complex variable interactions.

Although our results show the bias is independent of sample size, we simulated 10,000 samples to better visualize the effect at different measurement uncertainty ratios. To generate the predictions shown in Figure, we first fitted a linear regression using the observed training data (,) to obtain estimated coefficients, then used these coefficients to compute predicted values for the validation set. By regressing these predictions against true values, we obtainthat quantifies the attenuation bias in the label. The results demonstrate how the magnitude of measurement uncertainties affects predictions: with, the fitted regression line shows only 1% bias () compared to the perfect prediction line. When measurement uncertainties become comparable to the signal variance (), we observe substantial attenuation () where extreme values are systematically pulled toward the mean. Recall that, for linear regression, the theoretical derivation shows that the attenuation factor for the coefficientcoincides with the attenuation factor for the predictions.

We further validated the theoretical predictions across a range of measurement uncertainties, as shown in Figure. For the attenuation in the regression coefficient (), we performed 100 independent simulations at each error level and calculated the meanto obtain robust statistics. While error bars from these independent simulations are included in the plot, they are too small to be visible. The figure shows that for the linear case, the simulated attenuation factors (blue points) match the theoretical predictions (blue solid line) exactly across all values of, confirming the analytical derivation.

Figureextends our analysis to examine the attenuation in predictions () across different measurement uncertainties in bothand. Following the approach in Figure, we computedby performing linear regression on the predicted values ofagainst true values of. The horizontal axis spansfrom 0.1 to 10 logarithmically, while the vertical axis covers measurement uncertainties infrom 0.7 to 70, with both axes spanning two orders of magnitude. The true values ofspan from 0 to 25 with a standard deviation of approximately 7, meaning we examine scenarios where the ratio of measurement uncertainties to true spread inis comparable toin the-axis. As predicted by the theoretical framework, the vertical gradient pattern in both panels demonstrates that the attenuation bias depends solely on the ratio, while remaining independent of the measurement uncertaintyin the dependent variable.

The agreement between simulation and theory confirms that for univariate linear regression with known, we can predict the attenuation in both coefficients and predictions regardless of the uncertainty in the dependent variable. However, as we will demonstrate below, such calibration against this fundamental bias becomes highly challenging, if not impossible, in higher-order polynomial regression or multivariate cases for linear regression.

SECTION: Attenuation Bias for Higher-Order Regression
While the theoretical framework precisely describes attenuation bias in linear regression, many astronomical applications, particularly in spectral analysis, employ more complex models. For example, The Cannon employs quadratic spectral features, while The Payne and AstroNN use neural networks. Empirically, attenuation bias has been observed across various machine learning applications in astronomy, from generalized linear regression with extended features to more sophisticated approaches. Different applications exhibit varying degrees of attenuation bias, raising the question of how our framework extends to these cases.

Many machine learning methods can be viewed as linear regression models with transformed features. Neural networks, for instance, act as feature extractors where intermediate layers perform nonlinear transformations of the input before a final linear transformation layer. Similarly, Gaussian Processes can be interpreted as extended linear regression models where the kernel trick substitutes for explicit feature transformations. While this suggests our theory should extend to these methods, a key challenge emerges: for transformed features, even in simple polynomial regression, the relationship between measurement uncertainties and true scatter becomes non-trivial.

To develop intuition for this complexity, we examine the univariate case with polynomial features, where the relationship between observed quantities and physical parameters often follows higher-order dependencies. Consider the transformationwith the model:

whereis the coefficient andrepresents measurement uncertainty in. As before, we assume:

While this remains a linear regression in terms of, the attenuation bias now depends on the ratio between the range of(denoted) and its measurement uncertainty (). To estimate this ratio, we employ logarithmic variance analysis. Assuming small measurement errors (), we can approximate:

The variance propagates as:

where we approximate. For the-th power:

This leads to an approximation for the ratio of uncertainties in transformed space:

yielding an attenuation factor for the-th degree transformation:

This result reveals that higher-order terms suffer more severe attenuation bias: for the same measurement uncertainty, the attenuation factor decreases roughly quadratically with the polynomial degree. This has important implications for astronomical applications where higher-order relationships are common.

Figureillustrates how attenuation bias scales with measurement uncertainty across different functional relationships, comparing the theoretical predictions with numerical simulations. In our simulations, we generated 10,000 samples for each case and centered thevariables to minimize higher-order effects in the Taylor expansion approximation. For each measurement uncertainty level, as before, we performed 100 independent simulations to obtain robust statistics.

As shown in the figure, at any fixed measurement uncertainty, higher-order terms exhibit progressively stronger attenuation bias, evident in the systematic downward shift of the curves with increasing polynomial degree. Even at modest measurement uncertainties (), the attenuation becomes more substantial: while a linear relationship experiences about 1% attenuation (), a quadratic relationship shows approximately 4% attenuation (), and a cubic relationship exhibits nearly 8% attenuation ().

Also notably, for linear regression (), the simulated attenuation factors (blue points) match the theoretical predictions (blue solid line) exactly across all values of. However, for higher-order polynomials (), while the simulations follow the general trend predicted by theory, they show increasing deviation from theoretical predictions (orange dashed and green dotted lines) as measurement uncertainties become larger (). These deviations are expected because our non-linear approximation above relies on a Taylor expansion that assumesis small, allowing us to neglect higher-order terms. When this assumption breaks down, additional terms in the expansion become significant, leading to the observed discrepancies.

Since most sophisticated methods involve highly nonlinear transformations of input features, their attenuation bias cannot be analytically predicted or corrected and appears to be a fundamental limitation of data-driven models.

SECTION: Theoretical Framework - Multivariate Linear Regression
Having established the principles of attenuation bias in the univariate case, we now extend our analysis to multiple independent variables. This extension is particularly relevant for astronomical applications, where observations often involve high-dimensional data such as spectra with numerous wavelength pixels. While linear regression is widely applied in spectral analysis, we will show that without proper treatment, it can introduce non-negligible bias in downstream scientific tasks.

Consider a multivariate linear regression model withvariables, whererepresents the number of pixels. As before, we absorb the intercept into the feature vector and write:

whereare the true coefficients andrepresents the combined measurement uncertainty and intrinsic scatter in, withand. As established earlier in the univariate case, we need not distinguish between these sources of uncertainty inas they do not affect the attenuation bias.

In spectral analysis using discriminative models, eachrepresents a different wavelength pixel in the spectrum. For our analysis, we consider continuum-normalized flux values that contain distinct spectral features, while the dependent variablerepresents the physical parameter we aim to infer, such as stellar metallicity [Fe/H]. And as in the univariate case, we assume each flux pixel is observed with measurement uncertainty:

For simplicity, we assume that each normalized fluxhas uncorrelated and unbiased measurement uncertainties with equal variance, i.e.,whereare independent withand. While this is a simplification, it is reasonable for spectroscopic data where different wavelength bins often have similar noise properties due to common instrumental and observational conditions.

For multivariate linear regression minimizing mean-squared error, recall that the ordinary least squares (OLS) estimator has the analytic form:

To analyze how measurement uncertainties affect this estimator, we must consider the relationship betweenand. Letbe thematrix of measurement errors where each elementrepresents the measurement error for the-th observation of the-th input, withbeing the number of training samples andthe number of features.

Substitutinginto Equation ():

For largeor small measurement uncertainties, we can use the first-order approximation:

First, we computeby expanding:

Usingand independence ofand, the cross terms vanish. For, since measurement errors are uncorrelated across pixels and observations with variance, this matrix multiplication yields a diagonal matrix where each diagonal element is the sum ofsquared errors, giving usfor each diagonal element. Hence:

Next, we compute:

Taking expectations and noting that cross-terms involvingandhave zero expectation:

Therefore best estimate of the multidimensionalsimplifies as:

To further simplify the expression, we examine two distinct cases, beginning with independent features.

Let’s first examine the case where the featuresare mutually independent. This scenario commonly occurs when different factors contribute additively to an outcome without interacting with each other. Consider, for example, a student’s exam score, which might depend on separate factors like hours of study, quality of sleep, and completion of practice tests—each independently affecting the final result. However, we should note that this example, while illustrative, rarely applies directly to astronomical analysis, as astronomical featurestypically exhibit strong interdependencies. We will explore these more general and realistic cases in the next subsection.

When features are independent,simplifies to a diagonal matrix where each diagonal element represents the variance of the corresponding feature multiplied by the sample size:

whererepresents the variance of the true signal in the-th feature. Substituting this into our previous derivation:

This diagonal structure, combined with the independence of measurement errors across pixels, leads to a simple form for each coefficient’s expectation:

This derivation reveals that, for independent features, the multivariate case preserves the fundamental characteristics of attenuation bias observed in our univariate analysis. Each coefficient experiences attenuation governed by the ratio of its feature’s true variance to measurement uncertainty, suggesting that the underlying mechanism of bias remains unchanged in higher dimensions, though the magnitude may vary across features depending on their individual signal variance.

Further, for the variance of the multivariate coefficient estimator, we can extend our univariate analysis using similar reasoning. For independent features and if bothandare centered (mean substracted), andhave the same signal rangeand measurement uncertainty, (see the derivation in Appendix), the variance of each coefficient estimate follows the same form as the univariate case:

However, as established in our univariate analysis, in many astronomical applications it is more critical to understand the attenuation in terms of the predicted labelrather than the bias in individual coefficients. This is particularly relevant when these coefficients serve as the “machine” in a data-driven model.

For the multivariate case, the relationship between coefficient attenuation and prediction bias follows directly from our analysis. For any new observation, the predicted value is:

The expectation ofcan be derived by considering thatis determined from the training set whilecomes from new observations, making them statistically independent. Additionally, since measurement errors are assumed to be unbiased,. Thus:

A key limiting case arises when all features have equal signal variance, i.e.,for all. In this case, the expectation of the predicted value becomes:

where we have used that all coefficients share the same attenuation factorwhenfor all, and the last equality follows from the definition of. Thus, we recover the same form of attenuation bias as in the univariate case, where the bias in coefficients directly translates to bias in predicted labels.

This result reveals a crucial insight: for independent features of equal strength, the attenuation bias is independent of the number of features. In other words, the bias induced by measurement uncertainties in the input features cannot be mitigated simply by including more features in the model. The magnitude of attenuation remains constant regardless of the dimensionality of the input feature space, maintaining the same form we derived in the univariate case.

For the more general case wherevaries across features, the attenuation intakes a more complex form:

Comparing this to, we can define an effective attenuation factor for the predicted label:

This effective attenuationrepresents a weighted average of the individual coefficient attenuation factors, where the weights depend on the contribution of each feature to. For features that carry no information (e.g., continuum pixels with), bothandapproach zero, effectively removing their contribution to the overall bias. For the informative features where signal are comparable, i.e.is about the same for all, thenapproaches the arithmetic mean ofover the informative pixels (see Appendix).

The derivations above paint a bleak prospect for mitigating attenuation bias, as they apply to the case wherecausesand all dimensions of the inputcontribute toindependently, like the exam score example laid out earlier. However, in most astronomical applications, there is still a saving grace stemming from the fact that it is the label, the physical property of the object, that generates all the features, i.e.,causes. By definition, in many astronomical applications, the featuresare not independent (their measurement errors can be independent, but notthemselves). As we will demonstrate below, such correlation in features helps mitigate the attenuation bias as the number of features increases.

This is perhaps not surprising; intuitively, with multiple correlated features, the noise needs to conspire in a single direction to generate the bias. As the dimensionality increases, the likelihood of such action happening becomes smaller for any independent random noise.

For an arbitrary set of features with non-zero correlation, an analytic theory for the attenuation bias becomes intractable. This is another reason why such attenuation bias, while fundamental to many data-driven machine learning applications as we have demonstrated, is hard to eradicate from first principles in real practice.

To derive some analytic expressions and gain insights for this more general case, we will make the simplifying assumption of an optimal scenario where all features are completely correlated with a correlation of one. In this case, all features can be expressed as linear transformations of a single underlying variable, we can thus assumeare the true independent variables, withbeing known scaling factors. The scaling factorscapture the relative magnitudes of each feature with respect to this common variable. For example, in spectral models,approximately corresponds to the oscillator strength when all features lie within the linear portion of the curve of growth.

If we further assume that, whereis a scalar coefficient, i.e., the linear coefficients follow also the direction of the scaling factors, as we will show in Appendix, we can still analytically derive the attentuation bias factor for the regression coefficients. The expected value of the coefficient estimatesis approximately:

where the attenuation factoris:

Here,is the measurement error variance,is the sum of squares of the true signal,is the number of samples, andis the number of features.

As shown in the equation, similar to the univariate case, the attenuation bias in the correlated case depends on the ratio of the total measurement error variance () to the total signal variance indicator (). However, it also depends on the scaling factorsand, more importantly, the number of variables, which was not the case before. If we define an effective signal range vs measurement error as:

the attenuation factor can be rewritten in a more familiar form as before:

Interestingly, unlike the independent feature case, for a large number of correlated and non-trivial features (), we have, and the attenuation factorapproaches 1, indicating that the bias can be mitigated by increasing the dimensionality of the input space, provided the features are sufficiently correlated. The attenuation bias in the correlated case is less severe than in the independent case, as the correlation among features allows the model to leverage additional information to reduce the impact of measurement errors.

SECTION: Numerical Validations – Multivariate Linear Regression
To validate the theoretical framework of multivariate attenuation bias, we conducted numerical simulations examining both coefficient and prediction attenuation across varying numbers of input dimensions. We will first focus on the case where the features independently contribute to the output label.

We generated synthetic data following our established framework in the case of univariate: for each j-th dimension (“pixel”) of the input, we sample true values uniformly from [-5, 5] (yielding) and adding homogeneous Gaussian measurement errors with a variance ofindependent of. We first draw the values of the input. The value of all pixels are drawn independently, and their linear sum from the noiseless version with the predefined coefficient is then assumed to be the true labels.

To explore the relationship between measurement uncertainty and attenuation bias, we examined three different signal variance to measurement error ratio for the input (). For each dimensionranging from 1 to 100, we performed 100 independent simulations with 10,000 samples each to ensure robust statistics. In each simulation, we maintained identical statistical properties across all dimensions (uniform distribution, consistent measurement error inand zero measurement error in) and set all coefficientsto 2.0, ensuring equal contribution to the dependent variable.

Figurepresents our findings through a two-panel visualization comparing coefficient and prediction attenuation factors across different numbers of input dimensions. For each simulation, we first fit a linear regression model using the noised-up observedto predict the output label, obtaining estimatesfor the true coefficients. To quantify the coefficient attenuation, we calculate the ratiofor each coefficient. For prediction attenuation, we use the fitted model to make predictions on the observed features and then perform a linear regression of these predictions against the true values, where the slope of this regression gives us. The left panel shows the distribution of coefficient attenuation factors (), pooling results across all coefficients and simulations for each. The boxcar shows the distribution ofover allcoefficients and all simulations. The right panel displays prediction attenuation factors (), obtained by regressing predicted values against true values across all samples in each simulation, and the boxcar shows the distribution ofover all simulations.

As discussed in Section, in this simplifying limit where features independently contribute to the label, the attenuation factor for the coefficientsis analytically predictable. Furthermore, when both the coefficientsand the featuresare homogeneous across all pixels, we derived that the attenuation factor for predictionsshould exactly follow. A key prediction from the theoretical framework is that such attenuation cannot be mitigated simply by increasing the number of pixels.

Our results, as shown in Figure, demonstrate excellent agreement with these theoretical predictions across all tested input dimensions. The attenuation factors remain stable as the number of input dimensions increases, confirming our analytical finding that for independent features, additional dimensions do not mitigate the attenuation bias. For all threeratios tested, the observed attenuation closely matches the theoretical prediction of—the same relationship we derived in the univariate case—shown as dashed lines in both panels. Even in the optimistic case where the signal range is 10 times the measurement error, which represents an optimistic limit for spectral analysis at high-resolution and high-signal-to-noise (see Section), there remains a persistent 1% systematic bias when the features are independent. Further, the identical behavior between coefficient attenuation () and prediction attenuation () across all dimensions and signal-to-noise ratios confirms the theoretical framework.

In the case of independent features, not only does increasing the number of pixels fail to mitigate the attenuation bias, it actually comes with a cost. As we derived in Equation, the variance of the coefficients increases withdue to theterm in the numerator. This behavior is perhaps not surprising from a geometric perspective: as dimensionality increases while the number of data points remains constant, we encounter the curse of dimensionality, where estimation of the hyperplane becomes increasingly stochastic. The variance of these linear coefficients follows a predictable pattern that we can derive analytically. Already seen in the boxcar plot, the spread increases with. We further calculate the variance of these spreads. The inset plots in both panels reveal the variance behavior of our estimates as a function of dimensionality. As shown in the inset plot of the left panel, the empirical variance of coefficient estimates (points) closely follows the theoretical prediction (dashed lines) from Equation, demonstrating how the uncertainty scales systematically with the number of dimensions.

In the case where the label is simply the linear weighted sum of these variables, the increase in coefficient variance naturally leads to larger variance in the predictions. In the limit where the variance ofis dominated by the variance of, the sum of these random variables is still just a convolution of the various Gaussians, and thus larger variance intranslates into larger variance in the predicted. However, as shown in the boxcar plot on the right panel, the prediction ofitself remains stable over, since here we are taking the average over all pixels instead of the sum. In the inset plot of the right panel, the variance ofincreases with. We note that, to be conservative, we only calculate the variance of, since we know thatalso suffers from an attenuation bias, and hence the variance ofwould have a dependency on, but the variance ofitself does not.

A common suggestion for addressing measurement error effects is to increase the sample size. Having established that increasing the number of input dimensions cannot mitigate the attenuation bias, we now examine how prediction attenuation () depends on training sample size. Following exactly the same simulation setup as in Figure, where we generate independent input features drawn from uniform distribution and add measurement errors, we now fix the number of input dimensions () and vary the sample size. As shown in Equation, the variance of the coefficient estimates scales as, whereis the sample size. This fundamental statistical behavior suggests that while the bias remains fixed, the precision of our estimates should improve with larger samples. Figureshows this relationship across three different ratios of signal range to measurement error ().

The results reveal that as sample size increases, the attenuation factor for eachratio converges to its respective theoretical value at a rate that follows ascaling law (shown as dashed lines). This scaling behavior directly emerges from the variance ofin our theoretical derivation and propagates to the predictions through the linear relationship. However, the key finding is that at all training sizes, we observe persistent attenuation in predictions that depends on the ratio. Even in cases with larger sample sizes, predictions asymptotically approach their theoretical attenuation limits.

In summary, these results do highlight a fundamental limitation in multivariate regression with independent features: the systematic bias introduced by measurement uncertainties cannot be overcome simply by increasing the number of input dimensions or size of the training sample.

While our analysis of independent features presents a concerning picture for attenuation bias, there is reason for cautious optimism. In most astronomical scenarios, such as spectroscopic analysis, the observed features (flux at different wavelengths), treated as the input of discriminative models, are not independent but rather correlated through the underlying physics – it is the physical parameters that generate the spectra, not vice versa. As we have derived in Section, this correlation between features could potentially help mitigate attenuation bias.

To explore this scenario analytically, we consider the optimistic limiting case of perfectly correlated features. We generate synthetic data where all features are linear transformations of a single underlying variable, drawn from a uniform distribution over [0,1] (yielding). Each featureis scaled by a factor, where we chooseto vary linearly from 0.2 to 0.4 across thedimensions. We chose this range of, as it ensures the resulting labelsremain roughly of order unity across different, mimicking typical normalized label in spectroscopy analysis as we will describe below (e.g., metallicity).

We have established that deriving the analytic attenuation factor is challenging, if not impossible, for general cases for correlated features. To make the theoretical analysis tractable, as assumed in the theoretical framework, we set the regression coefficientsequal to the scaling factors, allowing us to derive analytical predictions for the attenuation factor of coefficients as shown in Equationand. If we further assume homogeneous and identical (but not independent), the attenuation factorthen translates into the attenuation factor of the label as we take the expectation through the linear transformation. With this framework, we can check with simulations to build insight but to ensure also that the theoretical framework is accurate.

Figuredemonstrates how attenuation bias behaves in this idealized correlated case. As in Figure, we perform 100 independent simulations for each dimensionality, but now with perfectly correlated features. In both panels, the markers show the mean attenuation factors, while dashed lines indicate the theoretical predictions. The left panel displays the distribution of coefficient attenuation factors () through box plots, with boxes showing the interquartile range. The variance is shown for all collected coefficients over all simulations. For better visibility, the box plots for differentratios are horizontally offset. The right panel shows the prediction attenuation factors () across simulations. The empirical results (box plots) closely match the theoretical predictions (dashed lines) across differentratios. Like in the case of independent features, the left panel reveals additional complexity through the spread in coefficient attenuation factors, particularly at higher dimensions, suggesting that individual coefficients still suffer from high degrees of variations even when their collective effect is better behaved.

The key difference from the independent case is immediately apparent: increasing the number of correlated features does help mitigate the bias. For, the attenuation factor improves from about 0.8 atto nearly 1.0 at. Even for more challenging cases with, we see substantial improvement from 0.13 (almost random prediction) atto 0.97 atas dimensionality increases.

It’s worth noting that this represents a best-case scenario for several reasons. First, perfect correlation between features is unrealistic in practice. Second, our assumption thatlikely overestimates the mitigation effect. In real applications, as the number of features increases, the individual coefficients typically become smaller to maintain reasonable output scales. This means the sumgrows more slowly than in our idealized case, resulting in less mitigation of the attenuation bias as we will see in the real scenario below.

SECTION: Attenuation Bias in Spectral Analysis
Having established the theoretical framework for attenuation bias in both univariate and multivariate cases, we now examine its implications in a specific astronomical context: the inference of stellar properties from observed stellar spectra. We use APOGEE-like mock spectra as a case study to demonstrate how attenuation bias manifests in spectral inference tasks when training data contains measurement noise.

Spectra present a unique challenge in astronomical data analysis due to their high dimensionality. Each spectrum typically contains thousands of flux measurements across different wavelength pixels, encoding stellar properties through absorption features. Understanding how measurement errors propagate through discriminative machine learning models—which are becoming increasingly prevalent in modern data-driven research—is crucial.

We chose APOGEE as our case study for several reasons. First, we can take advantage of a well-tested spectral emulator which allows us to simulate spectra efficiently. Second, and perhaps more importantly, APOGEE high-resolution stellar spectra represent a conservative test case. With its high resolution () and typical signal-to-noise ratio of, APOGEE should present the most favorable conditions where attenuation bias would be minimal. Stellar features generally exhibit sharper features with more observable variation compared to galaxy spectra or integrated light spectra from stellar populations.

However, even under these optimal conditions of APOGEE, attenuation bias remains significant. When varyingfromto, only the strongest absorption features show variations of order 10% in normalized flux, setting a natural scale of. This yields. As demonstrated in Section, even this favorable ratio produces an appreciable attenuation bias of 1% for a single feature. Given that most spectral features show variations significantly smaller than 10% in flux, attenuation bias remains non-negligible under most assumptions.

Such bias has been broadly observed in many machine learning tasks applied to APOGEE spectra (see for example fig 7-8 inor fig 9-11 in). While much of the consensus often attributes these effects to the precision of training labels or sample size limitations, the theoretical framework and numerical validation show that these factors have no impact on such attenuation bias.

Nonetheless, predicting the attenuation bias in APOGEE spectra requires consideration beyond the simple univariate case, as spectral features in physical systems are inherently correlated. When varying only one label (here) while fixing other parameters, the information content across pixels should be perfectly correlated through the underlying physics. As developed in Section, such correlation between input dimensions (while measurement noise remains independent) should help mitigate the attenuation bias.

The theoretical framework predicts that as the number of correlated featuresincreases, the attenuation factor should improve, potentially approaching unity as. However, the exact analytical prediction relies on several strong assumptions that may not hold in practice. Given these limitations, we must turn to numerical simulations to understand attenuation bias in realistic spectroscopic applications.

SECTION: Numerical Investigation of Attenuation Bias Using Synthetic APOGEE Spectra
For our analysis, we use The Payne spectral emulationto generate normalized synthetic APOGEE spectra, though the exact accuracy of the model is not critical to our conclusions about attenuation bias. We chose The Payne emulator because it is publicly available and allows us to efficiently simulate spectra across the APOGEE parameter range. Following APOGEE’s wavelength sampling rate, we assume three pixels per resolution element, and all SNR values discussed below refer to the SNR per such pixel.

Figuredemonstrates both the spectral variations and their relationship with metallicity. The left panel displays synthetic spectra around the Fe absorption line at 16200Å, illustrating how spectral features vary systematically with. Throughout this study, we focus on a20Å region around this wavelength that contains 179 pixels rich in absorption features, allowing us to study attenuation bias across varying numbers of input dimensions up topixels. To ensure the validity of our linear regression analysis, we restrict our parameter range to a regime where spectral features vary approximately linearly with stellar parameters. Specifically, we fix the effective temperature and surface gravity to typical red clump values (K,) and vary only metallicity () fromto, as this regime is well-tested in the emulation. For simplicity, we assume all elemental abundancestrace, allowing us to capture the full variation across all pixels.

The right panel examines individual pixels that show more substantial variation (in normalized flux fromto), demonstrating predominantly linear relationships between flux and. We validate this linearity in the middle panel, where for each pixel, we first fit a linear relation betweenand flux, then predict the flux for 1000 randomly generated spectra from The Payne using theirvalues. The residuals from these predictions show that most pixels fall within a 0.1% range at 1, and even in the worst cases only reach 0.5-1%, demonstrating narrow confidence intervals around zero across all wavelengths. This confirms that linear relationships provide a sufficient model for our restricted study. Further, recall from Section 2 that for polynomial terms of order, the attenuation factor scales as, suggesting that our analysis focusing on the linear case is conservative in estimating the impact of attenuation bias in more complex spectral modeling approaches, as in real applications, higher order fits are often shown to be better descriptors of how spectral features vary with labels.

Having established the validity of our linear approximation, we now examine how measurement noise affects spectral inference. Figureillustrates the progressive degradation of spectral features with increasing noise levels. Starting from a noiseless spectrum (), we add uncorrelated Gaussian noise corresponding to different SNR values: SNR = 100 (), typical of APOGEE and other high-resolution stellar surveys where features remain clearly discernible; SNR = 20 (), characteristic of low-resolution surveys like LAMOST where noise begins to significantly impact the spectrum; and SNR = 5 (), common for faint targets where spectral features become severely degraded.

To quantify the attenuation bias in this realistic setting, we generate training and validation sets of 1000 spectra each, with no noise in thelabels. As established in the theoretical framework, the size of the training set and label precision do not affect the expected attenuation bias. To make our analysis more physically relevant, we sort pixels by their “sensitivity” - calculated as their standard deviations across the full metallicity range. This approach mimics common practice in spectroscopic analysis where reliable absorption lines of particular atomic species are identified for parameter estimation.

Figureshows how attenuation bias manifests in this correlated case. The solid black line represents the theoretical prediction for the most sensitive pixel (), which serves as our best-case reference if we have one pixel or if all pixels are independent and equally sensitive. Using only the single most sensitive pixel (yellow points), our simulations closely follow this theoretical prediction, validating our framework in this simplified case where analytical treatment remains tractable.

As expected with correlated features, the empirical results demonstrate a clear departure from the independent features case (see Appendix)—increasing the number of correlated pixels consistently improves the attenuation factor. For 100 pixels at SNR = 100 (), the attenuation is nearly negligible (mean), compared toseen with independent features. However, this improvement diminishes at lower SNR; even with 100 pixels at SNR = 10, we find, demonstrating how even correlated features struggle to overcome some of the more reasonable flux measurement noise from modern spectroscopic surveys like LAMOST and DESI.

The practical implications become particularly relevant when considering realistic spectroscopic scenarios. With 10 informative pixels at SNR = 100, we observe, already introducing a 1% systematic error. This quickly degrades toat SNR = 10. For any reasonable spectral analysis, there are usually only finite features from an atomic species that contribute meaningfully. While major elements like Fe that have myriad of features can achieve minimal attenuation bias by combininglines at high SNR, the bias becomes increasingly problematic either as SNR decreases or when analyzing elements with fewer () clean spectral features. Indeed, for atomic species that haveone strong absorption feature (e.g., potassium and vanadium abundances for APOGEE), even at relatively high SNR = 30, we see, with rapid deterioration at lower SNR.

The top panels visualize predicted versus truerelationships at different noise levels for both 10-pixel and 100-pixel cases, where the 10-pixel case shows visible attenuation even at SNR = 100 that becomes severe at SNR = 10, while the 100-pixel case demonstrates better resilience against attenuation through correlated features, although the bias is still quite visible at SNR = 10.

These results demonstrate that while natural correlations in spectral features provide notable protection against attenuation bias, particularly at moderate to high SNR and for labels that contribute to multiple features, this mitigation is not absolute. The effect remains detectable and potentially significant in many practical scenarios encountered in spectroscopic analysis.

The analysis above, while being more realistic than the independent feature case, still represents an optimistic scenario. Our assumption of perfect correlation achieved by varying onlylikely overestimates the attenuation mitigation effect compared to real spectra. In practice, multiple stellar labels (especially stellar parameters) vary simultaneously. For example, variations in effective temperature alter the stellar atmosphere, affecting all spectral features—including those sensitive to—even at fixed metallicity values. This coupling between parameters weakens the perfect correlation between pixels that would arise fromvariations alone.

To show how this idealized scenario breaks down in practice, we examine a simpler case showing how variations in effective temperature affect the correlations between spectral features and their resulting attenuation bias. Figuredemonstrates this effect by introducing, on top of thevariation, different levels of temperature scatter around our fiducial red clump temperature (K) while still fixing the. Focusing on the 100-pixel case that showed promising mitigation in our previous analysis, we demonstrate that even in this optimistic high-dimension scenario with high SNR and resolution, adding realistic parameter variations can accentuate again the attenuation bias.

For each temperature range, we calculate the correlation matrix for our 100 most sensitive pixels. Taking the upper triangular portion of this matrix to avoid redundancy, we compute the distribution of unique pairwise correlation coefficients. The left panel shows these distributions using kernel density estimation (KDE with bandwidth 0.03), with vertical lines indicating the mean correlation for each temperature range. By definition with zero temperature spread, we only have correlation of one which is shown as the solid vertical line on right panel. As expected, increasing temperature variation progressively weakens pixel correlations. With no temperature variation (orange), the pixels show nearly perfect correlation as expected from our previous analysis. However, as we increase the temperature range toK (blue),K (green), andK (purple), the correlation distributions broaden and shift toward lower values even for these most sensitive pixels responding to. This decorrelation occurs because different spectral features respond differently to temperature changes—some features become stronger while others weaken, even at the same.

This weakened correlation then translates into a smaller mitigation of the attenuation bias as shown in the right panel. The impact on attenuation bias is clear in the right panel. As temperature variations increase and correlations weaken, the attenuation becomes more severe at all SNR levels (here we also assumedto calculate the SNR). Even at high SNR = 100, the attenuation factor becomesatK, but then reachesatK, dropping visibly with increasing temperature spread.

This demonstrates that our earlier analysis with fixed temperature represents an optimistic scenario—in real applications where multiple stellar parameters vary simultaneously corrupting the perfect correlations, the protective effect of correlated features is diminished.

Finally, throughout this study, we have focused on APOGEE’s high-resolution regime (R = 24,000) as our baseline case. Even in this optimistic scenario, we assumedbased on typical spectral variations. To examine how this assumption and our earlier conclusions might change with spectral resolution, we conduct a systematic analysis across different resolutions from APOGEE (R = 24,000) down to LAMOST/DESI-like resolution (R = 3,000).

To ensure a fair comparison across resolutions, we maintain the same number of pixels by proportionally increasing the wavelength range as resolution decreases, essentially assuming the same CCD real estate. When degrading the spectra, we account for the initial APOGEE resolution by calculating the effective Gaussian kernel width as.

The left panel of Figureillustrates the degradation of spectral features with decreasing resolution. At APOGEE’s high resolution (, red), individual absorption features are clearly resolved, as shown in both the main panel and the zoomed inset. As resolution decreases (blue through orange), these features become progressively blended (see also Figure), effectively reducing the information content per pixel as the range of variation () decreases. For clarity of visualization, we truncate thecase to the range 16,000Å-16,400Å. Following the theoretical framework, this reduction inat fixed SNR should lead to more severe attenuation bias at lower resolutions. While we maintain the same number of pixels across all resolutions, the larger wavelength resolution elements at lower resolution result in broader wavelength coverage.

The middle panel quantifies this degradation by showing the distribution of absolute flux differences between spectra atandfor the 50 strongest features at each resolution. The vertical dashed lines indicate the median difference at each resolution. Naively, we should expect the strength to decrease by the ratio of R, but in practice, the low resolution features regain some strength through blending effects.

This analysis refines our earlier assumption offor APOGEE resolution. While the maximum peak-to-peak variation in flux is indeed about 0.1, the actualis smaller when considering multiple pixels. For example, with 50 pixels, the effectiveis approximately a factor of 3 smaller (or more precisely,smaller assuming uniform distribution). Therefore, while our original assumption ofprovides useful intuition, the exact value depends on the number of strongest pixels considered in the analysis.

While we have quantified specific attenuation values, the key insight of this paper is more fundamental: even in optimistic scenarios, attenuation bias remains non-negligible. This conclusion stems from two critical factors: (a) the measurement uncertainty in flux () is typically only a factor of a few smaller than the intrinsic variation range (), and (b) although increasing the number of pixels can mitigate attenuation bias when features are correlated, this mitigation is moderate—especially when only  10 atomic features contribute significantly to the signal, even in the optimistic limit of perfect correlation.

The impact of this feature degradation on attenuation bias is shown in the right panel of Figure. For each resolution, we calculate attenuation factors following the same methodology as our earlier analysis, but now using the resolution-appropriate spectra. The results show progressively more severe attenuation at lower resolutions across all SNR values. Even at relatively high SNR = 100, the attenuation becomes significantly worse as resolution decreases. The systematic degradation of feature strength with decreasing resolution suggests that attenuation bias becomes increasingly important to consider in lower-resolution surveys, even when high SNR can be achieved through long exposures or instrument design. This highlights the fundamental trade-offs between resolution, SNR, and feature strength that must be carefully considered in survey design and analysis.

SECTION: Discussion and Implications
This study examines the fundamental nature of attenuation bias in astronomical machine learning applications, demonstrating that when directly mapping from input features to output labels—a common practice in machine learning—this bias is inherent and widespread. The challenge stems from how modern machine learning methods, especially neural networks, typically handle uncertainties. While output uncertainties can be incorporated into loss (objective) functions, the treatment of input uncertainties remains challenging within most standard frameworks. This stands in contrast to traditional Bayesian approaches where input uncertainties can be explicitly considered in the analysis, although such treatment of input uncertainties is also often overlooked in practice in Bayesian inference applications to astronomical studies.

We find that the magnitude of the attenuation bias depends primarily on input measurement precision, rather than on label precision or training sample size. Using spectral analysis as a case study, we investigate a domain where regression tasks are routinely applied. While we focus on spectroscopy, the implications of attenuation bias extend broadly across observational astronomy, where measurement uncertainties are ubiquitous.

Our analysis centers on linear regression, a choice motivated by two key considerations. First, linear regression permits analytical prediction of attenuation bias under certain assumptions, providing concrete mathematical insights into the effects of measurement uncertainties. Second, despite its simplicity, linear regression serves as a foundation for many astronomical analysis methods. Many power-law relations in astronomy (e.g., Kennicutt-Schmidt relation, Tully-Fisher relation, M-relation) can be transformed into linear regressions in logarithmic space. Furthermore, many classical machine learning approaches are effectively generalized linear regression with extended, curated features, and neural networks can be viewed as a combination of a feature transformer and linear regression. Indeed, we demonstrated analytically that superlinear relationships typically suffer from even more severe attenuation bias.

SECTION: Why Attenuation Bias Particularly Matters for Astronomy
While attenuation bias receives relatively little attention in general machine learning applications, it poses a particular challenge for astronomy due to the inherent dynamic range of astronomical measurements. For univariate or independent multivariate cases, the relevant scale is the ratio of signal variance to measurement error (). The theoretical framework predicts that even at a ratio of 10—meaning the dynamic range of the data is approximately tenfold the measurement error—coefficient and label bias manifests at the percent level. This roughly corresponds to a SNR of 10. While general machine learning applications typically operate with much higher SNR (e.g., natural image processing), astronomy frequently operates in this critical regime where measurement uncertainties are significant relative to the signal range.

This challenge pervades astronomy as we attempt to leverage order-of-magnitude measurements to achieve percent-level understanding through statistical samples. Consider the case study presented here: flux variations in normalized spectra versus flux measurement errors. Even in the optimistic scenario of high-resolution spectra, typical variations are only 5-10%, while measurement errors in dedicated spectroscopic surveys remain around 1%. This placessquarely in the-regime where attenuation bias becomes prominent.

Similar examples appear throughout astronomy. Stellar age measurements, except for the small subset of stars with precise asteroseismology, typically have uncertainties ofGyr—approximately 10% of the Hubble time. Elemental abundance measurements, even in dedicated studies, typically achieve precisions of 0.05-0.1 dex, again about 10% of the full dynamic range for [X/H].

The situation becomes particularly relevant for power-law relationships when transformed into log-log space for linear regression. For instance, supermassive black hole mass measurements spanning three-fiver orders of magnitude can only tolerate uncertainties of about 0.3-0.5 dex—corresponding to roughly a factor of three in linear scale—to avoid significant attenuation bias. Similar precision limitations appear in measurements of stellar mass and gas mass, wheredex or more uncertainties are common in multi-dex power law studies. Thus, attenuation bias systematically affects astronomical measurements exactly where precision matters most.

While some studies can tolerate attenuation bias when it merely compresses the dynamic range of labels while preserving relative ordering, this bias becomes crucial when comparing results to physical models or performing parameter estimation. Cosmological studies provide a compelling example: in photometric redshift estimation, high accuracy is essential because biases can propagate through cosmological analyses, significantly affecting parameter inference. This sensitivity to bias is commonly assessed through the Probability Integral Transform (PIT) test, where a uniformly distributed sample (similar to our study) is simulated to verify recovery of the uniform distribution.

Common mitigation strategies in cosmology focus on addressing potential bias sources through careful sample selection—for example, collecting training samples that span the Self-Organizing Map (SOM) space to ensure unbiased representation of galaxy properties, or gathering high-quality spectroscopic redshifts. However, these approaches implicitly assume that bias primarily stems from training sample distribution and label quality. Our theoretical framework demonstrates that this assumption may be incomplete, as attenuation bias persists regardless of training sample properties, suggesting that these mitigation strategies warrant reevaluation.

Similar misconceptions appear in other domains, such as the inference of stellar ages from asteroseismic data, where calls for collecting more asteroseismic ages reflect the same assumption. Our analysis shows that such efforts, while valuable for other reasons, are unlikely to address the fundamental issue of attenuation bias.

SECTION: Implications of Attenuation Bias for Spectral Analysis with Machine Learning
While this paper’s primary goal is to provide a general framework for understanding attenuation bias—which can be partially predicted in the simplistic limit of linear regression—we showcase a specific application: spectral analysis with machine learning. We chose this example for several compelling reasons. First, as noted above, the ratio between flux measurement error and flux variation naturally falls in the regime where attenuation bias becomes significant. Second, spectra provide an ideal test case for the theoretical framework, offering naturally correlated multivariate features that allow us to extend our analysis beyond univariate cases. Third, spectroscopy represents one of astronomy’s most important observational modes, yet remains largely unexplored in general machine learning literature, which has led the astronomical community to independently develop numerous specialized machine learning applications for spectral analysis.

This interest has been further fueled by the need to analyze vast datasets of low-resolution, often low signal-to-noise ratio spectra from surveys such as LAMOST, DESI, and 4MOST. The prevalence of low-resolution data stems largely from modern spectroscopic surveys being cosmology-driven, where high resolution, while crucial for stellar studies, is less critical for galaxy observations.

The challenge extends beyond traditional spectroscopy to even lower-resolution regimes, including narrow-band filters like the Gaia XP spectra and ground-based surveys such as J-PLUS, S-PLUS, and JPAS. Given the persistent challenges in generating robust ab initio spectral models and the systematic uncertainties in theoretical models at these resolutions, researchers increasingly favor data-driven approaches, using high-resolution spectral observations or other precise measurements (from eclipsing binaries, astrometry, asteroseismology, or high-quality spectra) as training labels for machine learning models.

While it is often argued that machine learning models can effectively transfer and match labels across different surveys, our study reveals potential problems with this assumption. Even for elemental abundances under optimal conditions—APOGEE resolution and SNR—attenuation bias remains non-negligible, compressing the dynamic range of inferred labels away from their original scale. Although this bias depends on the number of features employed and the range of stellar parameters (e.g., effective temperature as a nuisance parameter), and strongly correlated features can help minimize the impact, such mitigation is only possible for atomic species with numerous spectral features. Key elements, including widely studied species like oxygen, have limited atomic features in the optical wavelength ranges, leading to potentially severe attenuation (e.g. fig 13 in, fig 4 in). This effect is frequently observed in label transfer applications, particularly at low resolution, where one-to-one comparisons between training labels and predictions show more prominent skewing for elements with weaker and/or fewer features.

This attenuation bias can have important implications, particularly for fundamental measurements of Galactic structure. For example, spectra contain information about stellar parameters that, when combined with apparent magnitudes, enable estimation of absolute magnitudes and hence distances—a variation of spectroscopic-photometric distance determination. The systematic underestimation of distances due to attenuation bias, which to our knowledge has not been well recognized or treated, can have cascading effects on our understanding of Galactic dynamics and chemical evolution.

When these attenuated distances are used to map the Milky Way’s rotation curve, the radiusat which the orbital velocityis measured is systematically underestimated. Given that the enclosed mass within radiusis determined by, the percentage attenuation in distance measurements directly translates to the same percentage underestimation in the enclosed mass. As we have established in this study, attenuation bias in low-resolution surveys can be significant even under conservative estimates, reaching the percent level. This systematic underestimation of the Milky Way’s mass has far-reaching implications for our understanding of the Galactic rotation curve, dark matter distribution, and overall Galactic dynamics. These findings suggest the need to revisit data-driven spectroscopic-photometric distance determinations in studies of the Milky Way.

SECTION: Mitigation Strategies for Attenuation Bias
Given the significant implications of attenuation bias, it is natural to consider potential mitigation strategies. The most obvious approach involves calibration, either theoretical or empirical, acknowledging that this fundamental bias stems naturally from input uncertainty and is independent of training sample size and label accuracy. Importantly, the presence of this bias does not indicate that our machine learning models are incorrect or inadequately trained.

Yet, theoretical calibration through analytic calculations has its own limitations. While in this study we have shown that bias can be accurately predicted for univariate cases with homogeneous errors when uncertainties are well-measured, real data typically exhibits heteroskedastic (inhomogeneous) errors, making theoretical calculations substantially more complex. Although such calculations might be warranted for high-stakes studies of fundamental one-dimensional relationships, e.g., M-, Kennicutt-Schmidt relations, their full development lies beyond the scope of this paper.

More generally, as demonstrated in our multivariate linear regression analysis, theoretical prediction becomes challenging when features are not completely independent. Even in the linear case with homogeneous errors, the theoretical framework requires impractical assumptions such as perfect correlation between features and stringent constraints on linear coefficients. These theoretical limitations become even more pronounced for nonlinear machine learning methods with unknown causality structures between the variables.

Empirical calibration of attenuation bias presents significant challenges. In the simplest case where both inputand outputhave homogeneous errors, one might assume that the attenuation biasapplies uniformly to all data points and could be calibrated globally using validation data. However, in practice, both inputs and outputs typically exhibit heteroskedastic errors, leading to different attenuation factors for individual data points. Consequently, global calibration, while potentially mitigating the overall bias, likely introduces new systematic effects that are difficult to control.

Simulation-based calibration might seem like an alternative approach, as demonstrated in our spectral analysis. However, the need for data-driven machine learning often arises precisely because physical models contain non-negligible systematics. As we have shown, attenuation bias strongly depends on the statistical properties of correlated features (spectral features in our case), making simulation-based calibration potentially unreliable when these properties are not well understood. Nonetheless, our spectral analysis suggests a possible path forward: for a given set of underlying physical models, the attenuation bias scales predictably with SNR, potentially enabling data-driven empirical calibration based on the empirical SNR dependence of the attenuation bias.

Our study has shown that when features are not independent, their combined effect can help negate random input noise, thereby mitigating attenuation bias. This suggests that when possible, using data in its more raw form with multiple correlated estimators—each subject to independent noise—can be beneficial. For example, full spectral fitting based on individual pixels could help mitigate these issues compared to approaches using limited spectral indices or summary statistics that restrict the number of input features. Furthermore, more restricted models like linear regression might offer advantages in that their bias can be better predicted or calibrated, and they suffer less from attenuation compared to super-linear regression.

More broadly, while our study focuses primarily on error-ignorant discriminative models—which unfortunately remain common in astronomical applications, especially with modern architectures like neural networks—the community has developed several sophisticated statistical approaches to address measurement uncertainties. Errors-in-variables models explicitly incorporate measurement error variances in the input to provide unbiased parameter estimates. Bayesian hierarchical models offer another powerful framework, particularly well-suited to handling complex error structures and incorporating prior information. These approaches can account for measurement uncertainties at multiple levels, from individual observations to population-level parameters, providing a more robust foundation for astronomical inference.

SECTION: Discriminative Models versus Generative Models
Our analysis highlights an asymmetry in how measurement errors affect regression models: uncertainties in dependent variables (outputs) do not cause attenuation bias, while uncertainties in independent variables (inputs) systematically bias the results. This insight has important implications for choosing between discriminative and generative approaches in astronomical data analysis.

Discriminative models, which directly map from observables to physical parameters (e.g., from spectra to stellar properties), treat the noisy measurements as input features. As we have demonstrated through our theoretical framework, measurement uncertainties in these inputs inevitably lead to attenuation bias in an error-ignorant framework, regardless of training sample size or label quality.

In contrast, generative models first learn to predict observables from physical parameters, then use optimization methods (e.g.,minimization) or sampling approaches (e.g., MCMC) to infer parameters from data. This approach naturally accommodates measurement uncertainties in two ways. First, during the forward modeling phase, measurement uncertainties appear in the dependent variables where they only affect variance of the model estimate, not bias. Second, during the parameter inference phase, measurement uncertainties can be explicitly incorporated into the likelihood function or objective function.

This theoretical advantage becomes particularly relevant in scenarios where training labels have higher precision than the observed data. Consider, for example, the common task of analyzing low-resolution survey spectra using training labels derived from high-resolution observations. Discriminative models must contend with significant measurement uncertainties in their input features (the low-resolution spectra), leading to inevitable attenuation bias. Generative models, by learning the forward mapping from parameters to spectra, can better preserve the precision of the training labels.

SECTION: Conclusion
A persistent challenge in astronomical machine learning has been systematic bias where predictions compress the dynamic range of true values—the highest true values are consistently predicted too low while the lowest values are predicted too high. This bias is particularly relevant in modern astronomy, where data-driven models increasingly attempt to extract information from low-quality data by leveraging high-quality training labels from other companion observations. While conventional wisdom has attributed this bias to factors such as inaccurate labels, insufficient training data, or non-uniform sampling of training data, the theoretical analysis reveals a more fundamental origin, and surprisingly demonstrates that all of these commonly cited factors are irrelevant to solving this issue.

Through examination of linear regression with noisy data, we demonstrate that this systematic bias naturally emerges from attenuation bias (or regression dilution)—a direct consequence of how measurement uncertainties in input variables spread the observed distribution and fundamentally lead to underestimation of regression coefficients. This bias persists regardless of training sample size, label accuracy, or parameter distribution.

For univariate or independent multivariate cases, we prove that the attenuation factorfollows a simple relationship. This relationship reveals that even with signal-variance-to-noise ratios of 10 (), predictions suffer from 1% bias, with the effect becoming severe at lower ratio. Since this range of signal-variance-to-noise of 10 and lower is common in astronomical observations but rare in typical machine learning applications, this effect has been largely overlooked in the broader astronomy literature despite its significant impact on astronomical inference.

Our further investigation of multivariate linear regression yields several insights: for independent features, the bias depends solely on the ratio of measurement uncertainty to signal variance, independent of sample size or dimensionality. Adding more independent features cannot overcome this fundamental limitation. However, we demonstrate that correlated features can help mitigate this bias. The degree of mitigation depends on both the number of correlated features and their correlation strength, which is particularly relevant for spectroscopic analysis where features are naturally correlated through underlying physics. In the theoretical limit of infinite perfectly correlated features, the attenuation bias can be completely resolved, though this limit is rarely achievable in practice.

Using high-resolution stellar spectroscopy as a case study, we show that even in optimal conditions—APOGEE resolution () with high SNR—attenuation bias remains detectable. The effect becomes substantially more severe at lower resolutions or SNR values typical of surveys like LAMOST and DESI. This has particular implications for abundance measurements of elements with limited spectral features, where the protective effect of correlated features is reduced. Furthermore, we demonstrate that variations in other stellar parameters (like effective temperature) can weaken feature correlations, diminishing their ability to mitigate attenuation bias.

The attenuation biases of 1-10% we have identified can have profound implications for fundamental astronomical relationships, particularly when propagated through subsequent analyses. The naive application of machine learning models that ignore input uncertainties can introduce systematic biases in key parameter estimations. These biases could significantly impact our understanding of fundamental astronomical relationships across multiple scales: from galactic-scale relations like the Kennicutt-Schmidt law and the M-relation between black hole and galaxy properties, to more specific spectroscopic analyses such as age-metallicity correlations and the Milky Way’s rotation curve.

This attenuation bias stems from the fundamental asymmetry in how measurement errors affect regression models: uncertainties in dependent variables (outputs) only increase variance, while uncertainties in independent variables (inputs) directly contribute to attenuation bias. This asymmetry has important implications for model choice in astronomical applications. Generative models that predict observables (such as spectra) from physical parameters, followed by parameter inference through optimization or sampling, may be practically more robust than discriminative approaches that attempt to infer parameters directly from noisy observations. This advantage arises because in generative models, the noisier measurements appear in the dependent variables where they only affect variance, while in discriminative approaches, these same noisy measurements serve as independent variables where they inevitably introduce attenuation bias.

While our analytical treatment focuses on linear regression and spectral analysis, the insights about attenuation bias naturally extend to more complex nonlinear models as well as all applications of machine learning in the astronomical context. As astronomy continues to embrace larger surveys with varying data quality, understanding and accounting for attenuation bias becomes increasingly crucial for reaching more robust and accurate inference, ensuring that upstream label determination does not inadvertently propagate systematic errors to downstream scientific conclusions.

I am grateful to my colleagues at OSU whose lively discussions during our daily astro-coffee sessions have inspired some of this work. Special thanks to David Weinberg for his careful and invaluable feedback as this paper took shape. The proximity of our offices provided both intellectual stimulation and emotional urgency to complete this work. Some ideas in this research emerged from my ritual of bouncing thoughts off o1-preview and claude-3.5-sonnet - while their initial suggestions often led me down fascinating (and largely unproductive) rabbit holes, our back-and-forth exchanges proved invaluable. Alongside my human colleagues at OSU, I’d like to thank o1-preview and claude for their unwavering intellectual companionship during the course of this research.

SECTION: References
SECTION: Attenuation Bias Under Generalized Least Squares (GLS)
In the main text, we established that measurement errors in the independent variable () lead to systematic underestimation of regression coefficients, known as attenuation bias, when using ordinary least squares (OLS) regression. A natural question arises: can employing Generalized Least Squares (GLS) mitigate this bias? GLS can provide more efficient estimates than OLS by accounting for known heteroskedasticity or correlation structures in the residuals of the dependent variable. However, as we will demonstrate, the fundamental attenuation bias due to errors inremains unaltered by the switch from OLS to GLS.

Consider the univariate linear model:

with observed values given by:

where,,, and,are independent of. The variance of the measurement errors inmay vary with, so we defineas the GLS weight.

The GLS estimator for the slopetakes the form:

Substitutingand:

Taking the expectation and using, along with the independence assumptions, we obtain:

Since the weightsare just multiplicative factors, they cancel out in the proportional sense:

This result is exactly the same attenuation factor derived for OLS. Thus, even if the error model inis accounted for by GLS, the fundamental bias from measurement errors inpersists. The weighting of observations bydoes not alter the ratio that characterizes attenuation: it only affects the relative contribution of each data point, not the underlying relationship betweenand.

To demonstrate, we repeat the simulations described in the main text (see Sectionand Figure), but now using GLS instead of OLS. The simulation setup remains largely the same, with a two-dimensional parameter space defined by the ratio of signal range to measurement error () and the measurement uncertainty in the dependent variable (). The key difference lies in the regression method: we now employ GLS, with weights set toto account for the known variances in.

The GLS estimates for the coefficientsare obtained using the normal equation:

whereis the design matrix (observedvalues with a column of ones for the intercept) andis the diagonal weight matrix. We then compute the predictedvalues using these estimated coefficients and perform a simple OLS regression of the predicted values against the true values to obtain the attenuation factor(the slope of this regression).

The GLS-based simulation results, shown in Figure, reinforce the conclusions drawn from the OLS case. Despite GLS accounting for known variances inthrough appropriate weighting, the results closely resemble those obtained using OLS. The attenuation factorexhibits the same dependence on, with no discernible influence from. This confirms that GLS does not mitigate the fundamental attenuation bias introduced by measurement errors in the independent variable.

SECTION: Variance of Linear Coefficient Estimator
While the attenuation bias in the linear coefficient estimator is independent of both the training set sizeand the uncertainty in the dependent variable, the variance of the estimator depends on both quantities. Here we derive this dependence explicitly for centered variables (i.e.,).

To derive the variance ofin the univariate case, let’s begin with its fundamental expression and systematically show how it relates to the residuals. Starting with the OLS estimator:

We can substituteinto this expression:

Distributinggives us:

Now, to find, we note thatis constant, so:

To evaluate this variance, treatingas our constant (conditional on) and applying this property element-wise:

For large samples, we can use the fact thatapproaches. Therefore:

This result reveals several important insights about the precision of our coefficient estimates. First, unlike the attenuation bias discussed in the main text, the variance scales inversely with sample size, indicating that larger training sets do improve estimation precision even if they cannot address the fundamental bias. Second, measurement uncertainties in bothandcontribute to the variance through the numerator term, showing how both sources of error affect our estimation uncertainty.

Having derived the variance of the linear coefficient estimator in the univariate case, we now extend this analysis to the multivariate setting withfeatures. We maintain our previous assumptions of centered variables (,) and homogeneous uncertainties (for all features), while adding the requirement that features are independent and identically distributed with variance.

In the multivariate setting, the ordinary least squares estimator for each coefficient takes the form:

To analyze this estimator, we first substitutein the appropriate terms:

To compute, we condition on the observed features, treatingas constant, similar to our approach in the univariate case:

The independence of,, andallows us to decompose the variance term:

The crucial step comes in evaluatingconditional on:

Substituting back and simplifying:

For large samples,approachesfor each feature, yielding our final result:

This multivariate generalization reveals how the variance of each coefficient estimator depends not only on its own parameter value but also on all other coefficients in the model. The denominator retains the same form as in the univariate case, showing that the fundamental scaling with sample size and feature variance remains unchanged. However, the numerator now includes contributions from all features’ measurement uncertainties, weighted by their respective coefficients, reflecting how noise in any feature affects the precision of all coefficient estimates. As the dimensionincreases, the variance of individual estimators grows through the summation term. This behavior is not surprising: given the same constraints from the training data, the determination of the hyperplane becomes more stochastic at higher dimensions, reflecting the fundamental challenge of maintaining precision in high-dimensional parameter estimation.

SECTION: Derivation of the attenuation bias for correlated multivariate features
We have shown in Sectionthat in the case of independent features, the number of dimensions in the input would not mitigate the attenuation bias, and the attenuation bias calculation follows the case of the univariate scenario. However, when the features are correlated, the random noise would need to conspire to create the attenuation bias, making it possible to reduce the attenuation bias as the dimensionality increases.

In general, deriving an analytic theory for multivariate linear regression with correlated features is challenging. For simplicity, the results we have laid out in Sectionassume perfect correlation between all the features. In this case, without loss of generality, we can assume, whereis a scaling factor for the-th feature, withranging from 1 to, andis a scalar random variable representing the underlying true signal.

Recall that, from Equation, we have:

We will calculate the individual terms in this expression. First, we compute. Under the assumption of perfect correlation,can be expressed as:

whereis the random variable representing the underlying intrinsic input data, andis a fixedrow vector of scaling factors.

Now, we can calculate:

Let, which represents the evaluation of the expectation oversamples of the training data. Thus, the matrixcan be estimated as:

Substituting these expressions into Equation (), we have:

Let, which represents the relative magnitude of the measurement error variance to the signal variance scaled by the sample size. To compute, note thatis a rank-1 matrix ifis a non-zero vector. Letbe thevector of scaling factors. Then:

Therefore, the matrix we need to invert is:

We can use the Sherman-Morrison formula to find the inverse of:

Substituting back, we have:

Note thatis a scalar, so. Similarly,. Compute the numerator in the second term:

Therefore, the expression simplifies to:

Simplify the term in parentheses:

Therefore:

We have the expected attenuation factor:

However, in this form, we still cannot writeas a factor of the true. In general, this is not possible. However, if we further assume that, whereis a scalar coefficient, then, and the expected value ofbecomes:

Thus, each element ofis:

and we arrive at the analytic expression for the attenuation factor:

This is the expression we discussed in Section. We derived this analytic formula under strong assumptions about the scalability and alignment betweenand. While such explicit expressions may not exist for general multivariate linear regression, the goal of this calculation is to demonstrate a key principle: as the number of non-trivial and correlated features increases, the attenuation bias decreases, with the attenuation factor approaching unity.

SECTION: Empirical Validation of Independent Feature Predictions with Varying Feature Strengths
In Section, we demonstrated that for independent features with equal strengths, attenuation bias persists regardless of the number of features used. For features with varying sensitivities, the theoretical framework predicts that the effective attenuation factor represents a weighted average of individual feature attenuations, with the final predictions being pulled toward the mean of the parameter range. Here, we use synthetic APOGEE spectra to validate these theoretical predictions.

In spectroscopy, features are naturally correlated through underlying physics—when varying a single parameter like, all spectral features should respond in a physically consistent way. To test the theoretical predictions about independent features, we need to artificially break these physical correlations. We begin by generating 1,000 synthetic spectra using The Payne spectral emulator, focusing on the same20Å wavelength range around 16200Å as in our main analysis, withuniformly distributed fromto. For each pixel, we derive an empirical linear relationship between flux andfrom these synthetic spectra. As validated in Figure, these linear relationships adequately describe the pixel-wise dependence of flux on metallicity.

Using these empirical relationships, we create a pool of 10,000 samples by independently sampling flux values at each pixel, maintaining the observed flux distributions but breaking the natural correlations between pixels. For each sample, we estimateby inverting these linear relationships pixel-by-pixel, then take the mean across all pixels as the finallabel. This approach, by treating pixels independently, allows differentvalues to contribute to the final label from each pixel. Because we treat all pixels as contributing independently to, the final label average close to the mean of the input range ().

Using multivariate linear regression, we analyze the relationship between the label() and the normalized flux values () across wavelength bins, with each pixelhaving a corresponding coefficient. For each simulation configuration, we train the model using 1,000 mock spectra with knownvalues and validate on another 1,000 independent mock spectra.

Figuredemonstrates how the attenuation factor varies with measurement uncertainty and number of input pixels. The gray shaded regions show the theoretical predictions for attenuation factors expected across all 179 pixels, with darker and lighter bands indicating 1(16th-84th percentile) and 2(2nd-98th percentile) ranges respectively. This spread arises because different pixels show varying sensitivities to(different), leading to different attenuation factors at fixed.

To validate these predictions, we perform 100 independent trials for each configuration of pixel number () and measurement uncertainty (). The colored points show the mean attenuation factor () from these trials, with error bars indicating the standard deviation. These results empirically validate the theoretical predictions about independent features with varying strengths. When features contribute independently and have different sensitivities (), they experience different degrees of attenuation (), and their weighted average pulls the final prediction toward the mean.

Also as predicted by the theoretical framework for independent features, the mean attenuation shows no significant dependence on the number of pixels used—demonstrating that including more independent features cannot mitigate the bias. The variance does decrease as more pixels are included, reflected in smaller error bars for larger, because with fewer pixels, each random selection might include predominantly strong or weak features, leading to more variable attenuation.

The top panels show predicted versus truefor the 100-pixel case at different noise levels, illustrating increasing attenuation as measurement uncertainty grows. Notably, even at relatively high SNR = 100 (), we observe measurable attenuation.

We note that this represents a pessimistic limit of independent features. In real physical systems, where features are naturally correlated, increasing the number of features can help mitigate the attenuation bias—a point we explored in detail in the main text.