SECTION: Optimizing Multispectral Object Detection: A Bag of Tricks and Comprehensive Benchmarks

Multispectral object detection, utilizing RGB and TIR (thermal infrared) modalities, is widely recognized as a challenging task. It requires not only the effective extraction of features from both modalities and robust fusion strategies, but also the ability to address issues such as spectral discrepancies, spatial misalignment, and environmental dependencies between RGB and TIR images. These challenges significantly hinder the generalization of multispectral detection systems across diverse scenarios. Although numerous studies have attempted to overcome these limitations, it remains difficult to clearly distinguish the performance gains of multispectral detection systems from the impact of these “optimization techniques”. Worse still, despite the rapid emergence of high-performing single-modality detection models, there is still a lack of specialized training techniques that can effectively adapt these models for multispectral detection tasks. The absence of a standardized benchmark with fair and consistent experimental setups also poses a significant barrier to evaluating the effectiveness of new approaches. To this end, we propose the first fair and reproducible benchmark specifically designed to evaluate the training “techniques”, which systematically classifies existing multispectral object detection methods, investigates their sensitivity to hyper-parameters, and standardizes the core configurations. A comprehensive evaluation is conducted across multiple representative multispectral object detection datasets, utilizing various backbone networks and detection frameworks. Additionally, we introduce an efficient and easily deployable multispectral object detection framework that can seamlessly optimize high-performing single-modality models into dual-modality models, integrating our advanced training techniques. Our codes are available:https://github.com/cpboost/double-co-detr

SECTION: 1Introduction

Multispectral object detection is a powerful technology that leverages both visible light and infrared spectra for object detection, and it has been widely adopted in various real-world applications[1,2,3,4,5,6], including anomaly detection in surveillance systems[7,8,9,10,11], obstacle recognition in autonomous vehicles[4,12,13,14,15], defect identification in industrial inspection[5,6,16,17,18], and threat detection in defense and security[19,20,21], to name just few. While many traditional object detection algorithms[5,6,17,22,19]have primarily relied on information from a single modality, recent advancements have explored more sophisticated multispectral architectures[23,24,25,26,27,28,29,30]. In numerous cases, fully exploiting the information from multiple-modalities has demonstrated significant advantages[28]. For instance, in low-light conditions, leveraging infrared spectra can enhance the performance of visible light detection, or in complex scenarios, combining information from both spectra can improve detection accuracy[31,32,33,34]. Recently, with the rapid development of satellite remote sensing and thermal imaging technologies[17], many challenging detection datasets have emerged (such as low light and extreme weather conditions)[7,17]. Multispectral detection architectures have demonstrated strong performance on these datasets[6,17,22].

However, training multispectral object detection models is known to be highly challenging[23,28,29,35,36,37]. Beyond the common issues encountered in training deep architectures, such as vanishing gradients and overfitting[22,28], multispectral models face severaluniquechallenges that limit their strides on these datasets:

The first challenge lies ineffectively utilizing dual-modality data. Simultaneously processing visible and infrared data increases the complexity of dual-modality feature fusion, which may result in suboptimal integration of information from both modalities[23,36]. This issue is particularly pronounced in earlier multispectral models, where the fusion process often led to information loss, preventing the models from fully leveraging the strengths of both modalities[35,36]. Additionally, registration discrepancies between the two modalities and the lack of modality-specific enhancement strategies further constrain model performance[37].

The second major question is thelack of an effective optimization strategyfor converting high-performance single-modality models into dual-modality models. Despite the emergence of numerous powerful single-modality object detection frameworks in recent years[38,39,40,41], there has yet to be a robust method for effectively harnessing the potential of these models while addressing the unique challenges of multispectral object detection.

To addess the aforementioned challenges, the promising approaches can be categorized into
➀ dual-modality architectural fusion[26,27,28]and ➁ modality-specific enhancements[31,32,33], both of which we classify as “training techniques”. The former involves adapting single-modality architectures to dual-modality structures, integrating advanced backbone networks, and employing diverse feature fusion strategies. The latter focuses on processing data from both modalities using techniques such as modality-specific data augmentation and alignment calibration[6]. While these techniques generally contribute to the effective training of multispectral object detection models, their benefits are not always significant or consistent[35,36,37]. Furthermore, it is often difficult to distinguish the performance improvements achieved through more complex dual-modality architectures from those gained via these “training techniques”.

In some extreme cases, contrary to initial expectations, single-modality models enhanced with certain optimization techniques may even outperform carefully designed, complex dual-modality architectures[27,28,29,30]. This casts doubt on the pursuit of increased complexity, thereby rendering it a less attractive approach.These observations highlight a critical gap in the study of multispectral object detection: the lack of a standardized benchmark that can fairly and consistently evaluate the effectiveness of training techniques for dual-modality models.Without disentangling the effects of architectural complexity from the “training techniques” applied, it may remain unclear whether multispectral object detection should inherently perform better under otherwise identical conditions.

SECTION: Our Contribution

To establish such a fair benchmark, our first step was to conduct a comprehensive investigation into the design philosophies and implementation details of dozens of popular multispectral object detection techniques, including various backbone networks, dual-modality fusion strategies, and alignment techniques. Unfortunately, we discovered that even on the same datasets, the implementation of hyperparameter configurations (such as hidden layer dimensions, learning rates, weight decay, dropout rates, number of training epochs, and early stopping patience) is highly inconsistent and often varies depending on specific circumstances. This inconsistency makes it challenging to draw any fair or reliable conclusions.

To this end, we conducted a detailed analysis of these sensitive hyperparameters and standardized them into a “best” hyperparameter set, consistently applied across all experiments. This standardization provides a fair and reproducible benchmark for training multispectral object detection models. Subsequently, we explored various combinations of training techniques across several classical multispectral object detection datasets, leveraging common single-modality model backbones and optimizing them for dual-modality detection tasks.

The results of our comprehensive study were highly significant. Based on the characteristics of different single-modality model backbones, framework features, and detection sample characteristics, we developed several effective training techniques and optimization strategies, enabling us to achieve state-of-the-art results on multiple representative datasets. Furthermore, we proposed several optimization strategies with strong transferability, demonstrating excellent performance across multiple dual-modality public datasets111Our research was awarded the championship in the Global Artificial Intelligence Innovation Competition (GAIIC)https://gaiic.caai.cn/ai2024, out of over 1,200 participants, 1,000+ teams, and 8,200+ submissions..

Specifically, our contributions are as follows:

➥Multimodal Feature Fusion:we introduce advanced multimodal feature fusion techniques to effectively integrate visible and infrared data, enhancing the feature representation capabilities of multispectral object detection models, especially in complex environments.

➥Dual-Modality Data Augmentation:we employ modality-specific data augmentation strategies that cater to the distinct characteristics of visible and infrared data, improving the model’s robustness in varying environmental and complex scenarios.

➥Alignment Optimization:by implementing precise alignment techniques, we improve spatial consistency between visible and infrared data, reducing inter-modality misalignment and significantly enhancing performance in low-light object detection and multimodal information fusion.

➥Optimizing Single-Modality Models for Dual-Modality Tasks:we provide a new benchmark and training techniques to effectively adapt high-performing single-modality models into dual-modality detection models. Through these optimizations, single-modality models outperform even complex, large-scale dual-modality detection models, offering strong support for their migration to dual-modality tasks.

SECTION: 2Related work

SECTION: 2.1Multispectral Object Detection & Training Challenges

Multispectral object detection has achieved state-of-the-art performance in applications like autonomous driving and drone-based remote sensing[5,6,7,8,9,10]. However, implementing multispectral detection is challenging, especially when dealing with images from distinct spectra, such as visible light (RGB) and thermal infrared (TIR)[11,16,17]. Existing methods[42,43,44,45]face several issues, including spectral differences[46], spatial misalignment[47], and high sensitivity to environmental conditions[48], limiting their generalization across diverse scenarios. While recent studies have introduced various training techniques, they often struggle to deliver consistent performance improvements when applied to complex remote sensing data[49,50], differing from the dual-modality detection benchmarks discussed in this paper.

To address these challenges, techniques such as multimodal feature fusion, registration alignment, and dual-modality data augmentation have been developed in recent years[24,25,26,27,28]. The following sections provide a detailed exploration of these techniques and their applications.

SECTION: 2.2Multimodal Feature Fusion

In multispectral object detection, feature fusion plays a crucial role in enhancing model performance. Current fusion methods are generally categorized into three types: pixel-level, feature-level, and decision-level fusion. Pixel-level fusion[51,52,53]integrates RGB and TIR images at the input stage, allowing early information combination but potentially introducing noise or misalignment due to differences in resolution and viewpoints. Feature-level fusion[54,55,56,57,58]combines high-level features from both modalities at intermediate layers, utilizing techniques like concatenation, weighting, or attention mechanisms to better capture complementary information, though it may add computational overhead[59,60]. Decision-level fusion[50,61,62,63,64,65]merges independent detection results from each modality at the final stage, providing efficiency and stable performance, especially when the modalities offer relatively independent information.

SECTION: 2.3Dual-Modality Data Augmentation

In multispectral object detection, data augmentation is crucial for improving model generalization and reducing overfitting[27,28]. While traditional techniques like flipping, rotation, and scaling work well in single-modality detection[25,26], the fusion of RGB and TIR images introduces higher complexity. A common approach is to apply synchronized augmentation to both RGB and TIR images[3,6]to ensure consistency between the modalities. Techniques such as random cropping, scaling, and color transformations increase image diversity and help the model adapt to varying environmental conditions[66,67]. Additionally, some studies propose joint data augmentation methods, such as mixed modal augmentation, which exchanges pixels or features between modalities to enhance robustness against modality differences[4,5,68,69], ultimately improving detection performance in challenging scenarios.

SECTION: 2.4Registration Alignment

Registration alignment techniques are employed to address spatial discrepancies between images from different sensors, such as RGB and TIR. Differences in resolution and viewpoints often lead to misalignment and distortion, which can negatively impact feature fusion and detection performance[70,71]. Traditional alignment methods[72,73,74], such as scaling, rotation, and affine transformation, are used to align the images but tend to be limited in complex scenes or when nonlinear deformations are present. Recently, deep learning-based alignment techniques[64,75,76,77,78,79]have emerged, achieving pixel-level precision by learning feature mappings between RGB and TIR images, and using contrastive loss or self-supervised learning to ensure spatial consistency. Some methods also incorporate attention mechanisms to dynamically adjust feature alignment[47,70,74,80], enhancing both local detail and global consistency.

SECTION: 3Method

In this section, we systematically discuss how to improve existing dual-modality object detection algorithms. The discussion focuses on three key aspects:multimodal feature fusion, dual-modality data augmentation, and registration alignment.Specifically, Section 3.1 details the hyperparameter configurations and datasets used in our experiments, while Sections 3.2, 3.3, and 3.4 discuss multimodal feature fusion, dual-modality data augmentation, and registration alignment, respectively.

SECTION: 3.1Standardized Experimental Configuration

We conducted a comprehensive analysis of previous single-modality object detection models applied to dual-modality detection tasks. To further enhance model performance and improve the robustness of our benchmarking, we also performed hyperparameter optimization and fine-tuning on these models. Based on dual-modality datasets (including both RGB and TIR data), we systematically explored the adaptability of single-modality models in integrating multimodal information, with particular emphasis on their performance across different modalities. The key hyperparameter configurations are presented in TableI.

Through a grid search approach, we optimized the hyperparameters for all methods and identified the most generalizable and effective configuration. This configuration was selected based on the best performance of various single-modality detection models across multiple datasets, focusing on parameters such as learning rate, weight decay, and dropout. Ultimately, we proposed this “optimal hyperparameter configuration” and strictly adhered to it in our experiments.

Specifically, the final configuration consists of a learning rate of 0.01 with decay, a weight decay of 0.0001, and a dropout rate of 0.5.We believe that this setup provides stable and efficient performance across a range of multispectral object detection tasks, ensuring fair comparisons between different methods under the same conditions.

In each experiment, we trained for up to 200 epochs, with early stopping set to a patience of 20 epochs. To minimize the impact of random variations, each experiment was repeated 20 times, and the results were averaged to obtain the final performance metrics.

Our subsequent experiments utilized the KAIST[89], FLIR, and DroneVehicle[90]datasets. The KAIST dataset is a benchmark for pedestrian detection, combining visible and infrared images to evaluate multispectral detection algorithms. The FLIR dataset includes multi-class vehicle and pedestrian detection tasks with high-resolution thermal imagery. The DroneVehicle dataset focuses on multi-class object detection from a drone’s perspective, covering various complex scenarios.

Regarding performance evaluation, we employed task-specific metrics tailored to the characteristics of each dataset. For the KAIST dataset, we selected Miss Rate as the primary evaluation metric due to its sensitivity to missed detections, which is critical in this context. In contrast, for the FLIR and DroneVehicle datasets, we used mean Average Precision (mAP) as the evaluation metric, as these datasets involve multi-class object detection, and mAP provides a more comprehensive assessment of detection accuracy across classes. This dataset-specific approach ensures a thorough and accurate evaluation of each method’s performance in diverse tasks.

By leveraging a unified hyperparameter configuration and dataset-specific evaluation metrics, we ensure fair and consistent comparisons between different methods, providing a robust foundation for subsequent performance improvements.

SECTION: 3.2Multimodal Feature Fusion

Formulations.In multispectral object detection, multimodal feature fusion techniques aim to effectively integrate complementary information from both RGB and TIR images, enhancing detection accuracy and model robustness.Multimodal feature fusion can be categorized into three main approaches: pixel-level fusion, feature-level fusion, and decision-level fusion.

Pixel-Level Fusion.The fusion of RGB and TIR images at the pixel level involves a series of tensor-based transformations, incorporating both adaptive weighting and convolutional refinement to effectively integrate the complementary modalities. Letdenote the RGB image anddenote the TIR image. Initially, the TIR image is expanded to a three-channel format:

wheredenotes a tensor of shape, employed to replicate the TIR image along the channel dimension via the tensor outer product operation. The summation termindicates that this operation is applied independently to each channel, expanding the single-channel TIR imageinto a three-channel representationconsistent with the structure of RGB images.

Next, adaptive pixel-wise weighting matricesandare introduced for the RGB and TIR channels. The intermediate fused imageis defined as:

whererepresents a noise model parameterized by a Gaussian random vector, accounting for the inherent uncertainty in sensor measurements.

To refine the fusion process and incorporate spatial context, convolutional transformations are applied using modality-specific kernelsand. The convolutional outputs are expressed as:

wheredenotes the convolution operation, andandare the learnable bias terms.

The final fused imageis obtained through a non-linear fusion strategy that incorporates spatially adaptive weight mappings. We introduce non-linear mapping functionsand. Theis expressed as:

The functionsandcan be simplified as follows:

where,denotes the sigmoid activation function,represents the hyperbolic tangent function, andis a non-linear spatial filtering operation. The termis a learnable scaling tensor.

The proposed pixel-level fusion scheme integrates adaptive weighting, convolutional refinement, and a multi-layered non-linear transformation pipeline to enhance representation capacity. The noise modeling termimproves robustness, while the activation functionsandfacilitate non-linear interactions between the RGB and TIR modalities.

Feature-Level Fusion.The mainstream feature-level fusion methods primarily include convolution-based Network-in-Network (NIN) modules and bidirectional attention-based Iterative Cross-modal Feature Enhancement (ICFE) modules[91]. The following sections provide an in-depth introduction to each approach.

NIN Module.
To achieve independent localized non-linear transformations on the RGB and TIR modalities, we first designed a network-in-network module integrated with a residual structure. This module serves as a foundational step for subsequent cross-modal feature enhancement and interaction, where we leverage 1x1 convolutions to apply fine-grained, spatially localized non-linear mappings that improve the expressiveness of feature representations. Letanddenote the RGB and TIR modality feature maps at layer, respectively. We define learnable 1x1 convolution kernelsand, applying them with residual connections to each modality for localized feature transformations, as follows:

The residual connection within this transformation preserves original modality-specific information in the transformed feature, mitigating potential information loss or distortion. To achieve adaptive fusion of RGB and TIR modalities, we introduce dynamic weighting coefficientsand, computed through a transformationfollowed by a shared non-linear functionapplied to each transformed feature map:

The final fused featureat layeris given as follows:

Through these operations, the NIN module not only performs modality-specific, localized feature transformations but also enables adaptive and balanced feature fusion. This module strengthens the feature discriminability and robustness, while preserving localized information via non-linear activation and residual connections.

ICFE Module.
The ICFE module progressively enhances feature representations of RGB and TIR modalities by iteratively exchanging and refining complementary information, ultimately producing a single fused feature representation. Letandrepresent the initial RGB and TIR features, respectively, and let the final fused feature representation afteriterations be denoted as. The following outlines the detailed formulae of this process.

At the k-th iteration, multi-head queries, keys, and values are generated for both the RGB and TIR modalities. Suppose there are H attention heads, indexed by h. For the h-th attention head, we compute the query matrixfor RGB features, and the key matrixand value matrixfor TIR features:

whereare learnable projection matrices, andrepresents the dimensionality per attention head.

To obtain the cross-modally enhanced RGB features, we calculate the weighted matrix by applying the softmax function to the scaled dot product of the query and key matrices, then multiply it with the value matrix:

Then, we concatenate the features from all attention heads (denoted byas the concatenation operation) and project them back to the original feature space using an output projection matrix:

whererepresents the concatenation operation applied across all attention heads.

In each iteration, the RGB and TIR features are combined to produce an intermediate fused feature representation, with learnable weighting coefficientsandcontrolling the fusion:

whereis the cross-modally enhanced TIR feature obtained symmetrically to.

To further enhance non-linear representation capabilities, a non-linear activation functionis applied with residual connection to the fused feature in each iteration. Afteriterations, the final fused feature representation is given by:

Decision-Level Fusion.In decision-level fusion, RGB and TIR modalities undergo separate feature extraction and preliminary detection, and their fusion occurs at the final decision stage. Let the detection results for RGB and TIR modalities be denoted asand, respectively. The following describes two advanced fusion strategies for combining these decisions.

Confidence-Based Weighting with Normalization.To refine the fusion process, confidence scoresandreflect each modality’s reliability and serve as normalization factors. These scores are obtained through a scaling functionand normalized using:

The confidence-weighted fusion resultis:

whererepresents element-wise weighting, andis a small constant to prevent division by zero, thereby stabilizing the computation.

Hierarchical Fusion with Multi-stage Process.Hierarchical fusion enhances robustness by applying both local and global fusion steps. Initially, a region-based fusion is applied independently within each modality. This local fusion step can be represented as:

whererepresents the local fusion function, such as Simple Average, Confidence-Weighted Average, or Maximum Selection, andandare weighting factors specific to each modality.

After obtaining the locally fused results, a global aggregation function combines these results across regions or categories. The global fusion step is given by:

wheredenotes the global fusion function,is the number of local regions or categories, andare adaptive coefficients for each local fused region.

This hierarchical approach provides finer control over region-specific interactions, enhancing robustness in complex scenes.

Experimental Observations

We first evaluate the three fusion methods through experiments and identify feature-level fusion as the most effective approach. Building on this insight, we further optimize the combination of feature-level fusion modules to achieve the best performance.

Fusion Method Experiments.In our preliminary experiments, we compared the effects of the three feature fusion methods on the improved multispectral model. The experimental results can be found in TableII. It is evident that using different fusion methods had a significant impact on the detection accuracy of the optimized model.

Observations on Pixel-Level Fusion.Pixel-level fusion exhibits lower stability and detection accuracy compared to single-modality detection on most datasets, with only slight improvements observed in a few specific cases. This may be attributed to the fact that pixel-level fusion combines the dual-light images at the input stage, introducing a significant amount of redundant information and noise. As a result, the model struggles to effectively learn the key features from each modality.

Observations on Feature-Level Fusion.Compared to single-modality detection, feature-level fusion demonstrated significant improvements in both stability and detection accuracy across most datasets. This is likely due to the fact that feature-level fusion effectively utilizes high-level features extracted by the backbone, allowing for efficient fusion while minimizing redundant features and preserving as much valuable information as possible.

Observations on Decision-Level Fusion.Compared to single-modality detection, decision-level fusion can improve accuracy to some extent, but it demonstrates instability with certain methods, such as the RTMDet framework[93]. This instability may stem from the fact that decision-level fusion processes RGB and TIR modality information independently, merging them only at the decision stage. Consequently, this approach struggles to effectively leverage complementary information between the two modalities, especially in scenarios where such information is crucial, like varying weather conditions or significant changes in viewpoints.

Feature-Fusion Experiments.To determine the most effective fusion strategy, we selected the best-performing feature-level fusion method from prior experiments for further analysis. Using single-modality detection models as baselines, we introduced the NIN and ICFE modules under different input modalities. This approach enabled a systematic evaluation of their contributions to feature representation and fusion performance. Key results are shown in Figure1, along with notable findings.

Observations on Datasets.After applying fusion modules, all detection frameworks showed varying degrees of improvements. Notably, on datasets with significant changes in lighting conditions, shadows, and viewpoints (e.g., the FLIR dataset), both the NIN-structured fusion module and the ICFE-structured fusion module exhibited more pronounced performance. This enhancement is likely attributable to the fact that in scenarios where there are substantial differences between the two modalities, complementary information plays a crucial role in improving detection accuracy, which highlights the effectiveness of the fusion modules.

Observations on Fusion Modules.We found that different fusion module architectures exhibit high sensitivity to various backbone networks. Specifically, in detection networks using Resnet50 as the backbone, the NIN-structured fusion module showed notable improvements in detection accuracy. On the other hand, for backbones based on the Vit-L structure, the ICFE module demonstrated better performance when fusing data from the RGB and TIR channels. This difference in performance may be attributed to the fact that Resnet50 is a convolution-based architecture, where the NIN module effectively fuses local features, maintaining the continuity and consistency of convolutional features, thus leading to better results. In contrast, Vit-L excels at capturing global features, and the ICFE module, with its cross-feature and attention mechanisms, further enhances the fusion of global information, resulting in superior performance.

Observations on the ICFE Fusion Module Branches.For the branch inputs of the ICFE module, we experimented with various connection methods, as illustrated in Figure1. The experimental results show that using the ICFE module alone for fusion, regardless of the connection method, failed to consistently improve the detection accuracy. This outcome may be attributed to the fact that when only a single module is used for fusion with inputs from the same modality, the ICFE module may repeatedly amplify background noise or irrelevant features, causing the model to focus excessively on the noise rather than the target, thereby reducing detection performance. Furthermore, when inputs from different modalities (RGB and TIR) are used, their features are not deeply fused or integrated (e.g., through NIN’s nonlinear transformation), meaning the complementary information between modalities is not fully leveraged.

We further attempted to add an NIN connection structure after the iterative ICFE module, using different input methods. The experimental results indicate that using the R+T+NIN connection significantly improves the detection accuracy, while the R+R and T+T configurations, following NIN extraction, resulted in poorer performance. This is likely due to that the NIN module can more finely integrate and fuse cross-modality features, leading to notable improvements in detection performance.

Observations on Robustness.The experimental results indicate that different input configurations (e.g., R+T, R+R, T+T) have a significant impact on the model’s robustness. When using the same modality inputs (R+R or T+T), the model’s detection performance tends to be unstable and more susceptible to background noise. In contrast, when using the R+T combination, especially when coupled with the NIN module for feature fusion, the model demonstrates significantly higher robustness across various environmental conditions. These findings suggest that the complementary information between modalities plays a crucial role in enhancing the model’s ability to withstand environmental uncertainty and noise interference.

SECTION: 3.3Dual-Modality Data Augmentation

Formulations.Dual-modality data augmentation is a vital technique for enhancing the performance of multispectral object detection models. By applying consistent or complementary transformations to both modalities during training, this approach not only ensures the correlation between features from the two data sources but also enables the simulation of specific test scenarios (e.g., low-light conditions or small samples). Additionally, it effectively addresses information loss caused by feature dimensionality reduction, particularly in cases where the data distributions of the two modalities differ significantly. Mainstream dual-modality data augmentation strategies can be broadly categorized into three types:Geometric Transformations, Pixel-Level Transformations, and Multimodal-Specific Enhancements. These strategies will be detailed in the following sections.

Geometric Transformations.Geometric transformation strategies involve a range of spatial modifications designed to maximize the geometric diversity of training samples, enabling the model to generalize more effectively to varied object poses, orientations, scales, and viewpoints. The overall approach to geometric transformation strategies is outlined below, with most transformations formulated based on the following equation. Let the input image be represented by, the processed image by, and the geometric transformation function by. This transformation can be formalized as:

wheredenotes the composite affine transformation matrix, which integrates non-uniform scaling, complex rotation, and controlled mirroring. Therepresents the non-linear offset coefficient.

The matrixcan be decomposed as:

where each component transformation is defined as follows:

-represents a non-uniform scaling matrix, applying differential scaling along theandaxes:

whereandare the horizontal and vertical scaling factors, respectively, which may vary based on context-specific augmentation parameters.

-denotes the rotation matrix, which rotates the image by an anglein the 2D plane:

-represents the mirroring transformation, capable of inducing horizontal or vertical flips, denoted as follows:

whereis a stochastic parameter controlling the mirroring type, potentially following a probabilistic distribution to introduce randomness into the flipping process. This matrix may be further generalized to incorporate combinations of horizontal and vertical mirroring transformations, represented as:

-is the translation matrix, introducing positional shifts along theandaxes:

whereandrepresent horizontal and vertical translations, respectively. These shifts may vary based on contextual constraints to simulate different spatial orientations.

Pixel-Level Transformations.Pixel-level transformation strategies modify the pixel values of an image, such as by adding noise, adjusting colors, or altering contrast, to simulate various imaging conditions. This enhances the model’s robustness to lighting variations, noise, and diverse environmental factors. The following introduces pixel-level transformation strategies, with most transformations adhering to the approach outlined below. Let the pixel matrix of the image be, the transformation can be expressed through the following steps:

Noise Addition.To simulate sensor noise or environmental interference, Gaussian noisewith a standard deviation ofis added to the pixel matrix:

whererepresents Gaussian noise with variance.

Color Adjustment.To simulate different lighting conditions or sensor biases, color adjustment is applied using a scaling factor:

whereis the color adjustment factor that controls the brightness or saturation of each channel.

Contrast Adjustment.To enhance or reduce image details, contrast adjustment is applied using a contrast factor:

whereis the contrast adjustment factor andis the mean pixel value used for centering the pixel matrix.

Final Pixel Transformation.The final pixel transformation combines all the above operations:

Multimodal-Specific Enhancements.This class of strategies focuses on the unique characteristics of dual-light data, employing dual-channel synchronized or complementary enhancements tailored to specific test scenarios. By applying different augmentation methods to each modality, these strategies effectively enhance the cooperative performance of multimodal images and improve accuracy in targeted detection scenarios. Let the RGB image be denoted asand the TIR image as. The multimodal-specific enhancement can be expressed as:

whererepresents the multimodal enhancement function, which may include cross-modal alignment and modality-specific feature enhancement. Theandrepresent the enhanced RGB and TIR images, respectively. Specifically, the enhancement process can be further detailed as:

The functionsanddenote modality-specific enhancement operations applied to the input images, incorporating their corresponding aligned features. The matricesandare modality-specific alignment matrices, whileserves as the feature extraction function that identifies crucial features within each image for optimized information integration.

Experimental observations

Based on the single-modality object detection model Co-Detr, we made adaptive modifications to construct a baseline model suitable for multispectral object detection. As multispectral object detection augmentation strategies often need to adapt to specific application scenarios, test set sample characteristics, and varying weather and lighting conditions, we first conducted experiments exploring a set of synchronized augmentation techniques focused on geometric and pixel-level transformations. The experimental results are shown in Figure2. Building upon these methods, we further investigate specific augmentation strategies tailored to the unique characteristics of dual-light samples. The experimental results are shown in Figures3and4.

General Augmentation Strategy Experiments.In this section, we conducted dual-channel synchronized augmentation experiments using various geometric and pixel-level strategies, revealing several key insights.

Observations on Geometric Transformations.The experimental data indicates that applying a combination of random rotation, multi-scale scaling, and random cropping results in performance improvements across multiple datasets. However, strategies such as random flipping and random translation show poorer performance on the KAIST dataset. This could be attributed to the fact that the combination of random rotation, multi-scale scaling, and random cropping effectively simulates samples from various perspectives and angles, thus enhancing the model’s ability to adapt to different viewpoints, angles, scales, and deformations. On the other hand, strategies like flipping and translation may produce illogical images for certain samples (e.g., flipping upright pedestrians in the KAIST dataset leads to unnatural postures), which disrupts the inherent distribution patterns and modality alignment in some datasets, negatively affecting detection performance.

Observations on Pixel-Level Transformations.The overall performance improvements from pixel-level augmentation strategies are less significant compared to geometric transformations or spatial alignment methods. For instance, even the most effective combination in our experiments yielded only a 2.5% increase in recognition accuracy over the baseline, which is relatively modest when compared to methods such as feature fusion. Besides, a large number of pixel-level augmentation strategies (three or more) exhibit high sensitivity to different datasets. Specifically, we observed that the combination of Color Jitter+Random Sharpening+Random Blurring significantly improved recognition accuracy on the KAIST dataset, but the same combination performed poorly on the FLIR dataset. When more than four pixel-level augmentation strategies were applied, recognition accuracy often plateaued or even decreased across multiple datasets.

Experiments on Unique Augmentation Strategies.For specific scenarios, such as low-light/nighttime conditions and very small sample cases, we selected 500 images from the original dataset that exhibit these characteristics for targeted testing. We experimented with various combinations of dual-channel augmentation strategies, which includes dual-channel synchronized augmentation and complementary augmentation. Below are some interesting observations:

Observations on Strategies for Nighttime/Low-Light Samples.We conducted experiments comparing both synchronous and complementary augmentation strategies to identify the most effective combination for enhancing performance in low-light conditions. We found that complementary augmentation outperforms synchronous augmentation in improving overall recognition accuracy. This improvement is particularly pronounced in low-light conditions, where the strengths of complementary augmentation are more evident. Specifically, in low-light environments, the RGB modality tends to suffer from information loss, such as reduced contrast and increased noise, while the TIR modality, which captures thermal radiation, continues to provide stable target information even in the absence of illumination. Thus, adopting a complementary augmentation strategy allows each modality to better leverage its respective strengths. Besides, The complementary augmentation combination of random lighting and light enhancement for the TIR channel, paired with CLAHE for the RGB channel, achieved excellent results across all datasets. This success can be attributed to the complementary strategy’s ability to enhance the adaptability of the RGB channel to varying lighting conditions, while simultaneously improving the clarity of edges and shapes in the infrared samples.

Observations on Strategies for Small Samples.From the experimental data, it is evident that the augmentation strategy improves recognition accuracy. Specifically, the stitching operation proved to be highly effective in addressing the problem of very small samples individually, while the other two augmentation techniques did not consistently improve recognition accuracy. Independent use of both Stitcher and Fastmosaic led to notable improvements in recognition accuracy. In particular, Fastmosaic was the preferred choice for large-scale datasets (such as KAIST), while Stitcher performed better on more complex datasets (such as FLIR). Interestingly, when the two methods were combined, recognition accuracy decreased compared to their individual use. This outcome could be attributed to an imbalance in data distribution caused by the combination, which failed to provide the model with additional useful information.

SECTION: 3.4Registration Alignment

Formulations.In multispectral object detection tasks, factors such as sensor viewpoints, resolution discrepancies, and varying weather conditions can lead to spatial misalignment between RGB and TIR images. Such misalignment often introduces inconsistencies during feature fusion, thereby degrading detection performance. To address these issues, researchers have developed various registration and alignment strategies, which can be broadly categorized intoFeature Alignment-Based Methods and Feature Fusion-Based Methods. By applying these registration techniques at different stages of training and testing, the alignment between RGB and TIR images can be effectively improved, significantly enhancing recognition accuracy. The following sections provide a detailed discussion of these two categories.

Feature Alignment-Based Methods.The main goal of these methods is to address spatial misalignment between RGB and TIR images through precise feature matching and alignment. The Loftr approach exemplifies this objective by leveraging a Transformer-based architecture to achieve pixel-level feature matching between RGB and TIR images, allowing for high-precision geometric alignment[96]. This approach enables the calculation of transformation parameters (such as affine or perspective transformations) that can be applied to register the images effectively.

Let the RGB image be denoted asand the TIR image as. The coarse and fine features extracted from these images are represented asand, respectively. The matching functioncan be formulated as follows:

whererepresents matched point pairs across RGB and TIR modalities, andis a temperature parameter controlling the similarity distribution.denotes the softmax function, andrepresents the point with the highest matching score in the RGB modality for eachin the TIR modality.

The geometric transformationis then estimated based on these matched points by minimizing a distance-based objective:

whererepresents the transformed location ofin the TIR image space, withcontaining transformation parameters for an affine or homography matrixand translation vector. Once optimized, the transformation can be applied to obtain the aligned image:

To further improve the registration accuracy, a joint loss that combines a feature consistency loss and an alignment loss are introduced, expressed as:

wheremeasures the alignment quality based on transformation parameters, andandare weighting coefficients to balance the two loss terms. This method demonstrates exceptional alignment capabilities in scenes with pronounced parallax and varying viewpoints, enabling efficient image registration.

Feature Fusion-Based Methods.Feature fusion-based methods aim to effectively combine deep RGB and TIR features to generate a fused image, thereby achieving modality alignment. SuperFusion is a prime example, employing a multilevel fusion strategy that includes data-level transformation, feature-level attention mechanisms, and final Bird’s Eye View (BEV) alignment[97].

Given an RGB imageand a TIR image, the process begins by extracting feature mapsandthrough separate convolutional backbones. To enhance depth perception, a sparse depth mapis generated by projecting TIR depth information into the RGB image plane. A completion functionthen generates a dense depth map:

In the feature fusion stage, cross-attention is used to align features from both modalities, where RGB featuresguide the enhancement of TIR features. The cross-attention matrixincorporates depth information fromand is defined as:

whereandare learned weights andis a scaling factor, anddenotes the softmax function. This mechanism aligns features across modalities by using depth information to refine attention, allowing RGB features to enrich TIR information in the fused representation.

The resulting attention matrixis then used to enhance the TIR features:

whereis a learned weight matrix for generating the value matrix, andrepresents the TIR features enhanced by the RGB guidance.

Finally, a BEV alignment module refines the fused feature map by learning a flow fieldto warp RGB features, achieving better alignment with the enhanced TIR features. The aligned RGB imagecan be expressed as:

whererepresents the bilinear interpolation weights based on the flow fieldto adjust the alignment features. The interpolation weightscan be defined as:

These weights ensure that the spatial position of the RGB features is precisely adjusted according to the flow field, allowing for better alignment with the TIR features.

The entire process is optimized by a joint loss functionthat combines feature consistency and alignment error terms, weighted byand:

where the feature consistency termminimizes the difference between matched feature pairsin setM, and the alignment termmeasures the deviation from the ideal alignment.

Experimental observations

We utilized Loftr and SuperFusion to register RGB and TIR images separately and experimented by replacing the original RGB or TIR images with the fused images during model training and testing. The registration results in different scenarios can be observed in Figures5and6. The performance metrics of different registration methods can be found in Figure7. Below are some interesting findings:

Observation on Registration Performance.The experimental results demonstrate that Loftr and SuperFusion exhibit distinct advantages and characteristics in generating fused RGB images. Loftr focuses on precise feature matching and geometric alignment, ensuring that the fused RGB image is spatially well-aligned with the TIR image, with each pixel accurately corresponding to its counterpart. As shown in Figure5, Loftr performs well in images with high sample density, displaying strong spatial stability—likely due to the greater availability of feature mapping information provided by the dense samples. However, its performance deteriorates in sparser scenes, sometimes leading to issues such as ghosting and overlapping artifacts, making it challenging to proceed with subsequent detection steps.

In contrast, SuperFusion excels at handling sparse scenes where Loftr struggles, effectively preserving sample information and image features. However, it may impact the geometric characteristics of certain scenes, such as the vertical structures of bridges, whereas Loftr remains largely unaffected in such scenarios.

Observations on Registration Methods.The results in Figure7indicate that training the multispectral object detection model with Loftr-registered data yields a substantial increase in recognition accuracy, whereas training with SuperFusion-processed data shows limited impact. During testing, however, both Loftr and SuperFusion enhance recognition accuracy. This advantage is likely due to Loftr’s ability to address data inconsistencies via feature alignment during training, thereby improving data quality and facilitating more effective feature learning.

While SuperFusion is effective for multimodal fusion, it may introduce redundancy and complexity in the training data, potentially diverting the model’s focus from key features and limiting accuracy gains. In testing, both methods improve recognition accuracy by refining data quality or enriching feature information. Importantly, both registration frameworks perform best when generating RGB data based on the TIR reference, likely because the TIR-based RGB retains essential thermal information, supporting reliable performance in challenging conditions such as low light, smoke, or nighttime environments.

Observations on Application Scenarios.Experimental results indicate that Loftr excels in scenarios with significant rotational deviation or displacement between RGB and TIR images. This effectiveness is likely due to Loftr’s precise feature matching and geometric transformations, which effectively mitigate spatial misalignments. Conversely, SuperFusion demonstrates greater suitability in environments affected by adverse weather or low resolution, where it efficiently integrates multimodal data despite these challenges.

SECTION: 4Optimal Combination of Individual Techniques

In the previous section, we evaluated various training techniques for multispectral object detection under consistent conditions. However, extending a single-modality model to dual-modality with only one technique often yields suboptimal performance, as no single method fully addresses challenges like feature misalignment, overfitting, and fusion conflicts. Therefore, given the diverse methods in multispectral frameworks, relying on a single technique to enhance model performance is impractical. Our benchmark analysis highlights effective combinations of techniques and offers new insights for designing multispectral object detection models.

SECTION: 4.1Optimal Trick Combinations and Ablation Study

We have summarized the optimal technique combinations for the KAIST, FLIR, and DroneVehicle datasets above. Additionally, we conducted detailed ablation studies to validate the effectiveness of these combinations, as shown in Figure8. For each dataset, we tested 5 to 6 different combination variants by removing or substituting certain techniques. The results consistently demonstrate the significant effectiveness of our selected combinations, and the observed performance variations on specific samples are highly consistent with the conclusions we presented in Sections 3.2, 3.3, and 3.4.

SECTION: 4.2Comparison with Leading Frameworks

To further validate the effectiveness of the optimized single-modality model based on the best technique combinations, we compared it with other advanced frameworks specifically designed for multispectral object detection, including MBNet, MLPD, and MSDS-RCNN. As shown in TablesIV,V,VI, by organically integrating our training techniques into the single-modality model, the optimized model consistently outperforms previously well-designed multispectral detection frameworks on both small-scale and large-scale datasets.

SECTION: 4.3Transferring Technique Combinations

The final plausibility check is to determine whether certain technique combinations remain effective across multiple multispectral object detection datasets. To this end, we selected the combination of“Loftr for test alignment + ICFE for feature fusion”, as these two techniques consistently demonstrated optimal performance in the majority of scenarios covered in Sections 3.2, 3.3, and 3.4. This combination also performed comparably to other top-performing combinations on the FLIR and DroneVehicle datasets. Specifically, we evaluated this approach on two additional open-source multispectral detection datasets: (i) the LLVIP dataset, (ii) the CVC-14 dataset. In these transfer studies, we strictly adhered to the “best configuration point” settings outlined in Section 3.1.

As shown in TableIII:
the selected technique combination significantly improved the performance of the single-modality model on various multispectral datasets in most cases, particularly in scenarios with complex backgrounds and varying lighting conditions. This combination consistently enhanced model performance across different datasets, with the CVC-14 dataset showing a maximum accuracy improvement of over 31.23%. The strong transferability of this technique combination suggests its potential to serve as a robust baseline for future research in multispectral object detection, while also offering new training strategies for optimizing single-modality detection models.

SECTION: 5Conclusion

Multispectral object detection is a rapidly advancing field, yet significant challenges remain in effectively integrating multimodal information to adapt to diverse environmental conditions. In this study, we propose a standardized benchmark with fair and consistent experimental setups to drive progress in this domain. We conducted extensive experiments across multiple public datasets, focusing on three critical aspects of multispectral detection: multimodal feature fusion, dual-modality data augmentation, and registration alignment. Through a comprehensive analysis of our results, we identified the most effective technique combinations and established new performance benchmarks for multispectral object detection.

Additionally, we introduce a novel training strategy to optimize single-modality models for dual-modality tasks, laying the groundwork for adapting high-performing single-modality models to dual-modality scenarios. We believe that the strong baselines and optimized technique combinations presented in this work will facilitate fairer and more practical evaluations in multispectral object detection research. This work sets a robust foundation for future studies and opens new avenues for enhancing multispectral object detection performance.

SECTION: References