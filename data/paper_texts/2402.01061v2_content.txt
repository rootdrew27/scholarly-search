SECTION: On the power of linear programming for K-means clustering††thanks:The authors were partially funded by AFOSR grant FA9550-23-1-0123.

In[8], the authors introduced a new linear programming (LP) relaxation for K-means clustering. In this paper, we further investigate both theoretical and computational properties of this relaxation. As evident from our numerical experiments with both synthetic real-world data sets, the proposed LP relaxation is almost always tight; i.e., its optimal solution is feasible for the original nonconvex problem. To better understand this unexpected behaviour, on the theoretical side, we focus on K-means clustering with two clusters, and
we obtain sufficient conditions under which the LP relaxation is tight. We further analyze the sufficient conditions when the input is generated according to a popular stochastic model and obtain recovery guarantees for the LP relaxation. We conclude our theoretical study by constructing a family of inputs for which the LP relaxation is never tight. Denoting bythe number of data points to be clustered, the LP relaxation containsinequalities making it impractical for large data sets.
To address the scalability issue, by building upon a cutting-plane algorithm together with the GPU implementation of PDLP, a first-order method LP solver, we develop an efficient algorithm that solves the proposed LP and hence the K-means clustering problem, for up todata points.

Key words.K-means clustering; Linear programming relaxation; Tightness; Recovery guarantee; Cutting-plane algorithm; First-order methods.

SECTION: 1Introduction

Clustering data points into a small number of groups according to some similarity measure is a common task in unsupervised machine learning. K-means clustering, one of the oldest and the most popular clustering techniques, partitions the data points into clusters by minimizing the total squared distance between each data point and the corresponding cluster center.
Letdenote a set ofdata points in, and denote bythe number of desired clusters. Define a partition ofas a familyof non-empty subsets ofsuch thatfor alland. Then K-means clustering can be formulated as a combinatorial optimization problem:

It is well-known that Problem (1) is NP-hard even when there are only two clusters[3]or when the data points are in[19]. The most popular techniques for solving Problem (1) are heuristics such as Lloyd’s algorithm[16], approximation algorithms[13,10], and convex relaxations[26,25,5,22,12,15,8]. The two prominent types of convex relaxations for K-means clustering are semi-definite programming (SDP) relaxations[25]and linear programming (LP) relaxations[8]. The theoretical properties of SDP relaxations for K-means clustering have been thoroughly studied in the literature[5,22,12,28,15].
In this paper we investigate the power of LP relaxations for K-means clustering. In the following, we present an alternative formulation for Problem (1), which we use to construct our relaxations.

Consider a partitionof; let,be the indicator vector of
theth cluster; i.e., theth component ofis defined as:ifandotherwise. Define
the associatedpartition matrixas:

Definefor all.
Then Problem (1) can be equivalently written as (see[15]for detailed derivation):

It can be checked that any partition matrixis positive semidefinite.
Using this observation, one can obtain the following SDP relaxation of Problem (3):

whereis the trace of the matrix, andmeans thatis positive semidefinite. The above relaxation was first proposed in[25]and is often referred to as the “Peng-Wei SDP relaxation”. Both theoretical and numerical properties of this relaxation have been thoroughly investigated in the literature[5,22,12,15,27].

SECTION: 1.1LP relaxations for K-means clustering

In[8], the authors introduced theratio-cut polytope, defined as the convex hull of ratio-cut vectors corresponding to all partitions ofpoints ininto at mostclusters. They showed this polytope is closely related to the convex hull of the feasible region of Problem (3). The authors then studied the facial structure of the ratio-cut polytope, which in turn enabled them to obtain a new family of LP relaxations for K-means clustering. Fix a parameter;
then an LP relaxation for K-means clustering is given by:

By Proposition 3 of[8], inequalities (4) define facets of the convex hull of the feasible region of Problem (3). It then follows that for any, the feasible region of Problem (LPK) is strictly contained in the feasible region of Problem (LPKt). However, it is important to note that Problem (1.1) containsinequalities; moreover, an inequality of
the form (4) withcontainsvariables with nonzero coefficients. That is, while increasingimproves the quality of the relaxation, it significantly increases the computational cost of solving the LP as well. Hence, a careful selection ofis of crucial importance; we address this question in Section4.
In[5], the authors propose and study a different LP relaxation
for K-means clustering.
As it is detailed in[8], for any, the feasible region of Problem (1.1)
is strictly contained in that of the LP relaxation considered in[5](see Remark 1 in[8]).
In fact, the numerical experiments on synthetic data in[8]indicate that Problem (1.1) withis almost alwaystight; i.e., the optimal solution of the LP is a partition matrix.
To better understand this unexpected behaviour, in this paper, we perform a theoretical study of the tightness of the (1.1) relaxation. As a first step, we consider the case of two clusters. Performing a similar type of analysis foris a topic of future research.

Our main contributions are summarized as follows:

Consider any partition ofand the associated partition matrixdefined by (2). By constructing a dual certificate, we obtain a sufficient condition, often referred to as a “proximity condition”, under whichis the unique optimal solution of Problem (1.1) (see Proposition1and Proposition3).
This result can be considered as a generalization of Theorem 1 in[8]where the optimality ofequal-sizeclusters is studied.
Our proximity condition is overly conservative since,
to obtain an explicit condition, we fix a subset of the dual variables to zero. To address this, we propose a simple algorithm to carefully assign values to dual variables previously set to zero, leading to a significantly better dual certificate for(see Proposition5).

Consider a generative model, referred to as thestochastic sphere model(SSM), in which there are two clusters of possibly different size in, and the data points in each cluster are sampled from a uniform distribution on a sphere of unit radius. Using the result of part (i) we obtain a sufficient condition in terms of the distance between sphere centers under which the (1.1) relaxation recovers the planted clusters with high probability. By high probability we mean the probability tending to one as the number of data points tends to infinity (see Proposition4).

Since in all our numerical experiments Problem (1.1) is tight, it is natural to ask whether the (1.1) relaxation is tight with high probability under reasonable generative models. We present a family of inputs for which the (1.1) relaxation is never tight (see Proposition6and Corollary1).

Motivated by the surprising performance of the LP relaxation, we propose a scalable cutting plane algorithm, which relies on an efficient separation algorithm for inequalities (4), lower bounding and upper bounding techniques, and a GPU implementation of PDLP, a first-order primal-dual LP solver. Thanks to this algorithm, we are able to solve Problem (1.1) for real-world instances with up todata points in less than two and half hours. Interestingly forallinstances, by solving the LP relaxation, we solve the original K-means clustering problem to global optimality.

SECTION: 1.2Organization

The rest of the paper is structured as follows. In Section2, we perform a theoretical study of the tightness of the LP relaxation for two clusters.
In Section3, we present a family of inputs for which the optimal value of Problem (1.1) is strictly smaller than the optimal value of the K-means clustering problem. Finally, in Section4we develop a customized algorithm for solving the LP relaxation and present extensive numerical experiments.

SECTION: 2The tightness of the LP relaxation for two clusters

In this section, we consider the K-means clustering problem withand examine the strength of the LP relaxation theoretically. Recall that for, we must have; therefore, Problem (1.1) simplifies to:

where as before we setfor all. Throughout this section, whenever we say “the LP relaxation,” we mean the LP defined by Problem (2) and whenever we say “the SDP relaxation,” we mean the SDP defined by Problem (PW).
To motivate our theoretical study, in the following we present a simple numerical experiment to convey the power of the LP relaxation for clustering real-world data sets. Solving both LP and SDP becomes computationally expensive as we increase the number of points; that is, to solve an instance withefficiently, one needs to design a specialized algorithm for both LP and SDP.
Indeed, in[27]the authors design a specialized algorithm for the SDP relaxation and solve problems with. In Section4, we present a customized algorithm to solve the LP relaxation. In this section, however, we are interested in performing a theoretical analysis of the tightness of the LP and we are using our numerical experiments to motivate this study. Hence, we limit ourselves to solving problem instances withvariables.

We say that a convex relaxation is tight if its optimal solution is a partition matrix. In[26], the authors prove that a symmetric matrixwith unit row-sums and traceis a partition matrix if and only if it is aprojectionmatrix; i.e.,. Hence to check whether the solution of the LP relaxation or the SDP relaxation is tight, we check whether it is a projection matrix.
We collected a set of 15 data sets from the UCI machine learning repository[9]. The data sets and their characteristics, i.e., the number of pointsand the input dimensionare listed in columns 1-3 of Table1. For data sets with(i.e., Voice, Iris, and Wine), we solve the clustering problem over the entire data set. For each of the remaining data sets, to control the computational cost of both LP and SDP, we sampletimesdata points and solve LP and SDP using therandom instances. All experiments are performed on theNEOSserver[6];
LPs are solved withGAMS/Gurobi[11]and SDPs are solved withGAMS/MOSEK[1].
The tightness rate of the LP (), the tightness rate of the SDP (), and the (average) relative gap between the optimal values of SDP and LP are listed in columns 4-6 of Table1. Inallinstances, the LP relaxation is tight, while the SDP is only tight in a few instances.
The remarkable performance of the LP relaxation is indeed unexpected and hence in this paper, it is our goal to better understand this behaviour through a theoretical study.

SECTION: 2.1A sufficient condition for the tightness of the LP relaxation

We first obtain a sufficient “proximity” condition under which Problem (2) is tight; i.e., the optimal solution of the LP relaxation is a partition matrix.
A proximity condition for the SDP relaxation defined by Problem (PW) is presented in[15].
As our proximity condition is overly conservative, we then present a simple algorithm that certifies the optimality of a given partition matrix.
To establish our proximity condition, in Proposition1, we first obtain a sufficient condition under which a given partition matrix is an optimal solution of Problem (2). Subsequently in Proposition3, we address the question of uniqueness of the optimal solution. The next proposition
can be considered as a generalization of Theorem 1 in[8]where the authors study the optimality ofequal-sizeclusters.

In the following, for everyand, we define

Moreover, given a partitionof,
for each,, we define

We are now ready to state our sufficient condition.

Letdenote a partition ofand assume without loss of generality that.
Define

and

Suppose that

and

Then, an optimal solution of Problem (2) is given by the partition matrix

We start by constructing the dual of Problem (2). Define dual variablesassociated with (5),for allassociated with (6),for allassociated with (7), andfor allassociated with (8). The dual of Problem (2) is then given by:

where we letfor all.
To establish the optimality ofdefined by (13), it suffices to construct a dual feasible pointthat together withsatisfies the complementary slackness.
Without loss of generality, assume thatfor alland. Thenandsatisfy the complementary slackness if and only if:

ifandor ifand,

ifor.

After projecting outforand, we deduce that it suffices to findsatisfying:

To this end, let

whereandare as defined by (9) and (10), respectively.

First let us examine the validity of inequalities (17).
Substituting (19)-(22) in (17) yields:

where the last inequality follows from the definition ofgiven by (10) and the identity.

We now show that equalities (18) are implied by equalities (16).
In the following we prove that all equalities of the form (18) withare implied by equalities of the form (16) with; the proof of the case withfollows from a similar line of arguments.
Using (19) to eliminateforand, and using (21) to eliminate,, it follows that equalities (16) withcan be equivalently written as:

Using (21)-(23), inequalities (18) can be written as:

where we used the identity.
Moreover, it can be checked that

Therefore, to show that equalities (25) are implied by equalities (24), it suffices to have:

whose validity follows since.

Therefore, it remains to prove the validity of equalities (16).
First consider the case with. By (19) and nonnegativity offor all, we deduce that

wheredenote the absolute value function. Hence, using (21) to eliminate, we conclude that equalities (16) withcan be satisfied if

where we used the identity.
Lettingfor alland using (11), we conclude that inequalities (26) are valid. Similarly, it can be checked that inequalities (17) withcan be satisfied if

Lettingfor alland using (12), we conclude that inequalities (27) are valid and this completes the proof.
∎

We now consider the question of uniqueness of the optimal solution. To this end, we make use of the following result:

Consider an LP whose feasible region is defined byand, wheredenotes the
vector of optimization variables, andare vectors and matrices of appropriate dimensions.
Letbe an optimal solution of this LP and denote bythe dual optimal solution corresponding to the inequality constraints.
Letdenote
the-th row of. Define,. Letandbe the
matrices whose rows are,and,, respectively.
Thenis the unique optimal solution of the LP, if there exists nodifferent from the zero vector satisfying

We are now ready to establish our uniqueness result:

Suppose that inequalities (11) and (12) are strictly satisfied. Thendefined by (13) is the unique optimal solution of Problem (2).

Consider the dual certificateconstructed in the proof of Proposition1. We consider a slightly modified version of this certificate by redefiningdefined in (10) as follows:

for some. This in turn will imply thatfor all,. Notice that this is an admissible modification as we are assuming that inequalities (11) and (12) are strictly satisfied. In addition, we can choose

Therefore, by Proposition2, the matrixdefined by (13) is the unique optimal solution of Problem (2), if there exists nosatisfying:

From (29) and (30) it follows that

Moreover, from equations (31) we deduce that

Substituting (33) in (32) we obtain:for alland for all,
which together with (30) completes the proof.
∎

To have an intuitive understanding of our proximity condition, consider the special case withfor alland. This assumption is for instance satisfied, ifandandare uniformly distributed on two spheresandof equal radii. Denote bythe center of. Bybeing uniformly distributed on, we imply that for any rotation matrixfor which there exist two pointssuch that, we have. In this setting we haveand assumptions (11) and (12) simplify to:

These inequalities requires that the distance between any two pointsin the same cluster should be smaller than the average distance between the setsand, wheredenotes a point in the other cluster.

Using our proximity condition given by inequalities (11) and (12), we next obtain a recovery guarantee for the LP relaxation under a popular stochastic model for the input data.

SECTION: 2.2Recovery guarantee for the stochastic sphere model

It is widely understood that worst-case guarantees for optimization algorithms are often too pessimistic. A recent line of research in
data clustering is concerned with obtaining sufficient conditions under which a planted clustering corresponds to the unique optimal solution of a convex relaxation under suitable stochastic models[5,22,12,15,8]. Such conditions are often referred to as (exact)recoveryconditions and are used to compare the strength of various convex relaxations for NP-hard problems. Henceforth, we say that an optimization problemrecoversthe planted clusters if its unique optimal solution corresponds to the planted clusters.

Perhaps the most popular generative model for K-means clustering is thestochastic ball model, where the points are sampled from uniform distributions onunit balls in. As before, we letand we denote bythe distance between the ball centers. Notice that the question of recovery only makes sense when.
We denote byandthe set of points sampled from the first and second balls, respectively and without loss of generality we assume. In the following, whenever we saywith high probability, we mean the probability tending to one as.
In[15], the authors prove that the Peng-Wei SDP relaxation defined by Problem (PW) recovers the planted clusters with high probability if

whereis defined by (9). In the special case of equal-size clusters, i.e.,, the authors of[5]show that the Peng-Wei SDP relaxation recovers the planted clusters with high probability, if, while the authors of[12]show that the same SDP recovers the planted clusters with high probability if.
Again, for equal-size clusters, in[8], the authors show that Problem (2) recovers the planted clusters with high probability, if.

In this section, we obtain a recovery guarantee for the LP relaxation for two clusters of arbitrary size. For simplicity, we consider a slightly different stochastic model, which we refer to as the stochastic sphere model (SSM),
where instead of a ball, points in each cluster are sampled from a sphere (i.e., the boundary of a ball). We prove that our deterministic condition given by inequalities (11)-(12) implies that
Problem (2) recovers the planted
clusters with high probability, if

We should mention that, at the expense of a significantly longer proof, one can obtain the same recovery guarantee (35) for the LP relaxation under the SBM. We do not include the latter result in this paper because, while the proof is more technical and longer, it does not contain any new ideas and closely follows the path of our proof for the SSM.
Indeed, in case of equal-size clusters; i.e.,, inequality (35) simplifies to the recovery guarantee of[8]for SBM:.
Also note that while the recovery guarantee for the LP (35) is better than the
the recovery guarantee for the SDP (34) for, it becomes weaker for larger dimensions. As we detail in the next section, condition (35) can be significantly improved via a more careful selection of the dual certificate.

Throughout this section, for an event, we denote bythe probability of. For a random variable, we denote byits expected value. In case of a multivariate random variable, the conditional expected value in, withfixed, will be denoted either withor with.
We denote byandthe spheres corresponding to the first and second clusters, respectively.
Up to a rotation we can assume that the centers ofandareand, respectively, whereis the first vector of the standard basis of.
For a continuous function(and analogously for), we define

wheredenotes the-dimensional Hausdorff measure.

We are now ready to state our recovery result.

Suppose that the points are generated according to the SSM.
Then Problem (2) recovers the planted clusters with high probability if, whereis defined by (9).

By Proposition3, it suffices to show that for, inequalities (11)-(12) are strictly satisfied with high probability.
Namely, we show that there exists a universal constantsuch that, forwe have

and

whereare as defined in the statement of Lemma1.
Since the two inequalities are symmetric, their proof is similar and we will only prove inequality (36).
To this aim, for notational simplicity, define

Then we can compute

The first inequality follows from (47) in Lemma1, since; the second inequality holds by set inclusion, and the
third inequality is obtained by taking the union bound.
To complete the proof, we next estimate each of the terms in the last two lines of (37).
In the following,will always denote a universal positive constant, which may increase from one line to the next line and we will not relabel it for the sake of exposition. First, to estimate, we define

so that.
Recall that,,and. Since for, the recovery follows from a simple thresholding argument, we can restrict our attention to, i.e., we assume that. It then follows that

The first inequality holds by set inclusion and the third inequality follows from Hoeffding’s inequality (see for example Theorem 2.2.6 in[29]), since,are i.i.d. random variables for everyand. Next we show that

For notational simplicity, we denote theth point inby.
We notice that for anywe have

By symmetry, the same calculation holds forwith.
By (41), we have

Using Hoeffding’s inequality together with (41), we get

Hence, by the union bound, we obtain

from which we conclude the validity of (40).
Since by (38) and (42) we have

we can combine (39) with (40) to conclude that

We now observe that

where the first inequality follows from the linearity of expectation and the second inequality follows from the application of Hoeffding’s inequality and taking the union bound.
Finally, by Hoeffding’s inequality we have

Plugging inequalities (43)-(44)-(45) in (37), we conclude the claimed inequality (36).
∎

In order to prove our recovery result in Proposition4, we made use of the following lemma.

Suppose that the random points are generated according to the SSM.
Then the following inequalities hold provided that:

We first prove the following:

Inequalities (46)-(47) can be equivalently written as:

Proof of claimAs before, for notational simplicity, we denote theth (resp.th,th) point by(resp.,).
By (41) and (42), inequalities (46)-(47) read respectively

The first inequality, up to a change of variable, reads:

hence inequalities (46)-(47) read

which, expanding the squares, gives

Via a change of variables, we have

Hence (49) reads

which is equivalent to (48).

Thanks to Claim1, to conclude the proof of the lemma it suffices to show the following:

Inequalities (48) hold if and only if.

Proof of claimWe will prove that the maximum of the left-hand side of inequalities (48) over allis attained at, for all.
Sinceand, this in turn implies that inequalities (48) are satisfied if and only if

which, by, is true if and only if; i.e., the desired condition.
Define

Our goal is to show that for every

In Lemma2, we prove that

Hence, recalling thatand thatfor every, we conclude with the following chain of inequalities:

∎

In order to prove Lemma1, we made use of the next lemma, for which we provide a proof that is closely related to the proof of Lemma 1 in[8], in which the authors prove

whereis defined by (50) anddenotes a ball of radius one as defined in the SBM. For brevity, in the following, we only include the parts of the proof that are different, and when possible, we refer to the relevant parts of the proof of Lemma 1 in[8].

(52) holds.

For any, we denote bytheth component of.
We divide the proof in several steps:

Step 1. Slicing:

Letbe any pair of points insatisfying,,for all. In the special case, we consider.
Define

Then (52) holds if the following holds

for any pair of pointssatisfying,,for allin case, or, in case, for any point.

Proof of Step 1.Since

to show (52) it is enough to show that the function

is maximized in,, for everyDenoting, then

Hence, it is enough to prove that for everyand for every,

is achieved at.
Since Problem (55) is invariant under a rotation of the space around the axis generated by, we conclude that solving Problem (55) is equivalent to solving Problem (54).

Step 2. Symmetric distribution of the maxima:

Letbe any pair of points as defined in Step 1.
Define

In order to show that (54) holds, it suffices to prove that for

Proof of Step 2.The proof is identical (up to trivial changes) to the proof of Step 2 of Lemma 1 in[8].

Step 3. Reduction from spheres to circles:

To show the validity of (56), we can restrict to dimension.

Proof of Step 3.The proof repeats verbatim as in the proof of Step 3 of Lemma 1 in[8].

Step 4. Symmetric local maxima:

For any pairof the formand, we have

Proof of Step 4.Given such symmetric pair, the objective function evaluates to. Usingand, it suffices to show that

Since the functionon the right hand side of (57) is a convex parabola in,
its minimum is either attained at one of the end points or at, provided
that. Since, the pointlies in the domain only if.
The value ofatandevaluates toand, respectively,
both of which are bigger than. Hence it remains to show thatif, that is we have to show that

where we setand we use that. Since, the right hand side of the above inequality is increasing in;
hence it suffices to show its validity at; i.e.,

which can be easily proved.

Step 5. Decomposition of the circle:

To solve Problem (56), it suffices to solve

for every,,.

Proof of Step 5.The proof is identical (up to trivial changes) to the proof of Step 6 of Lemma 1 in[8].

Step 6. We solve Problem (58).

Proof of Step 6.The proof is identical (up to trivial changes) to the proof of Step 7 of Lemma 1 in[8].

∎

While the recovery guarantee of Proposition4is the first of its kind for an LP relaxation of K-means clustering, it is too conservative. We demonstrate this fact via numerical simulations. We let,,, and. For each fixed configuration, we generate 20 random trials according to the SSM.
We count the number of times the optimization algorithm returns the
planted clusters as the optimal solution; dividing this number by the total number of trials, we obtain the empirical recovery rate. We use the same set up as before to solve all LPs and SDPs. Our results are shown in Figure1, where as before we compare the LP relaxation defined by Problem (2) with the SDP relaxation defined by Problem (PW). Clearly, in all cases the LP outperforms the SDP. Moreover, our results indicate that the recovery threshold of the LP relaxation for SSM is significantly better than the one given by Proposition4. For example, for, condition (35) gives the recovery threshold, while Figure1(c)suggests. We should also remark that inallthese experiments, the LP relaxation is tight; i.e., whenever the LP fails in recovering the planted clusters, its optimal solution is still a partition matrix. In contrast, the SDP relaxation is not tight in almost all cases for which it does not recover the planted clusters.

SECTION: 2.3A stronger dual certificate

By Proposition1, if inequalities (11) and (12) are satisfied then the partition matrixdefined by (13) is an optimal solution of Problem (2) and as a resultis an optimal solution of the K-means clustering problem.
However, as evident from Proposition4and Figure1, our proximity condition given by inequalities (11) and (12) are overly conservative. In the following we present a simple algorithm that leads to significantly better recovery guarantees.

Recall that in the last step of the proof of Proposition1, our task is to identify conditions under which inequalities (26) and (27) can be satisfied. In the proof we letfor allandfor all, which in turn gives us inequalities (26) and (27). As we show next, a careful selection of these multipliers will lead to significantly better recovery results.
Define

and

We definefor all.
Then, by the proof of Proposition1, the partition matrixdefined by (13) is an optimal solution of Problem (2) if the following system of inequalities is feasible:

where as before we letfor.
We next present a simple algorithm whose successful termination serves as a sufficient condition for feasibility of system (2.3):

If AlgorithmCertifyterminates withsuccess = .true.,
then the partition matrixdefined by (13) is an optimal solution of Problem (2). Moreover, AlgorithmCertifyruns inoperations.

We prove that the system defined by all inequalities in system (2.3) withis feasible; the proof forthen follows.
Define

Then the system defined by all inequalities in system (2.3) withcan be equivalently written as:

If, then by lettingfor all, we obtain a feasible solution and the algorithm terminates withsuccess = .true..
Hence, suppose that; that is, the initialization step violates inequalitiesfor all.
Consider an iteration of the algorithm for some; we claim that if this iteration is completed withsuccess = .true., all nonnegative,remain nonnegative (even though their values may decrease), and we will have. Since the value ofdoes not decrease over the course of the algorithm, this in turn implies that if the algorithm terminates withsuccess = .true.,is feasible for system (61). To see this, consider somefor which we have. Notice that variable(which is equal to) appears only in three equations of (60) defining(with positive coefficient),(with negative coefficient), and(with negative coefficient). Since,, and, we can increase the value ofbyand keep the system feasible, this in turn implies that the value ofwill increase by, while the values ofanddecrease by. Therefore, if the algorithm terminates withsuccess=.true., the assignmentis a feasible solution for system (61). It is simple to verify that this algorithm runs inoperation.
∎

Let us now comment on the power of AlgorithmCertifyfor recovering the planted clusters under the SBM. Since we do not have an explicit proximity condition, we are unable to perform a rigorous probabilistic analysis similar to that of Section2.2. However, in dimension, we can perform a high precision simulation as the computational cost of AlgorithmCertifyis very low. First, we consider the case of equal-size clusters; i.e.,. We setand we observe that for, AlgorithmCertifyterminates withsuccess = .true.. We then conjecture the following:

Let, and suppose that the points are generated according to the SBM with equal-size clusters. If, then Problem (2) recovers the planted clusters with high probability.

If true, the recovery guarantee of Conjecture1is better than the recovery guarantee of the SDP relaxation given by (34) for. Clearly, any convex relaxation succeeds in recovering the underlying clusters only if the original problem succeeds in doing so. To this date, the recovery threshold for K-means clustering under the SBM forremains an open question.
Let us briefly discuss special casesand. In[12], the authors prove that a necessary condition for recovery of K-means clustering in dimension one is. In the same paper, the recovery threshold of the SDP relaxation in dimension one is conjectured to be(see Section 2.3 of[12]for a detailed discussion).
In[8], the authors prove that Problem (2) recovers the planted clusters with high probability (for every), if. It then follows that for, the K-means clustering problem recovers the planted clusters with high probability if and only if.
For, in[12], via numerical simulations, the authors show that K-means clustering recovers the planted clusters with high probability only if. If true, Conjecture1implies that in dimension two, the recovery thresholds for K-means clustering and the LP relaxation are fairly close. In[15], the authors prove that, if

then the SDP relaxation fails in recovering the planted clusters with high probability. They also state that “it remains unclear whether this necessary condition (i.e., inequality (62)) is only necessary for the SDP relaxation or is necessary for the K-means itself.” If true, Conjecture1implies that inequality (62) isnota necessary condition for the K-means clustering problem.

We further use AlgorithmCertifyto estimate the recovery threshold for the case with different cluster sizes, i.e., in dimension.
Our results are depicted in Figure2. As can be seen from the figure, the recovery guarantee given by AlgorithmCertifyis significantly better than that of Proposition4. For comparison, we have also plotted the recovery threshold for the SDP relaxation given by condition (34) for different input dimensions.
While Figure2suggests that the recovery threshold of the LP as given by AlgorithmCertifyquickly degrades asdecreases, our numerical experiments with the LP relaxation, depicted in Figure1for, indicates otherwise. Notice that AlgorithmCertifyprovidesa sufficient conditionfor feasibility of system (2.3). Obtaining a better sufficient condition or, if possible, a tractable necessary and sufficient condition for the feasibility of system (2.3) remains an open question.

SECTION: 3A Counter example for the tightness of the LP relaxation

As we discussed in the previous section, in all our experiments with both synthetic and real-world data sets, Problem (2) is tight; i.e., its optimal solution is a partition matrix. In particular, a large number of our experiments are done under the SBM and SSM. Hence, it is natural to ask whether the LP relaxation is tight with high probability under reasonable generative models. In the following we present a family of inputs for which the LP relaxation is never tight. This family of inputs subsumes as a special case the points generated by a variation of the SBM.
To prove our result, we make use of the following lemma, which essentially states that if several sets of points have identical optimal cluster centers, then those cluster centers are optimal for the union of all points as well:

Letdenote a set ofpoints infor some, with. For each, consider an optimal solution of Problem (1) for clustering thepointsand denote bythe vector with entries equal to thecenters of the optimal clusters. Iffor all, then there exists an optimal solution of Problem (1) for clustering the entire set ofpoints with cluster centers vectorsatisfyingfor all.

Letdenote a set ofpoints inthat we would like to put intoclusters and denote bythe vector of cluster centers. We start by reformulating Problem (1) as follows:

Notice that by solving Problem (63) one directly obtains a vector of optimal cluster centers and subsequently can assign each pointto a cluster at whichis attained. Now for each, define

It then follows that the K-means clustering problem for clustering thepointsfor somecan be written as:

while the K-means clustering problem for the entire set ofpoints can be written as

Clearly,. Hence, if for each, there exists a minimizerof Problem (64) such thatfor all, we conclude thatis a minimizer of Problem (65) as well.
∎

Now let us investigate the tightness of the LP relaxation defined by Problem (2). First note that by Proposition 5 in[8], if, then the feasible region of Problem (2) coincides with the convex hull of the feasible region of Problem (3). Hence, to find an instance for which the LP relaxation is not tight we must have.
Consider the followingpoints in dimension, which we refer to as thefive-point input:

In this case the vector of squared pair-wise distancesis given by

By direct calculation it can be checked that the partitionandis a minimizer of the K-means clustering problem (1)
with the optimal objective value given by

Now consider the following matrix

Notice thatis not a partition matrix.
It is simple to verify the feasibility offor the LP relaxation. Moreover, the objective value of
Problem (2) atevaluates to. Denote bythe optimal value of Problem (2). Since, we conclude that for the five-point input, the LP relaxation is not tight.

Now consider the following set of points, which we will refer to as thefive-ball input. Instead ofpoints located at each,defined by (66), suppose that
we havepoints for
some,, such that for each, we havepoints located inside a closed ball of radiuscentered at.
In the following we show that for, the LP relaxation always fails in finding the optimal clusters.

Consider an instance of the five-ball input for some. Then the LP relaxation is not tight; i.e., the optimal value of Problem (2) is strictly smaller than that of Problem (1).

We denote bythe index set of points located in theth ball; without loss of generality, letfor all.
Let us denote these points by,,.
We start by computing an upper bound on the optimal value of the LP relaxation.
Consider the matrixdefined as follows:

Clearly,is not a partition matrix.
First we show thatis feasible for Problem (2).
The equality constraint (5) is satisfied as we have. Equalities (6) are also satisfied since we havefor all, andfor all.
Hence it remains to check the validity of inequalities (7):
first note that if,,for, then the validity offollows from the validity of inequalities  (7) atdefined by (68). If,, thenfor all,. If,, thenfor all,. The remaining cases, i.e., when two of the points are in the same ball, while the third point is in a different ball, can be checked in a similar fashion.

Denote by,, the vector of squared pairwise distances. We have:

Hence, an upper boundon the optimal value of Problem (2) is given by

where the last inequality follows since by assumption, and
wheredenotes the optimal value of Problem (2) for the five-ball input.

Denote bythe optimal value of the K-means clustering problem (1) for the five-ball input.
Next, we obtain a lower bound on, and show that this value is strictly larger thandefined by (69) for any, implying that Problem (2) is not tight.
To this end, first consider the case with. Define the set of pointsfor all. Notice that since, we havefor all. For each, consider the K-means clustering problem withfor five input points, and denote the optimal cluster centers by. We then havefor all, wheredenotes the optimal cluster centers for the five-point input (66). Therefore, by Lemma3, we conclude that an optimal cluster centers for the five-ball input with, coincides with an optimal cluster centers for the five-point input. This in turn implies that

Now let us consider the five-ball input for some.
We observe that

Sincefor all, we havefor all.
Therefore, for every partition matrixwe compute

where the last equality follows from constraints (6).
Hence, taking the minimum over all partition matrices on both sides and using (70), we deduce that

From (69) and (71) it follows that by choosing

we getand this completes the proof.
∎

As a corollary of Proposition6, we find that the LP relaxation is not tight for a variant of the SBM:

Let. Suppose that the points are generated in, for someaccording to any generative model consisting of five measures,, each supported in the ball, whereis defined by (66).
Then Problem (2) is not tight.

Recall that in all our previous numerical experiments with synthetic and real-world data sets, the LP relaxation outperforms the SDP relaxation. That is, the optimal value of the LP is always at least as large as that of the SDP. Hence, one wonders whether such a property can be proved in a general setting. Interestingly, the stochastic model defined in Corollary1provides the first counter example, which we illustrate via a numerical experiment. We consider the stochastic model defined in Corollary1, where we assume the points supported by each ball are sampled from a uniform distribution. We setand generatepoints in each of the five balls to get a total ofpoints. Moreover we set ball radiiand for each fixedwe generate 20 random instances. Our results are depicted in Figure3, where as before we compare the LP, i.e., Problem (2), with the SDP, i.e., Problem (PW), with respect to their tightness rate and average relative gap. Recall that the relative gap is defined as, whereanddenote the optimal values of the LP and the SDP, respectively. That is, a negative relative gap means that the SDP relaxation is stronger than the LP relaxation. As can be seen from the figure, while the SDP is never tight, for, we often have.

We conclude this section by noting that the family of examples considered above are very special and our numerical experiments with real-world data sets suggest that such special configurations do not appear in practice. Hence, it is highly plausible that one can establish the tightness of the LP relaxation under a fairly general family of inputs. We leave this as an open question.

SECTION: 4A customized algorithm for solving the LP relaxation

As we detailed in the previous section, Problem (1.1) containsinequalities of the form (4). This in turn implies that even in the cheapest case; i.e.,, the computational cost of solving the LP relaxation quickly increases making it impractical for many real-world applications. To further illustrate this fact, we consider three state-of-the-art LP solvers:

Gurobi 11.02[11], we use the interior-point algorithm and turn off thecrossoveroperator to expedite the solver,

PDLP 9.10[4], a primal-dual
first-order method for solving large-scale LPs that is implemented in Google’s OR-Tools,

cuPDLP-C[18], an improved implementation ofcuPDLPin theClanguage, wherecuPDLP[17]is a GPU implementation ofPDLPand is written inJulia.

We first show that even for relatively small data sets i.e.,, none of the solvers listed above are able to solve Problem (1.1) withinhours. Again, we consider a number of data sets from the UCI machine learning repository[9]. The data sets and their characteristics, i.e., the number of pointsand the number of clustersare listed in columns 1-3 of Table2. We solve Problem (1.1) withusing the three LP solvers listed above.
The size of each LP, i.e., the number of variables, the number of constraints, and the number of nonzeros in the constraint matrixare listed in columns 4-6 of Table2. Henceforth, all experiments are conducted onGoogle Colabusing an Intel(R) Xeon(R) CPU @ 2.20GHz with 8 cores and 50 GB of RAM. The GPU used is an NVIDIA L4 with 22.5 GB of RAM.
The total time in seconds of solving the LP using each solver is listed in the last three columns of Table2. As can be seen from this table, for all problems that fit in the GPU memory,cuPDLP-Csignificantly outperforms other solvers. Nonetheless, for problems with, the LP becomes excessively expensive for all solvers. It is interesting to note that with the exception ofglassandWholeSale(with) data sets, for all instances, the LP relaxation is tight; that is, it returns a partition matrix upon termination. As we detail shortly, forglassandWholeSaledata sets, Problem (1.1) is tight ifand, respectively.

Problem (1.1) withis not tight.

Exceeds the maximum memory of the machine.

Motivated by this numerical study, we next a present a cutting-plane based algorithm that enables us to solve Problem (1.1) for significantly larger data sets. The key to the success of this algorithm is to solve a sequence of LPs each of which contains a small but carefully selected subset of inequalities (4).
Next, we describe the main ingredients of our algorithm111The source code as well as the data sets are avaialable athttps://github.com/Yakun1125/cutLPK/tree/main.

SECTION: 4.1Initialization

Llyod’s algorithm and its enhanced implementations such ask-means++are extremely fast and are often successful in finding high quality solutions for the K-means clustering problem. In our framework, we make use ofk-means++’s solution at the initialization step. First, the cost associated with this solution serves as an (often very good) upper bound on the optimal value of Problem (1.1). Second, to construct the first LP, we select inequalities (4) withthat are satisfied tightly at thek-means++’s solution. This in turn implies that if thek-means++’s solution is globally optimal, and if the optimal value of Problem (1.1) withcoincides with that of the K-means clustering problem, then our algorithm terminates after one iteration. For large scale data sets, however, the number of inequalities (4) withthat are satisfied tightly at thek-means++’s solution might be too large; in those cases, we randomly select a subset of at mostof such inequalities.

SECTION: 4.2Safe lower bounds

To further expedite the cutting plane algorithm for large scale problems, in the first few iterations, we solve LPs with a larger feasibility tolerance and terminate early, if needed. In such cases the solution returned by the LP solver is often infeasible. To generate valid lower bounds on the optimal value of Problem (1.1), we make use of a technique that is has been used in mixed-integer programming solvers[24]. Consider the following pair of primal-dual LPs:

Denote byan infeasible primal-dual pair returned by the LP solver satisfying.
Define the dual residualas. We then have. Letdenote an optimal solution for the primal LP. Using, we get.
Fromand, we deduce thatimplying that. Denote byan upper bound on. We then obtain the following “safe lower bound” on the optimal value of the primal LP:

SECTION: 4.3Rounding

Denote byan optimal solution of the LP relaxation at an iteration of the cutting plane algorithm and suppose thatis not a partition matrix. It is then desirable to “round” this solution to obtain a partition matrixwhose cost may serve as good upper bound on the optimal clustering cost. We make use of the rounding scheme proposed in[25], which is given below.

SECTION: 4.4The separation algorithm

Recall that the key to the efficiency of a cutting plane algorithm to solve Problem (1.1) is the efficient separation of inequalities (4). That is, each intermediate LP should contain a small number of inequalities (4) that are as sparse as possible.
Letbe the solution to the current LP. Fix.
For each, the separation problem is
to find a nonempty subsetwithsuch that

or to prove that no such subset exists. This separation problem is similar to the separation of clique inequalities (see for example[21]). In this setting, we can avoid repeated calculations using the following relation:

where we use the identityfor all.
However, this enumeration scheme is too expensive for large scale problems. To this end, we utilize the greedy strategy proposed in[21]for separating clique inequalities. Our heuristic is as follows: for each, instead of enumerating all subsets, we start by settingfor some. The algorithm then proceeds by expandingiteratively, where in each iteration, an indexthat maximizesis appended to.

An outline of the separation scheme is provided in Algorithmseparate.
Clearly, this algorithm is not exact, as the greedy expansion ofmay lead to exclusion of some violated inequalities. Hence we apply the following two step approach. We first run Algorithmseparate; if no violated inequalities are found andwhile, then we switch to enumeration. However, in our numerical experiments, for all problems instances, Algorithmseparatewas enough to close the optimality gap. We should also remark that, to separation inequalities (4), we runseparation problems, each for a fixed, in parallel. We make use ofOpenMP[7]to do the parallel computations.

We are now ready to present our iterative cutting-plane based algorithm. An overview of this algorithm is given in AlgorithmIterative LP solver. Notice that inIterative LP solver, the sparsity of the inequalities added to the LP is controlled by parameter, which is initially set to, and is increased by one, only if the number of violated inequalities (4) withthat are found by Algorithmseparateis below a certain threshold.

SECTION: 4.5Numerical Experiments

We start by testingIterative LP solverusing our three LP solvers as before for small data sets from UCI. Results are shown in Table3. Notice thatIterative LP solveris not a deterministic algorithm; that is, at initialization,
if the total number of eligible inequalities exceeds, a random subset is selected and added to the LP. Hence, for a fair comparison, we run each instancetimes and report average time and average number of iterations along with standard deviations.
We set the optimality tolerance. Results are summarized in Table3.
As can be seen from the table, even with the cut generation scheme,Gurobiis unable to solve instances with more thandata points withinhours of CPU time.
On the other hand, both first-order method LP solvers solve all instances within the time limit.Surprisingly, in all cases, the LP relaxation is tight; i.e., the optimality gapupon termination is smaller than the optimality tolerance.Moreover,cuPDLP-Cis on averagetimes faster thanPDLP. As a byproduct, we can verify that out ofinstances, the SDP relaxation is tight forinstances only. In addition,k-means++’s solution is optimal forinstances.

k-means++’s solution is not globally optimal for all five random trials.

Problem (1.1) withis not tight.

The SDP relaxation, i.e., Problem (PW) is tight.

Motivated by the impressive performance ofIterative LP solverfor small data sets, next we consider larger data sets. We consider the first-order LP solvers only, asGurobiis unable to solve any of the larger instances. Results are shown in Table4. WhilecuPDLP-Csolves allinstances,PDLPsolves onlyinstances within the time limit. Moreover, for the latterinstances,cuPDLP-Cis on averagetimes faster thanPDLP.Again, for allinstances, the LP relaxation is tight; i.e., the optimality gapupon termination is smaller than the optimality tolerance.On the contrary, the SDP relaxation is tight only forinstances. Moreover, thek-means++’s solution is optimal forinstances.

k-means++’s solution is not globally optimal for all five random trials.

Problem1.1withis not tight.

Unable to verify whether Problem (1.1) withis tight or not due to exceeding the time limit.

The SDP relaxation, i.e., Problem (PW) is tight.

In Tables3and4, all instances for which Problem (1.1) withis not tight or is not solvable withinhours are marked. As we mentioned before, by letting, all such instances are solved to global optimality within the time limit, indicating the importance of selectingin a dynamic fashion. Recall that by construction, ifor if an instance is solved in one iteration, then we have.
For the remaining problems, in Table5, we show the distribution ofassociated with all inequalities added to the LP in the course ofIterative LP solver. As can be seen from this table, while for the majority of the inequalities we have, for some instances adding inequalities withis beneficial as well.

The termination criterion forIterative LP solveris based on the optimality tolerance. An alternative strategy is to terminate when the LP solution is a partition matrix. In this case, no upper bounding is needed. However, we need to solve each intermediate LP with high accuracy implying that we cannot rely on early stopping and safe lower bounds. The impact of this alternative strategy (labeled as “without upper bounding”) on the performance ofIterative LP solveris detailed in Table6. In this table, we include the data sets for which the LP solution in the first iteration ofIterative LP solveris not a partition matrix; as otherwise the two strategies coincide. As can be seen from this table, without upper bounding,cuPDLP-Csolves onlyout ofinstances within the time limit. Moreover, for theseinstances, the algorithm with upper bounding is on averagetimes faster than the one without upper bounding.

Finally, in Table7we list the average time spent for different components ofIterative LP solverwithcuPDLP-Cas the LP solver.
As can be seen from this table, the vast majority of the time is spent for solving the LPs. The initialization time is particularly big for large scale problems in which case we need to select a random subset of inequalities that are binding at thek-means++solution. Other major components such as separation time and rounding time are rather insignificant.

We conclude this paper by acknowledging that further work is needed to makeIterative LP solverrelevant for larger data sets. Of course, as first-order LP solvers become faster and more powerful GPUs become available, one can solve larger instances. Exploring sketching techniques[23,2]for tackling larger data sets is an interesting topic of future research as well. An alternative strategy is to incorporate the proposed LP relaxation within mixed-integer nonlinear programming solvers[14]to expedite their convergence.

SECTION: References