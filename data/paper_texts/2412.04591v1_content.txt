SECTION: MetaFormerHigh-fidelity Metalens Imaging via Aberration Correcting Transformers

Metalens is an emerging optical system with an irreplaceable merit in that it can be manufactured in ultra-thin and compact sizes, which shows great promise of various applications such as medical imaging and augmented/virtual reality (AR/VR). Despite its advantage in miniaturization, its practicality is constrained by severe aberrations and distortions, which significantly degrade the image quality. Several previous arts have attempted to address different types of aberrations, yet most of them are mainly designed for the traditional bulky lens and not convincing enough to remedy harsh aberrations of the metalens. While there have existed aberration correction methods specifically for metalens, they still fall short of restoration quality. In this work, we propose MetaFormer, an aberration correction framework for metalens-captured images, harnessing Vision Transformers (ViT) that has shown remarkable restoration performance in diverse image restoration tasks. Specifically, we devise a Multiple Adaptive Filters Guidance (MAFG), where multiple Wiener filters enrich the degraded input images with various noise-detail balances, enhancing output restoration quality. In addition, we introduce a Spatial and Transposed self-Attention Fusion (STAF) module, which aggregates features from spatial self-attention and transposed self-attention modules to further ameliorate aberration correction. We conduct extensive experiments, including correcting aberrated images and videos, and clean 3D reconstruction from the degraded images. The proposed method outperforms the previous arts by a significant margin. We further fabricate a metalens and verify the practicality of MetaFormer by restoring the images captured with the manufactured metalens in the wild. Code and pre-trained models are available athttps://benhenryl.github.io/MetaFormer.

SECTION: 1Introduction

Metalens is an advanced optical device that leverages nanoscale engineering to manipulate light. Unlike traditional lenses relying on glass or plastic surfaces, the metalens harnesses a metasurface, a thin optical element consisting of nanostructures modulating optical properties such as phase, amplitude, and polarization. It can be fabricated in ultra-thin and compact sizes compared to the traditional bulky lens, making it an ideal option for various domains, including the medical industries, consumer electronics, and augmented/virtual reality (AR/VR)[95,75,87,31,67,54,43,52]. Despite its great potential for miniaturization, existing metalenses have faced critical challenges, such as aberrations and distortions in the captured images, hampering their broad uses across diverse applications.

Aberrations in optical systems occur due to practical constraints and physical properties of lens materials. Different types of aberrations arise for different reasons,e.g.chromatic aberrations are produced when lights with different wavelengths refract at slightly different angles when they pass through a lens, resulting in being focused at different points.
Another type of aberrations, spherical aberrations, occur when rays transmitting through the edges and center of the lens focus at different locations.

Several previous works[10,23,28]have attempted to correct these aberrations, primarily addressing them for general commercial cameras and not effective enough to mitigate intense aberrations of metalens. On the other hand,Tseng et al.[80]andChakravarthula et al.[9]devised aberration correcting methods for the metalens.Tseng et al.[80]adopted convolutional neural networks (CNNs) andChakravarthula et al.[9]employed a diffusion model[36,74]for restoration. While promising, these methods have yet to reach the desired restoration quality.

Besides the aberration corrections, numerous studies have tackled other image degradations such as blur, noise, raindrops, haze,etc. A line of research introduced[39,65,46,47]generative models[29,74,36]to produce clean images from degraded images, and other arts[60,93,63,32,49,91]adopted CNNs. Another line of works[92,55,13,16,81]harnessed Vision Transformer (ViT)[22]for image restoration. Although they have shown prominent results, straightforwardly applying them to metalens-induced aberration correction is undesirable. This is because metalens can manipulate multiple optical properties, resulting in complex aberration patterns that vary in response to different wavelengths, which lead to severe chromatic aberrations. Moreover, aberrations in metalens images can amplify noise and distort intensity variations across channels.
These aberrations are more complicated than the degradations encountered in conventional image restoration tasks, and those approaches devised for general image restoration lack in capacity to adapt to these distortions.

Among diverse frameworks, ViT-based approaches have proven effective for low-level computer vision tasks.
ViT can effectively capture global context by leveraging a self-attention that allows each pixel in an image to communicate with all others, unlike CNNs, where pixels can interact only with neighboring pixels.
This capability for modeling global relationships is especially crucial for image restoration, where understanding the overall structure plays a key role in removing noise or artifacts while preserving intricate details. Moreover, self-attention modules enable dynamic weighting of features, empowering the model to adaptively focus on the most relevant regions. While ViT has demonstrated its competence in various tasks, its potential for restoring metalens images remains largely unexplored.

It motivated us to propose MetaFormer, an aberration correction Transformer tailored for correcting the aberration of metalens-captured images. Particularly, we design a Multiple Adaptive Filters Guidance (MAFG) that employs several Wiener filters[86], which have been widely used to reduce noise in signals.
The advantages of MAFG are threefold: first, the representations deconvolved with different Wiener filters show diverse degrees of noise suppression and recovered fine details, allowing them to be processed complementarily in the restoration network. Secondly, MAFG provides a mechanism for handling unknown noise distributions. The Wiener filter penalizes noise based on the signal-to-noise ratio (SNR), which is usually obscure in real-world scenarios. By incorporating multiple Wiener filters with varied noise suppression parameters, MAFG can effectively address unknown noise distributions. Lastly, we adapt these parameters based on the image intensity, allowing for flexible handling across a broad range of images.

Furthermore, we extend Restormer[92]by incorporating both spatial and transposed self-attention and aggregating features from different attention modules, proposing a Spatial and Transposed self-Attention Fusion (STAF) module. Various previous arts[13,16,81]adopted encoder-decoder architecture with two types of attention mechanisms (spatial and transposed), but they apply them alternatively in both encoder and decoder, though they have different roles. The encoder focuses on capturing global structures, whereas the decoder emphasizes more on reconstructing fine details of images. Motivated by this, we implement spatial and transposed self-attention in parallel and fuse their features to guide the encoder in capturing global context and the decoder in enhancing fine reconstruction.

We conducted extensive experiments across various imaging scenarios to validate the effectiveness of the proposed MetaFormer. First, on synthetic aberrated images generated using point spread functions (PSF) from real metalenses, MetaFormer demonstrated notable performance improvements, outperforming the baseline model by 3.24 dB in PSNR and achieving state-of-the-art restoration quality across multiple evaluation metrics. Second, we extended and applied the proposed techniques to a video restoration model, where results further confirmed the proposed technique’s superiority in aberration correction in videos. Third, we tested MetaFormer on 3D reconstruction tasks using multi-view images (3D Gaussian Splatting[41]), demonstrating its capability to maintain both high-quality aberration correction and 3D consistency across views. Finally, we fabricated metalens using the open-source point spread function (PSF) of a metalens[80]and evaluated the proposed method on the images captured with the manufactured metalens. The corrected images demonstrated promising results, revealing new possibilities in metalens imaging.

To sum up, our contributions are as follows:

We devise a Multiple Adaptive Filters Guidance (MAFG) that employs several Wiener filters operating adaptively to various images to enrich features of representations.

We design a Spatial and Transposed self-Attention Fusion (STAF) module that applies spatial and transposed self-attention well aligned with encoder-decoder architecture.

We fabricated a metalens with the open-source PSF and constructed a metalens-captured image dataset.

We enhanced the baseline model by 3.24 dB in PSNR, accomplishing state-of-the-art restoration performance.

SECTION: 2Related Works

SECTION: 2.1Metalens

Recent advancements in nanofabrication have enabled the use of ultra-thin metasurfaces consisting of subwavelength scatterers that can independently adjust the amplitude, phase, and polarization of incident wavefronts, allowing for greater design flexibility[24,57,62,27].
This flexibility has motivated research into applications such as flat meta-optics for imaging[18,88,2,58,12,9,80,27], polarization control[4], and holography[97].
Notably, work byTseng et al.[80]introduced a differentiable design for meta-optics achieving high image quality within a 0.5 mm aperture but with limitations in field performance beyond 40∘.
More recently,Chakravarthula et al.[9]introduced a metalens array design that enhances image quality across a full broadband spectrum over a 100∘field of view (FoV) without increasing the back focal length, potentially allowing for one-step fabrication directly on the camera sensor coverglass.
However, existing meta-optics still face challenges with chromatic and geometric aberrations, limiting broadband imaging capabilities[88,57,3,83].
Although dispersion engineering methods have mitigated some chromatic aberrations[66,73,5,42,82], they remain restricted to very small apertures[68].
In this work, we introduce a framework to compensate for aberrations and overcome the challenges faced by such meta-optics imaging cameras.

SECTION: 2.2Image Restoration models

Image restoration aims to recover high-quality images from degraded observations. Traditionally, Wiener deconvolution[86]recovered images by solving complex inverse problems. However, it often struggled with noise and complex degradations, limiting their real-world applications. Afterward, deep learning has revolutionized image restoration with data-driven methods. Vision Transformer, in particular, (ViT)[22]has achieved remarkable performances in image restoration tasks, including denoising[81,11,85,26,92,53], deblurring[85,92,59,45,79], and super resolution[14,15,55,77,61,16], by exploiting self-attention that can capture global interactions effectively.

Restormer[92]stood out with its superior performance, introducing transposed self-attention mechanisms that apply self-attention across the channel of an image, enabling efficient processing of high-resolution images.
However, it might limit the exploitation of spatial information. X-Restormer[13]further incorporated spatial self-attention mechanisms into Restormer to handle these shortcomings.
This allowed the model to better capture spatial dependencies, still there is room for improvement in that it deployed attention blocks identically for encoder and decoder though they have different roles when learning the representation.

SECTION: 2.3Aberration correction models

Optical aberration arises from imperfections in optical systems, with chromatic and spherical aberrations being frequently observed in the metalens imaging process. Chromatic aberration, where different wavelengths of light focus at different points, leads to color fringing, and spherical aberration occurs when the rays passing through the edges and center of the lens are focused at different points, resulting in blurriness. Extensive research has been conducted to correct these aberrations, and they can be broadly categorized into non-blind and blind aberration correction.

Non-blind approaches[38,40,72,33,17,21,96]require PSF calibration before training the models. Once the PSF is well calibrated, it can be consistently applied across various images, as optical aberrations are independent of the scene content in general[89,90].
Especially, DWDN[21]leveraged Wiener deconvolution[86]in the feature space for multi-scale refinement andLi et al.[51]proposed a PSF-aware network using patch-wise deconvolution with deep priors to cope with spatially varying aberrations.

Blind aberration correction approaches[70,71,78,90,23,28]correct aberrations through estimating aberration kernels and true images without any prior PSF information.Eboli et al.[23]adopted a small CNN for color correction, andGong et al.[28]utilized the physical properties of lenses to improve image quality. These blind methods can be applied generally without demanding preprocess (e.g., PSF calibration), but restoration quality may be unsatisfactory due to uncertainties when estimating PSF.

On the one hand, there have been aberration correction models designed for metalens imaging[80,9].Tseng et al.[80]employed CNNs and Wiener filters for restoration, but it struggled to restore the fine details of the image because of the limited receptive field of CNNs.Chakravarthula et al.[9]introduced diffusion models[36,74]with Wiener filter to manage aberrations, yet it can introduce unrealistic details due to the stochasticity of the diffusion model.

Aberration correction poses unique challenges in the context of metalens imaging, but ViT has not been fully examined to address them. In this paper, we propose leveraging the ViT, adapting attention mechanisms to deal with complex aberration patterns in metalens imaging, to overcome the limitations faced by CNN and diffusion models.

SECTION: 3Methods

Fig.2depicts the overall architecture of MetaFormer. It consists of Vision Transformer (ViT) as a backbone network for restoration (Sec.3.2), Multiple Adaptive Filters Guidance (MAFG) (Sec.3.3), and Spatial and Transposed self-Attention Fusion (STAF) blocks (Sec.3.4). MetaFormer produces a residual image which is added to the input aberrated image to acquire the aberration-corrected image.

SECTION: 3.1Preliminary

Metalens ImagingAn imaging formulation using a spatially invariant point spread function (PSF) can be written as follows111While a more general approach involves a spatially varying PSF, this work focuses on restoring aberrated images using a spatially invariant PSF.,

wheredenotes the convolution operator, andis the observed image, which can be represented as the convolution of a clean image or an objectwith a single PSF uniformly across the entire image. Then the noise, such as shutter and natural noise, is added to generate the captured image.
Convolution can be replaced with simple element-wise multiplication whenandare in the frequency domains[8]. The PSF of metalens varies significantly with wavelength because different colors focus at different distances from the metalens. This produces color fringing and a low contrast in the captured image.

SECTION: 3.2Image Restoration using Vision Transformer

Chromatic aberrations of metalens differ across different wavelengths of light, implying that the aberrations affect each channel of the image distinctively. Vision Transformer (ViT) can model these channel-specific differences, leveraging their multi-head attention to process each channel properly. Additionally, metalens-induced aberrations can degrade the images at various scales, affecting both high-frequency details (e.g., color fringes at sharp edges) and low-frequency components (e.g., gradual intensity shifts). ViT can represent multi-scale features, which can help in correcting different types of distortions.

As discussed, various ViT-based approaches[81,55,85,59,15,77,16,92]have been developed for restoring images captured by traditional cameras. Among these, Restormer[92]has demonstrated remarkable restoration performance across multiple tasks, utilizing an encoder-decoder architecture with transposed self-attention (TA), a novel feed-forward network with gating mechanism[19], and depth-wise convolutions[37]. Nevertheless, applying Restormer directly to metalens image restoration does not yield satisfactory outcomes, as metalenses introduce more complex aberrations and distortions than conventional lenses, demanding a more sophisticated and tailored solution. Therefore, we employ Restormer as the backbone network and incorporate novel aberration-correcting modules to better align ViT with the specific challenges of metalens imaging. Additionally, we train ViT non-blindly, harnessing point spread function (PSF) for training, as the PSF for metalens can be easily obtained via calibration methods compared to the traditional cameras.

SECTION: 3.3Multiple Adaptive Filters Guidance

Before feeding the aberrated input images to the restoration network, we deconvolve the image first with multiple Wiener filters[86]. Wiener filter is a linear filter that has been used to reduce noise in signals, especially in the context of image processing and restoration[20,21,96,98,69,35]. It minimizes the mean square error between the estimated and true signal, providing an optimal approach to reconstructing signals degraded by noise. It can also balance noise suppression and detail preservation by exploiting the signal-to-noise ratio (SNR), which makes it particularly effective in image restoration tasks. Given a PSF, the Wiener filter in the frequency domain,, is defined as follows,

whereis an indexing operator, andandare horizontal and vertical spatial frequency respectively,is a fourier transformed PSF,is a complex conjugation of, andis an inverse of SNR. Note that each color channel has a different PSF, but we do not explicitly denote the channel for brevity, e.g.or.

As shown inFig.3,plays a pivotal role in penalizing the noise; a greaterleads to a smoother representation, whereas a smallercan preserve more high-frequency details, albeit with the increased noise.
Inspired by this, we propose to use multiple Wiener filters to guide aberration correction with several distinct representations.
It is not feasible to obtain an optimal Wiener filter with accurate SNR as noise distribution is unknown in the real world.
Instead of estimating the noise distribution, we adopt multiple Wiener filters with different.
We useWiener filters and deconvolve the input image to yielddifferent representations—some focused on noise removal, others with fine information.
Various representations are fed to the restoration model, and they can enrich the features complementarily, which in turn improves aberration correction.

Furthermore, we extend multiple Wiener filters to Multiple Adaptive Filters Guidance (MAFG), illustrated inFig.4, which determinesadaptively considering the image intensity. Image with higher intensity tends to have better signal quality, so brighter channels often experience less noise and are less sensitive to noise[25]. Thus, we penalize noise less and capture more information for bright images by adjustingwith the image intensity. Also, we treat each channel differently to avoid suppressing high-SNR details unnecessarily. This is important when addressing metalens-captured images since metalens exhibit wavelength-dependent aberrations where each color channel suffers differently due to the different chromatic responses. Based on this observation, we slightly modifyEq.2for MAFG as follows:

whereis the index ofdifferent filters (),is the average intensity of each color channel, andis a hyperparameter for scaling.
The proposed MAFG is adaptive to the input images, which allows flexible responses to the various images.

We obtaindifferent deconvolved imagesusing MAFG with the following equation:

wherefftandifftare Fast Fourier Transform (FFT) and the inverse FFT[8], respectively. ‘’ is the Hadamard product,andare pixel coordinates.are concatenated along the channel dimension and fed to the restoration network. It is also worth mentioning that the MAFG can be applied to any other restoration model, including video restoration, with negligible computational costs.

SECTION: 3.4Spatial and Transposed Self-Attention Fusion

As aforementioned, Restormer[92]relies solely on transposed self-attention to generate an attention map, often resulting in weak spatial context encoding.
To improve context modeling, a variety of subsequent studies[13,16,81]leveraged both SA and TA, by alternatively applying them as depicted inFig.5(a).
In particular, X-Restormer[13]added spatial self-attention to Restormer and proved the enhanced spatial context encoding capability.

Nevertheless, there remains room for improvement, as existing methods apply SA and TA uniformly across both encoder and decoder stages despite their distinct objectives. The encoder primarily focuses on capturing a global context, emphasizing the overall structure and relationships within the images. This is critical in image restoration tasks since understanding the broader contexts helps identify patterns and features that may be corrupted in the degraded images. In contrast, the decoder mainly aims to recover local details and textures essential for high-fidelity restoration.

Therefore, we propose a Spatial and Transposed self-Attention Fusion (STAF) module to further ameliorate image restoration. STAF implements SA and TA separately, not alternatively, and fuses SA and TA features with different weights in the encoder and decoder, as depicted inFig.5(b).
In detail, the outcome of SA and TA after feed-forward networks (denoted as,, respectively andare height, width, and channel of the input feature) are concatenated along channel dimension to obtain concatenated feature.is then fed to a small CNN, consisting of two 11 convolution layers with GELU activation[34], to mix features and halve channel dimension to, and produce, which is then treated differently in encoder and decoder.
At the encoder side, we employ average pooling on spatial dimensions to efficiently reduce resolution while preserving crucial feature representations and apply the sigmoid function toto obtain weight matrix.
On the other hand,is averaged along the channel dimension to selectively emphasize the most relevant feature maps and apply the sigmoid function to produce weight matrixfor the decoder.
This allows the decoder to prioritize different spatial areas, for instance, regions with more aberration can receive higher attention to be corrected.
In a nutshell,andcan be computed as follows:

whereis the sigmoid activation, andandare average pooling along the height and width dimension and channel dimension, respectively.andare then used to compute the final feature of each block whereandare mixed properly for the encoder and decoder with the equations:

wheredenotes the Hadamard product, and the index for each layer is omitted for brevity. The residual image obtained after passing through all STAF blocks is added
to the, an image deconvolved withwhereis a median of, to get the aberration-corrected image.

We adopted Overlapping Cross Attention (OCA), which partitions query matrix with non-overlapping windows and key and value matrices with overlapping windows, from HAT[15]for our SA module, following X-Restormer[13].
In addition, we employed the Gated-Deconv Feed-Forward network from Restormer[92]for our feed-forward network (FFN).
However, any SA and FFN can be a valid option.

Fig.6shows the LAM[30]analysis which implies that STAF can restore the target region entailing more information, leading to better restoration quality. X-Restormer[13]also employs both SA and TA but aggregates the features in a simple manner, resulting in lacking information involvement and inferior aberration correction.

SECTION: 4Experiments

SECTION: 4.1Experimental Settings

We trained MetaFormer for 300K iterations with a batch size of 8 on 4 RTX A6000 GPUs. We set the number of filtersand noise penalizing scalers. We followed the overall experimental settings of Restormer[92]and X-Restormer[13]. Please refer to Restormer and X-Restormer for more details.
We constructed synthetically aberrated datasets followingEq.1with metalens PSF fromTseng et al.[80], whereinEq.1is a sum of two different per-pixel noises, Gaussian and Poisson noise.

We conducted experiments across various tasks: aberration correction on synthetic (Sec.4.2) and real (Sec.4.3) images, video aberration correction (Sec.4.4), and 3D reconstruction with aberrated images (Tab.2). Ablation studies are delivered inSec.4.6.
Detailed experimental settings and more experimental results are described in the supplementary material.

SECTION: 4.2Synthetic Image Aberration correction

We synthesized aberration on Open Image V7[48,7]dataset with PSF fromTseng et al.[80]and sampled 12K and 1K images for training and validation, respectively222Training codes and pre-trained weights forChakravarthula et al.[9]were not available, so we could only compare withTseng et al.[80]..Tab.1andFig.7describe the quantitative and qualitative results on the aberrated Open Image V7 dataset, respectively.Eboli et al.[23], a blind deconvolution approach, failed to compensate for the aberrations, and other baseline models, including Restormer[92], struggled with restoring fine details. Meanwhile, the proposed method could ameliorate Restormer with a noticeable margin and outperformedTseng et al.[80]designed for metalens images.

SECTION: 4.3Real Image Aberration Correction

We fabricated a metalens with the PSF fromTseng et al.[80]as shown inFig.1(Left) and captured images to construct a real image dataset. It consists of 220 images, where 200 images are involved in fine-tuning the pre-trained MetaFormer, and 20 images are kept for validation.Fig.1(Right) shows the qualitative results on the metalens-captured images. Even though MetaFormer is trained only with the synthetic data, it can correct the aberration of the real image after fine-tuning with a small real dataset, which eases the cost of constructing a large-scale real image dataset.
Please refer to the supplementary material for details about metalens fabrication, image capture setup, model fine-tuning, and more experimental results.

SECTION: 4.4Video Aberration Correction

In this section, we assessed our method for the video aberration correction task. We employed VRT[56]as a baseline video restoration model and trained it with synthetically aberrated DVD[76]dataset. We incorporated the MAFG into VRT optimization (VRT + MAFG) and trained VRT with the output of pre-trained MetaFormer (VRTw/Ours) fromSec.4.2.Tab.2andFig.8show that VRT struggled with correcting the aberration of the input video while simply applying the proposed MAFG to VRT (VRT + MAFG) could mitigate the severe aberration and restore the fine details. Furthermore, VRT trained with MetaFormer outputs (VRTw/Ours) could enjoy both temporal consistency and clean representations, reaching the best restoration quality.

SECTION: 4.53D Reconstruction with Aberrated Images

We further extended our method to clean 3D reconstruction, training 3D Gaussian Splatting (3D-GS)[41]with the sets of multi-view aberrated images. We embedded MetaFormer into 3D-GS training pipeline as delineated inFig.9. Specifically, we simulated aberration during 3D-GS training usingEq.1to constrain 3D-GS learning clean representations.
The output () of the pre-trained MetaFormer, which is frozen during 3D-GS training, is involved in supervising the rendered image () with a losswhereis a reconstruction loss function defined in 3D-GS.
The aberration-simulated imageis used to compute, with the training image. We use a sum of these losses to guide 3D-GS learning clean features from the aberrated images.

Tab.3andFig.10provide the quantitative and qualitative results on the 3D reconstruction task, evaluated on LLFF[64]and Tanks&Temples[44]dataset, where we usedTruckandTraindata in Tanks&Temples dataset following 3D-GS.
While 3D-GS fails to render clean images, the proposed pipeline that combines 3D-GS and MetaFormer (3D-GS + MetaFormer) shows sound reconstruction performance, even outperforming 3D-GS optimized with the restored outputfrom MetaFormer (3D-GSw/MetaFormer).
This is because 3D-GS + MetaFormer can learn clean representation viaand meet the multi-view consistency withsimultaneously, resulting in better reconstruction quality.

SECTION: 4.6Ablation Studies

We conducted ablation studies of the proposed methods.Tab.4shows the comparison of MAFG and STAF module. Applying a single Wiener filter improved Restormer[92]by 1.80 dB in PSNR, but it elevated more when coupled with MAFG and STAF.
Other ablation studies are delivered in the supplementary material.

SECTION: 5Conclusion

We present MetaFormer, an aberration correcting transformer tailored for metalens imaging. We harness a Vision Transformer (ViT) that has not been fully explored in metalens imaging.
We devise Multiple Adaptive Filters Guidance (MAFG) to enrich the feature of the aberration correcting network.
The proposed Spatial and Transposed self-Attention Fusion (STAF) module further boosts aberration correction by implementing SA and TA separately and fusing the outcomes differently in the encoder and decoder. The comprehensive experiments across diverse tasks covering correcting aberrated images, videos, and 3D reconstruction with aberrated images show the proposed method achieving state-of-the-art performance on various tasks.
We also fabricate a metalens and restore the images captured in the wild, proving the practicality of MetaFormer.

SECTION: References

MetaFormer: High-fidelity Metalens Imagingvia Aberration Correcting Transformers:Supplementary Materials

SECTION: Appendix AMetalens Fabrication

SECTION: A.1Fabrication Details

A metalens with a diameter of 500m and a focal length of 1 mm was designed based on the optimization of a polynomial phase equation[80]. The SiN meta-atom library was generated using rigorous coupled-wave analysis (RCWA) simulations for circular pillars with a height of 750 nm. The widths of the selected meta-atoms ranged from 100 to 300 nm, with a lattice period of 350 nm.

A 750 nm thick SiN layer was deposited onto a SiO2substrate using plasma-enhanced chemical vapor deposition (PECVD; Oxford, PlasmaPro 100 Cobra) to fabricate the designed metalens. A 200 nm thick positive photoresist layer (AR-P 6200.09, Allresist) was spin-coated at 4000 RPM. The pattern of circular nano-pillar meta-atoms was then transferred onto the positive photoresist using electron beam lithography, as shown inFig.11(a), with a dose of 3.75 C/m2. To prevent charging, 100L of ESPACER (RESONAC, 300Z) was spin-coated at 2000 RPM for 30 s.

The exposed resist was developed in a 1:3 solution of methyl isobutyl ketone (MIBK)/isopropyl alcohol (IPA) for 11 min. Subsequently, a 40 nm thick chromium (Cr) layer was deposited as a hard mask using an electron beam evaporator (Fig.11(b)). The unexposed photoresist was removed through a lift-off process in acetone at room temperature for 1 hour, leaving the Cr hard mask intact. Patterning was finalized using inductively coupled plasma (ICP) etching (STS, multiplex ICP) with SF6(15 sccm) and C4H8(40 sccm) gases for 10 min. Finally, the Cr hard mask was removed using a chromium etchant (TRANSENE, CE-905N) for 5 min. The fabricated metalens is illustrated inFig.12.

SECTION: A.2Optical Setup for Imaging

An image capture setup is illustrated inFig.13.
An optical microscope system was set up to obtain images through the metalens. The images displayed on a 5.5-inch FHD display (FeelWorld, LUT5) were captured using a CMOS camera (Allied Vision, Alvium 1800 U-235c) coupled with a magnification system consisting of a 20x objective lens (Olympus, UPlanFL N 20x) with 0.5 NA and a tube lens (Thorlabs, TTL180-A).

The metalens was positioned such that its focal plane coincided with the focal plane of the objective lens using a linear motorized stage (Thorlabs, DDS100). Camera exposure time was adjusted using a white image prior to recording to prevent saturation.

The point spread functions (PSFs) were then acquired using the same setup with 450 nm laser (Thorlabs, CPS450), 532 nm laser (Thorlabs, CPS532), and 635 nm laser (Thorlabs, CPS635) for calibration and training of the model.

SECTION: Appendix BAdditional Experiments and Results

In this section, we provide more experimental results on each task that were not delivered in the main paper due to the page limit. We also attached videos of the 3D reconstruction and video aberration correction task results.

Evaluation Metrics.We assessed the proposed method with various evaluation metrics including Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM)[84], and Learned Perceptual Image Patch Similarity (LPIPS)[94].

SECTION: B.1Synthetic Image Aberration Correction

Fig.14shows extra qualitative results on the aberrated Open Image V7[48,7]dataset. The proposed method could mitigate severe aberrations and restore fine details. We also provide noise maps inFig.15and Restormer[92]showed noises on textureless regions or failed to match contrast, where noise and low contrast are especially more prominent in metalens imaging. On the other hand, MetaFormer could alleviate noises and achieve the correct contrast, reaching better results on multiple evaluation metrics.

SECTION: B.2Real Image Aberration Correction

Model Fine-tuning Details.We fine-tuned our model, initially trained on synthetically aberrated images inSec.4.2in the main paper, using real images captured with the fabricated metalens. The fine-tuning dataset includes 200 real images for training and 20 images for validation. Despite the limited dataset size, fine-tuning MetaFormer for 3,500 iterations
could produce promising and appealing results.

Results.Fig.16presents the qualitative results of images captured with the fabricated metalens and restored images using the proposed method. The captured images show harsh degradations and aberrations but MetaFormer could successfully restore fine details and textures. Such outcomes can demonstrate the adaptability of the proposed method to real-world scenarios.

SECTION: B.3Video Aberration Correction

Fig.17shows additional qualitative results on the aberrated DVD[76]dataset. Similar toFig.8in the main paper, the proposed method could effectively address aberrations in the video and produce aberration corrected representations.

SECTION: B.43D Reconstruction with Aberrated Images

More Method Details.We used the combined loss function, consisting ofandto train the proposed ‘3D-GS + MetaFormer’as following:

whereis a hyperparameter controlling weight of. We used the pre-trained MetaFormer trained with the aberrated Open Image V7[48,7]dataset fromSec.4.2in the main paper to get aberration corrected images,.

Experimental Settings.For the LLFF[64]and Mip-NeRF360[6]datasets, we set the densification thresholdof 3D Gaussian Splatting (3D-GS) model[41]to, the pruning thresholdand the loss weighting hyperparameter. We also inactivated periodical opacity reset for the LLFF dataset to preserve more points in the forward-facing dataset[50]. We set,, andfor the Tanks&Temples[44]dataset.

Results.We conducted experiments on the aberrated Mip-NeRF360[6].Tab.5andFig.18show the quantitative and qualitative results, respectively. Training 3D-GS with the output of MetaFormer (3D-GSw/Ours) produced aberration alleviated representations but it suffered from artifacts due to the lack of multi-view consistency. In contrast, extra multi-view consistency guidance viacould aid in mitigating those artifacts from 3D inconsistency and yield more clear representations.

SECTION: B.5More ablation studies

We conducted further ablation studies to demonstrate the effectiveness of the proposed methods. We evaluated all models after training 15K iterations.

Comparison on the Number of Filters for MAFG.Tab.6shows the results of comparing different numbers of involved filters in MAFG. The more filters we employed, the aberration correction performance was improved as it could enrich the input of the restoration network more. However, the performance dropped when we leveraged 4 filters in MAFG. We assume this is because excessive usage of filters may introduce over-redundancies and conflicting correlations within the representations, hampering correcting aberrations. Thus, we harnessed 3 filters in our experiments.

Ablation on Using Intensity for MAFG.We conducted an ablation study on using channel intensity information when defining filters for MAFG. We obtained multiple adaptive filters followingEq.3(MAFG) and simple multiple filters usingEq.2(MAFG w/o Intensity) in the main paper. We ran an experiment on a different test dataset, the aberrated DIV2K[1]dataset, to validate the adaptability of the proposed MAFG to unseen data. Exploiting channel intensity is simple, but it could impart the model flexibility to different images and facilitate aberration correction, especially showing a noticeable improvement in LPIPS as described inTab.7. We believe that a more sophisticated approach to incorporating image information, such as image intensity, into the filters can further enhance the restoration process.