SECTION: A Powerful Building-Block for Imaging, Inverse Problems,and Machine Learning
Denoising, the process of reducing random fluctuations in a signal to emphasize essential patterns, has been a fundamental problem of interest since the dawn of modern scientific inquiry. Recent denoising techniques, particularly in imaging, have achieved remarkable success, nearing theoretical limits by some measures. Yet, despite tens of thousands of research papers, the wide-ranging applications of denoising beyond noise removal have not been fully recognized. This is partly due to the vast and diverse literature, making a clear overview challenging.

This paper aims to address this gap. We present a clarifying perspective on denoisers, their structure, and desired properties. We emphasize the increasing importance of denoising and showcase its evolution into an essential building block for complex tasks in imaging, inverse problems, and machine learning. Despite its long history, the community continues to uncover unexpected and groundbreaking uses for denoising, further solidifying its place as a cornerstone of scientific and engineering practice.

SECTION: Introduction
Like most things of fundamental importance, image denoising is easy to describe, and very difficult to do well in practice. It is therefore not surprising that the field has been around since the beginning of the modern scientific and technological age - for as along as there have been sensors to record data, there has been noise to contend with.

Consider an image, composed of a “clean” (smooth) component, and a “rough” or noisy component, which we take to be zero-mean Gaussian white noise of variance, going forward:

where all images are scanned lexicographically into vectors. The aim of any denoiser is to decompose the imageback into its constituent components - specifically, to recover an estimate of, the underlying signal, by applying some operator (denoiser), parameterized by someas follows:

whereis a monotonic function of the noise variance, and therefore controls the “strength” of the denoiser.

As the description above indicates, a denoiser is not a single operator but aof boundedmaps. We expect “good” denoisers to have certain naturally desirable properties, which alas in practice, many do not. For the sake of completeness, and as a later guide for how to design good denoisers, we call a denoiserif it satisfies the following properties:

To convey some intuition for this property, consider the linear case. When a denoiser is linear:, we always require the matrixto be row-stochastic (meaning the rows sum to) in order to preserve the mean local brightness. Ideally, we also requireto be symmetric, which has the added advantage that the denoiser isin the mean-square sense. Propertyextends these notions to more general nonlinear denoisers.

The conservation Property 2 guarantees that the ideal denoiser is the gradient of a scalar field.This also implies thatis a Lipschitz map with some constant:

We naturally expectfor all; therefore, this Lipschitz condition implies. A non-expansive denoiser would require that. In the statistics literature, such operators are calledsmoothers.

The above properties impose the structure of anspaceon the class of ideal denoisers. Namely, any affine combination of ideal denoisers is also ideal. That is, if we let

it is easy to verify that Propertiesandare satisfied.Ideal denoisers satisfy:

,

,

Closed-ness under affine linear combination.

It is an unfortunate fact that in practice, most denoisers are not ideal. But this should not bother the reader, as by studying the broader class of denoisers we will learn how the above desirable properties are manifested or desired in practice, and which practical denoisers (approximately or exactly) satisfy them.

Rather than a survey of image denoising, this work focuses on defining ideal denoisers, their properties, and their connections to statistical theory and machine learning. We then demonstrate how these powerful components can serve as building blocks in various applications. Readers interested in a historical overview of image denoising are encouraged to consult the excellent resources in.
Our analysis specifically considers an additive white Gaussian noise model due to its broad applicability and relevance to the applications explored herein. A deeper examination of various noise models can be found in.

SECTION: Denoising as a Natural Decomposition
One of the remarkable aspects of well-behaved (even if not ideal) denoising operators is that we can employ them to easily produce a natural multiscale decomposition of an image, with perfect reconstruction property. To start, consider a denoiser. We can write the obvious relation:

The first term on the right-hand side is a(or denoised) version of, whereas the second term in the brackets is the residualwhich is an ostensibly “high-pass” version. Next, we can apply the same decomposition repeatedly to the already-denoised components:

wheredenotes the operator appliedtimes (i.e. a diffusion process), and(i.e. a residual process). For any, this-th order decomposition splitsinto a smooth componentand a sequence of increasingly fine-detail components.

It is important to note that applying the operatorsmultiple times does not necessary result in a completely smooth result. For instance, if we repeatedly apply a bilateral filter, the result is aconstant image. The process we’ve described here has been called, in certain instances, adecomposition in, mainly in the context of total-variation denoising. Our point of view is considerably more general, applicable todenoiser.

Returning to the decomposition above, it empowers us to do practically useful things. For instance, truncating the residual terms at some, we can smooth out certain high frequency features. More generally, we can null out any component in the sum; or better yet, recombine the components with new coefficients to produce aor modified image, as follows:

This approach was generalized and used in a practical setting into produce a wide variety of image processing effects, built on a base of well-established (at the time) non-local means denoisers. This is illustrated in Figure.
More generally, given paired examples of input and desired output images (), one can construct a loss function such as shown below, whereis a training loss, andis a regularization term. By minimizing this loss, we can learn both the parametersand.

Recently, inthe authors used a similar decomposition to create a zero-shot method to control each individual component of the decomposition through diffusion model sampling.

The concept of breaking down an image into layers of varying detail is closely related to the architecture of Residual Neural Networks(ResNets). Both share the principle that it’s simpler to model/learn residual mappings (the difference between the input and desired output) than to model/learn the complete transformation directly. While traditional deep neural networks try to learn this complex mapping in one go, ResNets use “skip connections” that allow the network to bypass layers, adding the original input to a later layer’s output. Lettingbe the desired complex mapping andthe input, a ResNet layer attempts to learn a residual functionsuch that:

The skip connection ensures that the original inputis preserved and added back to the output. Note the connection to (), where the residual term is exactly.
This decomposition and the use of skip connections simplify the network’s task, making optimization easier and mitigating the vanishing gradient problem that can hinder deep network training. Additionally, the preservation of the original input or its smooth approximation through skip connections ensures important information isn’t lost as data travels through the network. ResNets have been a major breakthrough in deep learning, enabling the training of much deeper networks and achieving state-of-the-art performance on image recognition tasks, with the concept of residual learning now being applied to other domains beyond image processing.

The natural decomposition of an image using denoisers has also been used for analyzing images, for example to detect anomaliesin images. The principle behind this is that anomalies, being infrequent occurrences, lack the self-similarity or smoothness typically observed in natural images. Drawing inspiration from patch-based denoising (e.g., non-local means), which employs self-similarity to differentiate between signal and noise, inthe authors introduce a method that effectively dissects an image into two components. The first is a self-similar component that embodies the background or ’smooth’ regions of the image given by the denoiser. The second is a residual component that encapsulates the distinctive, non-repetitive elements, which could potentially include anomalies and noise. The residual image, anticipated to resemble noise, is then subjected to a statistical test to detect any anomalies.

Next, we will describe various well-known classes of denoisers, including those derived from statistical optimality principles, and others which are pseudo-linear and derived from non-parametric or empirical considerations. We will also examine whether these classes of denoisers satisfy the above properties.

SECTION: The Structure of General Denoisers
SECTION: Bayesian Denoisers
Bayesian denoising invokes the use of a prioron the class of “clean”
images. This prior influences the estimate of the underlying signal away
from the observed measurement. We will describe the popular Maximum a-Posteriori (MAP) and the Minimum Mean-Squared Error (MMSE) denoisers below.

The contrast between the MAP and MMSE is highlighted in Figure. The two estimates tend to coincide when the posterior is
symmetric and unimodal, or when the noise varianceis small.

As the name indicates, the maximum a posteriori estimate is the value ofat which the posterior densityis maximized,

When the noise is Gaussian and white, the optimization
boils down to regularized least-squares

whereis the negative log-prior on the space of “clean” images, andis proportional to the noise variance. It would appear that the MAP denoiser does not have a closed form. However, the expression () is also known in the optimization literature as aproximal operatorwhenis convex, quasi-convex, or a difference of convex functions. It is well-knownthat to every proximal operatorthere corresponds a convex scalar-valued functionsuch that.

Furthermore, in the context of the MAP estimate,has an explicit form:

whereis aversion ofcalled its Moreau
envelope. As we will see below, the MMSE estimate shares a very similar form.

An example (for the scalar case) of the MAP denoiser foris shown in Figure, where the resulting denoiser is exactly the soft-thresholding operator.

While Maximum A Posteriori (MAP) denoising seeks the most probable estimate of a clean signal given a noisy observation, MMSE denoising aims to find the estimate that minimizes the mean squared error (MSE) between the estimate and the true signal

whereis the true signal,the noisy observation, andis the estimate ofgiven.

A fundamental result in estimation theory is that the posterior mean,, is the MMSE estimator. This can be shown by minimizing the MSE directly. Starting with the definition of MSE:

Since, minimizing the MSE is equivalent to minimizing the inner integral for each. Expanding the square and simplifying, we get:

Taking the derivative with respect toand setting it to zero, we find:

Thus, the posterior mean minimizes the MSE for any, and therefore minimizes the overall MSE.

While the MMSE expectation integral is generally difficult or impossible to evaluate directly, a key result known as Tweedie’s formulaenables us to write the expression for MMSE also in the form of the gradient of a scalar function:

whereandis the marginal density of the measurement, computed as.
It is apparent thatis effectively the priorwith the noise distribution (Gaussian in our setting).
Just like the MAP denoiser, the MMSE denoiser also has the form. More specifically, the MMSE denoiser can be rewritten as

where. This is more or less identical to the form of the MAP denoiser in ().
Figureillustrates the MMSE denoiser for the scalar case withpenalization, showcasing its behavior across variousvalues. A comparison between the MMSE and Maximum A Posteriori (MAP) estimators is presented in Figure.

The typical modern supervised approach to image denoising is to train a deep neural network with pairs of clean and noisy images, where the noise is often modeled as additive white and Gaussian(AWGN).
Let’s assume we have image pairswhererepresents a clean image, andis the noisy observation obtained by adding AWGN with a known standard deviation to:, where.

A typical regression approach would predictdirectly fromusing a trained model, by minimizing the expected reconstruction error:

In the case, this leads to an approximation to the ideal MMSE denosier,

As mentioned earlier, the MMSE denoiser is the average of all plausible clean signals given the noisy observation. This averaging can lead to a loss of details and a blurry appearance, especially when the noise level is high.
This is because minimizing average distortion (e.g., PSNR) can harm perceptual quality. To address this, alternatives including perceptualand adversarial losseshave been considered. A more powerful approach is tosamplefrom the posterior distribution, avoiding theregression to the meaneffect.

Denoising Autoencoders (DAEs) are a prime example of data-driven MMSE denoisers. These neural networks excel at learning robust data representations by training on noisy input and striving to reconstruct the original, clean data. This makes them not only valuable for denoising but also for tasks like data compression and feature extraction.

SECTION: Energy-based Denoisers
We’ve seen that both MMSE and MAP estimators are of the formwhereis some smoothed version of– differently smoothed in each case. These denoisers are also special cases of a general “energy-based” formulation:

where in the particular case of MMSE and MAP,

If the energy function satisfiesfor all(as do both the MMSE and MAP), then such denoisers are ideal. This is because the Jacobian of the denoiser can be written as

wheredenotes the Hessian operator which is, by definition, symmetric. In summary, all energy-based denoisers, including MAP and MMSE, are ideal, have symmetric Jacobians and are therefore conservative vector fields.

The energy-based formulation of denoisers provides a natural mechanism for principled empirical design of denoisers. This approach turns out to be consistent with the well-establishedempirical Bayesapproach as well.

Recall that the formulation of the denoising problem in () directly implies that the marginal densityis smooth because, wheredenotes convolution (i.e. blurring) with a Gaussian density. So by definition, this marginal density can be treated as a smooth function - the larger the noise parameter, the smoother is.

Now let’s write this marginal density inGibbsform:

whereis an energy function with. Thescore functionis related to the energy function as follows:

The smoothness ofimplies smoothness of the energy, thereby ensuring the existence of the gradient for both.

We can expand the energy aroundusing a first order Taylor expansion (with the Lagrange form of the remainder) to get

whererepresents the (symmetric) Hessian matrix ofevaluated at some (unknown) pointlying on the line segmentbetweenand.
Accordingly, the score function is

Meanwhile, Tweedie’s formula implies that the MMSE denoiser has the form:

Requiring thatimplies that the second term must be zero. Therefore, the MMSE denoiser has a simple (pseudo)-linear form:

To summarize, the resulting locally optimal denoiser can be written as

where the symmetric matrixis adapted to the structure of the input. This observation is consistent with earlier findingsthat such pseudo-linear filters -including those built from (bias-free) deep neural nets- are (a) attempts at empirical approximations of the optimal MMSE denoiser, (b) shrinkage operations in an orthonormal basis adapted to the underlying structure of the image, and (c) perturbations of identity. In particular, such denoisers can be written in the form, meaning that their local behavior is fully determined by their Jacobian, and therefore its spectrum.

Though these facts were neither historically clarified, nor the original motivation for their development, denoisers of the form () have always been heuristic/empirical approximations to the MMSE. These denoisers were hugely popular and effective (e.g.) for decades before the more recent introduction of neural networks. More recent work by Scarvelis et al.explores the use of a specific kernel approach to create a “closed-form” diffusion model that operates directly on the training set, without the need for training a neural network.

Next, we will describe these types of denoisers -using the language of kernels- in more detail.

SECTION: Kernel Denoisers
The basic idea behind kernel denoisers follows a non-parametric approach to modeling the distribution of (clean) images. Concretely, consider our basic setting given by

whereis zero-mean Gaussian white noise of variance. In practice the densityis unknown, but we may have access to examples, for. We can construct a naive empirical estimate of the distribution as follows:

The empirical density foris the convolution ofwith the Gaussian density, yielding:

Armed with this estimate, we can compute an empirical estimate of the score:

Invoking Tweedie’s formula, we have a closed form approximation to the MMSE denoiser as a (data-dependent) weighted averageof the clean data points:

In practice, we may only have access to the noisy image. In this scenario, we can treat each pixelas an independent sample (with independent noise) and apply the same reasoning directly to the noisy input, using it as a proxy for the clean signals:

This is a primitive instance of the pseudo-linear form alluded to earlier. In particular, the Gaussian “kernels”, motivated by the assumed (Gaussian) distribution of the noise, can be thought of more generally as one of a myriad of choices of positive-definite kernels that can be employed to construct more general denoisers, as described below.

The pseudo-linear form is very convenient for the analysis of practical denoisers in general. But even more importantly, it is a fundamental and widespread approach to denoising that decomposes the operation into two distinct steps. First is a nonlinear step where data-dependent weightsare computed. Next is astep where weighted averages of the input pixels yield each output pixel. More specifically, for each output pixel, the denoiser can be described as:

Gathering all the weights into a matrixreveals the denoiser inmatrix form:

Generally speaking, the weights are computed based on the affinity (or similarity) of pixels, measured using a “kernel” (a symmetric positive-definite function). When properly normalized, these yield the weights used to compute the output pixels as a weighted average. For instance, in the non-local meanscase

anddenotes a patch of pixels centered at. There exist many other possibilities, a practical few of which are shown in Table.

When normalized, these affinities give the weightsas follows

In more compact notation:

whereis a diagonal normalization matrix constructed from the row sums () of.

For common kernels such as those highlighted in the above table, the parametercontrols theof the kernel. Therefore, as, the kernel approaches a scaled Dirac delta:, or equivalently, the Kernel matrix is a scaled identity:. Consequently, normalizing gives. If in additionis symmetric, then the denoiser can be approximated as the gradient of an energy (see discussion in previous Section):

In practice, symmetry of the filter matrixis not a given. Despite the fact that the kernel matrixis symmetric, the resulting weight matrixis not so, due to the non-trivial diagonal normalization by. Fortunately, one can modifyto satisfying the symmetry condition as detailed in. This is accomplished by applying Sinkhorn balancing to(or equivalently to), resulting in a symmetric and-stochastic weight matrix, which can incidentally improve mean-squared error denoising performance over the baseline - see also.

Alternatively, one can take a different approach via a first-order Taylor series:

where. The right-hand side is evidently symmetric.

To give some additional context to this approach, note that when applying a filter to an image, standard practice is to normalize the filter coefficients in order to maintain the local brightness level from input to output image. This is particularly important where nonlinear filters are
concerned, where the effect on local brightness and contrast can be complex. The symmetrization approach presents a way of achieving the same level of control over the local filter behavior without the
need for this normalization.

As described in, the approximation works better - in terms of the distortion introduced to the output image - when the diagonal entries of the matrixare more tightly concentrated around their mean.

SECTION: Summary
The takeaway message from the above discussion is that denoisers we described share some important properties in common. Namely, they have the formwhereis the gradient of some scalar function. Furthermore, they are:

The ideal behavior of a denoiser when the noise is absent () is to give the input image back, unchanged. This is what we identified as Propertry 1 in the introductory Section. We’ve seen that both Bayesian (MAP, and MMSE) denoisers, and their (ideal) empirical approximations satisfy this condition.

The general formcan be interpreted as the “trivial” denoiserwith acorrectiontermthat pulls the components of the noisy input toward zero. It is remarkable that these denoisers have the same form as the original James-Stein estimator, wherewas interpreted as the maximum-likelihood estimator, andplayed the role of a Bayesian
“correction”. It has been observedthat such denoisers behave (at least locally) as shrinkage operations in an orthonormal basis adapted to the underlying structure of the image.

We noted that many denoisers can be written in the form. It is obvious that the right-hand side defines one step in a steepest descent iteration. Repeated applications of a denoiser have the effect of marching toward a local stationary point of the energy.

It has been pointed out elsewherethat if we accept the assertion that real-world images withpixels are approximately contained in low-dimensional manifolds of, then adding noise is equivalent to orthogonal perturbation away from the manifold, and denoising is approximately a projection onto the manifold. In particular, for small noise, denoising is precisely a projection onto the local tangent of said manifold. As such, the work of denoising is essentially analogous to manifold learning.

SECTION: Denoising, the Score Function, and Generative Modeling
A crucial link between denoising and the score function enables denoisers to learn complex probability distributions. In modeling real-world data, and images in particular, we are typically faced with a complex, high-dimensional probability density,. Explicitly modeling such a distribution can be computationally intractable or extremely difficult. The score function, defined as the gradient of the log probability density, can provide a way through.

Instead of modeling the distributiondirectly, we can learn, or approximate, the score function. Denoising techniques are a way to implicitly learn the score function roughly as follows: an estimate of the score function around a “clean” image is obtained by corrupting it with noise, training a model to reconstruct the original clean image from the noisy version, and measuring the denoising:

At first blush, it is not at all clear why this is a reasonable procedure. Yet there are a number of waysto motivate this idea -perhaps none more direct than by usingintroduced earlier in Eq. ():

Rewriting this establishes a direct andrelationship between score function and the MMSE denoiser:

Despite its elegance, the MMSE estimator is typically difficult to compute, or entirely inaccessible. Therefore as a proxy, often other denoisers are used, which may only be rough approximations of the MMSE (Eq. ()).

One can take a broader point of view by consideringdenoisers:

whereis of the form

Energy functions such as these can be learned, and the resulting denoisers have the appealing form:

Or equivalently

This illustrates again that the energy function is a proxyfor the score, and the resulting denoiser’s residual can be used as an approximation of the score.

SECTION: Denoising as the Engine of Diffusion Models
Denoising Diffusion and Flow generative modelshave become an important area of research in generative modeling.
They operate by progressively corrupting training data with noise until it’s indistinguishable from random noise, then learning to systematically reverse this corruption. By training a model to iteratively denoise, it gains the ability to generate entirely new, coherent data samples from a starting point of pure noise, effectively converting noise into meaningful structures like images or other data forms (Figure).

Despite their popularity, expressive power, and tremendous success in practice, there’s been relatively little intuitive clarity about how they operate. At their core, these models enable us to start with a sample from one distribution (e.g. a Gaussian), and arrive (at least approximately) at a sample from a target distribution. But how is this magic possible? Referring to Figure, let’s say we begin with a sample, where.

One simple way to activate this sampling process is to directly consider adifferential equation

where the right-hand side is the score function introduced earlier, andis the noise level at time. This differential equation, called a, by construction moves the initial condition gradually toward the distribution. Solving this equation requires (a) selecting a numerical scheme, and (b) having access to the score function.

If we have access to an MMSE denoiser at every, we can invoke Tweedie’s formula to write:

which we call aflow. As we’ve described in the previous sections, lack of access to the MMSE denoiser forces us to select a different denoiser and therefore solve only an approximate version of the desired Eq. ().

A key question arises: How is the velocity coefficient (the term multiplying theresidual) in () ODE determined? Let’s assume the process has a conditional varianceat time(i.e., noise level). The ODE is then constructed such that this variance evolves consistently, meaning.

A first order discretization of () yields:

This allows us to derive the conditional variance ofgiven:

where the final approximation holds for small. This demonstrates that the velocity coefficient in () effectively ensures the consistent evolution of the conditional variance, crucial for accurately capturing the underlying process dynamics.

A crucial point for our discussion is that the probability flow described by equation () can be proven to yield the same marginal distributions as the stochastic formulation presented in. This implies that, in the limit, if we initialize with samples from a Gaussian distribution, the solution is guaranteed to produce samples that match the data distribution.
While a comprehensive mathematical analysis of diffusion models is beyond the scope of this work, we encourage interested readers to delve into the foundational worksor the excellent introductory overviewsfor a deeper understanding.

SECTION: Denoisers in the Context of Inverse Problems
Consider the following formulation of a linear inverse problem: The data is given by the following model

whereis the forward operator (e.g., degradation or measurements operator),is additive white Gaussian noise, and the task is retrievingfrom.

A nominal solution can be obtained by solving this optimization problem:

wherecaptures the Gaussian nature of the noise, andis a regularization term intended to stabilize the solution, andis a regularization parameter.

Over the last several decades, a vast number of choices for the regularizerhave been proposed with varying degrees of success. Early approaches often relied on hand-designed priors to encourage desired properties in the solution, such as sparsity or smoothness.
Iterative Shrinkage/Thresholding (IST) algorithmsutilize the shrinkage/thresholding function (Moreau proximal mapping) derived from the regularizerto solve optimization problems. However, the non-smoothness of many regularizers and the scale of these problems pose computational challenges. Proximal methods like FISTAand ADMMpresent more efficient solutions by leveraging the proximal operator, which can be interpreted as applying a denoising step to intermediate solutions.

More recently, and independently of the machine learning literature, a fascinating connection has emerged between denoising algorithms and inverse problems. Powerful denoising algorithms, particularly those leveraging deep learning, have been shown to implicitly encode strong priors about natural signals. By incorporating these denoisers into the optimization framework, we can effectively leverage their learned priors to achieve state-of-the-art performance in various inverse problems. This approach effectively blurs the lines between traditional regularization techniques and modern denoising methods, offering a new paradigm for solving inverse problems.

Learning priors from data has a long history starting in the statistical literature with the concept of “empirical Bayes”(see e.g.). More recently, both implicit and explicit methods have been developed to learn the distribution of images. In particular, the vast recent literature on diffusion models is all about mapping a known distribution (typically a multidimensional Gaussian) to an empirical distribution (learned from collections of images in a desired domain).

As we described earlier, access to a high quality denoising engine affords us the possibility to learn, or at least locally approximate, the geometry of the image manifold. This approximate geometry is learned based on a residual: the difference between aimage and its denoised version. This enables us to formulate inverse problems as general optimization tasks, where the denoiser (or more specifically a functional based on it) is used as a regularizer.

In order to solve the optimization problem (), it is necessary to evaluate the gradient of the objective, which is as follows:

A key concern is how to compute. In this respect, classical choices ofsuch asnorms have been fairly convenient and successful; but also shown to have limited power to model natural images.

Another choice that has proved more effective is (image-adaptive)regularizersthat implicitly contain a (pseudo-linear) denoiser inside. Namely,

In, we developed a natural extension of this idea called(RED), where the regularizer is constructed from a more general denoiser:

Note the intuition behind this prior: the value ofis low if the cross-correlation between the image and the denoising residual is small, or if the residual itself is small due tobeing a fixed point of.

But with this generality comes a challenge: can the gradient of the regularizer be computed easily? The answer is yes, whenis ideal and locally homogeneous. This is not difficult to prove:

where the second line follows from the Jacobian symmetry of ideal denoisers; and the third line follows from local homogeneity and the definition of directional derivative:

Replacingfor the gradient in (), we have the following expression for the gradient of the objective:

The most direct numerical procedure for solving this equation is a fixed point iteration that sets the gradient of the objective to zero:

Equivalently,

where

Here,is the (fixed) linearsolution andis also a fixed matrix. Procedurally, we start with, denoise it, and then a linear operatoris applied and a biasis added - this leads to an updated estimate, and the process is repeated. Note that the structure of this iterative process is not altogether different from a denoising diffusion processwhere a denoiser is repeatedly applied. In fact, whenwe see the structure of adiffusion process:

In a general statistical setting, the scalar valuedis often the result of assuming a prior whose negative-log isas the regularizer:

However, in cases where a denoiser is used toa regularizer, the role of the regularizeris that of an energy function that we implicitly use to define a Gibbs distribution:

In the particular case of RED:, Equation () implies that an ideal and locally homogeneous denoiser has the form, which means that under these conditions, the RED regularization can be thought of as a () energy function:

SECTION: Posterior Sampling with Denoisers
An alternative approach to solving inverse problems is to leverage pretrained denoisers as priors for generating samples from the posterior distribution. Given measurements, our goal is to generate samplesthat follow the distribution, where the prioris implicitly defined by the denoiser.

To achieve this, we can adapt the generative sampling strategy from Equation () to sample from the posterior distributioninstead of the prior:

starting from. The second equality is given by Bayes rule.

We recognize the first term as the score function, which can be connected to the MMSE denoiser through Tweedie’s formula (). The second term in () quantifies how well the current sampleexplains the measurements, but this is generally intractable to compute.

One approach to address this intractability is the Diffusion Posterior Sampling framework. DPS approximates the intractable term with, based on the assumption that.

Considering a linear measurement model as in Equation (), this approximation leads to:

Substituting this into (), we obtain:

whereis a hyperparameter balancing the influence of the prior and the measurements. In practice, we utilize a denoiser networkto approximate the conditional expectation.

Denoising diffusion models are rapidly emerging as a powerful tool for solving inverse problems across various domains. This success often stems from combining the strengths of diffusion models with additional approximations or specialized techniques. A growing body of research explores these approaches (; seefor a comprehensive review).

SECTION: Conclusions
In this paper, we have explored the multifaceted nature of denoising, showcasing its far-reaching impact beyond the traditional task of noise removal. We have highlighted the structural properties of denoisers, their connection to Bayesian estimation and energy-based models, and their ability to act as powerful priors and regularizers in various applications. The surprising effectiveness of denoisers in tasks from generative modeling to inverse problems underscores their versatility and potential for future research. The continued evolution of denoising techniques, coupled with advancements in machine learning, promises to unlock even more innovative applications and deeper insights into the underlying structure of images.

The authors extend their sincere gratitude to Mojtaba Ardakani, Michael Elad, Vladimir Fanaskov, Mario A. T. Figueiredo, Ulugbek Kamilov, José Lezama, Ian Manchester and Miki Rubinstein for their valuable feedback.

We also extend our sincere thanks to the vast research community whose dedication over decades has driven remarkable progress in denoising. The advancements in this field are a testament to collective effort, and it is beyond the scope of this work to fully acknowledge the extensive and diverse body of literature on this topic.

SECTION: References