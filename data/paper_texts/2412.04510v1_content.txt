SECTION: A Taxonomy of System-Level Attacks on Deep Learning Models in Autonomous Vehicles

The advent of deep learning and its astonishing performance in perception tasks, such as object recognition and classification, has enabled its usage in complex systems, including autonomous vehicles. On the other hand, deep learning models are susceptible to mis-predictions when small, adversarial changes are introduced into their input. Such mis-predictions can be triggered in the real world and can propagate to a failure of the entire system, as opposed to a localized mis-prediction.
In recent years, a growing number of research works have investigated ways to mount attacks against autonomous vehicles that exploit deep learning components for perception tasks. Such attacks are directed toward elements of the environment where these systems operate and their effectiveness is assessed in terms of system-level failures triggered by them. There has been however no systematic attempt to analyze and categorize such attacks.
In this paper, we present the first taxonomy of system-level attacks against autonomous vehicles. We constructed our taxonomy by first collecting 8,831 papers, then filtering them down to 1,125 candidates and eventually selecting a set of 19 highly relevant papers that satisfy all inclusion criteria. Then, we tagged them with taxonomy categories, involving three assessors per paper. The resulting taxonomy includes 12 top-level categories and several sub-categories.
The taxonomy allowed us to investigate the attack features, the most attacked components, the underlying threat models, and the propagation chains from input perturbation to system-level failure. We distilled several lessons for practitioners and identified possible directions for future work for researchers.

SECTION: 1.Introduction

The advent of Autonomous Vehicles (AVs) has brought about a paradigm shift in transportation(kuutti2020survey,). They leverage an array of sensors, cameras, and Deep Learning (DL) models to perceive and navigate their surroundings for safer and more efficient travel. However, as these vehicles have become increasingly reliant on complex DL components, they also expose new attack surfaces. While highly effective in most cases, these DL components possess inherent vulnerabilities, such as being sensitive to adversarial examples(biggio2018wild,), that can lead to catastrophic failures. A mis-prediction at the DL model level can quickly escalate into a system-level failure, resulting in vehicle crashes or collisions with pedestrians, thus endangering lives.
A large body of research has focused on the model-level security of DL components, tested by introducing various attack (resp. defense) strategies to induce (resp. prevent) mis-predictions in these models(cina2023wild,). As evidenced by Wang et al.(wang2023does,), however, not all attacks on the DL model propagate to the system of an AV, meaning that the success of the model-level attack does not always translate to system failures. In their study, this was largely due to a lack of consideration for the physical properties of the underlying vehicle system, such as maximum and minimum acceleration/deceleration, steering rates, and sensor mounting positions. A few recent studies have extended the evaluation of attack methods beyond the model level, considering their system-level impacts in real-world settings, such as the frequency of vehicle crashes(shen2024soksemanticaisecurity,). To the best of our knowledge, no research has comprehensively taxonomized the attacks on AVs’ DL models that cause system-level failures.
This paper presents a taxonomy of system-level attacks on AVs, building upon existing research that has explored the propagation of model-level DL mis-predictions in the context of AVs. The contribution of this work is to provide a comprehensive understanding of the vulnerabilities and potential attacks on AV systems, which can help developers identify areas where defenses are most needed.

We adopted a bottom-up approach to the creation of a taxonomy for classifying the existing literature. We allowed the incremental identification and creation of new categories, as we analyzed the literature in depth. We collected papers by searching scientific databases using a carefully designed query. This initial search yielded 8,831 papers. Then, we manually analyzed and filtered these papers based on their venues, reducing our list to 1,125 potential candidates. Lastly, through snowballing and a thorough assessment by three reviewers, we identified 19 papers that formed the basis of our taxonomy. These papers were categorized into 12 distinct groups, including system-level failure-specific categories such as ‘System Under Attack’ or ‘System-level Results’ to differentiate our taxonomy from previous work that focused only on the characteristics of model-level attacks(biggio2018wild,).
We further provide statistics to highlight the distribution of properties in the taxonomy, as well as a tree-like visualization to enhance the readability and practical usage of the taxonomy. Finally, our discussion elaborates on the implications and potential future directions arising from our taxonomy and findings, opening up new avenues for future exploration in this emerging field.

The contributions of the paper are as follows:

The first taxonomy of security vulnerabilities that affect autonomous vehicles whose perception components are based on DL. The taxonomy consists of 12 top-level categories and several subcategories.

A detailed analysis of 19 papers that are highly relevant for system-level attacks against autonomous vehicles. We provide a thorough mapping between papers and taxonomy categories.

A discussion of the main findings obtained while constructing the taxonomy and analyzing the papers. The discussion includes implications for practitioners and possible avenues for future research.

The rest of the paper is structured as follows. Section2introduces some important notions about AVs and about DL security. Section3comments on the research questions that triggered and motivated the construction of our taxonomy. Section4describes the methodology adopted for paper selection and taxonomy construction. Section5presents our findings, including a description of the taxonomy and the mapping of the considered papers. Section6answers the initial research questions and discusses the implications of this work for practitioners and researchers. Section7comments on the related work. Section8enumerates the main threats to the validity of our findings and the ways in which we tried to mitigate them. Section9contains our conclusion and anticipates some possible future works.

SECTION: 2.Background

SECTION: 2.1.Autonomous Vehicles

Vehicles equipped with automated components can achieve different degrees of autonomy. For self-driving cars, five levels have been defined(J3016_202104,), ranging fromdriving assistancetofull driving automation. However, according to Faisal et al.(faisal2019understanding,),
an automated vehicle system can only be termedautonomouswhen the vehicle’s automated system can seamlessly perform all dynamic driving tasks across a diverse range of environments (i.e., only at level 5,full driving automation(J3016_202104,)). In this paper, we are not interested in differentiating among different degrees of automation vs. full autonomy, as we consider in the scope of our taxonomy all works that perform system-level security attacks against the deep learning components used to providesomeautomation to a vehicle, up to full autonomy.

The motion control of a vehicle can be divided into two primary tasks:lateralmotion control andlongitudinalmotion control. Lateral control, managed by the vehicle’s steering, aims to maintain the vehicle’s lane position and perform maneuvers such as lane changes and collision avoidance. This is typically achieved through DL components that process images from on-board cameras. Longitudinal control, managed by the gas and brake pedals, focuses on reaching/maintaining the desired speed, ensuring safe distances from other vehicles, and preventing rear-end collisions. This control relies on sensors like radar and lidar, as well as cameras, to measure relative velocity and distance(kuutti2020survey,). Other relevant tasks that are assisted by DL components include parking, driver’s attention checking, pedestrian and obstacle avoidance, traffic sign/light detection and recognition.

SECTION: 2.2.Adversarial Attacks

DL models have achieved impressive performance across various application domains. However, these technologies can be easily deceived by adversarial examples, such as carefully perturbed input samples designed to mislead detection systems during testing. Researchers have demonstrated that deep neural networks for object recognition or classification can be fooled by input images altered in ways that are imperceptible to humans(biggio2018wild,).
Amodel-level attackis an attack that exploits weaknesses of the DL
model in isolation to obtain a mis-prediction for specific, possibly manipulated, inputs.
Specifically, there are two primarymodel-level attackstrategies. Firstly,poisoning attacksoccur when an attacker, with access to the training data, inserts malicious data into the training set. Such data act as a trigger and when the trigger is present in the input at inference time, it forces the model to make incorrect predictions. Secondly,evasion attacksoccur when an attacker manipulates input data during testing without altering the model itself(biggio2018wild,;cina2023wild,). These attacks highlight the vulnerabilities of DL models in isolation, but they might be leveraged to trigger potential consequences at the system-level, in safety-critical applications such as AVs.

To define the scope of our taxonomy, it is important to clarify the notion of system-level attack and system-level failure in the domain of AVs. We define asystem-level attackas an attack that manipulates the environment where the AV operates, in order to cause a system-level failure of the AV. In turn, asystem-level failureis a deviation of the autonomous system’s behavior from its functional/safety requirements.
When mounting a system-level attack, one or more model-level attacks can be exploited by first propagating some malicious inputs into the models under attack, and then ensuring the mis-predictions propagate to a system-level failure.
This paper aims to build a taxonomy based on the literature that has explored system-level attacks and their impact in terms of system-level AV failures.

SECTION: 3.Research Questions

Research on system-level attacks against autonomous vehicles is an emerging, rapidly growing field.
Thegoalof our work is to acquire a better understanding of the research landscape by categorizing the existing works and defining the main dimensions that characterize them, More specifically, we build a system-level attack taxonomy to answer the following research questions:

RQ1 [Attack Features]:What are the prevalent features of the attacks, based on the paper distribution across taxonomy categories?

With this RQ, we investigate how extensively the research landscape has been explored, to highlight regions that are still relatively unexplored and to identify the features that characterize the most populated regions. This might indicate possible directions for future work, but also the reasons why some research directions are more feasible than others.

RQ2 [Attacked Components]:Which components of the system are the targets of the attack?

Deep-learning models are used for several tasks in AVs, including lane keeping, obstacle recognition and avoidance, and traffic sign/light recognition. With this RQ, we investigate if there are preferred attack targets among the deep learning components commonly integrated into AVs and why these are preferred over other alternatives. We aim to identify the features of those components that make them suitable for system-level attacks possibly operated by manipulating the environment where the AV operates.

RQ3 [Threat Models]:What are the threat models?

Different system-level attacks are mounted under very different assumptions about the knowledge available to attackers and their capability to manipulate the environment. Hence, some assumptions may be more reasonable or realistic than others, changing substantially the probability that an attack could be performed in practice. With this RQ, we aim to understand whether the research effort was directed mostly toward realistic attacks that are reasonably possible in the real world, or toward scenarios based on unrealistic assumptions that make them very unlikely.

RQ4 [Consequences]:What are the consequences of the attack and how do the failures propagate to cause system-level failures?

The main difference between a model-level and a system-level attack is that in the latter case, an attack is successful only if model-level mis-predictions propagate beyond the model and cause a system-level failure. With this RQ, we investigate the attack paths exploited in existing works, considering the attack vector (e.g., the environment element manipulated by the attack), its propagation into a model input, a model mis-prediction, and the propagation of such mis-prediction into external observable actions that eventually cause a system failure.

SECTION: 4.Methodology

Figure1presents an overview of our paper collection process. We adopted a systematic bottom-up approach, to curate a comprehensive collection of papers from diverse sources. We designed a search query to gather the papers from scientific databases such as Scopus (see below), spanning a 7-year period from 2017 to 2024111We selected 2017 as our starting year since research in the security of deep learning in AVs began to gain significant momentum after that year.. This was followed by automatic selection and manual filtering. Automatic selection refines the results based on venue reputation, so as to include only well-established sources, while manual filtering involves the examination of each paper by multiple assessors. Additionally, we included relevant papers discovered through snowballing, ensuring a detailed collection by leveraging references to/from other works.

SECTION: 4.1.Initial Paper Collection Through Scientific Databases

We employedFindpapers(grosman2020findpapers,)to collect our initial set of papers. This tool allows us to design a customized search query, focusing on paper titles and abstracts, and supporting multiple databases including IEEE Explore222https://ieeexplore.ieee.org/Xplore/home.jsp, Scopus333https://www.elsevier.com/products/scopus, and ACM Digital Library444https://dl.acm.org/. To ensure both comprehensive coverage and relevance, we iteratively refined our query based on two key criteria: the number of papers collected and the inclusion of three manually selected pivotal papers. Our pivotal papers contain two survey papers(biggio2018wild,;cina2023wild,)and DeepBillboard(zhou2020deepbillboard,), which deal with adversarial attacks on autonomous vehicles555These papers are excluded from the final taxonomy since they do not satisfy criteria introduced in later phases of the process (e.g., no survey; system-level failure propagation).. Through this iterative process, we balanced the query output, encompassing key papers while maintaining a broad search scope.

Given our focus on system-level attacks on autonomous vehicles, we designed a search query that requires the papers’ metadata (abstract, title, and keywords) to contain three key terms: ‘autonomous’, ‘vehicle’, and ‘attack’. Considering that not all papers may use these terms explicitly, we expanded our query to include synonyms. For ‘autonomous’, we include variations such as ‘automated’, ‘driverless’, ‘self-driving’, and ‘unmanned’, connected with theORoperator to account for any of these terms appearing in the metadata. Similarly, for ‘vehicle*’, we incorporate terms like ‘car*’, ‘drone*’, and ‘robot*’, also connected withOR. Note that the use of asterisks ensures coverage of plural forms. Lastly, for ‘attack*’, we include alternatives like ‘threat*’, ‘exploit*’, and ‘adversarial’, again joined byOR.

To ensure that each metadata contains at least one key term, all the term groups and their connected synonyms are linked with theANDoperator. Consequently, our final query is as follows:

Executing this query across three databases from January 2017 to January 2024, we retrieved a total of 8,831 papers. Note that terms like ‘machine learning’. ’deep learning’ and ‘system-level’ are not explicitly mentioned in this query as they might exclude too many relevant papers. However, the subsequent manual filtering process ensured that only papers in the scope of our study (i.e., system level attacks against deep learning models) are taken into consideration. Moreover, the term ‘robots’ is included to incorporate dynamic and mobile autonomous robots, such as delivery robots.

Next, our task was to refine this extensive pool of papers by filtering out those considered irrelevant or lacking in quality. Initially, we removed duplicate papers. Subsequently, we evaluated the publication venues based on established academic standards, such as the presence of a rigorous peer-review process and their focus on topics relevant to security. Papers from venues that did not meet these criteria were excluded. Employing avenue frequency filter, we reduced the number of papers from 8,831 to 1,125. This filter was designed by ordering journals and conferences separately by frequency of occurrence in the pool and then selecting the top 50 most prevalent venues. Out of them, we eliminated venues lacking security-related keywords in their titles. For example, communication-related journals and conferences, and multimedia journals and conferences are excluded. Recognizing that this approach may exclude reputable software engineering/security-focused venues, we created a white list comprising prominent venues, ensuring their inclusion after the filtering process. This venue white-list is available online athttps://zenodo.org/records/12806551.

We continued by manually filtering the 1,125 papers, focusing solely on their titles, which narrowed them down to 83 papers. From there, we delved deeper, examining the abstracts and further refining our selection to 30 papers.

SECTION: 4.2.Snowballing

The snowballing(wohlin2014guidelines,)phase is aimed at expanding the pool of relevant papers by following their connections with other relevant papers. We employed both backward and forward snowballing techniques on thepapers obtained from the previous phase.

Backward snowballing involves examining the references cited within each of the selected 30 papers and including additional papers based on their titles and abstracts. Conversely, forward snowballing involves identifying papers that cite the selected 30 papers. To achieve this, we utilized Google Scholar666https://scholar.google.comto locate the papers referencing each selected paper and considered as candidates to be relevant papers only those within the first two pages of the search results. We limited our selection to the first two pages because Google Scholar ranks results based on relevance. To prevent excessive iterations, we did not perform snowballing on the newly added papers.
In the end, our collection expanded to 50 papers. However, further filtering was still required, since selecting papers based only on their titles and abstracts is not necessarily accurate. This further selection was performed during the process of paper reading and categorization for taxonomy construction.

SECTION: 4.3.Creation of Taxonomy Categories

In this phase, we have read the 50 papers collected after snowballing, with the goal of creating the taxonomy categories and excluding papers that do not meet the inclusion criteria. In fact, sometimes it is necessary to read a paper fully to realize that it cannot be included in the taxonomy.

The process of category definition for the taxonomization of the papers was bootstrapped with the categories already available from the literature of attacks on DL models and autonomous vehicles. Initially, we adopted ten categories suggested by Biggio et al.(biggio2018wild,)and Cinà et al.(cina2023wild,). Subsequently, during taxonomy construction, we widened the scope to include concepts explicitly associated with system-level attacks and failures. This involved factors such as the system targeted by the attack and the specific system failures encountered during the attacks. In total, we identified 12 top-level categories, each listed in Table1with the corresponding explanations. A detailed description of these categories,a s weel as the sub-categories, is carried out below, in Section5.3.

In the phase of paper categorization and taxonomy refinement, we read each paper thoroughly. Specifically, each paper was read and categorized by at least two assessors. Weekly consensus meetings were held to resolve and negotiate any differences in the categorization. We excluded papers that do not specifically target the DL component of the AV under attack.
For instance, papers that attack the hardware sensors and cause them to malfunction, resulting in incorrect point cloud generation and subsequent failure to recognize targets, are discarded as they do not directly involve any DL model(cao2023you,). Additionally, papers that only evaluate their attack on a dataset without executing the system, or collect their own dataset using cameras mounted on non-autonomous, manually driven vehicles, are excluded(zhou2020deepbillboard,).
Papers that do not report any system-level failures, despite executing their attacks against autonomous vehicles(fu2022ad,;muller2022physical,), are also discarded. Although these papers may report mispredictions of the vehicle’s DL model, they do not provide information on the system-level vehicle’s behavior once the attack has caused a model-level misprediction. As a result of this strict refinement process, the number of relevant papers was reduced fromto.

SECTION: 5.Results

In this section, we first provide a summary of each paper. Following this, we present the taxonomy mapping — a table that classifies each paper according to the identified categories (multiple categories can be applied to each paper at the same time), offering a comprehensive overview of the existing research in the field. Finally, we comment on the taxonomy tree, which visually links each category and subcategory to the corresponding papers.

SECTION: 5.1.Summary of papers

A summary of the 19 papers that met our criteria is provided here to give an overview before delving into the detailed classification of each paper. The papers are organized in chronological order, starting with the most recent to the oldest.

“Slowtrack: Increasing the latency of camera-based perception in autonomous driving using adversarial examples”:
Ma et al.(ma2024slowtrack,)introduced SlowTrack, a framework designed to increase the execution time of camera-based autonomous vehicle perception by inserting fake bounding boxes. This attack targets both object detection and tracking systems, resulting in vehicle crashes with a success rate of 95%.

“Learning when to use adaptive adversarial image perturbations against autonomous vehicles”:
Yoon et al.(yoon2023learning,)worked on injecting malware into the system to manipulate the input image. By adding perturbations, the attacker can create fake bounding boxes in specific locations, deceiving the victim’s vehicle. This causes the vehicle to lose track of its intended target, potentially leading to a collision.

“Deepmaneuver: Adversarial test generation for trajectory manipulation of autonomous vehicles”:
Stein et al.(von2023deepmaneuver,)successfully manipulated the steering angle prediction model of a victim’s vehicle by installing a perturbation on a billboard on the left-hand side of the road. This caused the model to predict the wrong steering angle for a given input image, leading the vehicle to leave the road or collide.

“Kidnapping deep learning-based multirotors using optimized flying adversarial patches”:
Hanfeld et al.(hanfeld2023kidnapping,)proposed an attack method capable of hijacking multi-rotor drones using flying adversarial patches, or small images strategically positioned in the surroundings. As a result, the drone deviates from its intended trajectory, losing track of the target human and instead following the adversarial patch.

“Does physical adversarial example really matter to autonomous driving? Towards system-level effect of adversarial object evasion attack”:
Wang et al.(wang2023does,)explored the limitations of existing object evasion attacks in autonomous driving systems, noting their failure to achieve system-level effects due to inadequate understanding of the vehicle’s physical properties. These include parameters like maximum/minimum acceleration/deceleration, steering rates, and sensor placements. To overcome these limitations, the paper introduced SysAdv, a novel attack design that integrates such system-specific knowledge. SysAdv significantly enhanced the success rate of attacks by approximately 70%.

“On data fabrication in collaborative vehicular perception: Attacks and countermeasures”:
Zhang et al.(zhang2023data,)proposed an attack method that exploits LiDAR-based collaborative perception in AVs, which can spoof or remove objects at specified locations in the victim’s perception results, making all mainstream types of collaborative perception schemes vulnerable.

“Rolling colors: Adversarial laser exploits against traffic light recognition”:
Yan et al.(yan2022rolling,)proposed an attack targeting the color recognition systems of traffic lights in autonomous vehicles (AVs). They exploited the vulnerability of rolling shutters in CMOS cameras by directing a laser beam to create a colored stripe in the input image. This manipulation resulted in the misclassification of the traffic light’s color.

“Stop-and-go: Exploring backdoor attacks on deep reinforcement learning-based traffic congestion control systems”:
Wang et al.(wang2021stop,)researched poisoning a DRL-based controller in congested traffic. The attacker’s car sets a trigger based on a combination of speed and position, causing the victim’s vehicle to perform inappropriate actions like accelerating or decelerating incorrectly. This ultimately results in a collision with the vehicle in front, occurring on both one-lane and two-lane roads.

“Attack and fault injection in self-driving agents on the carla simulator–experience report”:
Piazzesi et al.(piazzesi2021attack,)modified the trained agent’s neurons and weights, as well as the input image, to cause the car to make incorrect steering decisions and mis-detect traffic lights. This resulted in collisions with surroundings, lane departure, going off-road, running through crossroads, and ignoring traffic lights.

“Dirty road can attack: Security of deep learning based automated lane centering under Physical-World attack”:
To deceive the victim vehicle’s automated lane-centering system, Sato et al.(sato2021dirty,)printed specially designed patches on the road that resembled dirty road markings. This manipulation caused the system to make incorrect steering predictions, leading the vehicle to engage in hazardous maneuvers such as turning left or right incorrectly, driving off-road, and causing collisions.

“Invisible for both camera and lidar: Security of multi-sensor fusion based perception in autonomous driving under physical-world attacks”:
Cao et al.(cao2021invisible,)conducted a physical sensor-based attack aimed at deceiving the camera and Lidar perception systems of the victim’s vehicle, specifically while driving in a single road lane. Their strategy involved 3D printing and placing an adversarial traffic cone on the road that was designed to appear broken and thus remain undetected by both sensors. This deception caused the vehicle’s systems to fail to recognize the object, resulting in collisions.

“Too good to be safe: Tricking lane detection in autonomous driving with crafted perturbations”:
Jing et al.(jing2021too,)implemented a technique where three white dots are painted on the road either upon completing the road lines or when there is space between them. These dots are designed to trick cars into mistaking them for real lane markers, leading the vehicles to follow the dots and veer into the opposite lane.

“Robust roadside physical
adversarial attack against deep learning in lidar perception modules”:
Yang et al.(yang2021robust,)utilized 3D printing to create a small adversarial object, which they placed on the road. The purpose was to deceive the victim’s car into mistaking the object for another vehicle, prompting it to either change lanes or come to a complete stop upon detecting the object.

“ML-driven malware that targets AV safety”:
Jha et al.(jha2020ml,)installed malware to introduce noise to the outputs of object detection, tracking, and sensor fusion systems. The malware modifies the outputs under specific conditions by adding perturbative data as sensor noise. This noise makes the victim’s car believe that a car in another lane is moving into its lane or that a car in the same lane in front of it has disappeared. Additionally, it can make the car think that pedestrians are moving into the street. This results in emergency braking or unwanted acceleration, leading to a car crash.

“Attacking vision-based perception in end-to-end autonomous driving models”:
In the research of Boloor et al.(boloor2020attacking,), when the victim’s car is at an intersection or on a turning road, the attacker paints adversarial road lines pointing in the opposite direction of the lane turn, causing the car to turn the wrong way.

“Feasibility and suppression of adversarial patch attacks on end-to-end vehicle control”:
Pavlitskaya et al.(pavlitskaya2020feasibility,)proposed manipulating an autonomous vehicle’s trajectory by placing a printed adversarial patch on the roadside. This action causes the vehicle to steer towards the patch, resulting in a collision.

“Phantom of the ADAS: Securing advanced driver-assistance systems from split-second phantom attacks”:
Nassi et al.(nassi2020phantom,)developed a method to project images of humans or traffic signs onto roads or flat surfaces. These projections, referred to as phantoms, are intended to deceive cars into perceiving them as real signs or pedestrians, prompting the vehicles to respond accordingly.

“Adversarial sensor attack on lidar-based perception in autonomous driving”:
Cao et al.(cao2019adversarial,)directed lasers from an adversarial vehicle toward the target vehicle’s Lidar system. This intentional interference aims to create false objects that do not exist, leading the car to freeze or suddenly brake.

“Trojaning attack on neural networks”:
In the work of Liu et al.(liu2017trojaning,), the attacker deceived the victim’s vehicle by poisoning its driving model with trojan data to cause a wheel angle misprediction. When the vehicle encounters the trojan attached to a billboard, it is instructed to turn right, leading to the vehicle driving off the road.

SECTION: 5.2.Mapping the papers

For the taxonomy mapping, we employed a rigorous process: each paper was independently reviewed and categorized by at least two assessors. To ensure consistency and reliability, the assessors regularly met to discuss and resolve disagreements in their mappings. The results of this process are presented in the Appendix, in Tables2,3, and4, where each column represents a distinct category, and each row corresponds to an individual paper and its associated categorization.

We conducted weekly meetings to ensure that the assessors’ perspectives aligned with each other while reading and categorizing the papers. Over time, the meetings led to a decrease in disagreements, which typically involved the category to choose (major disagreement) or how to precisely formulate a (sub-)category (minor disagreement).
For paper mapping, we used a shared spreadsheet containing 228 cells.
Among the 228 cells that were filled, around 20 cells (9%) faced major disagreements and approximately 80 cells (35%) faced minor disagreements. On average, 8 disagreements were resolved per meeting, resulting in a total of about 96 resolved disagreements over 12 meetings. Initially, the categories with the most disagreement were “Attacker’s System/Model Knowledge,” followed by “Attack/Error Specificity” and “Attacked Target.” Possible reasons for the higher disagreement in these categories were the
difficulty of giving a unique interpretation and purpose to these categories, and the difficulty of extracting the needed information from some papers, as it is not always directly provided by the authors.
The consensus meetings turned out to be a fundamental instrument to address such initial disagreements and to come out with a common understanding and interpretation, which was reflected in a much clearer definition of the categories and choice of the labels for the categories after such meetings.

SECTION: 5.3.Taxonomy tree

Figure2provides a visual representation of the resulting taxonomy as a tree.
The root node branches into 12 top-level categories, each further subdivided into relevant subcategories. The leaf nodes of the tree contain lists of papers associated with their respective (sub)categories.
Each category and subcategory is detailed in the following.

Application Domain:Domain or vehicle type that is the target of the attack, such as “Cars” or “Drones”.

DL Model Under Attack:The DL model under attack, which is responsible for the system-level failure, such as YOLOv5, ResNet-34, or DAVE-2. The “DL Model Under Attack” category has four subcategories. The first, “Object Detection & Tracking”, includes DL models (e.g., YOLO) that handle object tracking or detection. The “Steering Wheel Angle Prediction” covers models that control the car’s steering angle. “Road Line Detection” helps the system detect and follow road lines, while “Traffic Control” assists in navigating complex traffic situations.

System Under Attack:The autonomous system under attack, such as a driving agent like Baidu Apollo in the LGSVL simulator or the driving agent in the CARLA simulator. “System Under Attack” has two subcategories, “Simulation” and “Real AVs”, respectively indicating that the system under attack is simulated or is a real-world autonomous vehicle. “Baidu Apollo”(apollo,)and “CARLA’s Agent”(Dosovitskiy17,)represent examples of autonomous systems implemented in the LGSVL(rong2020lgsvl,)and CARLA simulators — the most popular simulation environments in our list of papers. Other less popular simulators include BeamNG(beamng_tech,)and Udacity(udacity,). “Tesla” is instead an example referred to by papers that used a real Tesla vehicle to test their attacks.

Attack Scenario:The scenario in which the attack is designed to be effective, such as when a car is at an intersection or stuck in a traffic jam. Hence, the “Attack Scenario” category provides details on thenecessary conditions for an attack, i.e., the conditions that must be satisfied in order for an attack to be successful. Some papers did not specify any constraints, so they fall under the “Without Constraints” subcategory. For those requiring specific conditions, we created the “With Constraints” subcategory, which includes three further distinctions. The “Roads” subcategory covers attacks that need modifications to the road itself. The “Objects & Cars” subcategory includes attacks that require objects, such as billboards or non-autonomous cars, to be positioned in specific places. Lastly, the “Driving Condition” subcategory relates to papers that do not require physical changes but need the AV to be in a specific scenario, such as approaching a traffic light, being at an intersection, or navigating a turn.

Attacked Target:The specific target of the attack, such as a billboard where to place adversarial noise or a road where to add adversarial lines. If an attack operates as malicious software that directly alters the input (e.g., image) received by the DL model, then the attacked target would be e.g. the input image. The subcategories of “Attacked Target” concern the specific elements that the attacker needs to change, manipulate, or perturb. This can include the “Input Image” fed into the DL model, the “Road” or “Roadside”, the AV’s “Sensors”, “Objects” like billboards or traffic “Signs” like stop signs, and the “Training Data” used to train the model before deployment.

Attacker’s Capability:The capabilities required by the attacker to execute the attack, such as the ability to alter the physical environment or to install malware into the system. The “Attacker’s Capability” category has four subcategories. Attacks requiring changes to the environment, such as placing objects on the road, fall under the “Environmental Manipulation” subcategory. Those needing alterations at the software level, like acting as a man in the middle, are categorized under “Software Interference”. Attacks that interfere with the sensors’ regular functionality, such as directing lasers at them, are included in the “Sensor Interference” subcategory. Lastly, those requiring manipulation of the model, such as adding backdoors to the training data, fall under the “Model Manipulation” subcategory.

Attack Type:The attacker’s general approach, such as altering training data (“Poisoning”) or producing mis-prediction at inference time (“Evasion”).

Attacker’s Knowledge:The attacker’s knowledge about the system or model, split into white-, black-, or grey-box knowledge:

“Model-level White-Box”: Full access to the code used to create the model and to the dataset used to train/test the model.

“Model-level Black-Box”: The attacker can only know the input and output vectors accepted/produced by the model.

“System-level White-Box”: The attacker has unrestricted access to the entire system, including the source code of the system, DL models, simulator (if any) and datasets.

“System-level Black-Box”: The attacker’s knowledge is limited to input (e.g., sensor) and output (e.g. actuator) data, with no visibility into the internal system components.

“System-level Grey-Box”: The attacker has some insights into the system but not to the full extent of a white-box setting (e.g., the code of some components is available, but not the code of the entire system).

Attack/Error Specificity:Some attacks are highly specific, targeting only one particular object or aspect (e.g., only traffic signs are attacked), which makes them “Attack-Specific”. On the other hand, the “Attack-Generic” subcategory represents a more general type of attack, which can be applied to a wider range of targets (e.g., any object on the side of the road). Regarding the model mis-predictions resulting from the attack, “Error-Specific” attacks aim to induce a particular output or trigger unique misbehavior in the model (e.g., confusing a 60 speed limit with a 90), while “Error-Generic” attacks allow any kind of misbehavior of the model (e.g., mis-classifying a traffic sign in any possible way).

System’s Failure Specificity:Similar to the previous category, this one indicates whether the attacker expects a particular system-level failure. For instance, if the attacker aims to make the vehicle suddenly brake, the attack is considered “Failure-Specific”. Alternatively, a “Failure-Generic” attack aims at a general system failure, without specifying the exact expected outcome.

Model-level Results:The outcome of the DL model resulting from the attack, such as a mis-classification or mis-detection by the object classifier or object detection component of the autonomous vehicle. In this category, papers causing a wrong steering angle prediction fall under the “Steering Angle Misprediction” subcategory. Those causing the object detection module to fail at detecting objects correctly are categorized under “Object Misdetection”. The “Misclassification” subcategory includes e.g. a paper where traffic light colors are incorrectly classified. Finally, the “Wrong Decision” subcategory is for instances where models make incorrect decisions, such as in traffic jams.

System-level Results:This category indicates the outcomes of the attack at the system level, such as collisions with other cars, collisions with the environment, or sudden braking, resulting from model-level misbehavior propagated into system-level faults. For the “System-Level Results”, we have four subcategories. Some papers cause AV cars to crash into various things. Therefore, under the “Vehicle Crash To” subcategory, we have three more subcategories to specify what the AV crashed into. If the AV crashes into another vehicle (often a non-autonomous car), it falls under the “Vehicles” subcategory. The “Objects” subcategory is for instances when the AV hits objects such as road curbs, billboards, traffic cones, etc. There’s also a subcategory for cases where the AV crashes into “Pedestrians”. For results that don’t fall under “Vehicle Crash To” we have additional subcategories. The “Losing the Path” subcategory covers cases where the AV loses its path and goes into another lane or off-road. The “Changing in Speed/Brake” subcategory includes attacks that cause the AV to accelerate, decelerate, or suddenly brake. Lastly, the “Sign Ignorance” subcategory is for instances where the AV ignores stop signs or traffic lights.

SECTION: 6.Discussion

In this section, we first answer the research questions and then point out to the takeaways of this work and the future research directions.

SECTION: 6.1.Answers to Research Questions

Figure3illustrates the distribution of papers across the taxonomy tree. It reveals substantial differences in the number of papers associated with some categories.

In Figure3(a), we observe that only two papers focus on the application domain of autonomous drones, compared to 17 papers on autonomous cars.
As autonomous drones are gaining traction in several sectors, the unbalanced distribution of existing papers points to a whole research area that could be investigated more deeply in the future.

Figure3(b)shows that most attacks target the object detection and tracking models of AVs, indicating that this component is the most vulnerable part of an AV system. Steering wheel angle prediction was also a notable target for attacks, as it is influenced by numerous factors on the road that can be the target of an attack(von2023deepmaneuver,;piazzesi2021attack,;sato2021dirty,;pavlitskaya2020feasibility,;liu2017trojaning,).

Figure3(c)clearly shows that the majority of papers focused on deploying attacks in a simulation environment. This trend may be due to the inherent dangers of the attacks, or legal constraints in various countries, making simulation environments more feasible for conducting such research.

Figure3(d)illustrates the approach chosen by attackers to deploy their attacks. Evasion attacks are prevalent, likely because poisoning attacks require access to the model’s dataset before training. This scenario is hard to achieve in the real world, making evasion attacks more realistic and feasible.

In Figure3(e), the model-level knowledge is predominantly considered to be white-box. While this may not be very practical in real-world scenarios,
it is often necessary for attackers to make their attacks effective. In this respect, the four papers(piazzesi2021attack,;jing2021too,;boloor2020attacking,;nassi2020phantom,)that utilized a black-box knowledge approach would represent more feasible options for real-world attacks.

Figure3(f)addresses the system-level knowledge required for attacks. It is noteworthy that almost every attack does not require in-depth knowledge or access to the source code of the system.
On one hand, this might be due to the practical difficulties that prevent the acquisition of white-box or grey-box knowledge of complex, possibly proprietary, systems. On the other hand, while at the model level, white-box knowledge is often a prerequisite that makes the attack otherwise impossible, at the system level plain knowledge of the environment elements and system input is often enough to introduce manipulations that cause a model mis-prediction and possibly a system failure.

In line with Figure3(b), Figure3(g)also shows that the outcome of attacks on the DL component is predominantly the misdetection of targets or objects the AV is trying to recognize. These objects can include traffic signs, cars, pedestrians, etc. This highlights a significant vulnerability in the object detection and recognition capabilities of autonomous vehicles.

Figure3(h)suggests that most crashes involving AVs were with objects such as obstacles on the road. The figure indicates that only one paper reported crashes involving pedestrians. This could be related to the difficulty in effectively targeting pedestrians as part of an attack.{mdframed}[backgroundcolor=cyan!15]RQ1 [Attack Features]: The distribution of papers across taxonomy categories is quite unbalanced. Most papers are focused on attacks against the object detection/tracking and the steering angle prediction components of simulated cars aiming for mispredictions (evasion). Most papers require white-box knowledge of the model, but black-box knowledge of the system. At the model level, they mostly aim at object misdetection, while at the system level, they mostly aim at crashing other vehicles or existing objects.

By examining the taxonomy tree, we observe that the DL models under attack consist of object detection & tracking, steering angle prediction, lane detection, and traffic control models.
Most of them support the perception tasks carried out by the AV, by identifying relevant features in the perceived input and manipulating them to cause a mis-prediction. Often, the perceived input is a camera image(ma2024slowtrack,;yoon2023learning,;hanfeld2023kidnapping,;piazzesi2021attack,;jha2020ml,), making it possible to reuse the wide range of attacks investigated in previous works on model-level evasion against images, achieved via adversarial noise patches(biggio2018wild,).
This is confirmed by the knowledge required by these attacks, which is white-box at the model level and black-box at the system level (see Figures3(e),3(f)).
Hence, the problem becomes how to manipulate the simulation environment in which the AV operates in order to introduce the attack patches that are known to trigger a mis-prediction at the model level, assuming that the induced error will propagate inside the system, which is known only as a black box, to eventually cause a system failure.

The systems under attack include Baidu Apollo, CARLA’s driving agent, Tesla, and several other AV systems. However, most attacked systems are executed within a simulator (see Figure3(c)). There are several practical reasons to prefer simulated AVs over real ones when experimenting with new attacks against AVs, including: (1) simulations are cheaper, faster and not affected by safety constraints; (2) simulations are less constrained and offer higher flexibility in the manipulation of the environment, which could be very expensive or infeasible in the real world; (3) simulations provide more information about the execution, e.g., obtained by monitoring and logging different types of execution/simulation data, which is useful to fine-tune an attack. Such information might be unavailable in the real world.{mdframed}[backgroundcolor=cyan!15]RQ2 [Attacked Components]: The most attacked components are image-processing neural networks, for which several model-level attacks are readily available in the literature. The problem is then how to manipulate the environment such that the model-level input resembles an adversarial input and how to ensure the propagation of a model-level error to a system-level failure. This seems easier to achieve in simulation than in the real world because a simulator offers higher flexibility, more information, and is generally cheaper, faster, and not potentially dangerous for human beings.

The most common assumption is that the attacker has white-box access to a DL model used in an AV system, which is known only as a black-box. Under this assumption, the papers included in our taxonomy explored various threat models:

Outside Attack: The attacker cannot manipulate the DL model input. Hence, the attacker can only manipulate the environment(von2023deepmaneuver,;hanfeld2023kidnapping,;sato2021dirty,;cao2021invisible,;jing2021too,;yang2021robust,;boloor2020attacking,;pavlitskaya2020feasibility,;nassi2020phantom,)or can interfere with the sensors(yan2022rolling,;cao2019adversarial,)to supply an adversarial input to the model.

Inside Attack at Inference Time: The attacker can manipulate directly the DL model input at inference time(ma2024slowtrack,;yoon2023learning,;wang2023does,;zhang2023data,;piazzesi2021attack,;jha2020ml,). This can be achieved e.g. by installing some malware that can intercept and occasionally replace the actual model input with an adversarial one.

Inside Attack at Training Time: The attacker can manipulate directly the dataset used to train the DL model(wang2021stop,;piazzesi2021attack,;liu2017trojaning,). This can be achieved e.g. by obtaining unauthorized access to the victim’s computers or with the complicity of an employer working at the victim’s company.

The above-listed threat models require an increasing amount of capabilities and permissions on the attacker’s side, with the Outside Attack threats being the easiest to mount and the Inside Attack threats the most difficult and challenging. It should be noticed that not all papers are explicit about the underlying threat model they assume, as a result, we often had to infer it from the algorithmic description of the attack. As the risk associated with an attack depends on the damage it can cause, but also on the probability that an attacker can successfully mount the attack, explicit knowledge of the threat model is important to understand and possibly estimate the likelihood of an attack.{mdframed}[backgroundcolor=cyan!15]RQ3 [Threat Models]: Threat models span from limited capabilities of manipulation of the environment where an AV operates, to the possibility of directly corrupting or replacing input values at inference or even at training time. Such a wide range corresponds to different degrees of attack likelihood and realism. Papers often lack explicit information on the assumed threat model, which makes it difficult for the reader to assess the feasibility of an attack in a given context.

Most attacks that cause a misprediction of the steering angle result in the vehicle losing its path and sometimes hitting road objects, such as side walls(von2023deepmaneuver,;piazzesi2021attack,;sato2021dirty,;pavlitskaya2020feasibility,;liu2017trojaning,).
On the other hand, object misdetection often causes the vehicle to fail to see an object or to perceive a fake object as real, leading to unexpected changes in speed or crashes(ma2024slowtrack,;yoon2023learning,;zhang2023data,;piazzesi2021attack,;cao2021invisible,;yang2021robust,;jha2020ml,;boloor2020attacking,;nassi2020phantom,;cao2019adversarial,).
Misclassifications by the model and incorrect decisions can cause the car to ignore traffic signs or collide with other vehicles(yan2022rolling,;wang2021stop,).

There are two main chains of attack propagation from the input to a system failure:

Targeted Propagation: one or more adversarial inputs propagate into the DL model and cause a misprediction, which goes in a specific direction, aimed at producing a specific system-level failure(hanfeld2023kidnapping,;wang2023does,;zhang2023data,;yan2022rolling,;wang2021stop,;piazzesi2021attack,;sato2021dirty,;cao2021invisible,;jing2021too,;jha2020ml,;boloor2020attacking,;pavlitskaya2020feasibility,;nassi2020phantom,;cao2019adversarial,;liu2017trojaning,). For instance, a patch is added to the environment in order to revert the numerical sign of the steering angle prediction, which is supposed to make the car go off its lane(boloor2020attacking,).

Untargeted Propagation: one or more adversarial inputs propagate into the DL model and cause an arbitrary misprediction. Propagation to an arbitrary system failure is conjectured and often possible, but it is not targeted explicitly(ma2024slowtrack,;yoon2023learning,;von2023deepmaneuver,;yang2021robust,;nassi2020phantom,). For instance, perturbations are added to the input image to cause arbitrary, incorrect bounding boxes. As a result of the model-level misclassification, the self-driving car misbehaves(ma2024slowtrack,).

Clearly, the targeted propagation chain is more effective as it causes a model-level mis-prediction that is specifically functional to an overall mis-behavior of the whole system. However, such targeted chains require deeper knowledge of the system, which is often unavailable or incomplete. Hence, often papers adopt the untargeted propagation chain and then just observe and evaluate its effectiveness empirically, with no a-priori warranty of success.{mdframed}[backgroundcolor=cyan!15]RQ4 [Consequences]: There are two main approaches to achieve the system-level consequences of an attack: (1) specific model-level mis-predictions are targeted in specific execution contexts, to ensure the desired consequence at the system level; (2) arbitrary model-level mis-predictions are triggered, and system-level consequences are just observed, but not controlled.

SECTION: 6.2.Implications for practitioners and researchers

Based on our analysis of the papers and based on the taxonomy extracted from them, we can distill a few lessons and implications that are potentially useful to practitioners and represent interesting opportunities for researchers.

Vulnerability of Image Processing Components: Based on the categories of “DL Model Under Attack” and “Attacked Target”, most system-level attacks manipulate the environment in order to affect some image processing components, which are vulnerable to adversarial attacks. It should be noted that incorporating a DL component for image processing requires viewing environmental objects as potential threats and designing defenses accordingly.

Transferability of Attacks: Since most attacks have been tested in simulation. While it is safe to assume that these attacks might not be transferable to the real world, Stocco et al.(stocco2022mind,)found that simulation based failure scenarios allow real-world testing to be anticipated and informed. This is either because the outcomes transfer reliably between simulation and real environments or because the uncertainty profiles in the simulator can help predict the uncertainty levels and outcomes in real-world scenarios.

Attack Propagation:
Wang et al.(wang2023does,)highlighted that some existing model-level object evasion attacks in AV fail to produce system-level effects. Therefore, it is crucial to analyze the propagation of these attacks, as the effectiveness of a model-level attack on a system depends on the propagation chain it exploits. During our review of related papers, we identified a lack of research addressing why certain attacks on DL models do not escalate to system-level failures. This gap represents a promising area for further investigation in the field.

System-level knowledge: Nearly every attack in the reviewed papers was conducted without any prior knowledge of the system, classifying them as black-box attacks. This demonstrates that effective attacks can still be carried out without access to the source code or detailed information about the autonomous vehicle. Consequently, it should not be assumed that concealing the AV’s system information would inherently enhance its security.

Additional Attack Targets: Existing attacks commonly involve creating adversarial objects or fake road lines to mislead the perception components of an AV. These objects are static and do not move. However, Hanfeld et al.(hanfeld2023kidnapping,)demonstrated an attack using a moving drone, showing that dynamic objects, such as autonomous or human-controlled units, can also be employed by attackers to carry out their attacks.

SECTION: 7.Related Work

Biggio & Roli(biggio2018wild,)proposed a comprehensive survey investigating the vulnerability of ML and the development of suitable countermeasures within the research field of adversarial ML. This work provides an in-depth overview of the evolution of this research area over the past decade and beyond, starting from early investigations into the security of non-DL algorithms to more recent studies aimed at understanding the security properties of DL algorithms in computer vision and cybersecurity. The authors reviewed the main threat models and attacks defined in this context and discussed the significant limitations of current research, as well as the future challenges in designing more secure learning algorithms.

In their subsequent work, Cinà et al.(cina2023wild,)provided a comprehensive survey that systematizes poisoning attacks and defenses in ML over the past 15 years. They begin by categorizing the current threat models and attacks, and then organize the existing defenses accordingly. While their focus is primarily on computer vision applications, they argue that their systematization also encompasses state-of-the-art attacks and defenses for other data modalities. The main difference between these two papers and our work is that their surveys
cover model-level attacks, while we are interested in system-level attacks.
Our paper aims to provide categorized information for readers interested in understanding how to mount an attack against a whole system, not just one of its DL components in isolation, and how this can lead to system-level failures.

Shen et al.(shen2024soksemanticaisecurity,)systematized the knowledge within the growing area of semantic autonomous driving AI security. They analyzed a total of 53 papers, categorizing them based on research aspects critical to the security field such as the attack/defense targeted AI component, attack/defense goal, attack vector, attack knowledge, defense deployability, defense robustness, and evaluation methodologies. While this paper is the closest to our work, there are remarkable differences.
First of all the focus, which issemanticAI security. As a consequence,
although this paper mentions attacks that can cause system-level failures, its approach to collecting and analyzing papers was not systematically focused on all possible system-level attacks, rather just on those exhibiting asemanticgap. Hence, they cover the existing literature on system-level attacks to a quite limited extent. Correspondingly, our taxonomy provides more detailed information about the relationship between each attack and the whole system under test, including how the attacks are performed in the environment, which system components they target, the resulting system-level failures, and the propagation chain that allows an attack to manifest itself.

SECTION: 8.Threats to Validity

SECTION: 8.1.Internal Validity

Internal validity threatsare caused by the tools and methodology employed to conduct the study. We have identified the following internal validity threats: (1) the query string; (2) the filtering phase; (3) subjectivity of categorization; (4) exclusion criteria.

Thequery stringwas crafted with a focus on attacks on autonomous vehicles and refined iteratively based on the number of retrieved papers and the presence of pivotal papers in our results. Running the query string from 2017 to 2024 delivered an initial list of approximately 8,800 papers. Although changing the query and pivot papers would alter this initial number, our subsequent steps — filtering and snowballing — compensate for this effect. So, we expect that any replications would yield results similar to ours.

In thefiltering phase, we excluded venues that did not meet our desired quality standards or fell outside the scope of our taxonomy. We also white-listed some high-reputation, related venues to ensure their inclusion, as determining the scope based solely on titles was not feasible.
While changing the chosen venues might affect the results, manual inspection of the retrieved papers, along with those identified in the snowballing phase, ensured that all included papers were high-value and fully in the scope of our work.

The selection of categories was done by starting from previous model-level surveys and creating new categories when needed. The resulting categories were defined through consensus in weekly meetings held among the authors. The primary task involved comprehensively reading the papers and categorizing them. Consequently, each paper was reviewed by at least two assessors, and weekly meetings were held to resolve any differences in categorization. These measures were taken to ensure accurate categorization and mitigate anysubjectivity threatpossibly affecting the categorization.

We had two main criteria for excluding papers. The first criterion was to exclude attacks not targeting any DL model. The second criterion was to exclude attacks evaluated only at the model level. While reviewing the papers, we encountered some that targeted the sensors rather than the DL component, such as those that blinded the sensors. In these cases, the attack could not be attributed to any vulnerable DL model. Thus, these papers were discarded. Some papers claimed real-world evaluation but only involved mounting a camera on a car and taking pictures of traffic signs and surroundings. Since no autonomous vehicle was involved, this merely resulted in creating their own dataset, leading to their exclusion from our taxonomy.
This manual exclusion of papers was carried out through meetings and discussions, and we tried to stick to the two criteria explained above as strictly as possible. However, we acknowledge that other researchers might have different perspectives.

SECTION: 8.2.External Validity

External validity threatsaffect the generalizability of our findings beyond the study conducted for this paper.
A replication of this study following our methodology is not ensured to produce exactly the same list of papers, taxonomy categories and taxonomy mapping. However, we have tried to document all the key decisions made in the process, describing the adopted methodology in detail. We have adopted the best practices available for the execution of a systematic literature review(ISTguidelines,). Hence, we think that our main findings (see our answers to the research questions in Section5) are to a major extent valid beyond our specific instance of the study.

SECTION: 9.Conclusion

While model-level adversarial attacks have received huge attention from researchers in the last decades, it is only more recently that researchers considered the possibility of injecting attack vectors into elements of the environment where the system under attack operates, to observe a system-level failure, not just a model-level mis-prediction. The literature on system-level attacks has grown quite rapidly in recent years, going in different directions. Hence, this is the time to organize the existing works into a taxonomy. For this purpose, we retrieved the 19 more relevant papers from the literature and we created a taxonomy to categorize them. Our key findings indicate an unbalanced distribution of papers across taxonomy categories; a prevalence of attacks against image processing components in simulation environments; a variety of threat models, which include black-box attacks and white-box attacks, the latter either at inference or at training time; two main propagation chains, consisting of targeted propagation, aimed at specific system-level failures, and untargeted propagation.

We also discussed a number of implications for practitioners and researchers. In the former case, they include guidelines and suggestions for security testing (e.g., on risk assessment and attack effectiveness). In the latter case, they indicate possible directions for future work, such as: performing attacks on DL components beyond image processing ones; investigating the transferability of attacks from a simulator to the real world; propagation of an attack from model to system; system level security testing for the creation of more secure vehicles, including dynamic entities among the environment elements that contribute to a system-level attack.

SECTION: Conflict of Interest and Data Availability

The implementations, source code, data, and experimental results are publicly available athttps://zenodo.org/records/12806551.

SECTION: References

SECTION: Appendix ATables with Paper Mapping

Below, we report the full tables with the paper mapping produced by the assessors and agreed upon in the consensus meetings.