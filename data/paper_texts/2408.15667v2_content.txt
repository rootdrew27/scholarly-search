SECTION: Towards Reliable Respiratory Disease Diagnosis Based on Cough Sounds and Vision Transformers
Recent advancements in deep learning techniques have sparked performance boosts in various real-world applications including disease diagnosis based on multi-modal medical data. Cough sound data-based respiratory disease (e.g., COVID-19 and Chronic Obstructive Pulmonary Disease) diagnosis has also attracted much attention. However, existing works usually utilise traditional machine learning or deep models of moderate scales. On the other hand, the developed approaches are trained and evaluated on small-scale data due to the difficulty of curating and annotating clinical data on scale. To address these issues in prior works, we create a unified framework to evaluate various deep models from lightweight Convolutional Neural Networks (e.g., ResNet18) to modern vision transformers and compare their performance in respiratory disease classification. Based on the observations from such an extensive empirical study, we propose a novel algorithmfor cough-based disease classification based on both self-supervised and supervised learning on a large-scale cough data set. Experimental results demonstrate our proposed approach outperforms prior arts consistently on two benchmark datasets for COVID-19 diagnosis and a proprietary dataset for COPD/non-COPD classification with an AUROC of 92.5%.

SECTION: 
Respiratory diseases such as COVID-19 and COPD are widespread in populations and early diagnosis is imperative for disease control and human well-being. According to, COPD was the third fatal disease in the world in 2022 and is expected to remain the third
cause of death through 2050 at the global level. The current diagnosis of COPD requires measurements of lung functions using clinical equipment in hospitals which is expensive and inconvenient for mass screening. As a result, a significant fraction of COPD patients are undiagnosed in China and are exposed to risks of developing into fatal cancers.

The advancement of machine learning and deep learning techniques enables digital diagnosis for various diseases with satisfying accuracy. These data-driven techniques can discover hidden patterns in patientâ€™s medical data and connect them with disease labels of interest. In general, machines are expected to understand medical data at an equivalent level as experts for disease diagnosis after being exposed to large-scale high-quality data for training.
Nowadays, various medical data modalities have been investigated for disease diagnosis. Comprehensive surveys have been made for deep learning techniques used in medical applications using medical imagery, physiological data, electronic health recordsand audio data.

Among many respiratory sounds such as wheezing, crackles, and breathing, cough sounds have been used to diagnose respiratory diseases in a long history as cough sounds contain indicative patterns closely related to pathomorphological alterations in the respiratory system. Researchers have tried to develop various digital methods for cough sound-based respiratory disease diagnosis. However, most existing studies use traditional machine learning methods and handcrafted features extracted by classic audio signal processing techniques or relatively lightweight deep models. This is mainly caused by the lack of large-scale cough data sets to pre-train or even fine-tune modern deep models like wav2vecand transformers. For the same reason, the performance reported on small datasets may be biased and unreliable.

Recently, Zhang et al.released a series of respiratory acoustic foundation models which are based on the latest self-supervised learning and are pre-trained on large-scale unlabeled respiratory sound data. In addition, the pre-trained foundation models are evaluated on a curated list of downstream tasks for respiratory sound classification or regression. The benchmarking in this work compares several modern deep models originally designed for audio event classification and demonstrates the superiority of their proposed models which are pre-trained specifically on respiratory sound data. However, their released models are based on backbone models of moderate sizes compared with the most advanced ones for audio event or image classification.

In this work, we focus on cough sound data and investigate the best practices for improving the accuracy of cough-based respiratory disease diagnosis. Motivated by existing works in general audio and image classification, we raise the following research questions in this study:
(1) Which pre-trained models are most appropriate for cough-based respiratory disease classification in terms of accuracy and efficiency?
(2) What are the best practices for fine-tuning a pre-trained model for enhanced cough-based respiratory disease classification?
(3) What are the best practices of model ensemble and selection for optimal classification performance?

We conduct extensive empirical studies to answer these research questions and propose a novel cough-based respiratory disease classification approach. The contributions of our work can be summarised as follows:

We create a unified framework to investigate the cough-based disease classification performance of various pre-trained models belonging to three categories: ImageNet-pre-trained models, audio data pre-trained models and respiratory sound data pre-trained models.

We propose an approach to cough-based respiratory disease classification based on both self-supervised learning and supervised learning on a large-scale cough data set.

We conduct extensive experiments on three datasets (two public datasets for COVID-19 classification and one proprietary dataset for COPD/non-COPD classification) and the experimental results demonstrate our proposed approach outperforms all others consistently on three tasks.

SECTION: 
In this section, we review existing works closely related to this study. Specifically, we first review the latest deep learning approaches to audio event classification as our employed method is based on and adapted from models for general audio event classification tasks. Subsequently, we review recent works on cough sound classification and discuss the limitations of commonly used approaches to cough sound classification for respiratory disease diagnosis.

SECTION: 
Audio event classification is a well-formulated research task that has attracted significant attention in the community. Deep learning models have dominated state-of-the-art approaches to this task in recent years. These approaches follow a similar framework in which the raw audio data are converted to log-mel-spectrogram images. Hence, innovations in image classification models can also benefit the audio event classification tasks by proper transfer learning. Such transfer learning is enabled by fine-tuning the pre-trained image classification models on large-scale audio datasets like AudioSet.

According to the model architectures, existing approaches to audio event classification in the literature can be categorized into two groups: Convolutional Neural Networks (CNN)and transformers. Whilst early works employing ImageNet-pretrained CNN models on audio spectrograms achieved competitive accuracy, they have been outperformed by transformer-based models. Audio Spectrogram Transformer (AST)was one of the first to apply vision transformers to audio event classification and performed superior to their CNN counterparts. Koutini et al.proposed the PaSST series by evaluating different variants of tailored ViT models and efficient training strategies for audio event classification. These works vary from one another in their employed backbone architectures and audio data preprocessing but share a similar transfer learning framework in which the ImageNet-pre-trained models are fine-tuned on audio spectrogram data for downstream tasks.

With access to large-scale audio datasets like AudioSet-2M, more recent works turn to self-supervised learning for enhanced audio representation learning. A framework consisting of self-supervised learning and supervised fine-tuning has been employed byand leads to state-of-the-art methods for audio event classification in various downstream tasks.

The aforementioned models are usually evaluated on several benchmark datasets for audio event classification. Those performing well in general audio classification tasks are expected to perform well in a specific downstream task like cough sound classification. We believe a thorough evaluation is imperative for choosing the most appropriate solutions to the specific cough sound classification task in practice. More recently, Zhang et al.made a thorough evaluation of self-supervised learning models across several respiratory sound classification tasks, however, their work aims at a wider scope than cough data classification and their released models were only pre-trained on respiratory sound data which lead to sub-optimal performance as observed in our comparative study.

SECTION: 
Cough sound-based respiratory disease classification has attracted much attention. Researchers have explored the possibility and efficacy of using cough data to diagnose various respiratory diseases including tuberculosis (TB), Asthma, pneumonic infections, and COVID-19.

However, most of these prior works use traditional machine learning approaches such as logistic regression, support vector machine, tree-based models, multi-layer perceptronsand classical convolutional neural networks like ResNet50. The evaluations were usually made on small private datasets, preventing a direct fair comparison across different works and making the practical use of proposed methods difficult.
For example, Pahar et al.evaluated various traditional machine learning methods for cough-based TB classification and the best logistic regression achieves an area under the receiver operator curve (AUROC) of 0.94 using 23 features selected from a set of 78 high-resolution mel-frequency cepstral coefficients. Later, the same group employed the Resnet50 classifier, and discriminated between the COVID-19 positive and the healthy coughs with an area under the ROC curve (AUC) of 0.98.

Along with the release of large-scale cough data sets including COUGHVIDwhich contains over 25,000 crowdsourced cough recordings and Coswarawhich contains more than 7,000 audio samples from around 1,000 participants for COVID-19 diagnosis. Attempts have also been made using modern deep-learning models for cough classification. Approaches falling into this category preprocess the raw audio data into log-mel spectrogram image data so that they can be fed into deep models originally designed for image classification. Xue et al.propose a novel self-supervised learning framework for COVID-19 cough classification. A vision transformer (ViT) is firstly trained on unlabeled cough data in a self-supervised learning manner and the pre-trained model is subsequently fine-tuned on the downstream classification task for COVID-19 screening. Valdes et al.also employ a ViT-based model Audio Spectrogram Transformer (AST)for cough signal feature extraction towards the classification cough types (e.g., dry, wet, whooping, etc.). The employed AST was pre-trained on a large-scale image dataset ImageNetand a large-scale audio dataset AudioSetsubsequently. However, features extracted from the pre-trained models are directly used in the downstream task. We believe the data distribution gap between general audio data (e.g., those in AudioSet) and cough data will restrict the capabilities of deep models without proper transfer learning. To address this limitation, in this study, we further fine-tune the ViT-based deep models on cough data to enhance their representation learning from cough data to discover disease signatures underlying cough sound data.

Although most existing works use the off-the-shelf deep models, Dentamaro et al.proposed a novel deep model, dubbed AUCO ResNet, by designing a trainable Mel-like spectrogram layer able to finetune the Mel-like-Spectrogram for capturing relevant time-frequency information. However, it is unclear if such a layer can still benefit ViT-based models when they are pre-trained and fine-tuned on large-scale audio datasets (e.g., AudioSet).

Distinct from prior works on cough-based respiratory disease classification, in this work, we followand use modern deep learning techniques adapted from the state-of-the-art deep models in audio event classification (c.f.). The conclusions drawn from our experiments are expected to provide insights into the practical use of cough data for respiratory disease diagnosis.

SECTION: 
In this section, we describe the process involved in developing a deep model for cough sound classification. Three steps are illustrated in Figure: audio data preprocessing (Figurea), self-supervised learning (Figureb) and supervised fine-tuning (Figurec). Whilst the audio data preprocessing step and the supervised fine-tuning step are required by all methods investigated in this study, the self-supervised learning step is only involved in the methods belonging to the respiratory sound data-based pre-training approaches.

SECTION: 
Collecting multiple cough sounds from one subject is practically useful for robust classification results. For this reason, a cough sound segmentation algorithm is needed to segment individual cough sounds from an audio recording. We propose a cough segmentation algorithm based on signal processing and hand-crafted rules.

A cough typically consists of two or three phases. In the first phase, a rapid burst of air breaks through the glottis. In the second phase, the glottis is fully open, allowing air to be steadily expelled from the lungs. Finally, there is possibly a phase where the airflow decreases as the glottis gradually closes.

Our pilot experimental results demonstrate that precise cough onset and offset localisation are not necessary for the classification task. Therefore the cough sound segmentation algorithm boils down to the cough onset detection. A cough event is segmented starting from its detected onset and continues until the segment reaches a predetermined duration.

:
Initially, the audio signal undergoes a Short-Time Fourier Transform (STFT) with a hop length of 0.016 seconds and a window length of 0.021 seconds, utilizing the Hanning window function. Subsequently, the magnitude spectrum is derived by taking the absolute value of the STFT coefficients. Next, the magnitude spectrum is integrated across the 120Hz-8000Hz frequency band to yield a univariate energy sequence. The ratio of the current frameâ€™s energy to that of the preceding frame is computed, and its logarithm is taken to generate a sequence representing the rate of energy change. This sequence is then smoothed using a Butterworth low-pass filter. Peaks in the energy change rate are identified by applying a threshold of 100 to this sequence. For each detected peak in energy change rate, the corresponding energy peak is located within a window of 5 frames centred on the peak. Ultimately, the cough onset is identified as occurring two frames before the located energy peak positions.

SECTION: 
It isstandard to convert raw audio data into 2D spectrogram images for audio event classification using image classification deep models. The generation of spectrogram images is based on STFT and the choice of optimal hyper-parameters is coupled with the type of deep models employed for the classification.

SECTION: 
In the deep learning era, a common practice is to use a pre-trained deep model and fine-tune it on the downstream task. It is crucial to choose an appropriate pre-trained model for good performance in the downstream tasks. For cough sound spectrogram-based disease classification problems, the candidate pre-trained models can be categorised into three groups according to the pre-training data modalities. In this study, three types of pre-trained models are considered: ImageNet-pre-trained models, audio data pre-trained models and respiratory sound data pre-trained models

ImageNet-pre-trained models have been widely used in a variety of computer vision tasks due to the fact they are extensively studied, rapidly evolved and easily accessible. These models are pre-trained on ImageNet and/or other large-scale natural image datasets of this kind. Prior works have proved the benefit of using such pre-trained models on audio classification tasksprovided the models are properly fine-tuned.
As ImageNet-pre-trained models require 3-channel images as inputs, we duplicate the generated 1-channel spectrogram three times to form 3-channel images. For models requiring specified input image sizes (e.g.,), we resize the images as per requirement.

Audio data pre-trained models close the gap between natural images and audio spectrogram images by pre-training deep neural networks on large-scale audio data. During pre-training, the models take audio spectrogram images as the input and hence can be directly applied to downstream tasks for respiratory sound classification. As the pre-trained models may have employed different parameters to generate the spectrogram, the same process for spectrogram generation as that used during pre-training must be employed during fine-tuning.

Respiratory sound data pre-trained models further close the gap between the general audio spectrogram and the respiratory sound spectrogram. Similar to the audio data pre-trained models, these models can be directly fine-tuned on downstream tasks once the classification head is adapted to fit the number of classes.

A comparative study will be presented in the section on experiments with typical pre-trained models selected from three categories for empirical evaluations.

SECTION: 
To adapt the pre-trained foundation models to a specific downstream task (i.e. cough sound classification in our case) which usually has relatively less labelled data for training, we employ different training paradigms: supervised training and self-supervised training. One individual or a hybrid training strategy can be chosen for the best performance based on the type of training data available in a specific application scenario.

In most cases, a supervised training strategy should be used to fine-tune the pre-trained models and adapt them to the downstream task. For this purpose, labelled data are required and the label space is usually different from that in the pre-training phase. As a result, one needs to replace the classification head in the model for fine-tuning.

The classification heads in modern deep learning architectures are usually implemented with a series of fully connected layers together with non-linear activation and normalisation layers. Whilst the classification heads can vary in the number of layers and the dimension of the input layer (determined by the dimension of the features from the backbone of the pre-trained model), the last layer should always consist of output neurons equalling the number of labels in specific downstream tasks.

: Sharpness-Aware Minimization (SAM)has been proven as an effective strategy for improving the model generalisation and training efficiency. It seeks parameters that lie in neighbourhoods having uniformly low loss; this formulation results in a min-max optimization problem on which gradient descent can be performed efficiently. Used together with optimizers such as Adamduring fine-tuning, it is expected to improve the performance of pre-trained models on the downstream tasks.

: Data augmentation is a commonly used strategy during supervised training to improve the model generalisation and performance. A two-stage data augmentation strategy is employed on the fly during supervised training. In the first stage, we use the Python libraryand apply three types of data augmentation to the raw audio data: adding Gaussian noises, multiplying the audio by a random gain factor and shifting the pitch up or down without changing the tempo.
In the second stage, the data augmentation is applied to the spectrogram images. We useandmethods to augment the training data in each mini-batch on the fly during training. Specifically, three types of augmentation are applied to the log mel spectrogram images: time warping, frequency masking and time masking.

It is common to have unbalanced data in medical applications where positive samples (with diseases) are far less than negative ones (healthy). To combat such an issue, we use a simple yet effective strategy which assigns label-dependent weights when computing the loss. The loss for samples belonging to the lower-represented class (positive) is multiplied by a higher weight. The weight can be calculated as the ratio between negative and positive samples in the training set.

Adapting pre-trained foundation models to a specific downstream task suffers from out-of-distribution issues when the supervised training data are limited. In such cases, self-supervised training on large-scale unlabeled data before the supervised training can bridge the data distribution gap between those used in pre-training and fine-tuning. In our targeted use cases, the models pre-trained either on ImageNet or AudioSet may suffer the out-of-distribution issue when applied to the cough data. To mitigate the performance degradation caused by the distribution gap, we use a two-stage fine-tuning pipeline (c.f. Fig.) consisting of a self-supervised training stage followed by supervised training on the downstream tasks.

We followand employ the teacher-student framework (c.f. Fig.b) for self-supervised training on large-scale unlabeled cough data. The teacher and the student models share the same architecture of a vision transformer. The student model weights are updated normally by gradient descent whilst the teach model weights are updated by the Exponential Moving Average (EMA) strategy. The objective of training the student model is composed of a global loss and a local loss. Both are implemented by the Mean Squared Error (MSE) loss. The global loss aims to align the final representations output by the student and teacher models.

whereare the local patch representations output by all the transformer blocks in the teacher model andis the decoder output which aims to approximate the local representations of the masked patches based on the representations of unmasked patches from the student model;is the number masked patches in the input spectrogram to the student model andis the number of transformer blocks;is the output representation from the student model andis the average pooling result of;is the dimension of the representations. The final loss is a combination of global and local losses.

SECTION: 
In this section, we conduct thorough experiments to compare several deep models and experimental settings on classifying respiratory diseases based on cough sounds. We demonstrate the effectiveness of modern deep models in such tasks in practical use cases and provide insights into the choice of best practices in different stages of model training and inference.

SECTION: 
Three datasets are employed in the experiments for a thorough evaluation across different tasks and data distribution.

Thedatasetis collected to evaluate machine learning models classifying COVID-19 status using multi-modal data including cough sounds. In this study, we use the â€œmatched" training-test split released along with the dataset. As a result, there are 2,599 COVID+ and 2,599 COVID- participants in the training set and 907 COVID+ and 907 COVID- participants in the test set. We follow the same data split of task 2 inand formulate a binary classification task classifying the status of COVID-19 on this dataset.

Thedatasetcontains over 25,000 cough sounds labelled with COVID-19 statuses. Following the definition of task 5 in, we use this binary classification task (COVID-19 vs healthy) as one of the test beds in our study.

Our privatedataset is challenging yet more clinically useful as the negative samples cover not only the coughs of healthy subjects but also the coughs of subjects diagnosed with various respiratory diseases which may share similar cough signatures to COPD cough sounds. Specifically, the negative samples in our dataset include those collected from patients diagnosed with asthma, upper/lower respiratory tract infection (URI/LRI), Bronchiectasis and other non-COPD respiratory diseases. The LucaCough dataset consists of cough data from 3000 subjects (272 COPD and 2728 non-COPD subjects) in the training subset and cough data from 651 subjects (66 COPD and 586 non-COPD) in the test set. The cough sounds are recorded via several different mobile phones to make the data more diverse and enable the generalisation capabilities of the developed models. The data are manually annotated by at least three clinical experts with the help of medical records and lung function test readings. Informed written consent was obtained from all participants. The study was approved by the Ruijin Hospital Shanghai Jiaotong University School of Medicine Ethics Committee (No. 2023-199), Ruijin Hospital Luwan Branch Ethics Committee (2023-HXK-V1), Shanghai Jingâ€™an District Central Hospital Ethics Committee (No. 2023-33), Shanghai Zhabei Central Hospital Ethics Committee (ZBLL2024030401001) and registered at ClinicalTrials.gov (NCT06082791).

SECTION: 
We implement the proposed method in PyTorch and all the experiments are run on a GeForce RTX 4090 GPU. In the self-supervised pre-training stage, we use the default experimental settings used in. For supervised fine-tuning, we use a learning rate of 2e-6 and a batch size of 24 throughout our study. The Adam optimizer is employed if not specified otherwise.

Since all downstream tasks investigated in our experiments are binary classification problems, we use the area under the receiver operator curve (AUROC) as the evaluation metric if not otherwise specified. Multiple runs with different random seeds are conducted for each experiment to get the mean values as reliable results.

SECTION: 
There exist plenty of deep models pre-trained on ImageNet. In this study, we use thelibraryas a convenient tool to access various pre-trained vision models in a unified framework. Firstly, we choose the ResNet series from ResNet10 to ResNet101 to investigate how the model complexity of Convolutional Neural Networks (CNN) affects the performance on three downstream tasks. The results are shown in Table. It is interesting to see models with fewer layers (e.g., resnet10t, resnet14t and resnet18) generally outperform those with more layers consistently on three datasets. The possible reason could be deeper models require more training data to achieve good performance than their shallow counterparts.

Based on the observations of ResNet performance, we conduct follow-up experiments on four small vision transformer models as listed in the bottom part of Table. These transformer-based models perform comparably well with their CNN counterparts and the best one achieves the highest AUROC on two tasks (i.e. LucaCough and UK COVID-19), the third highest on the COUGHVID dataset.

Experimental results shown in Tabledemonstrate deeper neural networks with more model capacities may not always be better choices for downstream tasks. Our empirical study provides insight into how to choose ImageNet-pre-trained models for downstream tasks when the data distribution gap is large (e.g., natural images and cough audio spectrogram) and the amount of data for fine-tuning is limited.

SECTION: 
In this section, we evaluate the performance of models pre-trained on audio data (e.g., AudioSet-2M). Specifically, we evaluate PaSST-S, EAT-base and EAT-largeon the benchmark datasets. PaSST-S is based on DeiT-B384which has the same architecture as ViT-B. EAT-base is based on ViT-B hence both PaSST-S and EAT-base have 86M parameters, 12 Multi-head self-attention (MSA) layers, 12 attention heads and the embedding dimension is 768. EAT-large is based on ViT-L and has 307M parameters, 24 MSA layers, 16 attention heads and the embedding dimension is 1024.

We use the code and checkpoints released by the authors of these two works and follow their instructions for fine-tuning on the three downstream tasks. All the checkpoints utilised in our experiments were trained on the large-scale AudioSet-2M dataset in a supervised learning way. In addition, the EAT models were also pre-trained on the AudioSet-2M dataset using a self-supervised learning strategy before fine-tuning on the same dataset. To further fine-tune the models on our downstream tasks, the classification head is replaced with a linear layer with two output neurons for the binary classification problems.

The experimental results are shown in Table. The investigated models perform comparably well on three downstream tasks. On the COUGHVID dataset, EAT-large achieves higher AUROC than the lighter version EAT-base which again outperforms PaSST-S. However, compared with the experimental results shown in Table, all three audio data pre-trained models perform significantly better than most of the ImageNet-pre-trained ones on three downstream tasks consistently. The comparison between experimental results in Tablesandprovides clear evidence that pre-training on audio data benefits the classification of respiratory data including cough sounds.

SECTION: 
We conduct experiments to evaluate models pre-trained on respiratory sound data. The models employed in these experiments include those released by Zhang et al.and EAT models further trained on cough data by ourselves. Firstly, we apply the released pre-trained models to the LucaCough datasets and present the experimental results together with those fromon the other two datasets. Subsequently, we continue the pertaining of EAT models on a large-scale cough sound dataset. As a result, we have a variant of the pre-trained EAT-large model dubbedas it was further trained on cough data. Themodel is subsequently fine-tuned on downstream tasks to obtain the classification results and compare them with other respiratory sound data pre-trained models (i.e. the OPERA series). To make a fair comparison, we use the official code released byto fine-tune the OPERA models on downstream tasks whilst only linear probing results were reported in the original paper.

The comparison results are shown in Tablefrom which several conclusions can be drawn. Firstly,performs the best on three tasks consistently. Particularly, it outperforms EAT-large on three tasks with the AUROC margins ranging from 0.9 to 3.3 percentage points. This proves that continuous pre-training of the model on cough data can further strengthen its capabilities of classifying diseases based on cough sound data. Secondly, the OPERA models, though pre-trained on respiratory sound data in a self-supervised learning manner, perform inferior not only tobut also to the models pre-trained on general audio data (i.e. PaSST-S, EAT-base and EAT-large). This may be attributed to the fact of low model capacities or the lack of pre-training on large-scale audio data like AudioSet-2M.

SECTION: 
In this subsection, we focus on exploring the effectiveness of the SAM optimizer in our particular cough classification tasks. Specifically, we compare experimental results without and with the use of SAM optimizer during fine-tuning whilst keeping all other experimental settings the same. The comparative study is conducted on the LucaCough dataset with three representative pre-trained models: PaSST-S, EAT-base, EAT-large and. The empirical results are shown in Table. The experimental results demonstrate the use of SAM improves the AUROC values consistently for all four models.

SECTION: 
This study presents a comprehensive evaluation of various deep learning models and their performance in classifying respiratory diseases such as COVID-19 and Chronic Obstructive Pulmonary Disease (COPD) using cough sound data. Our proposed approach, which leverages both self-supervised and supervised learning on a large-scale cough dataset, has demonstrated superior performance on three datasets.

The use of pre-trained models, particularly those pre-trained on respiratory sound data, has shown significant benefits in enhancing the classification of cough sounds. The continuous pre-training of models on cough data, as evidenced by the performance of themodel, further strengthens the modelâ€™s ability to discern disease signatures from cough sound data.

While our approach has shown promising results, there is room for further improvement and exploration. Future work could involve the integration of additional data modalities, such as clinical and demographic information, to enhance the predictive power of the models. The development of more sophisticated self-supervised learning techniques tailored to the unique characteristics of cough sounds could potentially uncover more nuanced patterns in the data.

In conclusion, our study represents a significant step towards reliable and accurate respiratory disease diagnosis using cough sounds. The insights gained from this research have the potential to inform the design of future diagnostic systems, ultimately contributing to improved disease control and human well-being.

SECTION: References