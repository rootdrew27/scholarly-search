SECTION: Density forecast transformations
SECTION: Introduction
Forecasting models are often specified to produce-step-ahead forecasts, which implies that the predictions do not contain information on their cross-horizon dependence. As a consequence, individual h-step-ahead predictive distributions cannot easily be transformed into predictive objects that depend on several horizons. For example, the literature on macroeconomic risk often uses quantile regression models that, in brief, producedensity forecasts of quarter-on-quarter (qoq) real GDP growth. However, it is not straightforward in this framework to construct, for instance, density forecasts for annual-average growth rates from the estimated quarter-on-quarter density forecasts because of their potential serial dependence.

To address this issue, we propose using Gaussian copulas to combine the information of the marginal-step-ahead predictive densities into a joint distribution, which reflects the serial dependence between the marginals. This enables the practitioner to draw from the multi-horizon distribution and, therefore, to construct predictive distributions that are functions of several horizons, which we label target-frequency predictive densities.The resulting multi-horizon predictive objects provide forecasts that are coherent with the underlying marginal predictive densities, in the sense that the moments of the multi-horizon objects are a function of the marginals and the copula parameters.

While separate forecasting specifications could be used to make predictions for the different predictive objects, there are several reasons why a single specification can be preferable. First, the scenario of having a single forecasting specification is particularly common among professional forecasters and institutions, such as central banks, where changing the forecasting process is costly and yet transformations of the existing forecasts to other target-frequencies are often required. Second, using a single forecasting specification leads to predictive objects with moments that are coherent across the target-frequencies. For instance, using our approach, the mean of the annual-average forecast is consistent with the mean of the quarter-on-quarter predictions.
Third, some density forecasts are derived from surveys that only report one frequency but not the required target-frequency. In general, our approach helps to broaden the usability of already available individual predictive densities that are based on aforecasting scheme.

As an alternative to our approach, the researcher could use a simple approach that assumes independence between the different marginal predictive densities, i.e., no correlation between the-step-ahead predictions at different horizons. However, this approach neglects the serial dependence typically present in macroeconomic indicators, which is important for the tails of the multi-horizon distribution. For example, accounting for the positive serial correlation in quarter-on-quarter GDP growth forecasts leads to considerably fatter tails of the annual-average predictive distributions. This is because an approach that constructs the annual-average growth while taking into account the path of quarterly GDP growth over several quarters and a positive correlation would reflect the feature that large positive (negative) growth is typically followed by positive (negative) growth. This makes our approach particularly appealing for macroeconomic and financial risk applications.

In this paper, we show analytically that the proposed copula approach for density forecast transformations may outperform substantially a benchmark approach that neglects the cross-horizon dependence of the marginal densities. We also show how the resulting gain in the forecasting performance depends on the persistence of the underlying process. Further, we show in several Monte Carlo studies that our approach provides a better approximation to the true underlying annual-average density forecasts under different DGPs. This result holds under misspecified forecasting models, i.e., when the true multivariate distribution is not Gaussian, and for small training samples for the copula parameter estimation.

For the application of our approach, the researcher only needs to compute the correlation between the empirical PITs of the individual-step-ahead predictive distributions for different horizons in a training sample. In particular, the forecaster needs to (i) compute the sequence of realized PITs for the marginal predictive densities at each forecast horizon, from a pseudo out-of-sample exercise over a training sample, and to (ii) combine the marginal distributions into a joint distribution via a multivariate Gaussian copula, where the maximum-likelihood estimator of the copula parameters is equal to the rank correlation of the realized PITs.

We demonstrate the usefulness of our methodology in three empirical applications, where we compare the forecasting performance of the proposed copula approach with a benchmark approach that ignores the serieal dependence between the multi-horizon marginal distributions. The first application is a large-scale forecasting exercise based on monthly data from FRED-MDentering bivariate ARDL models regressing a large number of pairs of randomly selected variables. We first compute density forecasts for month-on-month values from these regressions and we then use these predictive densities to compute quarter-on-quarter density forecasts. Results show that the copula approach outperforms the benchmark for the majority of randomly specified bivariate models.
The second empirical application aims at emulating a situation in which a forecaster dispose of predictive densities for year-on-year U.S. monthly inflation, but she needs to summarize the picture of the expected inflation environment by converting the target frequency of the predictive densities from monthly year-on-year rate to annual-average rate. Importantly, the year-on-year and annual-average predictions need to be coherent, i.e., they should be based on the same predictors and model type, and the central tendency of the forecasts across the two frequencies should be very similar. The results show that our copula approach provides significantly better density forecasts of annual-average inflation, in particular at the tails of the distributions, than the benchmark.
Finally, in the third empirical application, we use the predictive densities of quarter-on-quarter U.S. real GDP growth from, which are based onforecasts, and transform them into annual-average forecasts. We find again that the annual-average forecasts based on our copula approach leads to more accurate predictive densities than the benchmark approach.

Our paper contributes to the literature on density forecasts and economic risk prediction. Whileintroduced conditional copulas to economics with a focus on modeling the cross-sectional dependence of predictive objects (see, for review), we show how copulas can be used for density forecast transformations. In particular, compared to existing contributions, the proposed copula-based approach is designed to combine the marginal forecast densities, unimodalor multimodal, into new predictive objects of several horizons. Differently from, who use copulas to model both the time series and cross-sectional correlation of U.S. macroeconomic variables, we propose to use copulas for modeling the time series correlation and transforming marginal density forecasts into multi-horizon objects. Finally, compared with recent works also proposing to model the joint distribution of predictive objects (e.g.,, for the joint distribution of point forecast errors obtained from surveys via multi-variate stochastic volatility models;, for joint forecasts of hourly electricity prices from point forecasts;, for the construction of fixed-horizon density forecasts out of fixed-event survey density forecasts), we propose to use Gaussian copulas to combine marginal predictive densities of macroeconomic indicators to obtain predictive objects that are transformed to a new target-frequency.

The paper is structured as follows.provides an analytical example of our forecasting environment.describes the methodological framework.presents Monte Carlo results. Inwe provide a robustness analysis and discuss alternative simpler approaches.presents the results from the three empirical exercises. Finally,concludes.

SECTION: Motivating example
Consider the following simple mean-zero autoregressive model:

withand. The optimal-steap ahead prediction (under both iterated and direct forecasting approach) is given by

It follows that the forecast errorhas second moment

and auto-covariance and auto-correlation functions:

for.denotes the Pearson correlation. Thus, the process has the following conditional predictive distribution:

with the pdf ofdenoted by. Now consider a linear transformation of the forecast sequence, such as

witha vector of weights. This transformation is often useful in macroeconomic applications when the original forecasts need to be converted into a different target periodic measure of the same variable. For instance, ifis a month-on-month growth rate sampled at monthly frequency andis its-step ahead forecast, then forand, the transformed forecast

is (approximately) the 12-months ahead forecast of the year-on-year growth rate.

Following the example in (), the conditional predictive distribution of the “dependence-attentive” transformed forecast is:

which is the sum of random variables from the joint multivariate Normal forecast distribution

whereand

The pdf ofis denoted by. The predictive distribution in () can be compared to the predictive distribution of the forecaster who ignores the cross-horizon dependence of the forecasts. We label this forecaster the “dependence-inattentive” forecaster because no attention is paid to the potential correlation structure of the forecast errors across horizons. This forecaster draws from the following conditional forecast distribution:

where the pdf ofis denoted by. Note that for, the conditional predictive distribution of the “dependence-attentive” forecaster in () simplifies to:

The panels ofshow the average relative accuracy of the "dependence-attentive" density forecast in () compared to the "dependence-inattentive" density forecast in () with, evaluated through proper scoring rules, such as the quantile-weigthed continuous ranked probability score (QW-CRPS;) which emphasizes the tails, the continuous ranked probability score (CRPS;), and the 10% quantile score (QS(10%);). All these metrics show robust gains for the "dependence-attentive" forecast, which increases monotonically withand. For instance, withand(the year-on-year growth rate transformation for month-on-month predictions), the gain would stand about 7% according to the CRPS, while the QW-CRPS and the QS(10%) point to a gain of about 15-18%. Not surprisingly, the latter suggest that substantial accuracy gains can be obtained when evaluating the tails of the predictive densities, rather than their central region. This is due to the fact that the two densities differ solely in their variance, while the other moments are identical.

In this stylized example, the researcher knows the dependence structure across forecasting horizons and, therefore, the joint distribution of the. However, in practice when using direct forecasting schemes, this dependence structure is unknown and not estimated alongside the predictions. The next section, therefore, introduces a methodology to estimate the dependence structure for a given set of direct density forecasts.

SECTION: Constructing multivariate densities with copulas
This section first introduces the necessary notation and then explains the methodology to transform the marginal density forecasts into the desired target multi-horizon object.

SECTION: Notation
denotes the higher frequency prediction that the researcher would like to transform into. The variableis a function ofand potentially ofand its lags. The time subscriptand the forecast horizon subscriptdenote the time units of. For instance, ifdenotes monthly predictions thendenotes a transformation of the monthly forecasts using at most predictions up to month. Recalling the example of the previous section, ifis a month-on-month growth rate sampled at monthly frequency, then forand, the transformed forecast

is approximately the 12-month-ahead forecast of the monthly year-on-year growth rate. In this case, we shall denoteaof the forecast sequence.

Similarly, a linear transformation can map the forecasts generated at the original sampling frequency of the data into a forecast sequence sampled at a desired lower target frequency. For instance, letbe a month-on-month growth rate sampled at monthly frequency. Then, for, the transformed forecast

is (approximately) the two-quarter-ahead forecast of the quarterly quarter-on-quarter growth rate (see).In this case, we shall denoteaof the forecast sequence.

Throughout the paper, we use a boldface notation for vectors, matrices, and functions that take vectors as input.

SECTION: Methodology
From the previous section, it is clear that constructing the predictive density of transformed forecasts requires drawing from the joint predictive distribution across the forecasting horizon. However, this can be often impractical in empirical applications, such as those relying on direct multi-step forecasting.

To address this issue, in this paper we propose to resort to (Gaussian) copulas, which allow to model the marginals and the dependence separately while ensuring that the researcher obtains a valid multivariate distribution; seefor an introduction to copulas. Since we consider a forecasting environment, we work with the conditional copulas defined inbut drop the conditioning sets from the notation for simplicity.

Continuing with the example in, the predictive density, i.e. the density of, can be constructed by drawing from the joint distribution ofdescribed in eq. (). Denoting bythe CDF of the Gaussian predictive distribution ofevaluated at, we note that:

which is the Pearson correlation coefficients of the probability integral transforms (PITs). The Gaussian copula is given by:

wheredenotes the inverse CDF of a standard Normal,the joint CDF of a standard multivariate Normal with covariance matrix, and,, are the PITs of the univariate predictive densities over theforecasting horizons. Note thatis hence the correlation matrix, whose elements are defined in (). Further note that the predictive distribution of the “dependence-inattentive” forecaster in () is also equivalent to the joint distribution of the forecasts constructed through a Gaussian copula, but with. Sinceis the multivariate distribution of the random variables,, given the copula, it is easy to resample thefrom their joint distribution:

and then compute the desired (periodic or frequency) transformed density forecast from the sampled joint forecasts.

In the following we extend this approach to a more general forecasting environment. Assume the forecaster has a set of-step-ahead predictive densities forforecast origins, denoted byand with predictive cumulative distribution functions (CDF), for outcome variable; the subscriptdenotes the forecast horizon and the subscriptdenotes the forecast origin. Further assume that the set of predictive distributions,, is taken as given, for instance, due to institutional restrictions on the forecasting model to be used.

To illustrate the application of our methodology, but without loss of generality, we will assume that the predictive densityis a predictive density for quarter-on-quarter growth rates.In period, the forecaster is asked to provide predictive densities for the annual-average growth rates as well as for the conditional predictive density, henceforth called path-forecast, based on.

We propose to do this by using copula functions, developed by. A copula can be described as a function such that for any, whereis the multivariate distribution function of the random vector, there is a copula function, such that=, whereare the marginal CDFs of, respectively, anddenotes the parameter(s) that governs the dependence between. Inversely, a copula function, combined with marginal CDFs, gives a multivariate distribution.

A popular copula family is the Gaussian copula, denoted by, where the dependence between thevariables is governed by the correlation matrix, with ones on the diagonal and the rank correlation of variableandas the respective off-diagonal element.

Let thendenote the joint predictive CDF offor forecast origin, conditional on the correlation matrixand constructed using. In other words, we define. Further, letthe probability integral transform of the predictive densities, whereis the realized value. The forecaster can obtain an estimate ofby implementing the algorithm described below.

The resulting multivariate distribution allows to sample the-step-ahead predictions jointly, such that predictive objects that are functions of several horizons can be constructed. Note that the maximum likelihood estimator ofis, i.e. the maximum likelihood estimator of the correlation matrix under a Gaussian copula reduces to the rank correlation of the PITs.

To illustrate the use of Algorithm 1, consider the following example. The forecaster is asked in, which is the last quarter of the year, to provide a predictive distribution of the annual-average growth for the next year. The forecaster, however, has only a set ofquarter-on-quarter-step-ahead growth rate predictions available, for. To transform the quarter-on-quarter growth rates into annual-average predictions, the forecaster can obtain the setof draws, for, from. This can be easily implemented in standard statistics packages. First, do step 1 to 2 of Algorithm 1. Then, step 3 amounts to the following: compute the lower Cholesky decomposition of, denoted by. Next, for each:

Draw avector of independent standard Normals, i.e. drawwhereis theidentity matrix.

Compute the vector, whereis the CDF of a standard Normal distribution and.

Evaluateto get the vector of joint draws.

The set of vectors of joint drawsapproximatesand can be used, alongside with the observations, to obtain the predictive distribution of the annual-average growth rate using, for instance, an exact formula or a linear approximation formula as in eq. (), for each. The multivariate distribution also allows to obtain draws ofconditional onfor.

Our approach aims at constructing well performing forecasts but we note that there are several potential sources of misspecification. First, the choice of the Gaussian copula might not reflect the true underlying multivariate distribution of the data. Second, if the marginal distributions are misspecified, the joint distribution will also be misspecified. Third, a potential source of misspecification comes from the conditional copulas. As shown by, if the conditioning set is not identical for the marginals and the copula, then the constructed conditional joint distribution might not reflect the actual conditional joint distribution. Following’s () example, if the researcher conditionson,onand the copula on, such that, thenwill only denote the conditional joint distribution ofif the marginalis independent ofand the marginal ofis independent of(for details see). However, given the macroeconomic applications that we consider, this is unlikely to restrict the implementation of our approach for mainly two reasons. First, the marginals come fromforecasting models that use the same predictors for each forecasting horizon. Second, if the predictors differ across forecasting horizons that is typically because different predictors are relevant at that horizon, i.e., the marginals are independent of the non-included predictors.

Further, our estimation algorithm assumes that (i) dependence parameters do not depend on the predictors used for the marginals and (ii) restricts the copula dependence parameters into be constant. Assumption (i) is common in the literature and across model classes. For instance, flexible models such as VARs with stochastic volatility do not make the time-varying variance-covariance matrix an explicit function of dependent variables. However, while we propose a simple algorithm for practitioners, both assumption (i) and (ii) can be relaxed. For instance,suggests an ARMA-like specification to allow for time-variation in the copula parameters.propose a stochastic process for the copula parameters. Both specification could be extended to include exogeneous variables in the copula parameter equations.

Despite the various potential sources of misspecification, the Monte Carlo results in the next section show that our modelling approach outperforms the competitor approach and leads to forecasting performances that are often indistinguishable from the correctly specified predictive distribution.

SECTION: Monte Carlo study
We study the performance of our suggested copula approach via Monte Carlo simulations in a scenario where the forecaster has a model that produces-step-ahead predictive densities for qoq growth rates and then needs to transform the predictive densities into annual-average growth rates and yoy growth rates. The absolute and relative performance of the proposed approach with respect to a simple benchmark ignoring cross-horizon dependence of the forecasts (the “dependence-inattentive” forecaster described in) are evaluated through an out-of-sample exercise.

SECTION: Monte Carlo design
The underlying DGP of qoq growth rates, denoted by, takes the form of a VAR(1):

whereare two uncorrelated sequences of independent and identically distributed (iid) structural shocks. We set, with, but we consider three different specifications for the error term: () a Normal distribution, () a Skew-Normal distribution, or () a Skew-distribution.
For cases () and (), we adopt a location-scale-shape parameterization. All the distributions are calibrated to have mean zero and standard deviation, as well as negative skewness for cases () and () (with shape parameter). For the Skew-distribution, the degrees of freedom parameter is set to, which implies somewhat heavier tails than the Normal distribution. We consider these different specifications to allow for a varying degree of complexity in the DGP. As for the remaining parameters, we set,, and. To account for different degrees of serial correlation, and hence cross-horizon dependence in the multi-step forecasts,takes one of the following values:.

We consider two types of forecasting models, both of which are misspecified AR(1) and produce-step-ahead forecasts. The first forecasting model is used whenin the DGP is drawn from a Normal distribution:

with.

The second forecasting model is a quantile regression specification used whenin the DGP is drawn from the Skew-Normal or the Skew-distribution:

wheredenotes the quantile with, andanddenote respectively the quantile specific intercept and autoregressive parameter. To obtain a full predictive distribution, we smooth the five predicted quantiles using the Skew-of. Note that the latter introduces a second potential source of model misspecification, in addition to that implied by the specification in ().

The parametersandare estimated using a rolling-window estimation scheme with sample size, which we set to 200 (quarters). Estimated parameters are used to compute qoq predictive densities up to 12 quarters ahead, from which we getforecast paths. We then use simple approximating formulas to construct annual-average predictive distributions from those paths. For instance, assuming that the forecast origin is the last quarter of the year, for each forecast paththe one year-aheadannual-average forecast is given by:

and the four quarters-ahead year-on-year forecast:

Two competing approaches are here considered:the benchmark approach, which constructs the annual-average and year-on-year forecasts by directly transforming the quarter-on-quarter forecast paths with () and (), i.e., without accounting for the cross-horizon dependence; andthe copula approach, which in turn constructs the predictive distributions for the annual-average and yoy growth rates using first the methodology described in Sectionto obtain joint draws of the quarter-on-quarter growth rates, and then expressions () and () to transform the joint draws.

The out-of-sample size used to compute the correlation of the empirical PITs for the copula approach is denoted byand set to 50 (quarters).Similarly, the out-of-sample size used to evaluate the annual-average (resp. year-on-year) density forecasts is denoted byand also set to 50. In particular, we simulate 200 periods of quarterly data and we then produce annual-average (resp. year-on-year) forecasts every four quarters for horizons one, two, and three years ahead.Results are based on 500 Monte Carlo iterations. For each iteration we evaluate the performance of the benchmark and copula approach using both relative and absolute forecasting performance measures. For this purpose, in addition to the qoq growth rates, we also simulate the true annual-average and yoy growth rates generated by the DGP described in ().
As in, the relative performance measures include the QW-CRPS (calibrated to evaluate the predictive performance at the tails of the distribution) and the CRPS.The relative performance is evaluated both by directly comparing the two approaches and against the forecasts one could generate with knowledge of the true underlying DGP and the true parameter values. The absolute forecasting performance is measured through the test for the correct specification of the predictive distribution proposed by, which is a test for uniformity of the PITs.

SECTION: Monte Carlo results
shows the relative forecasting performance of the benchmark and copula approach, as well as the rejection frequencies of an equal predictive ability test when comparing either of the two approaches against the optimal forecast. The markers show the average score ratios (rejection frequencies) across all Monte Carlo iterations, while the horizontal lines denotebootstrap standard errors of the average score ratios (rejection frequencies). Panel (a) shows the ratio of the QW-CRPS of the copula approach and the benchmark approach, i.e., valuesindicate superior performance of the copula approach. Panel (b) shows the analogue ratio for the CRPS. As expected, for a serial dependence close to zero there is no improvement in the forecasting performance when using the copula approach. For medium to large values of, the copula approach outperforms the benchmark approach by about 5% to 15% in the case of the QW-CRPS and by about 3% to 7% in the case of the CRPS. Panel (c) and (d) show the average rejection frequencies of the null hypothesis of equal predictive ability using the unconditional test of. The scoring functions are the QW-CRPS (c) and the CRPS (d) and the test compares the copula (benchmark) approach against the optimal forecast. The nominal size is 5%, i.e., rejection frequencies above 5% indicate that the null hypothesis is rejected more often than expected given the nominal size. Panel (c) and (d) ofshow that for small values of, the copula approach performs slightly worse than the benchmark approach. We attribute this result to the additional parameter estimation uncertainty induced by the estimation of the copula parameters, which is likely to dominate the gain from taking the temporal dependence into account. In contrast, with increasing temporal dependence, i.e. increasing, the rejection frequency associated with the copula approach tends to be close to the nominal size, whereas, for the benchmark approach, the null hypothesis of equal performance is rejected much more frequently.

It is worth noting that the rejection rate for the copula approach is close to the nominal size when compared to the true predictive density. This is an important result, since for the case with Skew-Normal and Skew-errors, the Gaussian copula is potentially misspecified. Further, for the Skew-Normal case, the marginals are potentially misspecified since we use a Skew-distribution to smooth the quantile regression predictions. Importantly, the results suggest that these misspecifications have only a small impact on the overall forecasting performance, because the copula-based predictive distributions appear often indistinguishable from true predictive distributions in terms of forecasting performance. As regards the year-on-year forecasts, the results are reported inand are very similar to the annual-average results.

shows the rejection frequencies of the null hypothesis of a correct specification of the predictive densities, evaluated using the test of. Results show that a high temporal dependence implies that the benchmark approach leads to misspecified predictive distributions. In contrast, the copula approach adjusts for the temporal dependence and shows rejection frequencies only slightly above the nominal size. Hence the results insuggest that the consequences of the aforementioned misspecifications on the calibration of copula-based predictive densities are fairly negligible, as the rejection rates hovers around the nominal size of the test.

SECTION: Robustness of the copula and alternatives
This section provides some guidance on how the training sample size and the persistence of the process affect the performance of the copula approach and discusses potential alternative approaches.

SECTION: Robustness of the copula approach
This subsection further explores to what extent the performance of the copula approach depends on (i) the training sample for the copula parameter, and (ii) the persistence of the underlying process. Our main goal here is to provide a guideline to the practitioners by setting some rules-of-thumb when choosing whether or not implementing the copula approach on specific applications. Note, however, that the conclusions drawn in this section are, to some extent, dependent on the DGP considered.

For our first robustness check, we repeat the Monte Carlo experiment of, keeping the design of the simulations unchanged except for the number of in-sample observationsand the number of observationsused to estimate the copula correlation matrix. For this purpose, we use a grid of= 100, 200, and 400 and= 25, 50, 100, and 200 observations. Note that the values in the baseline Monte Carlo experiments discussed inareand. We focus on, the case of intermediately strong serial correlation, we compute the same scores as in, and we report the relative percentage gain of the copula approach with respect to the benchmark approach (neglecting cross-horizon dependence).reports the results for the one- and three-year-ahead annual-average predictions, and point to an overall stability of the relative performance of the copula approach across simulations. A slight improvement seems to arise from largerand, in particular when, but the results are often only marginally different and statistically indistinguishable according to (bootstrap) standard errors of the average relative scores. In other words, the Monte Carlo study suggests thata moderate training out-of-sample size should be sufficient for estimating copula parameter matrixand obtaining relatively accurate forecasting results from the copula approach, andthe training in-sample size used to estimate the model parameters appears less relevant.

For our second robustness check, we compute the relative forecasting performance of the copula approach for different levels of persistence measured by the determinant of the estimated correlation matrix,. On the one hand, in the extreme case of completely independent marginals across all the forecast horizons, the determinant ofis equal to one. On the other hand, in the extreme case of a perfect correlation across the marginals, the determinant is equal to zero. To evaluate the effect of different degrees of path-dependence across forecast horizons, we again repeat the Monte Carlo experiment of, but for each Monte Carlo iteration we now draw (with replacement) some of the VAR(1) parameters from the following uniform distributions:and. To attenuate the effect of finite samples on the results, we set the training sample atand we generateobservations for the out-of-sample evaluation of the transformed densities. For ease of analysis, we focus here on the Normal error case,, and on annual-average density forecasts.
Results are presented in, where the-axis denotes the relative performance gains and the-axis shows ranges ofcategorized into bins with width 0.1: smaller values on the-axis indicate a higher level of persistence. The figure shows results for the QW-CRPS and the QS(10%) in so-called violin plots for the one- and three-year-ahead annual-average predictions. The curves in the figure depict the distribution of the gains over the 1000 Monte Carlo iterations, and the boxes show the inter-quartile range. For high levels of persistence, the gains are on average large and the distribution exhibits a large right-tail towards regions of big relative forecasting gains for the copula approach. For medium levels of, the copula approach still shows substantial predictive gains relative to the benchmark approach. However, these gains tend to disappear with a decreasing persistence of the process.

SECTION: Alternative approaches
It could be argued that the copula approach described so far may be inefficient compared to simpler alternative approaches that provide transformed forecasts in one single step. For instance, the researcher interested in the transformation in () could run regressions using directly the transformed variable, instead of modelling the data at the original sample. Note that while this would violate the requirement of coherence between the moments across the different frequencies, this section nonetheless entertains the idea of a direct regression ofon its past history. However, it turns out that the forecasting regression based on the transformed sample, using the simplified example as in, does not lead to a significant superior forecasting performance.

Assume again that the DGP takes the form of the autoregressive modeland that the target forecast follows the linear transformation. A regression of the annual-average on its past values, or in our simplified illustrative example,on, leads to the following expression:

This suggests to construct a predictive density forcentered on the conditional meanand featuring a variance based on. First, note that the conditional mean is not optimal becauseis a linear combination of lags of, i.e., the forecaster partly conditions on "out-dated" information. This is akin to a situation where the forecaster only uses the lower frequency instead of taking a mixed-frequency approach. The information in the error terms,in () that already realized in. In other words, the conditional mean forecast is not efficient. Second, note that in the regressionandin () are correlated, i.e., the OLS estimator ofis inconsistent.

We repeat the Monte Carlo experiment ofusing the same DGP and running the alternative direct annual-average regression described in () usingas predictor for. In this experiment we evaluate its forecasting performance relative to the copula approach.reports the results for= 0.1, 0.4, and 0.7, three error distributions, and one- to three-year-ahead annual-average predictions (values <1 mean that the copula approach outperforms the alternative approach). The results interestingly point to a strong predictive gain of the copula approach compared to the alternative approach for the one-year ahead prediction, while from the two-years ahead onward the two approaches provide very similar results.
Abstracting from parameter estimation errors and the parameter consistency problem in the estimation of the direct annual-average regression, the forecasting gains by the copula approach can partly be explained by better conditional mean predictions of the higher frequency regression. Consider again the autoregressive DGPand assume for simplicity and without loss of generality that the calendar year is composed of two periods and that target forecast follows the weighted linear transformation:

which is akin to an actual one-year- and two-years-ahead annual-average forecasts if thewere to represent semi-annual growth rates. Note that the expression forembeds the observation at the forecast origin, in addition to the forecasts produced at forecast origin, whileincludes only forecast terms. The mean squared forecast errors ratio of the annual-average prediction provided by the copula approach () and the alternative direct annual-average approach () are:

For, it can be shown that the obtained expression in () (a ratio of two second-order polynomials) is always lower than 1 for, and it ranges between 0.8 and 0.6 for. Conversely, for, the ratio of mean square forecast errors simplifies to a ratio of two sixth-order polynomials, which is very close to 1 forand still above 0.9 up to. These analytical results are consistent with the simulation results reported in, where a slightly different DGP and additional metrics are considered. In summary, the expected forecasting performance of the direct annual-average approach is substantially worse than that of the copula approach, in particular forand in presence of strong persistence. The intuition is that, compared with the copula approach, the direct annual-average approach likely makes an inefficient use of the available short-term information. However, forthis inefficiency loses importance, and the predictive performance converges across the two approaches, because the underlying DGP is mean reverting and for longer horizons the initial information advantage in the copula approach becomes irrelevant. Finally,reports the results for the PIT test for the direct annual-average approach only. The results point overall to a correct specification of density forecasts, with rejection frequencies even closer to the nominal size than those of the copula approach reported in. The findings hence suggest that the alternative direct annual-average approach may be inefficient but correctly specified (see, for other examples of such predictive densities).

SECTION: Empirical applications
This section provides three distinct empirical exercises where the copula approach is evaluated in a forecasting environment with actual macroeconomic data. Note that we do not use real-time vintages in the forecasting exercises, so all “out-of-sample” forecasts are strictly speaking pseudo out-of-sample forecasts.

SECTION: Large bivariate exercise
In this section, we provide the results of a large-scale forecasting exercise based on monthly data from FRED-MD. In the construction of the forecasting environment, we closely followand consider pairs,, of two monthly series randomly drawn from the dataset. We first compute up to 12-months-ahead density forecasts of cumulative growth at a monthly frequency, withmonths and. Density forecasts are obtained through horizon-specific monthly-frequency autoregressive distributed lag (ARDL) direct-multistep regressions, estimated via OLS at each forecast origin. Then, we apply our proposed copula approach to the monthly density forecasts to compute up to four-quarter-ahead density forecasts of cumulative growth at a quarterly frequencyup to four quarters ahead. In particular, for each for, we run the following regression to obtain the monthly density forecasts:

with

anddenotes the levels or log-levels. Forecastsare then used to getand the quarterly average. Finally, the quarterly frequency transformation is obtained as:

For the sake of simplicity, we only consider the first month of each quarter as a forecast origin. The number of lags in () is either fixed ator selected through the Bayesian Information Criterion (BIC) among.

We consider two samples: a full sample starting in 1974:M1 and a reduced sample starting in 1984:M1, covering only the Great Moderation period. Different samples may help to assess the impact of potential breaks in the persistence of the series over time. The sample ends in both cases in 2019:M12. The sample is partitioned into (i) an in-sample part, of size, for the estimation of the parametersand, (ii) a calibration part, of sizeand spanning from 1995:Mto 2004:M12, for the estimation of the copula parameters, and (iii) a forecasting part, of sizeand spanning from 2005:Mto 2019:M12, for the out-of-sample evaluation in the target frequency. Note that as in,denotes the number of periods in the target frequency, i.e., in quarterly observations whereasandare denoted in the monthly frequency. The estimation is carried out using a rolling-window approach, updating at each forecast origin the model parameters, the PITs, and the copula parameters.

As a datasource we use the February 2023 vintage of FRED-MD. After dropping series not meeting some minimal conditions, the number of variables used in the bivariate exercise is 101, organized into 5 different groups as inand: (1) income, output, sales, and capacity utilization; (2) employment and unemployment; (3) construction, inventories, and orders; (4) interest rates and asset prices; (5) nominal prices, wages, and money.

Then, 650 random pairs of variables are selected from the database such thatandcome from distinct groups and an equal number of series pairscomes from each of the 10 possible group pairings. For each permutation of the series, we compute density forecasts at the quarterly target frequency, based on the monthly density forecasts, using either the copula approach or the benchmark approach, and we compare their forecasting performance using their average QW-CRPS and CRPS over the out-of-sample. For each lag selection method and forecast horizon, the 650 random pairs forming the systemsandprovide a distribution of 1,300 average score ratios.

shows these distributions after computing the relative % gain of the copula approach with respect to the benchmark. The labels “Full sample” and “Great Moderation” indicate the samples used for the forecasting exercise. The copula approach provides, on average, a better forecasting performance for all horizons, with the predictive gain increasing with the horizon up to around 10% according to the QW-CRPS and around 5% according to the CRPS, irrespective of the sample used for the estimation and the lag length. It is worth noting that these empirical predictive gains appear overall statistically positive, as the 95% confidence interval for the median and the inter-quartile range tend to exclude the zero value. To grasp the statistical significance of these results in a more formal way, we also compute for each pairing a test of unconditional equal predictive accuracy. The results are summarized in, where we provide the overall rejection frequency of the null hypothesis of equal predictive accuracy and its decomposition into the rejection frequency when the alternative is that the average loss of the copula is inferior to the average loss of the benchmark (the blue bar) and the respective opposite alternative (the red bar). The rejection frequencies appear relatively high across all samples and horizons, hovering around 0.6-0.8% for the QW-CRPS and 0.5-0.6% for the CRPS, and it can be largely attributed to the copula approach.

shows the empirical distribution of the determinant of the estimated copula parameter matrices. The determinant is a summary measure of the persistence of the forecastsacross the forecast horizons(see). The results suggest that for a majority of the randomly selected bivariate systems, the persistence inis considerably large. This should explain the superior average forecasting performance of the copula approach relative to the benchmark approach depicted in. The large average cross-horizon persistence is, in part, driven by defining the monthly density forecasts as cumulative growth rates such that shocks are accumulated across the horizons. However, predicting cumulative growth rates is not at all unusual and, in fact, the definition of the predictands in () is taken from.

SECTION: Inflation-at-Risk
In this section, we provide estimates of inflation at risk for year-on-year and annual-average inflation based on the U.S. Consumer Price Index. We use quantile regressions in combination with a Lasso, to select among many potential predictors, and produce monthly forecasts of the year-on-year inflation rate. Then, we construct predictive distributions of the (calendar year) annual-average inflation rates out of the year-on-year predictions. This forecasting environment aims at replicating the situation where a professional forecaster dispose of predictive densities for year-on-year inflation, but she is required to transform them into lower-frequency annual-average densities. This is a pretty common operational framework across institutions such as central banks, where the communication around the expected inflation environment is usually performed around annual-average rates, even though the monthly year-on-year rate represents the operational target.Importantly, the year-on-year and annual-average predictive densities need to be coherent, i.e., they should be based on the same predictors, model type, and the moments of the baseline frequency should be reflected in the moments of the new target frequency.

The underlying price index is the monthly and seasonally adjusted Consumer Price Index for all Urban Consumers(henceforth CPI) from 1960 to 2022. The baseline model predicts monthly year-on-year inflation, computed via the log-difference ofand, via a quantile regression model that uses a Lasso to select among a number of potential predictors. The quantile regression coefficient vector, for theth quantile, is estimated by minimizing the following objective function:

wheredenotes the monthly year-on-year inflation rate,denotes avector of predictors (including a constant),,is a hyperparameter that determines the degree of penalization (set here as recommended by),is the number of predictors,is the forecast horizon,is the in-sample estimation,, denotes the initial observation which changes due to a rolling window estimation scheme, and.

Note that because not all predictors are available over the entire range of the sample, not all predictors enter the model in all forecast origins (seefor details on the data series). Therefore, the predictor vectorcontains at most 23 exogeneous predictors, similar to the predictors used in, as well as two lags of the endogenous variable and the constant.

We use the model in () to produce out-of-sample forecasts for horizonsmonths ahead, with the first forecast origin being 1974:M12 and the last forecast origin being 2021:M12. At each forecast origin, the quantile regression is re-estimated over a rolling window of 15 years of monthly data, that is,. For example, for the forecast origin 1974:M12, the first observation used in the model estimation is 1960:M1.
We let the model produce quantile predictions forand we calculate the predictive CDF based on linearly interpolating between adjacent quantiles (see, for a similar approach; details on the interpolation are provided in).Finally, the generated year-on-year predictive densities are then transformed into annual-average inflation densities by drawing from the joint distribution of the year-on-year forecasts using either the copula or the benchmark approach. In order to approximate the annual-average, we first compute month-on-month rates from the year-on-year forecasts, and then we transform the month-on-month predictions into annual-averages using the approximation of.

For the estimation of the copula parameter matrix, we use about 10 years of data over a rolling window starting with the observations forecast origins 1974:M12 to 1984:M12, that is,. More precisely, we start by evaluating the empirical PITs of the predictive distributions of monthly year-on-year inflation using observations 1975:Mto 1985:Mfor forecast horizon. We then estimate the copula parameter based on the empirical PITs and we use the estimated copula parameters in combination with the forecasts for 1985:M,, to construct an annual-average inflation predictive distribution for 1986. Next, we use the predictive distributions for year-on-year inflation from 1976:Mto 1986:Mfor horizon, and re-do the steps to construct the annual-average inflation predictive distribution for 1987. We then repeat this until we have an annual-average forecast for each year from 1986 to 2022. In addition, we construct annual-average forecasts as described above but using the benchmark approach, which is identical to setting all off-diagonal elements ofequal to zero. Therefore, the out-of-sample evaluation period for annual-average inflation forecasts spans from 1986 to 2022, leading to 37 out-of-sample (calendar year) annual-average predictive distributions, that is,.

The results can be summarized as follows. Regarding the performance at the tails of the predictive distributions, the QW-CRPS ratio of the copula approach relative to the benchmark is 0.79, i.e., the copula approach provides a substantial predictive gain of about 21%. Similarly, the ratio of the QS(10%) and QS(90%) is respectively 0.72 and 0.85, i.e., the copula approach provides a relative gain of 28% and 15% for predicting the risks of low and high inflation, respectively. When evaluating the entire density forecasts, the CRPS ratio of the copula approach relative to the benchmark is 0.91, i.e., the copula approach provides a relative predictive gain of 9%. All these results are statistically significant at the 1% level according to the test for unconditional equal predictive ability of.

illustrates the superior predictive ability of the copula approach. Panel (a) shows the predictive density of the annual-average inflation for 2001 and panel (b) shows the predictive density of the annual-average inflation for 2011. In both cases, the benchmark approach assigns ex-ante zero-probability to the realizations, whereas the copula-based density forecasts can better predict tail-risks. In particular, the latter displays fatter tails as a result of the high correlation between adjacent months’ year-on-year growth rates. It is also worth noting that the predictive distributions of the copula-based approach exhibit notably stronger asymmetry.

SECTION: Growth-at-Risk
In a recent influential contribution,use quantile regressions to producedensity forecasts of quarterly quarter-on-quarter U.S. real GDP growth and show that financial conditions, captured by the National Financial Conditions Index (NFCI), are an important predictor for downside risks to GDP growth. We apply our methodology to transform their quarter-on-quarter density forecasts into annual-average densities.

The total data sample ofranges from 1973:Q1 to 2015:Q4 and, starting with forecast origin 1993:Q1, the authors produce (pseudo) out-of-sample forecasts using quantile regressions for up to four quarters ahead.Thus, using their exact specification leaves us with a series of out-of-sample forecasts from 1993:Qto 2015:Q4, for forecast horizons.

To construct annual-average predictive distributions, we proceed as follows. We first evaluate the predictive distributions for the forecast targets 1993:Qto 2001:Q, with, and we obtain a series of 32 empirical PITs for each forecast horizon. From the PITs, we calculate the copula parameter as described in. We then compute the annual-average predictive distribution for 2002 from the quarter-on-quarter densities for horizon, with origin 2001:Q4 (and forecasting targets 2002:Q1 to 2002:Q4). Then, we move four quarters ahead and repeat the exercise to construct the annual-average predictive distribution for 2003. In total, we repeat this algorithm until we have a series of 14 annual-average predictive distributions spanning from 2002 to 2015. The obtained annual-average predictive distributions are hence based on the original quarterly predictive distributions ofand takes into account the serial correlation across the quarterly growth rates. The benchmark annual-average predictive distributions are constructed similarly.

shows the results for the annual-average predictive density of 2008 for both the copula approach (solid line) and the benchmark approach (dashed line), alongside the realized annual-average growth rate (dotted line). The copula approach leads to annual-average forecasts with larger tails due to the positive correlation between the quarterly growth rates. Indeed, the rank correlation, i.e., the elements in the Gaussian copula correlation matrix, is around 50% to 60% for adjacent quarters, depending on the forecast horizon. Importantly, the larger tails of the copula approach help to assess the downside risk to the annual-average real GDP growth for 2008, i.e., during the onset of the financial crisis. This underlines the usefulness of our approach and provides anecdotal evidence of incorrect risk assessment if the serial correlation between the original predictive distributions is not taken into account when constructing multi-horizon objects.

Turning to the full evaluation sample, the QW-CRPS and CRPS ratios point to a predictive for the copula approach with respect to the benchmark of about 2.3% and 2.5%, respectively. However, the QS(10%) ratio points to a substantial predictive gain of about 25% for the copula approach. In other words, even though the sample is too short for formal testing procedures, the empirical analysis suggests that the copula approach may provides a considerable improvement for predicting lower tail risks on activity, which are carefully and closely monitored by policy-makers.

SECTION: Conclusion
In this work, we propose a method to combineforecasts to obtain new predictive objects that are function of several horizons. The approach is useful in a situation where the forecaster has a set offorecasts available and has to use the same set ofpredictive densities to construct a new predictive object; for instance, if the forecasters already has a set ofquarter-on-quarter growth rates but also needs annual-average predictions. These type of situations typically arise, but are not limited to, in institutions where the forecasting process is rigid.

In a Monte Carlo exercise, we show that our methodology outperforms the benchmark approach in terms of forecasting performance whenever the serial correlation across different forecasting horizons is close to zero. In terms of the absolute forecasting performance the copula approach provides density forecasts that largely pass a correct specification test based on evaluating the uniformity of the PIT, whereas the benchmark approach fails to pass this test whenever the serial correlation is not close to zero.

In a first empirical application, we investigate in a large-scale forecasting exercise, based on monthly data from FRED-MD, the performance of our methodology for a large number of outcome variable and predictor combinations. In this exercise, we transform month-on-month predictive densities to quarter-on-quarter density forecasts through our proposed copula approach and results show that copula approach outperforms the benchmark approach for the majority of outcome variable and predictor combinations.

In the second empirical application, we show the usefulness of the approach by transforming year-on-year predictive densities for inflation into annual-average predictive densities. The copula approach significantly outperforms the benchmark approach both in terms of the CRPS and quantile tick loss evaluation.

In the third empirical application, we transform the quarter-on-quarterforecasts of U.S. real GDP growth ofinto annual-average forecasts, and provide anecdotal evidence that the copula approach provides a better forecasts of the growth at risk during the Great Recession period.

SECTION: References
SECTION: Appendix
SECTION: Additional results and data
SECTION: Additional simulation results
SECTION: Inflation at Risk data
shows the predictors used in. All data is seasonally adjusted were applicable. The transformation codes imply the following: 1 — no transformation; 4 — log(); 5 — 100[log() - log()]

SECTION: Inflation at Risk quantile interpolation
Letdenotes the predictive cumulative distribution function inandhorizons ahead, evaluated at. Then, given the setof predictive quantiles, we computeas follows:

wheredenotes predictive value of quantile, andandare such that. For values ofand, we approximate the slope asandand the distance asand.

Similarly, we sample from the distribution given by the conditional quantilesas follows. Letdenote the-th draw from the uniform distribution and. Then,

wheredenotes drawof the predictive distribution for, conditional on information in. The two endpoints are treated analogously to the procedure described for equation ().