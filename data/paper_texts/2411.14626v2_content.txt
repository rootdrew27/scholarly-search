SECTION: Evaluating the Impact of Underwater Image Enhancement on Object Detection Performance: A Comprehensive Study

Underwater imagery often suffers from severe degradation that results in low visual quality and object detection performance. This work aims to evaluate state-of-the-art image enhancement models, investigate their impact on underwater object detection, and explore their potential to improve detection performance. To this end, we selected representative underwater image enhancement models covering major enhancement categories and applied them separately to two recent datasets: 1) the Real-World Underwater Object Detection Dataset (RUOD), and 2) the Challenging Underwater Plant Detection Dataset (CUPDD). Following this, we conducted qualitative and quantitative analyses on the enhanced images and developed a quality index (Q-index) to compare the quality distribution of the original and enhanced images. Subsequently, we compared the performance of several YOLO-NAS detection models that are separately trained and tested on the original and enhanced image sets. Then, we performed a correlation study to examine the relationship between enhancement metrics and detection performance. We also analyzed the inference results from the trained detectors presenting cases where enhancement increased the detection performance as well as cases where enhancement revealed missed objects by human annotators. This study suggests that although enhancement generally deteriorates the detection performance, it can still be harnessed in some cases for increased detection performance and more accurate human annotation.

SECTION: IIntroduction

Underwater computer vision has gained more attention in recent years with an increasing number of applications such as ocean mapping, ecological monitoring, and inspection. This is greatly attributed to the recent availability of large underwater datasets facilitated by the widespread use of Remotely Operated Vehicles (ROVs). Most recent datasets such as[10,11,12]are fully or partially obtained by ROVs due to their relatively low cost, availability, and ease of use. Because light propagates differently in water compared to air, underwater images are usually associated with limited visibility, reduced contrast, mono-color casts, and poor overall quality. In particular, light underwater suffers from absorption by which the energy of the light is dissipated and scattering by which the direction of light is changed[13].

Because of these challenges with underwater imagery, researchers have developed numerous enhancement approaches to mitigate these effects[1,4,6]. In addition, some studies[14,15]suggested using image enhancement as a preprocessing step to increase classification and detection performance. However, very little previous work has addressed the use of Underwater Image Enhancement (UIE) for high-level tasks such as Underwater Object Detection (UOD) and classification. Thus, more work is required to investigate the effect of image enhancement on detection performance.

Pei et al.[16]concluded that image degradation negatively impacts classification performance but image enhancement algorithms do not help regain classification performance. The authors of[17,18]concluded that image enhancement has a negative effect on detection performance when used as a preprocessing step. Conversely, Chen et al.[19]concluded that mixing enhanced and non-enhanced images to train a classifier does not lead to higher performance, but rather more robust performance.

Motivated by previous studies, we conducted a thorough evaluation of the underwater enhancement-detection framework and tried to overcome the shortcomings of previous studies by quantitatively and qualitatively linking image quality with detection, investigating the underlying reasons behind the degradation of detection performance after image enhancement, and showcasing how enhancement has the potential to improve detection.

To support our study, we selected representative image enhancement models with innovative and very recent architectures based on the taxonomy presented in SectionIIwith focus on recent learning-based methods, then used two publicly available datasets to ensure the generalizability of our conclusions: CUPDD[20]and RUOD[17]. Furthermore, the RUOD is one of the largest publicly available underwater datasets combining images from different datasets and containing different color casts, environments and aquatic creatures, while CUPDD is a relatively smaller dataset containing severely degraded challenging images. Since we do not have reference images (i.e. ground truth images with no water effects), we opt for a reference-free evaluation, similar to the work done in[21], using four reference-free metrics: 1) the Underwater Image Quality Measure (UIQM)[22], 2) the Underwater Color Image Quality Evaluation (UCIQE)[23], 3) an underwater metric based on light absorption and scattering characteristics (CCF)[24], and 4) the Matlab implementation of Entropy[25].

For detection, we opted for the state-of-the-art (SOTA) YOLO-NAS[26]Single Shot Detector (SSD), which utilizes Neural Architecture Search strategy, and made a major advancement in the YOLO series. YOLO-Nas has been successfully used in underwater environments outperforming other detectors from the YOLO family[27,28,29]. This work conducts extensive experiments and analyses to shed light on the effectiveness of various image enhancement techniques in improving object detection performance for underwater imagery leading to the contributions listed below:

individual cases are presented where enhancement improves object detection performance.

The underlying reasons behind the deterioration of the detection performance in enhanced images is analyzed.

A Quality Index (Q-index) is proposed that combines enhancement metrics to compare the quality distribution of the original and enhanced images.

The relationship between image quality, as measured by enhancement metrics, and object detection performance, as measured by mAP is examined.

A comparison is made between the cascaded approach of underwater image enhancement followed by object detection with that of detection performance with non-enhanced images.

A qualitative and quantitative evaluation of SOTA image enhancement models on two datasets, namely RUOD and CUPDD, is performed.

The remainder of this work is addressed as follows. SectionIIsurveys enhancement-detection studies, state-of-the-art underwater enhancement models, detection models, enhancement metrics, and detection datasets. The adopted evaluation procedures and metrics are discussed in SectionIII. SectionIVanalyzes the obtained image enhancement results while object detection results are analyzed in SectionV. Then we analyze and discuss the effect of image enhancement on the detection performance in SectionVI. Finally, we conclude the paper with SectionVII.

SECTION: IILiterature Survey

We summarized and extended current underwater image enhancement taxonomies[30,31,32]comprising three main categories: non-physical, physical, and deep learning-based as illustrated in Fig.1. These are further broken down into seven enhancement subcategories: histogram, fusion, retinex, dark channel prior, optical imaging, CNN-based, and GAN-based. In general, non-physical models work directly on images using image processing techniques without using any prior knowledge, so they are fast and simple but lead to sub-optimal enhancement details[33]. Conversely, physical models rely on simulating the underwater imaging process[34]and addressing ill-defined problems by using physical priors to mitigate the degradation process and obtain enhanced images[35]. However, the assumptions made for unknown parameters may not be entirely suitable or precise in intricate underwater environments, which restricts their use cases[36]. Learning-based methods are based on neural networks and can generate good enhancement results but require parameter tuning and a sufficient amount of data[37]. Since many recent enhancement algorithms are CNN-based, we select multiple models from this subcategory and insured that the selected models are very recent and well-designed. We also divide the combined task of enhancement and detection into two frameworks: 1) the cascaded framework[17]in which detection is sequentially performed after enhancement, and 2) the joint-learning framework in which enhancement and detection are considered simultaneously[35,17]. The joint-learning framework is further divided into two subcategories: 1) shared loss[17]in which the loss from the detector is used to train the enhancer enabling the enhancer to produce images that are favored by the detector rather than humans, and 2) reinforcement learning[38]in which image features are used as states, different UIE algorithms as actions, and loss reduction as rewards. It is worth mentioning that the joint-learning framework can only be implemented with learning-based enhancement algorithms. Therefore, we only consider the cascaded approach in this study.

SECTION: II-ACombined Enhancement-Detection Studies

In this section, we summarize the studies that addressed the problem of combined image enhancement and object detection. For instance, Fu et al.[17]proposed a new method for combining image enhancement and object detection called shared loss, indicating a shared loss function for the enhancer and detector to be trained simultaneously. They proposed RUOD dataset comprising 14,000 images and 10 classes and used it to evaluate their method. The finding of their research reveals that the cascaded enhancement-detection approach leads to a lower detection performance while a joint learning approach increases it. The authors do not provide much details about the implementation of their proposed joint-learning framework.

The authors of[19]studied the effect of different and mixed domains, and environments, on detection and addressed the enhancement role in the process. The study used the Underwater Robotic Picking Contest (URPC2018)[39]dataset and split it into three domains based onCIELABcolor space analysis[40]. Then, a restoration technique was applied and multiple experiments were conducted using different detectors. The authors concluded that although image restoration does not increase within-domain detection performance, but does produce a more generalizable performance. One of the drawbacks of the approach presented in[19]is that the effects of different enhancement models are not studied.

Aligning with the same topic, Pei et al.[16]studied the effect of synthetic image degradation and restoration on classification performance. In particular, the Caltech-256 dataset[41]was used, on which multiple degradation effects were applied including haze, motion blur, fish-eye, underwater, low-resolution, salt and pepper noise, white Gaussian noise, Gaussian Blur, and out-of-focus blur. The summary of the work presented in[16]is that although image degradation severely affects the classification performance, image degradation-removal does not lead to a restored or improved performance. We believe that this study should be extended to other higher-level tasks such as object detection.

In another study[42], a new video dataset called Underwater Visual Object Tracking (UVOT) and an enhancement model were proposed with the aim of achieving higher object tracking performance with enhanced videos compared to the originals. The effect of the proposed method on detection was compared to the effect of other SOTA enhancement models. The study claimed an improvement of up to 5% in terms of detection performance when using the enhancement methods as compared to other baseline enhancement models. However, no statistical significance analysis was provided in the study to validate the authors claimed improvement.

Another group of researchers[18]further questioned the effect of enhancement on detection and conducted a thorough analysis of object detection behavior after enhancement. A total of 18 image enhancement models and seven detectors were used with two datasets: RUOD[17]and URPC2020[11]. The authors used the Toolbox for Identifying Object Detection Errors (TIDE)[43]to conduct their analysis, which revealed that enhancement increases the False Positive (FP) rate because it changes the objects’ edges that are essential for detection. Furthermore, the utilized enhancement evaluation revealed a discrepancy between human perception and enhancement metrics which further prevents us from drawing reliable conclusions about enhancement and detection.

Last but not least, the authors in[38]introduced a novel enhancement-detection joint-learning framework based on reinforcement learning (RL). A RL agent is trained on the URPC2018[39]dataset with an action space of a group of enhancement effects, state space as image features, and detection performance as a reward. This ensures selecting an enhancement type that is better for detection rather than human perception. The proposed method in[38]outperformed the traditional cascaded enhancement-detection cascaded approach. This work could expanded by extending the action space to include more enhancement effects and algorithms.

SECTION: II-BUnderwater Image Enhancement (UIE)

Underwater image enhancement is categorized into three main groups: non-physical, physical, and learning-based methods. The most recent models belonging to each category are summarized in this section, beginning with non-physical methods.

Zhang et al.[1]introduced a method for enhancing underwater images using a combination of various techniques. Their proposed method included attenuated color channel correction (ACCC), a fusion-based contrast improvement method, and multiscale unsharp masking (MSUM) strategy to further sharpen the edges and details of the result. Extensive experiments were conducted on four benchmark datasets for underwater image enhancement. The results showed that their approach outperforms 12 other existing methods in both qualitative and quantitative metrics. Moreover, their method generalizes well to low-light images and hazy images. Nevertheless, there are some limitations associated with their method where the color of quality-degraded underwater images may be reduced after enhancement. Besides, the introduction of a dual-histogram-based iterative threshold may increase computational complexity. However, certain limitations were identified in their study. First, quantitative values may not always be the best. In certain special cases, images with higher evaluated scores may not necessarily have a visually pleasing appearance. Second, when dealing with rough images that contain a lot of unexpected noise caused by the camera, the noise may be amplified after enhancement.

Another algorithm, called Bayesian retinex, for enhancing a single underwater image with multi-order gradient priors of reflectance and illumination was proposed in[3]. The approach is based on combining a simple and effective color correction approach to remove color casts and recover naturalness. Then a maximum posterior formulation is utilized by imposing multi-order gradient priors on both reflectance and illumination. The comparative evaluation results demonstrated that Bayesian retinex outperforms other several traditional and state-of-the-art enhancement approaches on qualitative and quantitative assessments, and demonstrated the effective performance of the method in terms of color accuracy, challenging underwater scenes, an ablation study, parameter evaluation, and algorithm convergence.

The Texture Enhancement Model based on Blurriness and Color Fusion (TEBCF) model is presented in[2]based on multi-scale fusion in which two inputs are merged; one to improve contrast based on dark channel prior in the RGB space and another to improve color based on the CIELAB color space[40]. The authors used two datasets, UIEB[44]and U45[45], for evaluation with other baseline approaches. Although the method introduced in[2]showed a significant quantitative performance advantage over other approaches, the enhanced images were not equally performing in terms of subjective analysis, which highlighted the weakness of the utilized enhancement evaluation metrics.

Contrary to the non-physical methods described, physical methods incorporate some kind of prior knowledge based on the physics of the underwater image formation process. Zhang et al.[4]developed an approach that begins with a color cast correction method for each color channel. Subsequently, various enhancement methods are applied to enhance the base and detail layers of theVchannel in theHSVspace, which is decomposed using the spatial prior and texture prior. The researchers applied different enhancement strategies in different layers in order to enhance the contrast and texture detail of underwater images. Extensive experiments on several benchmark datasets were conducted, and the qualitative and quantitative comparisons showed that the proposed method outperforms 11 state-of-the-art methods. Furthermore, their method showed good generalization capability for fog and low-light images.

In another enhancement approach[5], a new model called illumination channel sparsity prior (ICSP) was developed and a new dataset called Non-Uniform Illumination Dataset NUID was proposed. The new model was based on the observation that there are some low-intensity pixels in the illumination (I) channel of an underwater image with uniform lighting in theHSIcolor space. Using this observation, the authors developed a variational model based on the retinex theory[46]. The authors claimed superior quantitative and qualitative results compared to other baseline approaches but our own evaluation in this paper reveals that the ICSP produces overexposed images, reducing some image details in the process. Next we turn to reviewing approaches based on deep learning.

In addition to the aforementioned approaches, the field of underwater image processing has seen several other notable contributions. One such contribution is the Mean Teacher-based Semi-supervised Underwater Image Restoration (Semi-UIR) framework proposed in[7]. This framework focuses on incorporating unlabeled data into network training. Through extensive experiments, the results demonstrated the superior performance of their method over other SOTA algorithms, quantitatively and qualitatively.

Fu et al.[8]introduced an UnSupervised Underwater Image Restoration (USUIR) method by leveraging the homology property between a raw underwater image and a “re-degraded” image. Their proposed idea decomposed the underwater image into three latent components. Then they generated a re-degraded image by randomly mixing up the estimated scene radiance and the raw underwater image, leveraging the homology constraint between the raw underwater image and the re-degraded image. Their experiments on two real-world underwater image datasets demonstrated the effectiveness and efficiency of their method in both inference time and restoration quality. And the success of the unsupervised framework highlighted the effectiveness of leveraging the homology property for unsupervised training through a suitable re-degradation operation.

In[9], a Two-phase Underwater Domain Adaptation network (TUDA) was introduced, which simultaneously minimized the inter-domain and intra-domain gaps. First, a triple-alignment network was designed to jointly perform image-level, feature-level, and output-level alignment using adversarial learning for better closing the inter-domain gap. Then, a simple efficient rank-based underwater image quality assessment method was performed, which can more accurately assess the perceptual quality of underwater images with the aid of rank information. Finally, coupled with the proposed RUIQA, an easy/hard adaptation technique was developed, which effectively reduced the intra-domain gap between easy and hard samples. Extensive experiments on four real underwater benchmarks demonstrated that their proposed method performed significantly better than other state-of-the-art algorithms, particularly in eliminating color deviation, increasing contrast, and avoiding over-enhancement.

Tang et al.[6]applied NAS to address the challenge of processing severely degraded underwater images. Their approach involved automatically designing a deep neural network architecture using NAS to search for the optimal U-Net architecture specifically tailored for underwater image enhancement. They proposed a new search space including diverse operators, applying the NAS mechanism to the transformer, and then proposed a selectable transformer structure that assigns the substantial learning capability to the proposed deep model. Their extensive experiments demonstrated that the proposed architecture could exploit the multi-color spaces in the underwater scenarios, that their framework obtained an optimal neural network, and achieved competitive performance on widely used benchmark datasets.

A different approach was explored in[47], where underwater image enhancement was modeled as a Markov decision process (MDP). This approach was based on reinforcement learning frameworks that selected a set of image enhancement actions and organized them into an optimal sequence. Experimental results validated the effectiveness of the reinforcement learning framework in underwater image enhancement, and the capability to easily integrate new image enhancement techniques was demonstrated, thereby enhancing the overall comprehensiveness of the framework.

SECTION: II-CUnderwater Object Detection (UOD)

Object detection underwater is challenging due to a number of factors, such as the lack of massive underwater datasets compared to other vision-task datasets, and the severe degradation of the images caused by underwater effects. Nevertheless, researchers have developed a number of underwater two-stage and one-stage detectors. In general, one-stage detectors are faster compared to two-stage detectors but have lower performance.

We will first talk about two-stage underwater detectors. Mandal et al.[48]integrated the Faster R-CNN[49]with three distinct backbone networks to enable automatic detection and identification of various fish species in underwater footage. Lin et al.[50]introduced a data augmentation technique that uses candidate region fusion to create training samples that mimic overlap, occlusion, and blurring, thereby enhancing the model’s accuracy and robustness. Xu et al.[51]developed a scale-aware feature pyramid detector for identifying targets in underwater settings. The approach presented in[51]crafted a unique backbone sub-network to capture the fine-grained features of smaller targets. Furthermore, a multi-scale feature pyramid was built to boost the feature representation for prediction by merging shallow-level texture details and high-level semantic information. This method successfully detected ocean targets but not in real-time. To effectively identify small underwater targets, Qi et al.[52]introduced a two-stage underwater small target detection network with a deformable convolutional pyramid structure. This was designed to tackle issues related to target deformation, occlusion, and scale variation.

Other works have focused on one-stage underwater detectors. For instance, Sung et al.[53]focused on addressing the challenge of detecting fish underwater in real-time. They trained their network using images of fish targets and randomly selected seabed pictures, utilizing the YOLO-v1[54]detection framework, attaining a classification accuracy of 93% at a detection rate of 16.7 frames per second. In a similar vein, Hu et al.[55]developed a detection network for sea urchins, employing the SSD algorithm[56]. A novel multi-directional edge detection technique was introduced to better capture the distinctive spined edge characteristic of sea urchins, thereby improving feature representation. Chen et al.[57]introduced a neural network architecture called SWIPENet, designed for the detection of small targets in underwater environments. This network was built upon the framework presented in[58]and incorporates a sample weighted loss function. The purpose of this function was to prioritize samples with higher weights while diminishing the focus on those with lower weights, thereby enhancing the detection performance for small-scale targets.

The YOLO architecture has been used in several works. A lightweight approach for recognizing underwater objects based on the YOLO-v4[59]architecture and multi-scale attentional feature fusion was introduced by Zhang et al.[60]A mix of depth-wise separable convolution and MobileNet-v2[61]was proposed to reduce the model size achieving higher inference speed but compromised the performance. Liu et al.[62]suggested TC-YOLO, a novel underwater object detection technique that includes attention mechanisms by incorporating a coordinate attention module and a transformer encoder into YOLO-v5[63]. By leveraging temporal context and the YOLO architecture, TC-YOLO enhances detection performance in challenging underwater conditions with poor visibility and complex backgrounds. YOLO-NAS[26]is a significant upgrade over previous YOLO models, addressing several shortcomings. The main features of YOLO-NAS are a new basic block that is quantization-friendly and a novel algorithmic optimization engine known asAutomated Neural Architecture Construction(AutoNAC). This guarantees optimal hardware utilization while maintaining baseline performance.

SECTION: II-DUnderwater Image Datasets (UIDs)

Acquiring a substantial volume of underwater imagery can be challenging. Nevertheless, the availability of comprehensive underwater datasets is essential to the development of underwater image enhancement and object detection algorithms, especially for learning-based methods that need a large sample size for effective training. This has led to a concerted effort by researchers to compile comprehensive underwater image datasets.

To support image enhancement efforts, many UIE datasets have been recently curated. For example, the Stereo Quantitative Underwater Image Dataset (SQUID)[64]contains images taken in different locations with varying water properties. The Enhancing Underwater Visual Perception (EUVP) dataset[65]contains separate sets of paired and unpaired image samples of both good and poor perceptual quality. The Segmentation of Underwater IMagery (SUIM) dataset[66]contains over 1,500 images with pixel annotations for eight object categories. The Underwater Image Enhancement Benchmark Dataset (UIEBD)[44]includes 950 real-world underwater images, 890 of which have a corresponding reference image. The Real-world Underwater Image Enhancement (RUIE) dataset[35]is divided into three subsets targeting three main challenges in underwater enhancement; those being quality, color casts, and task-driven.

Several datasets for object detection have also been recently proposed. The USOD10K dataset[67]is a significant contribution to the field of underwater salient object detection. It is the first large-scale dataset specifically designed for this purpose, containing 10,255 underwater images. These images cover 70 categories of salient objects across 12 different underwater scenes, providing a comprehensive range of environments and object types for researchers. The WildFish dataset[68]includes an impressive array of 1,000 fish species across 54,459 images, facilitating the development of robust models for fish species classification. Fish4Knowledge[69]is another widely utilized dataset, teeming with dynamic marine life such as fish, sea anemones, and various aquatic flora. The Brackish dataset[70]offers video footage of marine life like fish, crabs, and starfish in murky brackish waters, complete with annotations. Datasets from the underwater robot picking contests, like UDD[71], DUO[72], and UODD[10], feature marine organisms such as sea urchins and scallops, compatible with popular dataset formats such as Pascal, VOC, and COCO, making them ideal for underwater detection tasks. For broader underwater scenarios, Fu et al. introduced RUOD dataset[17], which comprises 14,000 images spanning 10 marine categories, also capturing the intricate challenges of underwater environments such as haze, color distortion, and lighting issues. The Challenging Underwater Plant Detection Dataset (CUPDD) presented in the work of Saleem et al.[20], consists of 414 images and categorizes aquatic plants into three broad types: bushy, leafy, and tapey. Now that we have covered the breadth of algorithms and datasets used for underwater enhancement and object detection, we turn to our experiments.

SECTION: IIIEvaluation Procedures and Metrics

In this section we describe our experimental settings including the utilized datasets, metrics, and image enhancement and object detection models.

SECTION: III-ASelected Datasets

We implemented our experiments on two underwater datasets: 1) RUOD[17], comprising 14,000 high-resolution underwater images, 9,800 of which are used for training, with approximately 75,000 annotations of 10 aquatic objects, such as divers, plants, and different types of aquatic animals; and 2) CUPDD introduced in[20], comprising 414 images, 313 of which are used for training, with three general categories of aquatic plants collected in the Great Lakes region in the US. The use of both datasets help us to draw generalized conclusions since RUOD is very large, recent and extensive combining images from different smaller datasets and containing different color casts, environments and aquatic creatures while CUPDD is very challenging and has the potential of testing enhancement algorithms at the extreme end.

SECTION: III-BSelected Enhancement and Detection Models

To accurately represent underwater image enhancement, we select several SOTA image enhancement models covering major categories and subcategories of image enhancement[30,31,32], including physical, non physical, and learning-based image enhancement categories. We also insure that the selected models are very recent, well-designed and acquired from credible sources. Since learning-based algorithms became recently very common, we select multiple models from this category to better represent it. The selected models include ACDC[1], TEBCF[2], BayesRet[3]are non-physical methods that use histogram, fusion, and retinex-based enhancement, respectively. The physical methods are PCDE[4]and ICSP[5], which use a dark channel prior and optical imaging properties-based enhancement, respectively. The deep-learning methods are AutoEnh[6], which uses a vision transformer; Semi-UIR[7], which uses contrastive learning; USUIR[8]and TUDA[9], which are based on CNNs and GANs, respectively. The source codes and instructions to run these models can be found in a Github link within their corresponding provided references. We implemented all enhancement models without modification or additional training, using the source codes and the trained models provided by the original authors. On the detection side, the YOLO detection family has been extensively used in underwater scenarios showing its efficiency[54,60,62]with YOLO-NAS being one of the recent top performers[27,28,29]. In this study, the SOTA YOLO-NAS[26]is fine-tuned with the original images and the enhanced images from each image enhancement algorithm for both datasets totaling of 20 model instances, each trained and evaluated separately. The image resolution for both datasets is set to. We used the SuperGradients[26]library to implement the detector on a Linux server with two Nvidia Tesla V100 GPUs. The training was performed with abatch sizeof 16 and the AdamW optimizer with adecayof 0.00001. We use the YOLO-NAS large (L) architecture and start from the COCO pre-trained weights.

SECTION: III-CSelected Evaluation Metrics

Researchers have developed a number of ways to quantitatively evaluate underwater images, which mainly fall into two categories: full-reference and reference-free evaluation[73]. In full-reference evaluation, a ground truth reference image, resembling an image without water effects, is compared with the degraded image. In contrast, reference-free evaluation does not require a reference image, and instead is measured directly on the degraded image. In this study, we opted to use reference-free evaluation because reference images do not exist for the selected datasets[20,17]. We also believe that this is a more practical study, as reference images are often impossible to obtain, except in very controlled laboratory experiments.

We used four underwater image enhancement metrics:

UIQM[22], which is comprised of color, contrast, and sharpness indices. A higher UIQM score indicates better image quality.

UCIQE[23], representing a linear combination of saturation, contrast and chroma. A higher UCIQE score indicates better image quality.

CCF[24], which is a linear regression model of colorfulness, contrast, and fog density indices. A higher CCF score indicates better image quality.

Entropy, a statistical measure that can be used to describe the randomness of the texture in images[74]. Here we calculated it as the Discrete Entropy[25]using Matlab. A higher Entropy score indicates richer details, i.e., higher image quality.

All of the utilized metrics are used without any modification and are implemented with original weights and hyperparameter values from the original papers.

SECTION: III-DProposed Quality Index (Q-index)

We use the selected metrics described in Section.III-Cto generate a quality index (Q-Index) which is later used to describe and analyze the quality distribution of images. The use of the Q-index would facilitate such analysis by making it more focused and providing a general impression about the quality of the images. Essentially, the Q-index is generated from the four selected metrics through outlier removal, global rescaling, and averaging processes to produce a single bounded metric between zero and one as introduced in Algorithm1. A value is flagged as an outlier if it is three Median Absolute Deviations (MAD) away from the median as in the Matlab implementation in[75]. MAD is defined as

and erfcinv is the Inverse Complementary Error Function[76].
Moreover, global re-scaling means that the minimum and maximum values are taken across all images from the original and enhanced datasets to fairly compare the Q-index values across different models. On the other hand, local Maxima and Minima refers to the Maxima and Minima of a certain metric belonging to a certain enhancement model as shown in Algorithm1

SECTION: IVEnhancement Evaluation

In this section, a quantitative and qualitative evaluation of the selected image enhancement models is presented for both RUOD and CUPDD datasets. In addition, we generate the Q-index based on the selected metrics to show the quality distribution of the images after enhancement and to provide a mixed quantitative and qualitative analysis.

SECTION: IV-AQuantitative Evaluation

The images from the selected datasets are enhanced using the selected enhancement models and evaluated using the selected enhancement metrics. The results for CUPDD dataset are shown in TableI. These show the superiority of the TEBCF model[2]based on all metrics, especially in terms of the UIQM valued at 4.07 and CCF valued at 25.03. This should indicate its capability of producing vibrant colors, marked contrast, soft edges, and removing fog effects. The Semi UIR model also performs notably with a UIQM of 3.21 and high UCIQE and Entropy values (0.55 and 7.56 respectively), suggesting that it offers a balanced image quality enhancement with rich details. Meanwhile, the USUIR model stands out in UCIQE (0.60) and Entropy (7.67), reflecting its proficiency in improving colorfulness and retaining image detail. The ACDC is another notable contender that signifies a marked enhancement capability with a UIQM of 3.07 and balanced other metrics. Similarly, BayesRet and TUDA demonstrate a balanced approach with a commendable Entropy at 7.72 and 7.77, respectively, indicating substantial detail retention alongside reasonable color and contrast enhancements. Conversely, models like ICSP and PCDE, with lower UIQM and CCF values, reflect lesser capabilities in quality enhancement and color correction. These comparative insights highlight the strengths and potential applications of each model in underwater image processing, emphasizing the importance of selecting a model based on the desired balance of quality, color correction, and detail retention. It is worth noting that all models are performing better than the Original except for the ICSP indicating that it is possibly corrupting colors and introducing noise. The PCDE and the BayesRet are also inferior to the Original indicating their limited capabilities of enhancing underwater images.

The results for RUOD dataset are shown in TableII. These show in general higher CCF and Entropy scores. This could be attributed to the fact that RUOD has clearer images with richer details. Like CUPDD, the TEBCF[2]is a top performer in the UCIQE and the CCF at 0.62 and 31.5, respectively. The TEBCF is followed by the BayesRet at a top UIQM and entropy values of 3.85 7.74, respectively. This might indicate the capability of the TEBCF in removing the fog effect and the capability of the BayesRet in adding more details. The ACDC also shows robust performance with a high UIQM (3.76) and balanced metrics, rendering it a versatile candidate for underwater image enhancement. TUDA further demonstrates solid capability with a UIQM of 3.40, a UCIQE of 0.58, a CCF of 20.88, and a high Entropy of 7.65, highlighting it as a strong performer overall, particularly excelling in maintaining detail. Models like AutoEnh. and Semi-UIR are notable for their good colorfulness and contrast enhancement at a UCIQE of 0.59 and 0.60, respectively, with reasonable overall quality and detail retention. In contrast, PCDE and ICSP exhibit lower UIQM (2.42 and 1.14) and CCF (14.76 and 25.59), reflecting their relatively less impactful enhancements. Aligning with the results from TableI, All models outperform the original except for the ICSP and the PCDE on some metrics reflecting the limited capabilities of both models.

SECTION: IV-BQuality Distribution

We used the proposed Q-index in Sec.III-Dto facilitate a quality distribution analysis of images before and after enhancement by each enhancement model. In particular, a violin plot is used to show the Q-index distribution of original images and the change in the Q-index distribution after enhancement, as shown in Figs.2and3. The change in the Q-index (of Q-index) for an image is simply calculated as

whererefers to the enhanced image andrefers to the original. This helps us understand how images of different qualities are changing the distribution after enhancement, i.e., whether enhancement is uniformly increasing the quality of images belonging to different quality bins. By examining Fig.2, it is observed that the entire original image dataset ranges from 0.25 to 0.75, with an almost bimodal distribution and a median of approximately 0.45. One peak is centered around the median and the other at just below 0.4, indicating that most images lie around those two peaks. Moving to the distribution of images after enhancement, we notice that all but two models produced a positive distribution. PCDE, with almost half the distribution in the negative, and ICSP, with the entire distribution in the negative, produce images that have less quality—according toof Q-index—than the Originals. The TEBCF is a top performer at a median of approximately 0.38, followed by the ACDC, BayesRet, Semi-UIR, USUIR and TUDA at a median of approximately 0.2. In contrast, AutoEnh showed an inferior performance with a bimodal distribution with most of the images centered around 0.1. It is also noted that for ACDC, TEBCF, BayesRet, Semi-UIR and USUIR, the distribution has peaks at the top and tapering off towards the bottom indicating that these models are shifting low-quality images into high-quality images. A small number of images became worse after enhancement with the ACDC, TEBCF, AutoEnh, Semi-UIR, USUIR, and TUDA models. On the Other hand, the distribution of the Original RUOD images shown in Fig.3is a clearer bimodal distribution with a median of nearly 0.52 and two distinct peaks at 0.6 and 0.35. Furthermore, most enhancement modals have a median at 0.1 except for the PCDE and ICSP observing similar negative trends as in Fig.2. A distinguished observation about the distribution of the enhanced images in Fig.3is that most models have an inverted distribution compared to the Original with a bigger peak at the bottom. This means that enhancement isdeteriorating the quality of high-quality imagesin RUOD dataset. ON the contrary, CUPDD had the bigger peak on top. This could be attributed to the fact that CUPDD images are mostly severely degraded images and have significantly lesser quality than RUOD which supports thatenhancement is only beneficial for low-quality images.

SECTION: IV-CMixed Quantitative-Qualitative Evaluation

As a further and final evaluation of image enhancement, we divide the Q-index into 10 quality bins and randomly select an image representing each quality bin for both datasets, namely CUPDD and RUOD as shown in Figs.4and5. Some quality bins at both ends are omitted since no images are found in those extreme bins. The Q-index value is shown under each image and color-mapped for easier inference of the quality. Fig4shows a random image from each quality bin available in CUPDD dataset with their enhanced versions and corresponding Q-index values. A gradual but not accurate improvement in quality can be seen as the Q-index value of the Original images increases from left to right. However, we would argue that the image belonging to the 0.5-0.6 quality bin looks more visually pleasing than its successor with soft details, less noise, and a mild color cast. The successor image appears to have a severe color cast and more noise but yet achieved a higher Q-index. This observation is further demonstrated by the enhanced images, especially with the TEBCF images where they all look very sharp and noisy but achieve very high Q-index values of higher than 0.75. In contrast, the Q-index accurately predicted our qualitative evaluation of the PCDE and the ICSP models at values ranging from 0.52 to values as low as 0.08 with the first producing very foggy images and the latter producing extremely overexposed images further deteriorating the quality of original images. The ACDC and BayesRet produce less colored tones but maintain the details of the images. The AutoEnh, Semi-UIR, USUIR and the TUDA seem to be producing the most visually pleasing images with vibrant colors, less noise and fog effects compared to other models. However, this is not always reflected by their Q-index values. It is notable that the average increase in the Q-index value of low-quality images is much higher than the average increase in the Q-index values of high-quality images. For instance, the Q-index of the image from the lowest quality bin almost doubled with some models while the Q-index of the image from the highest quality bin barely witnessed a slight improvement with most models. In accordance with CUPDD, the random images from RUOD dataset shown in Fig.5observe a similar gradual improvement from left to right. Images with low Q-index tend to have a mono-color cast with very few details and low visibility. A less severe color cast with more details and colors can be noted as the Q-index increases. Images with high Q-index tend to have a richer red channel, more vibrant color, and clear fine details. Although it is hard to distinguish a slight difference in the Q-index, the general trend of quality improvement with the increase of the Q-index is evident. For instance, the quality of the first two images at 0.16 and 0.21 is hardly distinguishable. The performance of the enhancers on RUOD is very similar to CUPDD with TEBCF being quantitatively the most superior but qualitatively the most inferior. We elect the TUDA and the Semi-UIR as the best enhancers for producing visually pleasing images. We conclude thatenhancement techniques are most effective on low-quality images, and deep learning-based enhancement models are better at producing visually pleasing images. In addition, the Q-index provides a general indication of image quality; however, it can be easily misled by artifacts produced by enhancement models, revealing the sensitivity of its composing metrics.

SECTION: VDetection Evaluation

In this section, we conduct a per class quantitative evaluation of the trained YOLO-NAS models on the Original and enhanced images from both datasets CUPDD and RUOD using mean Average Precision (mAP50_95). Furthermore, we conduct a qualitative analysis using random samples from both datasets.

SECTION: V-AQuantitative Evaluation

We have a total of nine selected enhancement models and two datasets, thus we train a total of 20 YOLO-NAS models including the Original images from both datasets. We call YOLO-NAS models that are trained on the Original images asOriginal detectors, and YOLO-NAS models that are trained on enhanced images asdomain detectors. All models are trained using the same parameters and configurations. The first experiment is conducted on CUPDD and is shown in Tab.IIIwhere the Original detector outperformed all domain detectors by a margin of 4% at 0.38 mAP. Moreover, the Original detector excels in the bushy class by a fair margin of 2%. In contrast, it is outperformed by the AutoEnh. domain detector in the leafy class by a notable margin of 3% possibly due to the fact that the bushy class has a more complex shape and edges that are being lost by the enhancement noise compared to the simpler leafy class. In addition, the images of CUPDD are severely degraded with very low visibility explaining the low Original performance. Surprisingly the PCDE achieves very close overall performance to the Original detector although we have discussed in sectionIVhow the PCDE produces very low-quality and non-visually pleasing images. On the contrary, the ICSP has a severely deteriorated mAP performance at 0.22 aligning with the severe deterioration in its enhancement performance. However, we consider the ICSP as a special case since it is producing extremely overexposed images. The ACDC and TEBCF show lower Leafy class mAP at 0.22 and have overall scores of 0.34 and 0.33, respectively. BayesRet shows consistently lower performance across all metrics with an overall mAP of 0.30, while the AutoEnh. exhibit strong results in the Leafy class at 0.31 and an overall score of 0.37. Semi UIR provides consistent detection score across all classes with an overall mAP of 0.34, matched by TUDA, which also balances its performance well. Finally, USUIR stands out in the Tapey class (0.40) and achieves a commendable overall mAP of 0.35. These observations signify the variability in detection performance across different methods, highlighting specific strengths and weaknesses pertinent to each class. On the other hand, Tab.IVshows the detection performance on RUOD dataset with aligned results with Tab.IIIwhere the Original detector outperformed all other domain detectors apart from the AutoEnh. which has a matching overall performance. Other models achieved moderate performance, including ACDC, TEBCF, BayesRet, and USUIR that show reasonably consistent mAP scores across most classes, resulting in overall mAP values between 0.59 and 0.61. Conversely, ICSP consistently ranks with the lowest scores across nearly all classes, yielding the lowest overall mAP at 0.55. There is a noticeable trend where all detectors perform best in the Diver and Cuttlefish classes, often registering mAP scores above 0.70. Those two classes are mostly found in clear and high quality images and have relatively bigger instance size than other classes. An observable note is that the Jellyfish class achieved with the TUDA and BayesRet domain detectors around 3% higher than the Original detector. We conclude thatwhile enhancement may degrade or maintain the overall detection performance, it has the potential to increase the detection performance for specific object classes. This is likely linked to the type of objects and their surrounding backgrounds.

SECTION: V-BQualitative Evaluation

The inference from the original and domain detectors on five random testing images from both CUPDD and RUOD is provided Figs.6and7. For a more convenient evaluation, we opt for a confidence threshold of 0.5. As shown in Fig.6, The original model achieves an acceptable top performance with most objects getting detected correctly apart from the second and last images where some objects got missed resulting in False Negatives (FN) and other objects being confused resulting in False Positives (FP). With domain detectors, we notice that the detected bounding boxes, even if True Positive (TP), are not as accurate as those from the Original images. Since we are using the mAP50_95, a lesser Intersection over Union (IoU) with the ground truth will greatly impact the detection performance. This could be attributed to the enhancement diffusing the edges and introducing noise on some occasions. All models fairly performed well except for the PCDE and ICSP which have very foggy or overexposed images with little to no details present. In the second image, we observe an interesting case where the Original domain detector was not able to detect the Leafy plant but most domain detectors were able to detect at least parts of the Leafy plant. Although such detected bounding boxes would still not be considered as TPs because the IoU is still less than 0.5, such case might reveal hidden potential in image enhancement for detection. Another interesting case is in the third image where the Semi-UIR and the USUIR detected a leafy plant that is not in the ground truth. This encouraged us to further investigate such cases as we will present in Sec.VI. On the Other hand, similar trends are found in Fig.7of RUOD dataset where the Original detector outperformed domain detectors in most images. Some models, although not producing visually pleasing images, were still able to achieve good performance such as the ACDC. In contrast, some models that produce visually pleasing images perform poorer than other domain detectors. The TEBCF images are very noisy resulting in a higher number of FPs compared to other models. The third image is a special case where most domain detectors outperformed the Original detector by a large margin. We conclude that:1) visually pleasing images do not necessarily lead to higher detection performance, and 2) image enhancement can increase the detection performance in some cases.

SECTION: VIEnhancement-Detection Discussion

In this section, we provide a combined enhancement-detection analysis. In particular, we study the correlation between the enhancement metrics and the mAP. In addition, we conduct a further subjective analysis and provide cases where enhancement improves detection performance. To this end, we generate scatter plots for the mean enhancement metrics and the mean mAP for all selected models and for both datasets as shown in Fig.8. Since the Q-index is made up of the four selected metrics, it will show similar results, hence, we opted for a more detailed analysis of individual metrics. The prevailing noticeable trend in Fig.8across all metrics is that the increase in enhancement metrics is not leading to an increase in the mAP. In fact, enhancement metrics and the mAP have a slightly inverse relationship. For example, the mAP maintained similar values with the increase in the UIQM for CUPDD while slightly decreased by the increase of the UIQM in RUOD. Furthermore, some enhancement models achieved lower scores on some metrics compared to the original, yet performed similarly in terms of the mAP. For instance, the PCDE, ICSP, AutoEnh, TUDA, BayesRet and the Semi-UIR all performed worse than the original in terms of the CCF on RUOD dataset. The ICSP model is an outlier on most plots and gained lower enhancement and mAP values due to its extremely overexposed images. We, conclude thatenhancement performance does not predict detection performance. This discrepancy can be attributed to two primary factors: 1) enhancement metrics are sensitive to noise and may not always align with human perception and 2) object detectors rely on different visual cues than humans to identify objects..

To further investigate the detector’s behavior, we conduct a further subjective analysis of the detection inference results. This time we only look at cases where a domain detector performed better than the Original as shown in Figs.9and10. Starting with CUPDD in Fig.9, the ACDC increased the contrast and removed the color cast of the Original image resulting in an image that is not necessarily visually pleasing but increased the detection performance with a much tighter box around the ground truth object. The TEBCF, BayesRet and TUDA achieved a good enhancement performance with clearer object’s edges and turned an FP into a TP. Surprisingly, the ICSP and PCDE also had cases where they enhanced the detection performance despite their overexposed or foggy images. Moreover, the AutoEnh, Semi-UIR, and USUIR had similar performance and recognized undetected objects. On the other hand, Similar trends can be seen in Fig.10for RUOD dataset. It is worth noting that some models enhance one aspect of the image and deteriorate others but still achieve better detection performance. For instance, the TEBCF removed the color cast of the image but reduced too much noise in the process but was able to detect the diver object in the image anyway. This could explained by the diver class being one of the easiest and most distinct classes to detect in RUOD dataset. The TUDA produced a very visually pleasing image making it easier for its domain detector to distinguish the turtle from the underlying rocks. Since most of the presented cases in Figs.9and10belong to low Q-index bins, we hypothesize that enhancement for low quality images could increase the detection performance compared to enhancement of the entire dataset. We conclude thatenhancement has the potential to increase detection performance. This improvement may be due to the fact that the enhancement process reduces the disparity between low and high-quality images, as discussed in SectionIV-B. i.e., By making low-quality, hard-to-detect testing scenes more similar to high-quality easy-to-detect training scenes within the same dataset, enhancement can facilitate better detection outcomes. This finding contradicts previous studies[17,19,16,18]which suggested that enhancement negatively impacts detection performance. While such negative impacts may be observed when evaluating overall enhancement performance, our evaluation demonstrates that specific cases exist where enhancement can indeed boost detection performance. Therefore, we recommend a more detailed quantitative analysis to explore these effects further.

Finally, during our investigation, we discovered cases where the enhancers revealed objects that were hard to see in the original images and were missed by the human labelers but got detected by the domain detectors and rarely by the original detector. Unfortunately, those instances were flagged as false positives (FPs) by the detectors despite being actual true positives due to wrong ground truth as shown in Figs.11and12. For example, the ACDC revealed a Bushy plant in the background while the TEBCF revealed a hidden Leafy plant in the background in Fig.11with similar trends across all classes and domain detectors. Clearer examples can be seen in Fig.12for RUOD dataset where many Scalop, Echinus, starfish, and other objects were revealed from the background and detected by the domain detectors. For instance, the AutoEnh was able to reveal and detect a Holothurian object that was almost impossible to see in the original image. The BayesRet detects an extra fish in the upper background despite producing a worse image than the original in color, contrast, and sharpness. We believe this is because the BayesRet provided a similar noisy scene for its domain detector in the training data. Some cases are presented in Figs.11and12exhibit high Q-index, while others show low Q-index. Therefore, we differentiate between two types of annotation errors: those caused by severe degradation of the underwater scene, and those due to complex underwater scenes. This distinction is more evident in the BayesRet, PCDE, Semi-UIR cases of RUOD dataset, as shown in Fig.12. We conclude thatenhancement should be utilized at an earlier stage in the enhancement-detection framework, specifically during the annotation phase. This approach would help annotators to recognize hidden objects. In addition, while enhancement can make it easier for the human annotator to avoid missing objects, sometimes simply running a detector on the Original images can reveal unnoticed objects. This finding suggests an addition to the claim made by the authors in[18]that enhancement is increasing the false positive rate by tampering with the objects’ edges, which are essential for detection. Although this might be true, there are other underlying factors to be thoroughly investigated, such as the cases of improved detection performance after enhancement and annotation errors presented in this section.

SECTION: VIIConclusion

In this work, we conducted a comprehensive investigation of underwater image enhancement methods and their effects on object detection. Our analysis included a quantitative and qualitative evaluation of different SOTA enhancement models, each representing a distinct enhancement category using two datasets, namely RUOD and CUPDD. A Quality index (Q-index) was proposed to assess the quality distribution of the images produced by these models. Subsequently, separate object detection models were trained and tested on enhanced and original images. A correlation study is also conducted between image quality and detection performance. Our findings reveal that enhancement significantly improves the quality of low-quality images but detrimentally impacts high-quality images. In addition, we found no significant relationship between enhancement metrics and detection scores. Although the detection model of the original non-enhanced images outperformed the detection models of the enhanced images, we revealed that enhanced images sometimes outperformed the original images. This enhancement effect is likely due to the reduction in the disparity between low and high-quality images, making challenging low-quality testing scenes more comparable to high-quality training scenes within the same dataset. Moreover, we provided cases where the detection models of the enhanced images detected cryptic objects that were missed in the ground truth annotations of the original images. Based on these findings, we recommend the following future directions: 1) enhancement should be used during the annotation process to help annotators identify hidden objects and thus generating more accurate datasets. 2) running preliminary detection on original images can also aid in revealing unnoticed objects. 3) the cases where the enhanced images outperformed the original images in terms of detection should be quantified and analyzed to check if enhancement can be used conditionally for an improved overall detection performance.

SECTION: Acknowledgment

The authors thank the United States Geological Survey (USGS) for supporting this research under grant number G23AS00029.The authors also appreciate the contributions of our colleagues and the technical assistance from the Great Lake Research Center and the Institute of Computing and Cybersystems at Michigan Technological University.

SECTION: References