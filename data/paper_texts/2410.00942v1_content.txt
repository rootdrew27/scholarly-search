SECTION: AR-Sieve Bootstrap for the Random Forest and a simulation-based comparison with rangerts time series prediction

The Random Forest (RF) algorithm can be applied to a broad spectrum of problems, including time series prediction. However, neither the classical IID (Independent and Identically distributed) bootstrap nor block bootstrapping strategies (as implemented in rangerts) completely account for the nature of the Data Generating Process (DGP) while resampling the observations. We propose the combination of RF with a residual bootstrapping technique where we replace the IID bootstrap with the AR-Sieve Bootstrap (ARSB), which assumes the DGP to be an autoregressive process. To assess the new model’s predictive performance, we conduct a simulation study using synthetic data generated from different types of DGPs. It turns out that ARSB provides more variation amongst the trees in the forest. Moreover, RF with ARSB shows greater accuracy compared to RF with other bootstrap strategies. However, these improvements are achieved at some efficiency costs.

Keywords:Block Bootstrap, Forecasting, Machine Learning, Resampling, Time Series Analysis,

SECTION: 1Introduction

Random Forest (RF)[Breiman,2001]has become one of the go-to models for data analysis because of its flexibility and performance. RF also appears to perform well in time series forecasting[Huang et al.,2020, Kane et al.,2014, Naing and Htike,2015].

The relationship between the accuracy of RF and the correlation among its trees has been established[Hastie et al.,2009, Lee et al.,2020]. The less correlated the trees, the more accurate the model. In a seminal paper,Breiman [2001]used classical bootstrapping[Efron,1979], also known as IID Bootstrap, to construct the trees.

However, when dealing with time series data, the IID assumption is not valid since the observations are dependent by nature, and thus their dependency could be broken.Goehry et al. [2021]proposed replacing the IID bootstrap with different block bootstrap strategies[Carlstein,1986, Künsch,1989, Politis and Romano,1994a,1991]and coined the new method rangerts. They exemplified that this approach can be better than the standard RF. However, they only used two benchmark datasets and an extensive simulation study confirming these findings is still missing. Moreover, although IID and block bootstrapping work well in practice, they do not consider the complete nature of the DGP.

In this paper, we propose to use the AR-Sieve Bootstrap (ARSB)[Bühlmann,1997, Kreiss,1988]instead of the IID bootstrap to construct the trees of the RF. The ARSB draws the bootstrap samples from a fitted autoregressive (AR) model and has already been shown to perform well for other time series models such as ARMA (AutoRegressive Moving Average) models[Kreiss et al.,2011].
To assess the performance of this new RF model, we compare its predictive accuracy with that of five RFs variants and a benchmark model for time series forecasting based on an autoregressive model fit in extensive simulations.
For this purpose, we consider six different classes for the DGP: AR-[Jürgen Franke and Hafner,2008a], MA- (Moving Average,[Jürgen Franke and Hafner,2008a]), ARMA-[Jürgen Franke and Hafner,2008a], ARIMA- (AutoRegressive Integrated Moving Average,[Jürgen Franke and Hafner,2008a]), ARFIMA- (AutoRegressive Fractionally Integrated Moving Average,[Granger and Joyeux,1980]), and GARCH (Generalized AutoRegressive Conditional Heteroskedastic,[Jürgen Franke and Hafner,2008b]) processes, see sections3and4for the explicit definition of the DGPs.

We start with a brief introduction to RF in Section2and its different bootstrap strategies used in the literature. We then present the new approach with the ARSB and compare its computational complexity with that of the other bootstrap methods in Section3. Section4concludes with the results and an outlook.

SECTION: 2Random Forest

The Random Forest learning algorithm is a bagging (Bootstrap aggregating)[Breiman,1996]technique. A bagging algorithm merges the predictions of multiple base learners to obtain a better prediction than its individuals.
The more diverse the learners, the more accurate the ensemble. As its name suggests, RF uses decision trees[Breiman et al.,2017]as base learners. As a first step, variability is achieved in RF through bootstrapping, which determines the observations to be fed for each tree construction.

A tree is built by recursively splitting the observations of a node into two disjoint partitions, starting from the root node, which contains all the bootstrap observations. Only a randomly chosen subset of sizemtryof the features is considered for the split. This is the second source of diversity in RF.
It is either tuned as a hyperparameter during training or chosen from the default settings: For a dataset withfeatures, the default choice formtryisin classification andfor regression tasks. The best split is done at the point along one of themtryfeatures’ axes which minimises the average impurity of the resulting child nodes. In a regression context, the impurity is quantified via the variance[Breiman et al.,2017]– the lower the variance, the purer the node. Other impurity criteria, such as the least absolute deviation[Roy and Larocque,2012], can also be used.
The tree construction has to be stopped once some criteria are met to avoid over-fitting. Some of those criteria include theminimum node sizeto attempt a split or themaximum depthof the tree. Once built, the final prediction is obtained by averaging the individual predictions of each tree. Therangerpackage[Wright and Ziegler,2017]provides a fast implementation of RF with a wide choice of parameters to tune.

SECTION: 3Bootstrap strategies for Random Forest

There exist different bootstrap strategies for the RF in the literature. The most common one is the IID bootstrap[Efron,1979]which is implemented in therangerpackage. Moreover, for time series forecastsing several block bootstrap strategies have been suggested recently[Goehry et al.,2021]and were implemented in therangertspackage[Goehry et al.,2017]. We explain the latter together with the ARSB in the sequel. To this end, we consider a time series model of lengthgiven by real-valued random variables,.

SECTION: 3.1Block bootstrapping

The main idea of block bootstrapping is to keep some portion of the data together in the form of blocks to avoid breaking their dependency[Künsch,1989].

Künsch [1989]proposed the so-called Moving Block Bootstrap (MBB). Here, the time series is first divided intooverlapping blocks of length,, where the blockstarts at time index,. Thenblocks are drawn independently with replacement and joined together in the order in which they were drawn to recover the original length of the time series. Figure1illustrates this strategy for a time series of lengthusing a block length of.

The MBB is easy to implement but also has some drawbacks. Beyond the choice of a ’good’ block length, it neglects the dependency between blocks in the and the bootstrap sample will in general not be stationary, see e.g.Politis and Romano [1994b]. Several variants exist that partially solve these problems: The Stationary Block Bootstrap (SBB,[Politis and Romano,1994a]) allows for different block lengths, while the Circular Block Bootstrap (CBB,[Politis and Romano,1991]) assumes the time series to have a circular shape, and the Non-Overlapping Block Bootstrap (NBB,[Carlstein,1986]) builds blocks with no common observations.

Nevertheless, the IID Bootstrap is the benchmark resampling strategy used for RF and can be seen as a specialisation of the above block bootstrap strategies in which the blocks’ lengths are chosen as.

SECTION: 3.2AR-Sieve Bootstrap

The AR-Sieve bootstrap (ARSB)[Bühlmann,1997, Kreiss,1988, Kreiss et al.,2011]uses residual resampling by fitting an autoregressive process on the data. The fitted model is linear and additive in the noise term with the following form for:

whereis the mean of the stationary time series,the order of the model,withthe model coefficients, andthe white noise process. The ARSB consists of four steps:

Fit the model and obtain the estimated Yule-Walker coefficientsand residuals,

Center the residuals around 0 if the fitted model has no intercept:,

Draw from (centred) residualswith replacement,

Construct.

In this paper, we use the Levinson-Durbin recursion[Franke,1985]to fit the model because it ensures the resulting bootstrap seriesto be stationary[Kreiss et al.,2011]. To determine the order of the fitted model, we use Akaike’s Information Criterion (AIC), which is asymptotically equivalent to the Leave-One-Out Cross-Validation for the model’s selection[Shao,1993]. Moreover, AIC is not time-consuming. It is particularly suited for simulations as no manual checking is required. In practice, other methods or criteria, e.g. based upon Auto-Correlation Functions (ACFs) plots, can be used.

Although the fitted model is linear with Gaussian errors, the approach is theoretically valid for more general DGPs, seeKreiss et al. [2011]for details. We evaluate the use of ARSB in the RF for forecasting different type of linear and non-linear processes in the simulation study in Section4. Before that we shortly discuss the computational complexity of all bootstrap methods.

SECTION: 3.3Computational complexity

Block bootstrap methods are all index-based. Only the samples’ indices need to be known for a bootstrap dataset to be created, making block bootstrapping strategies efficient. They typically perform intime and only needspace (Figure2, right) for each tree to be built.

On the contrary, ARSB is less efficient since it is a residual resampling technique.
The model fitting can take up to[Franke,1985]operations to solve the Yule-Walker (YW) equations using the Levinson-Durbin recursion, andtime to reconstruct the time series. In practice,, such that ARSB can be executed in linear time.

The AR-Sieve strategy also requires more memory space. The bootstrap time series and its firstlags need to be stored (Figure2, left), requiringspace. However, the additional effort required by the ARSB may prove beneficial as it helps to create more diverse trees, which may increase the RF’s accuracy. Whether this intuition is really true will be evaluated in the following section.

SECTION: 4Simulation study

SECTION: 4.1Setup

To compare the models’ performances, we conduct a Monte Carlo experiment withiterations. Data are generated from AR-, MA-, ARMA-, ARIMA- and ARFIMA as well as GARCH processes with Gaussian white noise. The first five DGPs are described in the most general (ARFIMA) form as

while GARCH processes are generated via

For each DGP we consider different parameter configurations as given below:

AR(1):{0.2, -0.2, 0.5, -0.5, 0.8, -0.8},

MA(1):{0.2, -0.2, 0.5, -0.5, 0.8, -0.8},

ARMA(1,1):,

ARIMA(1,1,1):,

ARFIMA(1,0.3,1):,

GARCH(1,1):.

We thus consider twenty-five (25) time series DGPs, each in three different sizes{100, 500 and 1000}, yielding a total ofparameters configurations.
For each configuration, we find the Yule-Walker estimates for the ARSB coefficients and train RFs with the bootstrap strategies presented in Section3. One-step () and five-step ahead () predictions are then made using the recursive multi-step forecast[Taieb et al.,2012]method. At each iteration, data is generated, the different models are fitted, and their performances are evaluated via the Mean Square Error (MSE). To obtain a unified metric over theiterations, we use the median of all MSEs[Tyralis and Papacharalampous,2017].

The simulations are realised with theRprogramming language[R Core Team,2021], and the existing RF models are created using therangertspackage[Goehry et al.,2017], which is an extension of therangerpackage[Wright and Ziegler,2017]for time series prediction. We perform no tuning and use the parameters’ default values provided by the package. We further extendedrangertsto support ARSB and made the extension accessible onGitHub.

SECTION: 4.2General results

Only a summary of the results is presented to avoid redundancy. Detailed results can be found in appendicesAandB.

We observe an improvement for the new RF of up to 13% and 16% for one-step and five-step ahead prediction, respectively (Figure3) for the median of MSEs compared to the other RF variants. ARSB successfully creates more diverse trees than its counterparts. The new RF’s performance is comparable with that of the YW estimator. This similarity shows that the properties of the fitted AR model have been conserved during tree construction with ARSB. However, it performs less good if the DGP has a high value on the MA part of Equation2(Figures15-16,22-24) but still performs better than the other RF models.

The models’ performances were also ranked (tables1and2) on each of the 72 simulation configurations and DGP. Ties were resolved using mean ranks.

Although the new RF was not the best performing model overall, the results are still encouraging given that no parameter tuning was done. Moreover, different to the classical YW time series approach, RF can also support exogenous variables. It also performs better on long-term (h=5) than short-term prediction (h=1) and when the AR part dominates the DGP (figures9-10).

Overall, Benchmark RF and RF with block bootstrap show similar performances on non-seasonal time series, contrary to the Goehry et al.[Goehry et al.,2021]study, where the time series were seasonal. This hints at better adequacy of block bootstrapping for seasonal time series.

Regarding the running time, RF with ARSB has the highest value (Figure4) as expected and was approximately up to two times slower than the other RF models. Nevertheless, its running time does not increase linearly with the order of the fitted model (table3). It remains low enough with an average value ofseconds for an average time series length ofand an average fitted order of.

SECTION: 5Conclusion and Perspectives

We introduced the AR-Sieve bootstrap (ARSB) method for the random forest (RF) algorithm, a residual resampling strategy based upon a fitted AR process. A simulation study on synthetic data showed better forecasting accuracy of the proposed approach compared to IID and Block Bootstrap strategies. However, ARSB appeared to be computationally more demanding than its counterparts but remained fast enough for practical applications. Moreover, RF with ARSB appears to conserve most properties of an AR process but struggles with DGPs having high coefficients on the MA part.

The conducted study was empirical and a more detailed and theoretical study regarding validity and consistency to support these findings needs to be done. This could also help to find out whether extensions of the ARSB[Fragkeskou and Paparoditis,2018]are worthwhile for the RF. A similar study with additional hyperparaemter tuning[Bischl et al.,2017]as well as exogenous information, e.g.,
on several benchmark datasets, would also provide a better insight into the model’s performance.

SECTION: References

SECTION: Appendix AOne step ahead forecast

SECTION: A.1AR Models

SECTION: A.2MA Models

SECTION: A.3ARMA Models

SECTION: A.4ARIMA Models

SECTION: A.5ARFIMA Models

SECTION: A.6GARCH Models

SECTION: Appendix Bfive-step ahead forecast

SECTION: B.1AR Models

SECTION: B.2MA Models

SECTION: B.3ARMA Models

SECTION: B.4ARIMA Models

SECTION: B.5ARFIMA Models

SECTION: B.6GARCH Models