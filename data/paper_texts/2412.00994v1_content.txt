SECTION: DSSRNN: Decomposition-Enhanced State-Space Recurrent Neural Network for Time-Series Analysis

Time series forecasting is a crucial yet challenging task in machine learning, requiring domain-specific knowledge due to its wide-ranging applications. While recent Transformer models have improved forecasting capabilities, they come with high computational costs. Linear-based models have shown better accuracy than Transformers but still fall short of ideal performance. To address these challenges, we introduce the Decomposition State-Space Recurrent Neural Network (DSSRNN), a novel framework designed for both long-term and short-term time series forecasting. DSSRNN uniquely combines decomposition analysis to capture seasonal and trend components with state-space models and physics-based equations.
We evaluate DSSRNN’s performance on indoor air quality datasets, focusing on CO2 concentration prediction across various forecasting horizons. Results demonstrate that DSSRNN consistently outperforms state-of-the-art models, including transformer-based architectures, in terms of both Mean Squared Error (MSE) and Mean Absolute Error (MAE). For example, at the shortest horizon (T=96) in Office 1, DSSRNN achieved an MSE of 0.378 and an MAE of 0.401, significantly lower than competing models.
Additionally, DSSRNN exhibits superior computational efficiency compared to more complex models. While not as lightweight as the DLinear model, DSSRNN achieves a balance between performance and efficiency, with only 0.11G MACs and 437MiB memory usage, and an inference time of 0.58ms for long-term forecasting.
This work not only showcases DSSRNN’s success but also establishes a new benchmark for physics-informed machine learning in environmental forecasting and potentially other domains. To facilitate further research and application, we provide our datasets and code at https://github.com/ahmad-shirazi/DSSRNN.

SECTION: 1.Introduction

The prediction of complex time series data stands as a critical challenge in machine learning (ML), particularly where accuracy and computational efficiency intersect(Zhou et al.,2021; Lim et al.,2021; Zhou et al.,2022; Liu et al.,2022; Zhou et al.,2021; Zeng et al.,2023). Outlier events in time series, such as spikes or dips, can disrupt forecasting models, necessitating robust detection and handling methods to maintain accuracy(Blázquez-García et al.,2021). Time series are omnipresent in today’s data landscape, underpinning critical applications across various domains, from traffic flow estimation(Pamuła and Żochowska,2023)and energy management(Chen et al.,2023)to financial investment(Barra et al.,2020; Sezer et al.,2020). In time series analysis, the challenge isn’t just long-term or short-term forecasting(Zeng et al.,2023)but also preparing datasets, often impeded by missing data(Che et al.,2018; Emmanuel et al.,2021). The integration of state-space models, founded on physics principles, with advanced ML techniques, presents a versatile solution that transcends domain boundaries(Kirchgässner et al.,2023; Fu et al.,2022; Xue et al.,2020). This synthesis transforms physics-based equations into computational algorithms, providing a robust framework capable of capturing the complex temporal dynamics across a range of real-world problems—from forecasting energy consumption in buildings to predicting building energy consumption(Somu et al.,2021; Pham et al.,2020; Olu-Ajayi et al.,2022; Zhang et al.,2021)and weather patterns(Bochenek and Ustrnul,2022; Markovics and Mayer,2022; Kashinath et al.,2021).

A significant portion of prior work in this area has focused on the use of transformers(Wen et al.,2022). However, transformers face several challenges in time series prediction. For long-term forecasting, their self-attention mechanism, which scales quadratically with sequence length, leads to high computational costs and memory usage. Capturing long-range dependencies effectively can be problematic, potentially diluting important signals over time. Transformers also suffer from the vanishing gradient problem and limitations of fixed context windows. In short-term prediction, transformers are prone to overfitting due to their high capacity, struggle with noise sensitivity, and may not generalize well with limited data. Despite shorter sequences, they remain computationally expensive compared to simpler models. Additionally, transformers require extensive hyperparameter tuning and are less interpretable than traditional models(Zeng et al.,2023). Our methodology addresses these limitations through the use of state-space models that capture the dynamics of complex systems(Friedland,2012), essentially marrying the precision of physics with the adaptability of machine learning. This integrated approach enables prediction accuracy while optimizing computational resources.

Our research, though broadly applicable, specifically targets the domain of Indoor Air Quality (IAQ), with a particular focus on predicting carbon dioxide () levels. This variable is of critical health relevance and presents significant predictive challenges due to its nonlinear behavior and sensitivity to numerous factors, including human occupancy, activity patterns, and ventilation systems(Taheri and Razban,2021). The complexity ofdynamics makes it ideal for demonstrating our models’ efficacy in handling intricate dependencies and varying conditions.

We validate our work in a range of indoor environments, showing that our model surpasses existing benchmarks in accuracy and efficiency. Its success in forecasting using data with a range of characteristics demonstrates a potential for wider application in time series forecasting.

This paper aims to achieve the following objectives:

Design and implement advanced neural network models that integrate state-space methodologies with physics-based equations, aiming to significantly improve the accuracy of IAQ predictions across both regression and classification tasks for time series analyses spanning short-term and long-term intervals.

Develop robust strategies for managing missing data within time series analyses by implementing efficient data imputation techniques, thereby enhancing the reliability and integrity of the dataset.

Prepare and release four comprehensive IAQ datasets to address the current lack of publicly available IAQ data, providing valuable resources for the research community.

The remainder of this paper is structured as follows: Section 2 reviews related work in the field, Section 3 describes the methodology behind our model, Section 4 presents the results of our experiments, and Section 5 discusses the implications of our findings and concludes the paper.

SECTION: 2.Related Work

Given that we explore two crucial aspects: imputation, which addresses missing data to ensure dataset integrity, and prediction, focusing on forecasting methodologies, we present background work on both these topics.

SECTION: 2.1.Review of Imputation Methods

Accurate imputation in time series is crucial for dataset preparation, as in our case detailed in Table1. Traditional RNN-based methods, such as GRU-D(Che et al.,2018), have introduced time decay factors to address missing data. Approaches like BRITS(Cao et al.,2018)and M-RNN(Yoon et al.,2018)enhance this with bidirectional RNNs, yet they grapple with the inherent complexities of long-term dependencies and computational intensity. Generative models, including GANs and VAEs, provide alternative strategies(Luo et al.,2018; Liu et al.,2019; Casale et al.,2018; Fortuin et al.,2020), but their training complexity and interpretability issues limit their practical scalability.

The emergence of self-attention-based models has introduced a new paradigm in time series imputation. Models like DeepMVI(Bansal et al.,2021)and NRTSI(Shan et al.,2023)leverage Transformer architectures for handling multidimensional and irregularly sampled time series data. Despite their novel approach, these models face challenges in computational efficiency and generalizability. Du et al. (2023) recently advanced this field with SAITS, a model that optimizes imputation through diagonally-masked self-attention blocks, addressing both temporal dependencies and feature correlations with enhanced efficiency(Du et al.,2023).

Our approach diverges significantly from these existing methods. As noted in the introduction, we integrate domain knowledge with ML. This enhances the accuracy of our model while also addressing the limitations of existing approaches. Additionally, leveraging neural networks within this framework allows for efficient processing of larger datasets, balancing robustness with computational agility.

SECTION: 2.2.Review of Prediction Techniques

Transformer models by Vaswani et al. (2017)(Vaswani et al.,2017), have revolutionized time series forecasting through their innovative use of self-attention mechanisms, enabling the model to focus selectively on relevant parts of the input. This model’s parallelizable architecture significantly enhances its efficiency in handling large datasets. There are also newer models based on transformers such as PatchTST(Nie et al.,2022), Informer(Zhou et al.,2021), and Autoformer(Wu et al.,2021).

Furthermore, Informer model by Zhou et al. (2021)(Zhou et al.,2021), which addresses the quadratic time complexity and memory usage inherent in traditional Transformer models, introduces the ProbSparse self-attention mechanism, a distilling operation, and a generative decoder to manage long-sequence time-series forecasting effectively. This model demonstrates a marked improvement in prediction capabilities for long-sequence forecasting tasks by mitigating the limitations of the encoder-decoder architecture.

The FEDformer model, introduced by Zhou et al. (2022)(Zhou et al.,2022), represents another advancement in time series forecasting by integrating Transformer architecture with seasonal-trend decomposition. This hybrid approach enables the FEDformer to capture both the global trends and the intricate details within the time series data. By leveraging the sparsity in the frequency domain, FEDformer achieves enhanced efficiency and effectiveness in long-term forecasting, outpacing conventional Transformer models in both performance and computational efficiency.

In contrast, the Autoformer model by Wu et al. (2021)(Wu et al.,2021)incorporates an Auto-Correlation mechanism and a novel series decomposition strategy within its architecture. This approach allows the Autoformer to dissect and analyze complex time series data, focusing on seasonal patterns while progressively incorporating trend components. This model’s unique design facilitates a deeper understanding of temporal dependencies, particularly in sub-series levels, thereby offering a robust solution for long-term forecasting challenges.

In addition, the introduction of a linear decomposition model, Dlinear, by Zeng et al. (2023)(Zeng et al.,2023)challenges the prevailing dominance of Transformer-based models in long-term time series forecasting. By simplifying the model to focus on linear relationships within the data, Dlinear astonishingly surpasses more complex models in performance. This revelation underscores the potential of minimalistic approaches in extracting temporal relationships effectively, thereby questioning the necessity of complex architectures for certain forecasting tasks.

Moreover, the Modern Temporal Convolutional Network (ModernTCN) by Luo et al. (2024)(Luo and Wang,2024), leverages convolutional operations to model temporal dependencies. ModernTCN employs dilated convolutions, residual connections, and layer normalization, resulting in a highly effective time series forecasting model with reduced computational overhead. However, ModernTCN may suffer from limited representation capabilities due to its lightweight backbone, potentially leading to inferior performance in certain tasks.

Additionally, SegRNN by Lin et al. (2023)(Lin et al.,2023)revisits RNN-based methods by introducing segment-wise iterations and parallel multi-step forecasting. These techniques reduce the number of recurrent steps and enhance forecasting accuracy and inference speed, making SegRNN a highly efficient tool for long-term time series forecasting. Despite these improvements, SegRNN can be constrained by the inherent limitations of RNNs, such as vanishing gradients and difficulty in capturing long-term dependencies.

Lastly, TiDE by Das et al. (2023)(Das et al.,2023)combines the strengths of linear models and multi-layer perceptrons (MLPs). TiDE’s dense encoder architecture captures both linear and nonlinear dependencies in time series data, offering robust performance without the excessive computational demands of more complex models like transformers. Nonetheless, TiDE can struggle with modeling non-linear dependencies and may face challenges when dealing with complex covariate structures.

Our DSSRNN model builds upon decomposition techniques used in Dlinear, Autoformer, and Fedformer, while significantly enhancing computational efficiency. By optimizing its architecture, DSSRNN achieves faster training and inference times than leading transformer-based models, making it particularly suitable for real-time applications.

SECTION: 3.Decomposition State-Space Recurrent Neural Network (DSSRNN)

Our proposed model, the Decomposition State-Space Recurrent Neural Network (DSSRNN), builds upon two innovative physics-infused ML models: the State-Space Recurrent Neural Network (SS-RNN), Figure2, and its advanced iteration, the Decomposition State-Space Recurrent Neural Network (DSSRNN), Figure3. The SS-RNN framework synergizes the robustness of state-space physics with the flexibility of recurrent neural networks, while the DSSRNN employs moving-average decomposition to filter out high-frequency noise inherent in temporal datasets.

Finally, note that DSSRNN, uses the same framework for the imputation as well as the prediction tasks. This dual functionality underscores the versatility of our model, providing a cohesive approach to managing and forecasting time series data.

SECTION: 3.1.Data

In our study, we utilized data from Lawrence Berkeley National Laboratory’s Building 59 in California, covering the period from August 19, 2019, to December 31, 2021. This unique indoor air quality (IAQ) dataset, notable for its rarity due to the generally limited availability of comprehensive data in this area, contains 20,760 data points from office settings. It encompasses hourly indoormeasurements along with other crucial environmental variables—a collection of data infrequently encountered in existing studies. The location of each office numbered 44, 45, 62, and 68 within the building is detailed in a map provided in(Luo et al.,2022), offering contextual insight into the data collection environment. A key aspect of our research is the prediction of indoorconcentration (), using five current environmental variables such as current indoorlevels, time of day (Hour), day of the week (Num_Week), and both indoor and outdoor temperatures (and)(Mohammadshirazi et al.,2022,2023).

In Figure1, we present a comprehensive depiction of the environmental conditions across four different office settings through the analysis of indoor temperature, outdoor temperature, and indoorconcentration.
The subfigure1(a)focuses on the indoorconcentration levels. Across all offices, the medianconcentration is consistent, indicating a general adherence to acceptable IAQ standards. However, the presence of numerous high outliers in each office suggests episodic events where thelevels exceed typical ranges. These instances could potentially be linked to variable occupancy levels, inconsistent operation of ventilation systems, or activities within the offices that temporarily increaseproduction.

The indoor temperature, illustrated in the subfigure1(b), reveals a notable variance between offices. Office 1 exhibits a broad temperature range with several high outliers, indicative of sporadic deviations from the norm, which could be attributed to intermittent HVAC system performance or external factors affecting the indoor climate. In contrast, Offices 2 and 3 display a narrower interquartile range, suggesting a more consistent application of temperature control measures. Office 4, while also showing a relatively stable temperature range, has more outliers on the lower end, pointing to occasional dips in the indoor temperature.

The subfigure1(c)captures the outdoor temperature conditions for each office location. Offices 1, 3, and 4 show a wider range of temperature values, which may reflect either a larger variance in outdoor climatic conditions or a more extended period of data capture. Office 2 demonstrates tighter temperature controls with minimal outliers, suggesting either less climatic variability or more effective mitigation of outdoor temperature fluctuations. Overall, the multitude of data eliminates the statistical biases, and the variety of the data supports the training of a robust model.

Analysis of the dataset reveals the presence of missing values, as detailed in Table1, corresponding to the period from January 2020 to April 2020. The missing data is imputed, and appropriate sections of the dataset are selected for training, testing, and validation to maintain the integrity of our analysis. The upcoming section elaborates on this method and its significance in our research.

SECTION: 3.2.Imputation Methods

In this section, we explore the imputation methods utilized to address the challenge of missing data within our datasets. Table1provides a detailed examination of the data absence across various input variables collected from four distinct office environments, spanning from January 2020 to April 2020. To effectively manage this dataset, we segregated it into training, validation, and testing subsets, with all instances of missing data allocated to the testing subset. Our proposed DSSRNN model is uniquely designed to tackle both prediction and imputation tasks using the same architecture. It evaluates performance metrics such as Mean Squared Error (MSE) and Mean Absolute Error (MAE) during the validation phase to accurately impute missing values. Given our development of the DSSRNN model to serve dual purposes—both prediction and imputation—using a unified architecture, we will delve into a comprehensive explanation of our DSSRNN model in the subsequent section dedicated to prediction.

SECTION: 3.3.Prediction Methods

In the prediction section of the paper, we delve into the architecture of our advanced model, the Decomposition State-Space Recurrent Neural Network (DSSRNN). This section outlines the foundational State-Space Recurrent Neural Network (SS-RNN) structure and its extension into the DSSRNN model, which incorporates decomposition techniques for enhanced prediction capabilities. We will first elaborate on the SS-RNN architecture, explaining its design and how it integrates the robustness of physic-based and state-space models with the adaptability of recurrent neural networks. Subsequently, we will discuss the DSSRNN and how it utilizes decomposition to refine the prediction of time series data, including the identification and forecasting of outlier events. This approach underpins the robustness of our model in predicting critical, rare events within the datasets, which we illustrate through a comparative analysis with leading models in the domain.

The State-Space Recurrent Neural Network (SS-RNN) architecture represents a novel blend of traditional state-space models with the dynamic capabilities of recurrent neural networks (RNNs). At its core, SS-RNN aims to model time series data by capturing both the underlying physical processes and temporal dependencies. This is achieved through a combination of state variables that embody the physical state of the system and RNN layers that process sequential data, allowing for the integration of prior knowledge and temporal context. The SS-RNN architecture is designed to efficiently handle the flow of information through time, making it particularly adept at predicting complex dynamics in time series data, such as those encountered in IAQ monitoring.

The transformation from physic to ML equations begins with the foundational state-space representation of indoordynamics(Persily,2022), as shown in Equation1. This equation models the temporal change in indoorconcentration by relating it to variables that capture both the physical characteristics of the environment and the dynamics ofexchange. Specifically, the mass flow ratereflects the air exchange due to ventilation, represented by,signifies air density,stands for the volume of the indoor space, andrepresents the internal generation of, such as those generated by occupants or appliances within the space. The variable, meanwhile, captures the concentration ofoutside the building. Together, these variables form the basis of our state-space approach, setting the stage for its translation into an ML framework.

Equation2is where the conversion to an ML framework takes place. The termrepresents the state matrix, encapsulating the system dynamics, andthe input matrix, incorporating external influences. These matrices interact with the state vector(Equation3) and input vector(Equation4) to form a discretized version of the physics-based model, respectively. This interaction is key to defining the subsequent state vectorand the differential state change. These state vectors, embedded within the ML architecture, allow neural networks to approximate the intricate relationships found within the dataset. The continuous state update fromto(Equation3), facilitated by the differential state, is a discrete analog of the physical process. This reformulation allows us to leverage the power of neural networks to approximate the complex relationships within the data, retaining the interpretability and structure provided by the physical model.

The SS-RNN model, presented in Figure2, is meticulously crafted to process inputs within each discrete time step. The model assimilates the current state vectorand the input vectorto advance its internal state. The transformation ofalongsideis governed by a nonlinear function, resulting in the interim state change, as shown in equation5. This intermediate state is then sculpted by another nonlinear function, culminating in the updated state(Equation6). Weight matricesand, alongside bias vectorsand, underpin this evolution, modulating the influence of past and present information and embedding non-linearity via ReLU activation functions, denoted asand(see equation7).

Based on Figure2representation, the SS-RNN ML model state space and its changes are updated as follows:

It should be noted that the equation referenced in1, while primarily developed for modeling, can be adapted to encompass a variety of indoor pollutants such as,,,, and. This adaptation necessitates modifying the generation termfor each specific pollutant’s source strength and incorporating particular deposition or removal processes. Such modifications allow the SS-RNN model to extend its applicability across a range of indoor pollutants, demonstrating its versatility in different environmental conditions. In the following subsection, we will expound on the intricacies of the Decomposition State-Space Recurrent Neural Network (DSSRNN) and its implementation for enhanced predictive performance.

DSSRNN architecture advances the SS-RNN framework by incorporating decomposition techniques to dissect and model the temporal data into trend and seasonal components by incorporating a decomposition mechanism to capture and model both global and local temporal structures within the data. Specifically, the trend component, represented as a moving average calculated over a 24-hour window, captures the global patterns. In contrast, the seasonal component—obtained by subtracting this trend from the observation window—reflects local fluctuations. As depicted in Figure3, the observation window provides input to two parallel pathways: one for isolating seasonal patterns processed by the SS-RNN, and another for delineating the trend via a linear model. The outputs of these pathways are then synergistically combined to yield the final output, capturing both the nuanced cyclicity and underlying directional movement in the data. This bifurcation allows the DSSRNN to adeptly handle the intricacies of time series forecasting, enhancing its ability to identify and predict complex patterns influenced by both long-term trends and short-term seasonal variations.

In order to demonstrate the robustness of our model in predicting outlier events within thedata, we employ a binary classification approach based on the box-and-whisker plot method. Specifically,measurements above the upper whisker—calculated aswhereis the third quartile andis the interquartile range—are classified as outliers and encoded as ’1’, while non-outliers are assigned ’0’ (See the threshold value for each office in Table2. This binary transformation allows us to specifically focus on the prediction of outlier occurrences. We then compare the performance of our model in predicting these binary outcomes against SoTA models, showcasing our model’s capability to identify significant deviations in air quality. The number of outlier events detected for each office is systematically cataloged in Table2. The comparative results and analysis are presented in detail in the Result section of the paper.

SECTION: 4.Experiments

In the results section of our paper, we evaluate the DSSRNN’s performance using the robust NVIDIA A100 GPU at the Ohio Supercomputer Center (OSC)111https://www.osc.edu/. The forthcoming subsections will present a detailed account of the model’s imputation and prediction capabilities, each scrutinized through rigorous experimentation. These findings will illustrate the DSSRNN’s strengths in accurately forecasting and handling missing data within the context of indoor air quality assessment.

SECTION: 4.1.Imputation Analysis

In the results section outlined in Section3.2, we present a comprehensive evaluation of our DSSRNN model in comparison with other imputation methods. The evaluation, focused on the imputation accuracy, is crucial given the presence of missing values documented in Table1. Our approach, embodied by the DSSRNN model, is meticulously compared against alternative methodologies, including DLinear (a linear-based model)(Zeng et al.,2023)and SAITS (a self-attention-based model)(Du et al.,2023), in terms of their ability to handle missing data effectively. The comparative analysis, as presented in Table4, demonstrates the superior performance of DSSRNN in minimizing both MSE and MAE metrics. For office 1, DSSRNN attains an MSE of 0.357 and an MAE of 0.375, which are the lowest among the methods compared. This pattern of outperformance continues across Office 2, Office 3, and Office 4, with DSSRNN maintaining the lead in minimizing error metrics, signifying its robustness in dealing with incomplete data.

DLinear, while being a robust model based on linear methodologies, exhibits higher MSE and MAE metrics compared to DSSRNN. This indicates that despite its strengths, DLinear encounters limitations when processing datasets with missing information. In contrast, DSSRNN, which transcends the linear approach of DLinear, captures complex temporal dynamics through a deeper understanding of the data’s underlying processes, thereby offering a more comprehensive representation of patterns. However, SAITS, despite embodying the forefront of self-attention-based imputation techniques, falls short of DSSRNN’s performance, as evidenced by its higher MSE and MAE figures. This disparity highlights potential challenges SAITS faces in effectively managing the complexities associated with missing data.

Through this evaluation, DSSRNN’s advanced architecture proves to be instrumental in accurately learning and imputing data patterns, outshining the state-of-the-art models that were specifically chosen for their superior performance in their respective categories. The bold metrics in Table4underscore DSSRNN’s exceptional capability to reconstruct missing data with unparalleled precision, solidifying its standing as the leading imputation model for tackling data sparsity in various analytical applications.

SECTION: 4.2.Prediction Analysis

In the comprehensive assessment of forecasting methodologies detailed in Table3eclipsing a spectrum of advanced models. Our experimental procedures, aligned with the benchmarks set by Zeng et al. (2023)(Zeng et al.,2023), ensured a balanced comparison across the forecast horizons T=96, 192, 336, 720, catering to the input length for long-term time series forecasting. DSSRNN consistently delivered the most accurate forecasts, achieving the lowest MSE and MAE values at all forecast lengths and across all office environments. For example, at the shortest horizon (T=96) in Office1, DSSRNN reported an MSE of 0.378 and an MAE of 0.401, establishing its proficiency in capturing intricate temporal dynamics.
Among the linear-based models, Dlinear showed commendable performance, especially at medium to long forecast horizons, marking it as the best within its category. Within the transformer-based models, ’Transformer’ excelled at the shortest horizon (T=96), while ’Informer’ demonstrated its strength at extended lengths (T=192, 336, and 720), adept at handling long-range data dependencies.

Our research into short-term time series forecasting evaluates the DSSRNN model across various sequence lengths ranging from 24 to 720-time steps, examining its performance in four office settings for both short-term (e.g., 24-time steps) and long-term forecasting (e.g., 720-time steps). As detailed in Figure4, DSSRNN performs greatly in most settings and although TIDE has a superior accuracy in some cases, considering its high computational cost, our model can be seen as a better choice, striking a balance between computational cost and accuracy.
These findings underscore DSSRNN’s adaptability to various forecasting horizons and its transformative potential in time series analysis.

Classification

In an effort to further comprehend the regression capabilities of our DSSRNN model, we transformed the regression task into a classification problem, as meticulously explained in Section3.3.3. The results of this transformation are depicted in Table5, where we delineate the thresholds set for event detection, the count of events, and their respective percentages for each office environment.

The classification approach allows us to dissect the model’s performance in distinguishing between high-concentration and normal event occurrences. While the DLinear model exhibits commendable recall for high-concentration events—likely a result of its decomposition capability, breaking down time series into seasonal and trend components—the Autoformer model shows a preference for normal events. This tendency is attributed to its Transformer-based architecture, which is proficient in modeling long-range dependencies within the data.

Notwithstanding the strengths displayed by DLinear and Autoformer in their respective domains, the DSSRNN model demonstrates a clear overarching superiority. It transcends the typical decomposition approach by integrating state space physics concepts, offering a profound understanding of the dynamics of the time series data. DSSRNN consistently maintains higher True Positive rates alongside lower False Positive rates, indicating an adeptness at correctly identifying events while minimizing false alarms—a critical aspect for real-world applications. The True Negative rate is also the highest for DSSRNN, reflecting its strong discriminative power in differentiating non-events.
Precision and Recall are metrics where DSSRNN distinctly outshines its counterparts. The model’s precision is indicative of the reliability and exactitude of its predictions, while the higher recall rates—especially for high-concentration events—demonstrate an increased sensitivity and a lower propensity to overlook significant events.

Computational Efficiency

In our comparative analysis, delineated in Table6, we scrutinize the computational efficiency of our proposed DSSRNN model compared to DLinear and a suite of Transformer-based architectures. This evaluation is predicated on the Office #1 air quality dataset, adopting an input sequence lengthand a prediction sequence length. The criteria for this comparison encompass multiply-accumulate operations (MACs), the aggregate number of model parameters, the average inference time over a quintet of runs, and the requisite memory allocation during computational processes.

DLinear distinguished itself as the most computationally efficient model, demanding the least computational resources with a mere 0.04G MACs and minimal memory usage of 196MiB, alongside a remarkable average inference speed of 0.15ms. This exceptional efficiency renders it particularly advantageous for resource-constrained settings. Close on its heels, DSSRNN stands as the second most efficient model, consuming slightly more resources at 0.11G MACs and 437MiB of memory but still delivering a swift inference time of 0.58ms, validating its efficacy.

SECTION: 5.Conclusion and Future Work

In this study, the Decomposition State-Space Recurrent Neural Network (DSSRNN) was introduced, integrating physics and state-space methodologies to adeptly impute missing data and forecast indoor air pollutant concentrations. The architecture of this model is designed for computational efficiency, surpassing the performance of conventional transformer-based models. The DSSRNN has shown exceptional proficiency, outperforming established benchmark models in both MSE and MAE, which highlights its potential for accurate regression and classification predictions and reliable data imputation. The success of the DSSRNN paves the way for the application of physics-informed machine learning models in environmental data analysis and potentially in other predictive domains.

For future work, we aim to apply the integration of physics-based concepts and state-space models with ML techniques to broader domains, such as the modeling and forecasting of energy consumption in buildings. This will involve analyzing usage patterns alongside external factors like weather conditions. Additionally, we will explore the optimization of climate control systems, focusing on heating, ventilation, and air conditioning to adapt efficiently to environmental variations. This expansion will harness the DSSRNN model’s strengths in addressing both indoor and outdoor pollutant dynamics and energy efficiency challenges.

SECTION: References