SECTION: Generating Realistic Adversarial Examples for Business Processes using Variational Autoencoders

In predictive process monitoring, predictive models are vulnerable to adversarial attacks, where input perturbations can lead to incorrect predictions. Unlike in computer vision, where these perturbations are designed to be imperceptible to the human eye, the generation of adversarial examples in predictive process monitoring poses unique challenges. Minor changes to the activity sequences can create improbable or even impossible scenarios to occur due to underlying constraints such as regulatory rules or process constraints. To address this, we focus on generatingrealisticadversarial examples tailored to the business process context, in contrast to the imperceptible, pixel-level changes commonly seen in computer vision adversarial attacks. This paper introduces two novellatent space attacks, which generate adversaries by adding noise to the latent space representation of the input data, rather than directly modifying the input attributes. These latent space methods are domain-agnostic and do not rely on process-specific knowledge, as we restrict the generation of adversarial examples to the learned class-specific data distributions by directly perturbing the latent space representation of the business process executions.

We evaluate these two latent space methods with six other adversarial attacking methods on eleven real-life event logs and four predictive models. The first three attacking methods directly permute the activities of the historically observed business process executions. The fourth method constrains the adversarial examples to lie within the same data distribution as the original instances, byprojectingthe adversarial examples to the original data distribution.

SECTION: IIntroduction

Machine Learning (ML) and Deep Learning (DL) methods have become central to a wide range of applications such as loan application processes[2], criminal justice[26]and, disease diagnostics[25]. Similarly, there has been a growing interest in using ML and DL techniques within Process Mining (PM), a field focused on analyzing event data generated during the execution of business processes. PM enables organizations to uncover, monitor, and improve real-world processes by extracting knowledge from modern information systems. Through data mining and machine learning techniques, PM reveals valuable insights into the processes of organizations, such as discovering process models, detecting inefficiencies, and predicting future behavior.

Predictive Process Monitoring (PPM) is a subfield of PM that focuses on providing insights into ongoing business processes, including predicting the remaining time, the next event, or the outcome of a process. The latter field is also known as Outcome-Oriented Predictive Process Monitoring (OOPPM) and aims to predict the future state of ongoing cases. For instance, in a loan application process, all the events record specific activities-referred to as control flow attributes—leading up to the acceptance or rejection of the loan request. The primary goal of OOPPM is therefore to address critical questions, such as whether the loan application will be approved. This means that it is required to monitor and analyze a vast amount of data from complex systems, such as financial transactions or patient admissions. In this context, it is essential for predictive models to effectively handle malicious, faulty, noisy, or unpredictable inputs. Adversarial testing has emerged as a valuable approach for assessing model robustness under such challenging data conditions[42,28,23,32,30,3], addressing trustworthiness concerns in critical decision-making contexts[30,3].

This work extends research[30,32]on the vulnerability of OOPPM methods to input perturbations. In[30], we already assessed how adversarial noise affects OOPPM predictive performance and explanation robustness across training and testing data. In this work, we introduced adversarial noise to the input data by perturbing dynamic event attributes in business process data, similar to subtle pixel changes in images that can significantly influence predictions. For example, in a loan application, modifying dynamic activity attributes like theCredit ScoreandIncome Levelaffected the outcome predictions. The limitation of this work was that we avoided permuting the control flow attributes, as changes to activities and their position within a case are inherently noticeable due to the discrete nature of sequences. Even small modifications, such as substituting activities with those in nearby positions in the embedding space, such as movingCredit CheckbeforeDocument Verification, can lead to unrealistic scenarios, as the sequence of events in a business process is often critical to its execution. In[32], we addressed the prior limitation of avoiding perturbations to control flow attributes with the use of manifold learning, by projecting the generated adversarial examples to the learned class-specific data distributions, which means that the adversarial examples are more likely to preserve the essential characteristics of the data. However, this method does not guarantee that the adversarial examples are indistinguishable from the original business process cases, as multiple activity changes across various event positions may have happened.

In this paper, we improve, extend, and benchmark these methods with novel adversarial attacking methods.
Thelatent space attacks(i.e. thelatent sampling attackand thegradient steps attackidentify adversaries within a dense and continuous representation space of the data rather than within the input data space. Latent space representations are able to capture high-level structural or sequential patterns within the data, and therefore preserve relationships between activities that would be lost in direct sequence manipulation. Furthermore, the generation of these latent space attacks is domain-agnostic and does not require specific process knowledge, as we introduce noise directly into the latent space representation of business processes, specifically targeting discrete event sequences in PPM. Unlike traditional methods that require direct manipulation of original event sequences, our latent space attacks generate adversarial examples from this compressed representation, producing realistic event sequences, while increasing the success rate of the attacks. Although previous studies have highlighted the effectiveness of perturbations within latent spaces[48,1], these methods are not capable of handling discrete event sequences. We address this gap by encoding business processes into a continuous latent space, and we generate adversarial examples within this space and map them back to the original input data space. In this way, we are able to manipulate event sequences without relying on domain-specific knowledge, a capability not covered by prior research. Figure1illustrates how such adversarial examples look in latent space. The gradient steps approach, however, is a white-box attacking method and is used for benchmark purposes only.

Next, we provide an extensive benchmark of eight different attacking methods, with multiple black-box and white-box adversarial attacks on business process executions, assessing model vulnerability through attack success rates. The successfulness of an attack is dependent on the difficulty of the adversarial attack, and the robustness of the model.

To summarize, we investigate the following research questions:

How can we generate realistic adversarial examples in the latent space, while conforming to the same data distribution and real-world process constraints, without relying on domain-specific knowledge?

How vulnerable are OOPPM models to theserealisticadversarial attacks?

The rest of this work is structured as follows. In sectionII, background information on adversarial machine learning (in PPM) is given. Next, sectionIIIprovides information on the eight adversarial attack methods. sectionIVreports on an experimental evaluation with eleven real-life event logs and four different classifiers. In sectionVI, the related work is discussed. sectionVIIconcludes the paper and points to elements for future work.

SECTION: IIBackground

This section provides an overview of the preprocessing steps needed for OOPPM purposes and discusses key concepts from the field of adversarial machine learning and manifold learning.

SECTION: II-APredictive Process Monitoring

An event logconsists of events grouped by cases, forming process execution traces. An eventfrom the event universeis represented as a tuple, wheredenotes the case ID,represents the activity (i.e., thecontrol flowattribute), andindicates the timestamp. Each event also includes event-related attributes, ordynamicattributes, which can vary during the course of the case and are represented by. Attributes that have the same value for every event with the same case identifier are calledstaticattributes, represented by. A trace is a sequence of events, withas the case ID andas the index in the trace, ordered by event timestamps. An eventin caseof the event logis denoted as. Each trace has a class label, with= {0,1} in the case of a binary outcome. This class labeling depends on the needs and objectives of the process owner[38]. An example event log is provided in TableI, containing the data about Tom (Case A) and Sarah (Case B). The outcome is defined by whether the loan application isacceptedorrejected.

To progressively learn from different stages of traces, a prefix logis extracted from the event log, encompassing all prefixes from the complete traces. A trace prefix of casewith lengthis defined as, where. Given the varying lengths of each (prefix) trace, a sequence encoding mechanism is necessary when using machine learning models with diverse attribute quantities. The aggregation encoding mechanism from[6]employs one-hot encoding to convert a dynamic categorical attribute into several transformed attributes. Frequency vectors are then extracted for each unique attribute value, representing the frequency of occurrence in the (prefix) trace.

Step-based models such as recurrent neural networks can use the activity labels and dynamic attributes associated with subsequent events one by one, preserving the sequential nature of the process data, and allowing the model to effectively capture its inherent temporal relationships. Categorical features are often converted into a set of binary vectors through one-hot encoding.
To provide a low-dimensional representation of discrete attributes, embeddings are often used. This method transforms categorical attributes into a vector of continuous numbers meaningfully, avoiding the high-cardinality issue and nonsensical vector distances associated with one-hot encoding. The transformed attributes are denoted by. The transformed prefix log is used to train a predictive model, which predicts the prefix (trace)as.

SECTION: II-BAdversarial Machine Learning

In the context of machine learning, addressing vulnerabilities against adversarial attacks can be an important tool for mitigating the risks posed by adversaries who exploit model weaknesses, or test the ability to reliably perform on possible out-of-domain samples or unseen in-domain samples that were not present during training.

This latter perspective on robustness ensures that machine learning models can generalize effectively across a variety of different inputs.

Adversarial Machine Learning (AML) focuses on examining the vulnerability against adversarial attacks, and their ability to withstand both adversarial perturbations and unexpected variations in data. In[39], a taxonomy of adversarial defenses and attacks is introduced to the field of AML, providing a structured framework for categorizing various vulnerabilities and defense mechanisms.
This taxonomy is visualized in Figure2, showing both adversarial defenses and attacks. However, this paper focuses exclusively on adversarial attacks. The adversarial attacks are divided into three categories:location-specific,knowledge-specificandintent-specific. Poisoning (evasion) attacks are when the training (testing) data is perturbed to mislead the ML model into incorrect predictions.
In this paper, we only focus on evasion attacks. Next, adversarial attacks can be categorized into two knowledge-specific types: white-box attacks and black-box attacks.

In white-box attacks, the attacker has full access to the model, including its architecture, parameters, and training data. This allows them to exploit this detailed information to craft highly targeted attacks. In contrast, black-box attacks are conducted with limited access, relying only on the output or predictions of the model without insight into its internal workings. In this paper, we employ both black-box methods (e.g., theregular attack[30], which use only the output of the model to guide the attack), and white-box methods (e.g., thegradient-based attack, which leverage full access to the internal gradients, see SectionIII). Additionally, attacks can also be divided intotargetedandnon-targeted attacks. Non-targeted attacks, such as regular adversarial examples, are designed to induce misclassification without steering the prediction toward a specific class. On the other hand, targeted attacks, like manifold-based attacks[32], aim to intentionally mislead the model into predicting a specific, often predefined, class or outcome. Note that, as we are in a binary classification setting, our non-targeted regular adversarial attack will result in the same outcome as it would in a targeted fashion. In this paper, we focus exclusively on adversarial attacks on the test data (evasion attacks) to ensure consistent and comparable results. By keeping the training dataset and model unchanged throughout the experiments, we eliminate variability introduced by attacking the training process (poisoning attacks). This approach allows for a direct comparison of performance across different attack strategies and methods on the same test dataset, ensuring that observed differences arise from the nature of the attacks rather than changes in the training configuration.

Two types of adversarial attacks have already been introduced to OOPPM[30], theattack (A1)and theattack (A2). In[30]the A1 attack targets the last event in the trace, modifying its dynamic attributes, resulting in a trace.
In this paper, however, we focus exclusively on permuting theactivityvalues within each trace. This approach contrasts with[32], where both theactivityandresourceattributes can be permuted. In this paper, the final event therefore looks like this:. In[30]and[32], the A2 attack modifies the dynamic attributes of all events in the trace, resulting in, where. In this paper, we consider the A2 attack as:, where

Valid adversarial attacks must adhere to the label invariance assumption, which states that the true class label of a data point remains unchanged even if the input data have been perturbed or manipulated. In other words, a theoretically perfectly robust and reliable model (or classifier) should not alter its predicted label in response to changes (attacks) made to the input.

Label invariance assumption.The label invariance assumption refers to the property that the underlying class label of a data point remains unchanged even if the input data have been perturbed or manipulated, such as in the case of adversarial attacks.

The label invariance assumption can be expressed as:

Consider an original traceand its true label. Letbe an adversarial example generated fromby a certain adversarial attack. The true labels of bothandshould be the same, but the predicted labelsandcould differ. Additionally, the original examplemust be correctly predicted, i.e.,. An adversarial attack is consideredsuccessfulwhen the original prediction made by the model changes.

For image data, the concept of adversarial examples is relatively straightforward. The visual features can be perturbed with imperceptible noise, leading to misclassifications while maintaining the original label. In contrast, generating adversarial examples for process data presents more significant challenges. Process data is structured, typically temporal, and context-dependent. Perturbations must carefully maintain the underlying control flow and domain-specific constraints of the process, making it difficult to ensure that the adversarial example remains a valid process instance. This complexity arises because the relationships between events and activities are critical, and perturbations can easily break these relationships, leading to implausible attacks, possibly not in line with the overall or the class-specific process behavior.

Successful Adversarial Attack.An adversarial attack is successful when the original prediction made by the model has changed. Thesuccess ratetherefore measures how many adversarial examples have misled the model relative to the number of input data points. By contrast, a predictive model is consideredrobustagainst minor input changes if the predicted labels of the original and adversarial instances remain the same.

Adversarial robustness.Adversarial robustness is the ability of the model to maintain predictive performance despite being presented with adversarial examples designed to deceive it. Early theories attributed this vulnerability to rare cases[34]or the linear nature of deep learning networks[10]. However, recent works state that regular adversarial examples are seen as unlikely in real-life scenarios and often lack semantic meaning[48,27]. This has led to a focus on generating more natural,on-manifoldadversarial examples, which are examples generated within the data manifold, making them more realistic and relevant to real-world scenarios. Some works even suggest that regular adversarial examples deviate from the manifold, and can therefore change their true label concerning the data distribution, potentially violating the label invariance assumption[33,9].

SECTION: II-CManifold Learning

The manifold hypothesis[5]posits that high-dimensional data can be effectively represented on a lower-dimensional manifold, an idea central to various methods aimed at ensuring adherence to the data distribution[14,22,27,7,40,41]. Generative models such as Variational Autoencoders (VAE) and Generative Adversarial Networks (GANs) are used in manifold learning to map low-dimensional latent vectors to high-dimensional data while adhering to an assumed prior distribution, typically Gaussian[16]. We refer to this process asmanifold learning.

Unlike a vanilla (non-variational) autoencoder that focuses on reconstructing the inputthrough the encoding process of an encoderto generate a latent representation, VAEs frame this as a probabilistic challenge. The objective of a VAE is to maximize the Evidence Lower Bound (ELBO), balancing the reconstruction loss (”how accurately can we reconstruct the original data point?”) with the Kullback-Leibler (KL) divergence (”how closely does the learned latent variable distribution match the chosen prior distribution?”). In VAE training, the prior distributionis often the standard normal distribution, and the learned latent variable distributionis the distribution approximated by the VAE encoder.

The reconstruction loss is typically measured by the Negative Loss-Likelihood (NLL) loss:

The notationis used to emphasize the dependency of the predicted probability on the model parameters. The KL divergence is the sum taken over all components of the distributions (i.e. the number of latent variables in the latent space,, and), where):

In the context of OOPPM, class-specific manifolds are learned by creating manifolds specifically for the prefixes of each class label, as introduced in[32]. The Long Short-Term neural network (LSTM) VAE is thus trained using an LSTM-based architecture on these class-specific prefixes, which leverages the sequential information inherent in process data. By focusing on the prefix data for each outcome class separately, the model more effectively captures the specific temporal dynamics and uncovers the underlying structure or manifolds of high-dimensional data specific to certain classes, while discarding noise and redundancy. Within this setup, the encoder networks compute a low-dimensional latent representation of data, and the decoder network reproduces samples from the latent space. By imposing a prior over the latent distribution, the variational aspect of the VAE regularizes the encoder and structures the latent space.

SECTION: IIIGenerating Adversarial Examples for Outcome-Oriented Predictive Process Monitoring

In this section, we detail four attack strategies—regular,projected,latent sampling, andgradient steps—that together yield eight distinct adversarial attack methods. We explain how each attack strategy addresses the challenges posed by the sequential nature of process data. Theregularandprojectedstrategies operate directly within the original input space, representinginput space attacks. These two strategies employ three distinct approaches for attacking individual events within the data sequence, resulting in six methods. In contrast, thelatent samplingandgradient stepsstrategies generate adversarial examples within the latent space, categorized aslatent space attacks. Together, these approaches result in eight unique adversarial attack methods.

SECTION: III-AAdversarial Example Generation

Algorithm1presents a general pipeline for generating adversarial examples, applicable across multiple attack strategies. This process begins with an iteration over prefixes (line 3), followed by the application of specific attack techniques (lines 4-20), which is further described in the subsequent subsections. Next, we select the adversarial prefix that is closest to the original in latent space, using pairwise Euclidean distances to measure the proximity between the latent representation of the original trace and the latent representation of the generated adversarial prefix (lines 22-29).

Theregular attackstrategy, illustrated in Figure3, follows the approach introduced in[30]. In this strategy, we directly attack the original trace, where the resulting adversarial example is the modified trace after applying the attack. We distinguish between three types of regular attacks: thelast event attack (A1), which modifies only the last event of the trace (introduced in[30]); theall event attack (A2), which modifies every event (introduced in[30]); and the novelk event attack (A3)targets a selected subset ofevents within the trace.
Thek event attackis similar to the A1 and A2 attacks but additionally considers the positional constraints of activities within a case, making the attack more context-aware and reducing unrealistic perturbations that might otherwise break the realistic flow of the process. Specifically, this attack accounts for the fact that certain activities can only be executed (and therefore recorded as events) at specific positions within a process execution, performing perturbations only when an activity has already occurred in a similar position within the case. The A3 attack therefore only modifiesactivityvalues based on tuples of (index, activity) extracted from the event log. This allows us to randomly selecttuples, where each selectedactivityof the tuple being permuted if (1) theactivitydiffers from the original and (2) the index has not already been modified, ensuring relevance to frequently occurring activities and contextual appropriateness.

Formally, given a trace, the A3 attack produces a modified trace:

where for each modified event, we have:

In contrast, theprojected attackstrategy, also shown in Figure3, builds on the concept of on-manifold attacks introduced in[32]. In this approach, we learn class-specific encoders and decoders that approximate the data distribution for each class by training them on class-specific prefixes. Then, after attacking the original trace, we project the adversarial example onto the class-specific data distribution. This approach adheres to the label invariance assumption, as the generated adversarial examples for a particular class remain within the boundaries of the data distribution of that class. This is in contrast to the traditional attacks that might result in unrealistic adversarial examples that would never occur in practice.
An important consequence of this projection is that the length of the adversarial prefix may differ from the original prefix length, i.e. projecting an adversarial example onto this manifold can result in either longer or shorter adversarial examples after decoding. This is due to our approach of masking out the padding tensor (see sectionIV-C). What we could have done to prevent this is, adding a constraint for preserving a similar prefix length relative to the original trace. Another option could be to enforce sparsity in the generated examples. Enforcing sparsity would mean explicitly encouraging the generated examples to differ minimally from the original trace, typically by modifying only a small number of attributes to preserve as much of the original sequence as possible. Nonetheless, our primary objective in this paper is to generate successful and realistic adversarial examples rather than to produce adversarial examples of identical lengths.

Different works argue that adding noise in the latent space allows for the generation of more natural adversarial examples, compared to adding noise in the input space. Many works argue that rather than permuting unimportant background pixels (in the case of image-based adversarial attacks), adversarial examples should be generated through perturbations in the latent space[12,27]. The first novel attacking method we propose is thelatent sampling attack, displayed in Figure4, and it is a black-box method that exploits the stochasticity introduced by the variational inference of the LSTM VAE manifold to create adversarial examples. This technique ensures that the instance stems from the same mean and variance as the original instance. Specifically, the reparameterization trick generates, whereis the latent variable,is the mean,is the standard deviation, andis a random variable sampled from a standard normal distribution. This approach ensures that the adversarial example is generated from the same latent space representation, making the generated adversarial examples morerealistic.

Conversely, thegradient steps attack, displayed in Figure4, is a white-box approach that utilizes the gradients of the classifier to iteratively move across the decision boundary. In each iteration, the algorithm calculates the gradients of the classifier with respect to the input trace, and it identifies the most impactful directions in the latent space to apply perturbations, creating adversarial examples that are likely to deceive the model while remaining close to the original trace in latent space. In this paper, we adapt the REVISE counterfactual generation method[14]to handle the sequential structure of business process executions. The adapted algorithm is tailored to create latent space representations based on the one-hot encoded structure of activity sequences, and the gradient-based optimization minimizes Euclidean distance in the latent space. This ensures adversarial examples with minimal perturbations by default. However, despite its effectiveness in creating adversarial examples, the gradient steps attack is primarily useful for benchmarking purposes and not for practical use in real-world settings. The key reason for this limitation lies in the white-box nature of the attack: it requires full access to the model, including the gradients of the classifier. In real-world scenarios, adversaries typically do not have this level of insight into the model, and thus cannot use such an attack strategy.
Similarly to the projected attack, the length of the adversarial prefix for latent space attacks can again differ from the original prefix length.

SECTION: III-BClosest Adversarial Example Selection

In VAEs, the latent space is generally modeled as a multivariate Gaussian distribution. The encoder learns the mean and variance of the distribution, allowing the latent points to be smoothly distributed within a Euclidean space, often viewed as a continuous low-dimensional manifold[4]. This design enables the encoder to map inputs to specific points in the latent space, where the Euclidean distance between points reflects the similarity between their latent representations. Figure5provides an illustration of the motivation for minimizing distances in the latent space, while Figure6demonstrates the process of selecting an adversarial example based on the smallest distance in the latent space.

SECTION: III-CClass Label Prediction of Adversarial Examples

We then use a classifier to predict the label outcome of the adversarial examples. An attack is considered successful if the predicted label of the adversarial example differs from the original label of the prefix trace.

SECTION: IVExperimental Setup

In this section, we describe the different event logs and their specifications, the benchmark models commonly used for OOPPM purposes, and the hyperoptimization settings and implementation details of various setups.

SECTION: IV-AEvent Logs

This research utilizes four distinct real-life event logs available at the 4TU Centre for Research Data website111https://data.4tu.nl/, which are frequently employed in the domain of Outcome-Oriented Process Prediction and Monitoring (OOPPM)[38,17,43,11,19]. The event logs are segmented using Linear Temporal Logic (LTL) rules as detailed in[38], establishing objectives for the processes. The segmentation is based on the labelling functions defined by the four LTL rules, resulting in four separate binary prediction tasks. TableIIprovides the specifications of these event logs.

The first event log, BPIC2012, captures the execution of loan application processes in a Dutch financial institution. The event log includes the events related to a particular loan application, with the labeling indicating whether the final outcome of a case was either accepted, declined, or cancelled. Similar preprocessing steps for trace prefixing and cutting are employed as in[38].

The BPIC2015 event log consists of events from the building permit application processes in five Dutch municipalities. An LTL rule is applied to the event log, with a split for each municipality. The rule stipulates that the activitysend confirmation receiptmust always be followed byretrieve missing data, and the latter must always occur if the former does. No trace cutting is performed on this event log.

The Sepsis cases event log contains discharge information for patients with sepsis symptoms in a Dutch hospital, from emergency room admission to discharge. Here, the labelling is based on patient discharge rather than LTL rules[38].

SECTION: IV-BBenchmark Models

Four different predictive models, which are commonly used in OOPPM[38,17], e.g. Logistic Regression (LR), XGBoost (XGB), Random Forest (RF), and an LSTM neural network, are used to benchmark the findings. These models serve as the most representative of the statistical, machine learning, and deep learning facilities. LR is a transparent predictive model that is commonly used for classification, and both XGB and RF are popular tree-based ensemble models due to their fast convergence and high predictive accuracy. Finally, a long short-term memory LSTM neural network is used due to its popularity in OOPPM research[44,18].

SECTION: IV-CImplementation

In this section, we describe the implementation details and assumptions made in this work.

A temporal split is used to divide the event log into training and testing cases[38]. The cases are ordered by start time, with the first 80% used for training the predictive model and the remaining 20% for evaluating its performance. This split is done at the level of completed traces, ensuring all prefixes of the same trace stay in the same set (either train or test). To avoid overlap, events in the training cases that occur during the test period are discarded.

Then, we extract prefix logs and do the necessary preprocessing steps as detailed inII. For the machine learning models, aggregation encoding[38]is used to encode the data for the machine learning algorithms. Note that aggregation encoding is unique to process data. On the other hand, the deep learning models are built to work with sequential models and therefore do not need the use of such a sequence encoding mechanism.

There are two different types of models that need to be trained: a classifier and class-specific LSTM VAEs. The classifier is used to predict the outcome of the instances, whereas the LSTM VAEs are used to project or generate the adversarial examples.

The classifier is trained on the extracted prefix loss, The LSTM VAE, on the other hand, needs additional steps. First, to ensure that the LSTM VAE could learn the ending points of traces, we added an artificial End Of Sequence (EoS) token to all the (prefix) traces. During decoding, whenever the LSTM VAE produced the first EoS token, we masked the subsequent tokens with padding values. To prevent the model from learning irrelevant information from the length of the traces or the padding tokens, we masked the padding token value during backpropagation. It is important to note that this masking and the use of the EoS token were not necessarily for training and fitting the predictive models. Additionally, we provided the option to remove duplicate traces in the event logs, as duplicates are prevalent when only considering control flow. Furthermore, we allowed for the removal of ambiguous duplicates—identical traces with different labels, since such cases could mean that a trace serves as its own adversarial example. The implementation of the LSTM VAE is based on the work done in[31].

The implementation of the benchmark setting is provided in theGitHub repository, as we have transformed the extensive counterfactual generation benchmark named CARLA[21], for the use of adversarial example generation.

SECTION: VExperimental Evaluation

In this section, we evaluate our work based on the answers provided to the research questions.

The first research questionRQ1can be answered with the use of sectionIII, by showing how the two novel methods, i.e. thelatent sampling attackand thegradient steps attackensure that the generated adversarial examples conform to the original data distribution while adhering to real-world process constraints. Importantly, the proposed approaches circumvent the need for domain-specific knowledge by leveraging latent representations derived from generalizable process mining models.

The second research questionRQ2evaluates and compares the performance of eight distinct attack strategies—both black-box and white-box—using metrics such as attack success rates and their implications on OOPPM methods. To address this, we calculate the success rate, as this is one of the most commonly used evaluation metrics to evaluate the vulnerability against adversarial examples[46]. Thesuccess rateon attacks is calculated by counting the percentage of adversarial examples that have a flipped prediction. A prediction is successfully flipped if the probability decreases/increases until it is lower/higher than the predefined optimal threshold for that classifier. Figure8presents box plots of the success rates for the classifier and attacking method combinations, aggregated over the different event logs. Note that the gradient steps attack method was only applicable to the LSTM neural network. Even though XGBoost uses gradients as well, it is not inherently differentiable. Decision trees, and by extension XGBoost, work by recursively partitioning the input space and making piecewise constant decisions based on learned thresholds for the attributes. Since decision trees are piecewise constant, there is no continuous gradient to compute for the model with respect to the input data. The decision boundary of the model is therefore not smooth but is highly non-linear and discrete.

SECTION: V-ABenchmark Results

First, the classifiers can be ranked by their average AUC scores as follows:LRwith 80.24,LSTMwith 82.35,RFwith 83.93, andXGBwith 84.54. In contradiction with the findings of[17], the DL model (LSTM) does not outperform the classical ML ensemble methods. First, we attribute the slight underperformance of the LSTM to the results for the event logBPIC2015 (2)andSEPSIS (1). Note that the results for the SEPSIS logs are volatile in general. Second, the performance of the DL is not fully exploited due to the trace cutting, as the DL models are better capable of handling long-term dependencies.

The attack methods, ranked from least to most successful, are as follows:

(1) Last Event (Regular) with a success rate of 12.17%

(2) 3 Event (Regular) with a success rate of 13.83 %

(3) Gradient Steps with a success rate of 33.17 %

(4) All Event (Regular) with a success rate of 38.05 %

(5) Last Event (Projected) with a success rate of 45 %

(6) All Event (Projected) with a success rate of 49.04 %

(7) 3 Event (Projected) with a success rate of 50.08 %

(8) Latent Sampling with a success rate of 50.17 %.

The first insight from the ranking is that theLatent Samplingmethod achieves the highest success rate. Additionally, the projected attack method introduced in[32]outperforms the regular attack method that was introduced in[30]. Increasing the number of iterations beyond the default 1500 could potentially further enhance success rates. Second, based on Figure8, the LSTM seems to be the most vulnerable OOPPM model, as it consistently exhibits the highest average success rates across most attack methods. Furthermore, theLatent Samplingattack method remains the most effective across all the four OOPPM methods. By contrast, the three regular attacking methods—Last Event (Regular),All Event (Regular), and3 Event (Regular)—have the lowest success rate. Finally, Figure7reveals an interesting pattern: for most adversarial example generation methods (excludingLast Event (Regular)and3 Event (Regular)), the number of successful adversarial examples increases with prefix lengths up to 30. This finding is counterintuitive, as one might expect that permutations in shorter prefixes would have a greater impact on the prediction.

In Figure8, we present the normalized frequency of successful adversarial attacks across different trace prefix lengths, averaged over the event logs, for various classifiers and attack methods. We focus on prefix length as it serves as an important factor in business process analysis, influencing the amount of information available to a predictive model at each point in a process. Longer prefixes typically provide a more comprehensive context, which can make models less sensitive to small perturbations, impacting the success rate of adversarial attacks. The results for theLast Event (Regular)attack method indicate a negative relationship between trace length and attack success rate, suggesting that classifiers become less vulnerable to adversarial attacks when the activity in the last event of longer traces is permuted, compared to shorter ones. In contrast, for theprojected,latent sampling, andgradient stepsattack methods, there is a clear positive relationship between prefix length and success rate, as the normalized frequency generally increases with longer prefixes. A similar trend is observed for the RF and XGB models under theAll Event (Regular)attack method, but this pattern does not hold for the LR and LSTM models.

SECTION: V-BResults per Event Log Group

In TableIII, we present the results for the three groups of event logs, averaged per group.

The Euclidean distance represents the distance in the latent space, where lower values indicate that adversarial examples are more perceptibly indistinguishable from the original instances. Interestingly, the Euclidean distance in the latent space for regular attack strategies (apart from theAll Event (Regular)) is lower than those for projected strategies, such asLast Event (Projected),All Event (Projected), and3 Event (Projected). This is counterintuitive, as one might expect that projection-based attacks, designed to be close in the latent manifold, would yield adversarial examples that are actually closer. This contradicts previous findings in[32].

TheEMDcaptures the distance between the original prefix and its adversarial example to provide a meaningful measure of similarity between probability distributions, which, in the context of adversarial examples, reflects howclosethe generated adversarial example is to the original prefix trace in terms of its underlying data distribution. TheEMDserves as a measure of distance in the input space, with lower values indicating adversarial examples that are more perceptually indistinguishable from the original instances. In adversarial machine learning, this is crucial because we want adversarial examples to remain realistic and perceptually similar to the original input while still achieving the goal of deceiving the model.

Another interesting remark is that, in general, theEMDdistances of the adversarial examples are lower for theprojectedattacking strategy compared to theregularattacking strategies. For the event log groupBPIC2012, we can see that theregularattacking strategy methods have the lowest average distance in the latent space (Euclidean distance), but also the highestEMDvalue of the adversarial examples after decoding. To understand this intuitively, we need to understand how the decoder functions. First, minimizing distances in the latent space encourages adversarial examples to stay close to the original representations at an encoded level. However, this does not guarantee that the decoded outputs will remain perceptually close to the original inputs. This is because the decoder is trained to reconstruct training instances, and the sensitivity of the decoder to changes in the latent space may vary based on the characteristics of the dataset it was trained on. As a result, even small shifts in latent space can lead to disproportionately large changes in the decoded output if the decoder is highly sensitive.

SECTION: V-CCluster Profiles of Attacks

Instead of relying solely on statistical analysis, we define these clusters as profiles that capture common patterns in the nature of adversarial changes. To group attacks by their unique characteristics, we apply thresholds derived from quartile values, calculated after normalizing both theEMDandDL Editscores according to the prefix length. This normalization ensures comparability across traces of varying lengths. The quartile-based thresholds provide a straightforward yet effective method for distinguishing between attack cluster profiles.

The resulting profiles categorize attacks into four primary groups:Subtle,Aggressive,Sequence Perturbations,Distribution Shift. Attacks that do not exhibit sufficientlydistinctivecharacteristics are classified asOthers. Below, we detail the criteria used to define and classify each profile.

AggressiveAggressive attacks are characterized by significant changes to the trace. Specifically, an attack is classified as aggressive if theDL Editdistance andEMD distancefall within the third quartile, indicating extensive changes to the trace and its overall distribution. While these attacks achieve moderate success rates (around 40%), their broad and noticeable changes to the trace make them relatively easier to detect. It is clear to see that theAll Event (Regular)category is predominantly represented within this cluster, as these attacks indeed target all the events in a trace.

Subtle AttacksSubtle attacks are those that make only minimal changes to the original trace. An attack is considered subtle if both theDL Editdistance and theEMDvalue fall within the first quartile. These minimal changes reflect the conservative nature of such strategies and their focus on remaining undetected by only performing small perturbations that are less likely to be detected in the event log. As expected, theLast Event (Regular)and3 Event (Regular)profiles are best represented by this classification. These attacks are the least successful attacks, with a success rate of below 20 %.

Sequence Perturbations.Sequence Perturbations are changes that specifically target the order, dependencies, and relationships between events in the trace. Attacks with a highDL Editare adversarial examples with aDL Editdistance in the third quartile, but also with anEMD valuethat is lower than the median value. This kind of attack primarily targets the sequence order of events without altering the underlying distribution of event types, and they are highly successful, with a success rate of up to 60 percent. This group is mostly represented by theAll Event (Regular)attack.

Distribution Shift.Distribution shift attacks are attacks that made changes to the trace that, while not necessarily altering individual events in an obvious or direct way, lead to a significant shift in the overall structure or distribution of events.
Attacks with a highEMDvalue are adversarial examples with anEMDdistance in the third quartile. These attacks are not that successful, with a success rate close to 20 percent. This group is mostly represented by the3 Event (Regular)attack and theLast Event (Regular)attack.

Others.These attacks can be characterized by a combination of small changes in sequence structure (as indicated by a low to moderate DL) and slight shifts in event distribution (reflected by a low to moderate EMD), yet they are still successful. Although these attacks modify the sequence structure and event distribution of the trace, they are subtle enough to the extent that they would not be easily detectable. Despite these modest changes, the attacks remain successful, with a success rate of up to 50%.

In conclusion, theOtherscluster represents the most desirable attack profile, as these attacks generated adversarial examples with a low to moderateEMDdistance and low to moderateDL Editvalue, while maintaining high success rates. The fact that these attacks introduce more or lesssmallchanges in both sequence structure and event distribution, yet remain effective, suggests they exploit vulnerabilities that are not easily detected. The attack methods most representative of this category include theLast Event (Projected), theAll Event (Projected), the3 Event (Projected)and theLatent Samplingattacks. These methods showcase how moderate perturbations to the trace can yield successful results without major alterations to theoriginal process flow.
This showcases the need for projected and latent sampling-based approaches, as they can be effective without needing any particular engineering which resorts to strong insights into the sequential information of the processes.

SECTION: VIRelated Work

SECTION: VI-APredictive Process Monitoring

Predictive process monitoring is an important field that involves monitoring and analyzing process data extracted from business information
systems. In recent years, researchers have proposed various solutions to tackle different prediction tasks, such as predicting
the most probable next event or suffix of a case[35], estimating
the remaining time[8], or predicting the outcome[38]. For
a more comprehensive overview of the field, we refer to the systematic literature review of Neu et al.[20].

Recent works have already issued the lack of reliability of deep learning models in the context of predictive process
monitoring[29,24]with issues such as the compromised faithfulness of post-hoc explanations[29]. Incremental adaptations to predictive models have also been suggested to address prediction stability over time[24].

SECTION: VI-BAdversarial Machine Learning

Deep neural networks are powerful tools for learning complex tasks, but their adoption in high-stake decision-making is often limited due to their lack of robustness against adversarial examples[34,10]. Adversarial Machine Learning (AML) aims to test and enhance this robustness by generating adversarial examples designed to deceive the algorithms. Adversarial attacks on time series prediction models have already been investigated[45].

In our previous work, we examined the vulnerability of OOPPM models and their explainability methods to adversarial attacks, highlighting risks to decision-making accuracy[30]. These attacks are (naively) engineered to fool models into producing incorrect predictions and pose a risk to the accuracy of high-stakes decision-making processes. In more of our recent work[32], the idea was to explore these adversarial attacks as a means to enhance robustness against such threats.

The field of AML is also focused on improving robustness model and generalization[33,37,36,42,15]. Defense mechanisms against adversarial examples and taxonomies of such examples have been proposed[47]. Generative Adversarial Networks (GANs) have also been adapted for robust predictive modeling, with innovations like the closed-loop adversarial training using Encoder-Decoder (ED) architectures[36,37]to improve the suffix and remaining time prediction of event sequences. Next, robust predictive models that adapt to spurious correlations and varying data distributions have demonstrated improved performance in real-world event log evaluations[42]. Simple noise-based transformations have been proposed to enhance event log data for next activity prediction[15].

SECTION: VI-CManifold Learning

Manifold learning aims to uncover the underlying structure of high-dimensional data, particularly useful in multivariate variable-length time series[13]. Although early theories attributed model vulnerability to rare cases or the linear nature of DNNs[34,10], recent works emphasize the importance of generating realistic, on-manifold adversarial examples[48,27]. In predictive process monitoring, VAEs have been used to learn latent space data representations of process data[32,31].

SECTION: VIIConclusion

In this paper, we compared and evaluated four distinct adversarial attack strategies, resulting in eight different attacking methods, all specifically tailored to the process-based analysis domain. Our work addresses critical challenges in OOPPM by thoroughly evaluating model vulnerabilities to adversarial attacks and generating realistic adversarial examples. We benchmarked a variety of black-box and white-box attacks, highlighting that even small perturbations, whether in the original data space or the latent space, can significantly impact the performance of predictive models. This underscores the susceptibility of predictive models in PPM to adversarial examples, which can undermine their reliability in real-world applications.

One of the key insights from our analysis is that adversarial robustness in PPM cannot be approached in the same way as in traditional domains. Business processes have unique characteristics—such as the sequential nature of events, their dependencies, and the historical context of activities—that require specialized adversarial attack methods and defense mechanisms. Our introduction of three novel attacks, including a data-aware approach that respects the historical positioning of activities and two latent-space-based attacks, illustrates the importance of designing attacks that account for these intricacies. By doing so, we provide a more nuanced understanding of how adversarial examples manifest in business processes and how predictive models respond to them.

Excluding the fact that adversarial examples were generated only from instances that the original model initially predicted correctly, we can conclude that all the attack generation methods, except for the gradient-based approach, are model-agnostic methods. This means the attacks generated from these can be stored in a new test set useful for evaluating and comparing robustness across various models. The gradient-based attacks, which are generated with the help of using a trained OOPPM (LSTM) model, can also be collected into a robustness test set. However, this should not be used to compare the robustness of that particular model to other models, since the attacks are explicitly made based on its gradients.

Moreover, our study emphasizes the broader challenge of ensuring trust in PPM models. This trust can only be established if the models are robust against adversarial examples, which could otherwise lead to incorrect predictions and costly decision-making errors. To address this, we introduced a comprehensive robustness evaluation framework. This framework facilitates the generation of diverse adversarial examples, allowing models to be tested and trained under adversarial conditions.

Future work could be to identify which types of traces (e.g., common patterns, rare outliers, or specific structural characteristics) are most prone to adversarial perturbation. Additionally, examining how process models change before and after attacks can provide valuable insights into the nature of adversarial examples and inform strategies for mitigating their impact. Another important direction is investigating whether models should be trained separately on each attack type or on a mixture of multiple attacks to enhance robustness. Visualizing the structure of the latent manifold, particularly through techniques such as convex hull visualization, could further deepen our understanding of how adversarial examples navigate the latent space and interact with the underlying data.

SECTION: References