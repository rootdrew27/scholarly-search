SECTION: Distance-Adaptive Quaternion Knowledge Graph Embedding with Bidirectional Rotation

Quaternion contains one real part and three imaginary parts, which provided a more expressive hypercomplex space for learning knowledge graph.
Existing quaternion embedding models measure the plausibility of a triplet either through semantic matching or geometric distance scoring functions.
However, it appears that semantic matching diminishes the separability of entities, while the distance scoring function weakens the semantics of entities.
To address this issue, we propose a novel quaternion knowledge graph embedding model.
Our model combines semantic matching with entity’s geometric distance to better measure the plausibility of triplets.
Specifically, in the quaternion space, we perform a right rotation on head entity and a reverse rotation on tail entity to learn rich semantic features.
Then, we utilize distance adaptive translations to learn geometric distance between entities.
Furthermore, we provide mathematical proofs to demonstrate our model can handle complex logical relationships.
Extensive experimental results and analyses show our model significantly outperforms previous models on well-known knowledge graph completion benchmark datasets.
Our code is available athttps://github.com/llqy123/DaBR.

Distance-Adaptive Quaternion Knowledge Graph Embedding with Bidirectional Rotation

Weihua Wang1,2,3,††thanks:Corresponding Author. Email:wangwh@imu.edu.cn.,
Qiuyu Liang1,
Feilong Bao1,2,3,
Guanglai Gao1,2,31College of Computer Science, Inner Mongolia University, Hohhot, China2National and Local Joint Engineering Research Center of Intelligent
InformationProcessing Technology for Mongolian, Hohhot, China3Inner Mongolia Key Laboratory of Multilingual Artificial Intelligence Technology, Hohhot, China

SECTION: 1Introduction

Knowledge graphs (KGs)Liang et al. (2024a)are powerful tools for representing valid factual triplets by capturing entities and their relationships in a graphical format.
Owing to the well-structured graph, KGs are often used for various Natural Language Processing tasks including but not limited to, question answeringMendes et al. (2024); Faldu et al. (2024), entity alignmentWang et al. (2024), KG-based recommendationLiang et al. (2024c)and KG enhanced Large Language ModelWen et al. (2024).

However, KGs are usually incomplete, and the incompleteness limits their application.
As an effective tool for completing missing facts, knowledge graph completion (KGC) has received much attention from researchers.
Typically, researchers transform KGC tasks into knowledge graph embeddings (KGEs).
KGE refers to learning representations of entities and relations in a low-dimensional space while preserving graph’s inherent structure and semantic properties.
In this representation space, a scoring function can be defined to measure the plausibility of each triplet, where valid triplets should receive higher scores than these invalid ones.

Quaternion contains one real part and three imaginary parts, which providing a more expressive space for learning embeddings of entities and relations.
Rotation in quaternion is often used to model the KGs.
For example, QuatEZhang et al. (2019)learns semantic information about entities by treating relations as rotations from head entities to tail entities.
TransERRLi et al. (2024)encodes the KG by rotating the head and tail entities with their corresponding unit quaternions.
These models use either semantic matching or distance scoring functions to measure the plausibility of the triplet, respectively.
However, it appears that semantic matching diminishes the separability of entities, while the distance scoring function weakens the semantics of entities from our visualization analysis in Figure1111For more information about queries, see Section6.4..
Specifically, as shown in Figure1, we can observe that QuatE using semantic matching as a scoring function overlaps in each queries.
The entities of TransERR using the distance scoring function are also indistinguishable from each query.

To address this issue, we propose aDistance-adaptive quaternion knowledge graph embedding withBidirectionalRotation model, named asDaBR.
Our model combines semantic matching with entity’s geometric distance to better measure the plausibility of triplets.
Specifically, in the quaternion space, we perform a right rotation on the head entity and a reverse rotation on the tail entity to learn the rich semantic features.
This process is called bidirectional rotation.
We conducted extensive experiments on multiple well-known benchmark datasets of knowledge graph completion task.
The experimental results and analyses demonstrated the effectiveness and robustness of our model.

Our contribution is summarized as follows:

We propose to perform a right rotation on the head entity and a reverse rotation on the tail entity to learn the rich semantic features.

We propose to learn the embedding distance between entities by incorporating distance adaptive translations.

We provide mathematical proofs to demonstrate that our model can handle rich logical relationships.

Extensive experiments show that our model provides consistent and significant improvements over previous models on the vast majority of metrics.

SECTION: 2Related Work

For KGE models, the design of the scoring function directly affects these models’ performance and effectiveness.
Based on the calculation methods of scoring functions in previous models, KGE scoring functions can be mainly categorized into semantic matching- and geometric distance-based.

Semantic matching.Semantic matching scoring functions capture the interactions between entities and relations through inner products on embedding vectors.
The hypothesis is that entities connected by relations are close to each other in the semantic space.
For example, QuatEZhang et al. (2019)obtains entity semantic information through the Hamiltonian rotation of the head entity on the relation in quaternion space.
DualECao et al. (2021)further enhances QuatE to model knowledge graphs in dual quaternion space.
QuatRENguyen et al. (2022)associates each relation with two relation-aware rotations, which are used to rotate the quaternion embeddings of the head and tail entities, respectively.

A common feature of these models is the computation of the inner product between the head entity and the tail entity after a relation transformation.
However, these models overlook the geometric distance properties between entities in the knowledge graph, leading to distorted embeddings of the learned entities.

Geometric distance.Geometric distance scoring functions assess the plausibility of triplets by calculating the distances between embedding vectors in the representation space.
The goal of this scoring function is keep the head/tail entity vector closer to the tail/head entity vector after being transformed through the relation vector.
For example, TransEBordes et al. (2013), considered the first model to employ a geometric distance scoring function, assumes that tripletsin knowledge graphs should satisfy the expression.
However, TransE struggles with more complex relation types, such as one to many (1-to-N), many to one (N-to-1) and many to many (N-to-N).

To address this limitation, several models using distance-based scoring functions have been proposed.
For example, Rotate3DGao et al. (2020)maps entities to an 3D space, defining the relation as a rotation from the head entity to the tail entity.
Trans4ENayyeri et al. (2021)performs rotations and translations in a quaternion space.
RotateCTDong et al. (2022)transforms entity coordinates and represents each relation as a rotation in complex space.
Rotate4DLe et al. (2023)employs two distinct rotational transformations to align the head embedding with the tail embedding.
DCNEDong et al. (2024)maps entities to dual complex number space, using rotations in 2D space via dual complex number multiplication to represent relations.
TransERRLi et al. (2024)encodes knowledge graphs by rotating the head and tail entities with their corresponding unit quaternions.

A common feature of these models is that the plausibility of the triplets is evaluated by calculating the distance between the head entity and the tail entity after transformation.
However, these models fail to consider information about entities within the semantic space, leading to performance degradation.

SECTION: 3Preliminaries

This section begins with a definition of the knowledge graph completion task, followed by a brief background on quaternion algebra.

SECTION: 3.1Knowledge Graph Completion

Knowledge graph completion is the task of predicting missing elements in a triplet.
This task can be broken down into three sub-tasks: predicting the head entity, predicting the relation, and predicting the tail entity.
Following previous research, our work focuses on predicting the headand tailentities.
It is because relation information is needed in the training process.

SECTION: 3.2Quaternion Algebra

The quaternion extends complex number system to four dimensions.
In-dimensional quaternion space, a quaternionconsists of one real component and three imaginary components.
It can be formalized as:, whereare real numbers andare imaginary units.
The imaginary part satisfies the Hamilton’s rulesHamilton (1844):.

Addition.Given two quaternionsand, quaternion addition is defined as:

Norm.The normalization of quaternionscan be defined by the following:

Inverse.The inverse of quaternionscan be defined by the following:

whereis the conjugate of.

Hamilton product.Given two quaternionsand.
The quaternion rotation of these two quaternions can be performed by the Hamilton product:

wheredenotes the element-wise product.

SECTION: 4Methodology

In this section, we describe our model in detail, which consists of two main parts:

Bidirectional rotation: Performing a right rotation on the head entity and a reverse rotation on the tail entity to learn the rich semantic features.

Distance-adaptation: Incorporating a distance adaptive translation to learn the geometric distance between entity embeddings.

SECTION: 4.1Symbol Description

A knowledge graphis a collection of triplet, whereandare the entity set and relation set.andrepresent the number of entities and relations, respectively.
Given a triplet, the embeddings of head entity, relationand tail entitycan be represented by quaternions:

SECTION: 4.2Part One: Bidirectional Rotation

In Figure2, we show the differences between our proposed bidirectional rotation and previous methods when modeling entity semantics.
Specifically, QuatE (Figure2(a)) performs a right rotation for head entity.
QuatRE (Figure2(b)) performs two times right rotation for head entity and a right rotation for tail entity.
Our model (Figure2(c)) performs a right rotation for head entity and a reverse rotation for tail entity.

We first normalize the relation quaternionto a unit quaternionto eliminate the scaling effect by dividing by its norm (Equation2):

Then, the head entityis right rotated using the relation, i.e., the entity vector and the relation vector do a Hamilton product (Equation4):

Similarly, the inverse of the relation unit quaternionis used to make a reverse rotation of the tail entity:

Sinceis a unit quaternion, we have:

whereis the conjugate of.

Therefore, the scoring functionfor the bidirectional rotation modeling entity semantics is defined by:

SECTION: 4.3Part Two: Distance-Adaptation

As shown in Figure2, the previous QuatE (Figure2(a)) and QuatRE (Figure2(b)) can only learn the semantic information of an entity but ignore the geometric distance attribute of an entity.
Our DaBR effectively addresses this limitation by adding a distance-adaptation (Figure2(c)).

Therefore, to model the geometric distance information, we initialize a distance-adaptive relation embedding.
Finally, the geometric distance part scoring functionis defined as:

whererepresents thenorm.
Despite its simplicity, we find that the proposed method is effective enough in providing distance information for our model.

SECTION: 4.4Scoring Function

After obtaining the scoring functions for modeling entity semantics and entity geometric distances, respectively.
We fuse these scoring functions into a new scoring function for model training:

whererepresents the semantic matching scoring function,represents the geometric distance scoring function, andis an adaptive parameter that learned by our model.

SECTION: 4.5Loss Function

FollowingTrouillon et al. (2016), we formulate the task as a classification problem, and the model parameters are learned by minimizing the following regularized logistic loss:

whereanddenote the embedding of all entities and relations.
Here we use thenorm with regularization ratesandto regularizeand, respectively.is sampled from the unobserved setusing uniform sampling.represents the corresponding label of the triplet.

SECTION: 4.6Discussion

As described inChami et al. (2020), there are complex logical relationships (such as symmetry, antisymmetry, inversion and composition relationships) in the knowledge graph.
In this part, we analyze the ability of our DaBR to infer these relationships.

Lemma 1DaBR can infer the symmetry relationship pattern. (See proof in AppendixA.1)

Lemma 2DaBR can infer the antisymmetry relationship pattern. (See proof in AppendixA.2)

Lemma 3DaBR can infer the inversion relationship pattern. (See proof in AppendixA.3)

Lemma 4DaBR can infer the composition relationship pattern. (See proof in AppendixA.4)

SECTION: 5Experiments

In this section, we first introduce the datasets, evaluation protocol, implementation details and baselines.
Subsequently, we evaluate our model on four benchmark datasets.

Datasets.
To verify the effectiveness and robustness our model, we conducted extensive experiments on four standard knowledge graph completion datasets including WN18RRDettmers et al. (2018), FB15k-237Toutanova and Chen (2015), WN18Bordes et al. (2013)and FB15kBordes et al. (2013).
The WN18 and FB15k datasets are known to suffer from a data leakage problem, leading to models being easily inferred and consequently performing well on metrics.
WN18RR and FB15k-237 were derived as subsets of WN18 and FB15k respectively.
These datasets are designed to address data leakage concerns and thereby present a more realistic prediction task.
The detailed statistics of the four standard datasets are shown in AppendixB.

Evaluation protocol.
Similar to previous workZhang et al. (2019); Li et al. (2024), we employed the filtered evaluation setup described in referenceBordes et al. (2013)to filter out real triplets during the evaluation process.
This was done to avoid flawed evaluations.
We used evaluation metrics encompassed Mean Rank (MR), Mean Reciprocity Rating (MRR) and Hits@n (n=1, 3 or 10).
Where a smaller value on the MR indicates a better model.
The final scoring model on the test set is derived from the model with the highest Hits@10 score on the validation set.

Implementation details.
We conduct all our experiments on a single NVIDIA GeForce RTX 4090 with 24GB of memory.
The ranges of the hyper-parameters for the grid search are set as follows: the embedding dimension () is selected from {300, 400, 500}; the learning rate () is chosen from {0.01, 0.02, 0.05, 0.1}; and the number of negative triplets sampled () per training triplet is selected from {5, 10}.
The regularization ratesandare adjusted within {0.01, 0.05, 0.1, 0.5}.
We create 100 batches of training samples for different datasets.
We optimize the loss function by utilizing AdagradDuchi et al. (2011).
All our hyper-parameters are provided in AppendixC.

It is worth noting that our modelsdo notemploy the training strategies of self-adversarial negative samplingSun et al. (2019)or N3 regularization with reciprocal learningLacroix et al. (2018).

Baselines.
To verify the effectiveness of our model, we compared DaBR with several powerful baseline models, including both well-known and recently proposed ones with outstanding results.
We divide these models according to the scoring function as follows:

1) Semantic Matching:TuckERBalazevic et al. (2019), QuatEZhang et al. (2019), DualECao et al. (2021), QuatRENguyen et al. (2022).

2) Geometric Distance:ATTHChami et al. (2020), Rotate3DGao et al. (2020), Trans4ENayyeri et al. (2021), RotateCTDong et al. (2022), Rotate4DLe et al. (2023), CompoundEGe et al. (2023), HAQELiang et al. (2024d), DCNEDong et al. (2024), FHRELiang et al. (2024b)and TransERRLi et al. (2024).

For a fair comparison, we report the optimal results for these baselines from the original papers.

SECTION: 5.1Main Results

The main results of our DaBR and the baselines for the WN18RR and FB15k-237 datasets are listed in Table1.
We categorize the baseline models into two main groups based on scoring functions, namely semantic matching and geometric distance scoring functions.
The models based onSemanticMatching are listed in the upper part of the table, while theGeometricDistance based methods are listed in the lower part of the table.
It is worth noting that our model’s scoring function is the unique scoring function that simultaneously measures bothSemantic andGeometric distances.

From Table1we can clearly see that our model achieves the best results on both datasets, except for the H@1 metric on the WN18RR dataset.
Specifically, compared to the best performing of the semantic matching model, QuatRE, our model drops from 1986 to 899 on the MR metric and absolutely improves 3.4%, 5.0%, 3.6% and 2.5% on the MRR, H@10, H@3 and H@1 metrics on the WN18RR dataset.
On the FB15k-237 dataset, our model decreases from 88 to 83 on the MR metrics, and absolutely improves on the MRR, H@10, H@3 and H@1 metrics by 1.6%, 1.5%, 1.4% and 1.8%.

Compared to the latest and best performance of the geometric distance model, TransERR, our model decreases from 1167 to 899 on the MR metric and achieves an absolute improvement of 1.8%, 2.8%, and 3.4% on the MRR, H@10 and H@3 metrics on the WN18RR dataset.
On the FB15k-237 dataset, our model decreases from 125 to 83 on the MR metrics, and absolutely improves on the MRR, H@10, H@3 and H@1 metrics by 3.6%, 3.0%, 3.5% and 3.7%, respectively.

The KGC results on WN18 and FB15k datasets, as shown in Table2.
The Table2illustrates our model superiority over any previous model on the FB15k dataset.
On the WN18 dataset, our model achieves the best results on all metrics, except for the H@1 metric which achieves second place.
In conclusion, our model not only achieves optimal results compared to semantic matching models, but also achieves competitive results compared to geometric distance models.

SECTION: 6Analysis

To demonstrate the superiority of our model, we have conducted in-depth analysis experiments from various aspects.
The obtained experimental results and analysis are as follows:

SECTION: 6.1Ablation Analysis

In this section, we aim to evaluate the efficacy of bidirectional rotation and distance-adaptation within our DaBR.
We have designed the following model variants:

Variant I: We remove the rotation of the tail entity and keep the rotation of the head entity.

Variant II: We removed the distance-adaptation.
The DaBR degenerates into a semantic matching model.

We show the results of the ablation experiments in Table3.
From the table, we can obtain the following conclusions:
1) The rotation of the tail entity and distance-adaptation are important parts of our model.
2) When our model removed the tail rotation, the model (i.e.,Variant I) still achieved the best results compared to the models in the Table1and Table2.
We attribute this to the fact that our model can measure both the semantics of entities and the embedding distance of entities.
3) When our model removed distance-adaptation, the model (i.e.,Variant II) performance decreased dramatically on all datasets.
It is worth noting that our model still achieves optimal results on most datasets compared to the semantic matching model on most datasets.

SECTION: 6.2Parameter Comparison Analysis

To analyze the number of parameters compared to other models, we compared our DaBR with the best semantic matching model (QuatRE) and the best geometric distance model (TransERR).
Given the same embedding dimension, QuatRE and TransERR haveparameters, while our DaBR hasparameters, whereandare the entity set and relation set.
Compared to QuatRE and TransERR, our model achieves better results with fewer parameters.

SECTION: 6.3Relationship Type Analysis

To explore the robustness of our model in the face of different relation types (one-to-many (1-to-N), many-to-one (N-to-1) and many-to-many (N-to-N)), we compared DaBR with QuatE and QuatRE in WN18R dataset.
For the results of the QuatE and QuatRE, we reproduce these models following the hyper-parameter settings of their paper.

In accordance with the calculation rules set out inBordes et al. (2013), the test set of WN18RR has been divided into three categories: 1-to-N, N-to-1 and N-to-N.
The division results are shown in AppendixD, whereandrepresent the average degree of head and tail entities, respectively.

We show the MRR scores for the QuatE, QuatRE, and DaBR models for 0 to 5200 training epochs in Figure3.
This demonstrates the effectiveness of our model in modelling different types of relationships.
In particular, the model is superior in dealing with 1-to-N relationship.
“1-to-N” means that a head entity can form a fact triplet with multiple tail entities.
We attribute this superior enhancement to the distance-adaptive embedding of our model.

SECTION: 6.4Visualization Analysis

In this section, to explore the embedding results of our model after distance adaptive embedding, we visualize the the tail entity embeddings using t-SNEvan der Maaten and Hinton (2008).
Suppose (,) is a query whereandare the head entity and the relation, respectively.
If (,,) is valid, the entityis the answer to query (,).
We selected 9 queries in FB15k-237 dataset, each of which has 50 answers.
For more details about the 9 queries, please refer to the AppendixE.

We then use t-SNE to visualize the semantic matching models QuatE and QuatRE, the geometric distance model TransERR, and our combined semantic and geometric distance DaBR to generate the answer embeddings for epoch 1 and epoch 100, respectively.
Figure4shows the visualization results222Refer to AppendixFfor more visualization results..
Each entity is represented by a 2D point and points in the same color represent tail entities with the same (,) context (i.e. query).

Specifically, our model (Figure4(g)) in the first epoch have demonstrated better embedding compared to QuatE, QuatRE and TransERR.
At epoch 100, our model (Figure4(h)) show clear inter-cluster separability, with entities within each cluster (intra-cluster) being well-separated from one another.

However, the semantic matching model QuatE (Figure4(b)) and QuatRE (Figure4(d)) heavily overlap entities within clusters despite inter-cluster separability.
The geometric distance model TransERR (Figure4(f)) clusters are indistinguishable from each other and entities within the clusters (intra-clusters) are distinguishable.

Table4summarizes our analysis above, which we attribute to the fact that our model combines semantic matching with entity geometric distance to better measure the plausibility of triplets.

SECTION: 6.5Visualization Ablation Analysis

In Figure5, we visualize that our model removes the distance adaptive embedding in the first epoch.
We can find that the visualization without the distance adaptive embedding (Figure5(b)) is worse than the with one (Figure5(a)).
By visualizing the ablation experiments, we can further illustrate the advantage of distance adaptive embedding.

SECTION: 7Conclusion

We note that existing quaternion models based on semantic matching diminishes the separability of entities, while the distance scoring function weakens the semantics of entities.
To address this issue, we propose a novel quaternion knowledge graph embedding model.
By combining semantic matching with entity geometric distance, our model provides a robust and comprehensive framework for knowledge graph embedding.
We provide mathematical proofs to demonstrate our model can handle complex logical relationships.
Visualization results show that our model can learn the geometric distance property between entities to achieve both inter-cluster and intra-cluster separability.

SECTION: Limitations

The H@1 metric performance of our model on the WN18 and WN18RR datasets is not optimal.
In addition, like most knowledge graph embedding models, our model is unable to predict new entities that do not exist in the training data.

SECTION: Acknowledgements

This work is supported by National Natural Science Foundation of China (No.62066033);
Inner Mongolia Natural Science Foundation (Nos.2024MS06013, 2022JQ05);
Inner Mongolia Autonomous Region Science and Technology Programme Project (Nos.2023YFSW0001, 2022YFDZ0059, 2021GG0158);
We also thank all anonymous reviewers for their insightful comments.

SECTION: References

SECTION: Appendix

SECTION: Appendix AProof

Given, whereis a unit quaternion after normalization operation.
We can makeand then our scoring function can be simplified as follows:

whereis the Hamilton product,denotes the element-wise product, and “” is the inner product.

SECTION: A.1Proof of Symmetry pattern

In order to prove the symmetry pattern, we need to prove the following equality:

The symmetry property of DaBR can be proved by setting the imaginary parts ofto zero.

SECTION: A.2Proof of Antisymmetry pattern

In order to prove the antisymmetry pattern, we need to prove the following inequality when imaginary components are nonzero:

We expand the right term:

We can easily see that those two terms are not equal as the signs for some terms are not the same.

SECTION: A.3Proof of Inversion pattern

To prove the inversion pattern, we need to prove that:

We expand the right term:

We can easily check the equality of these two terms.
Sinceis a unit quaternion, we have.

SECTION: A.4Proof of Composition pattern

For composition relationships, we can get that:

SECTION: Appendix BDataset statistics

The detailed statistics of the four standard datasets are shown in Table6.

SECTION: Appendix COptimal hyper-parameters

Table7shows the optimal hyperparameter settings for our model on the four benchmark datasets. The optimal parameters come from the highest scores of our model on the validation dataset.

SECTION: Appendix DClassification rules

The classification rules and classification results for WN18RR dataset in the Table8.

SECTION: Appendix EThe queries in t-SNE visualization

In Table5, we list the nine queries used in the t-SNE visualization (Section6.4in the main text).
Note that a query is represented as, wheredenotes the head entity anddenotes the relation.

SECTION: Appendix FMore visualization results

Figure6shows more visualization results.