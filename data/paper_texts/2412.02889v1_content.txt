SECTION: Deep-Learning Based Docking Methods: Fair Comparisons to Conventional Docking Workflows

The diffusion learning method, DiffDock, for docking small-molecule ligands into protein binding sites was recently introduced. Results included comparisons to a number of more conventional docking approaches, with DiffDock showing nominally much better performance. Here, we employ a fully automatic workflow using the Surflex-Dock methods to generate a fair baseline for conventional docking approaches. Results were generated for the common and expected situation where a binding site location is known and also for the condition where the entire protein was the nominal target of docking. For the known binding site condition, Surflex-Dock success rates at 2.0Å RMSD far exceeded those for DiffDock (Top-1/Top-5 success rates, respectively, were 68/81% compared with 45/51%). Glide performed with similar success rates (67/73%) to Surflex-Dock for the known binding site condition, and results for AutoDock Vina and Gnina followed this pattern. For the unknown binding site condition, using an automated method to identify multiple binding pockets, Surflex-Dock success rates again exceeded those of DiffDock, but by a somewhat lesser margin. DiffDock made use of roughly 17,000 co-crystal structures for learning (98% of PDBBind version 2020, pre-2019 structures) for a training set in order to predict on 363 test cases (2% of PDBBind 2020) from 2019 forward. The Surflex-Dock approach madeno useof prior information from pre-2019 structures, either for binding site identification or for knowledge to guide the docking and pose-ranking process. DiffDock’s performance was inextricably linked with the presence of near-neighbor cases of close to identical protein-ligand complexes in the training set for over half of the test set cases. DiffDock exhibited a roughly 40 percentage point difference on near-neighbor cases (two-thirds of all test cases) compared with cases for which a near-neighbor training case was not identified. DiffDock has apparently encoded a type of table-lookup during its learning process, rendering any meaningful application scenario beyond its reach. Further, it does not perform even close to competitively with a competently run modern docking workflow.

KeywordsDiffDockSurflex-DockPSIMGlideAutoDock Vinamolecular dockingbinding site identification

SECTION: 1Introduction

Deep learning approaches have recently generated significant interest in the field of computer-aided drug design (CADD), and this phenomenon is exemplified by the excitement produced by the introduction of the DiffDock method[1]. This short report is not intended to review the field or to differentiate substantive and meaningful deep-learning contributions to CADD from lesser contributions. Rather, the focus is to make the general CADD community aware of the proper context in which to interpret DiffDock’s recently reported results. Our work should be considered alongside another recent contribution, a paper from the Deane group, which offers additional context for understanding the results of AI-based docking methods like DiffDock, highlighting, in particular, extremely strained conformations in predicted docked poses[2].

Briefly, the performance of DiffDock was demonstrated by training on 98% of the co-crystal structure data from PDBBind version 2020 (pre-2019 structures) and testing on the remaining 2% (2019 forward). In what follows, we will lay out two main points. First, a mature and fully automated docking workflow far outperforms DiffDock on its own test setwithoutusing any prior knowledge of other co-crystal structures. This is true whether the ligand’s binding site location is provided to the docking system or not (so-called “blind docking”). Second, near-neighbor training cases, where both the protein binding siteandthe cognate ligand are nearly identical to those from a test case, exist for the majority of DiffDock’s PDBBind testing set. DiffDock performance on these near-neighbor test cases wasmuchbetter than on non-near-neighbor cases.

SECTION: 2Results

Details of the specific computational procedures will be presented in the Data and Methods Section (with additional details in the Appendices), below. The goals of the work presented here were two-fold: 1) to establish a reasonable baseline for the performance of conventional docking workflows on the DiffDock test set; and 2) to try to understand the underlying performance driver for DiffDock’s apparent success.

SECTION: 2.1DiffDock Performance: As Reported compared with a “Clean” Test Subset

The PDBBind 2020 data set was used to train and test DiffDock, making use of roughly 17,000 protein-ligand complexes for training and 363 complexes for testing (the “Full Test Set”). Here, we made use of a fully automated pipeline to process the PDB complexes, following previously reported protocols[3]. The processed PDB complexes consisted of protein and ligand files, with bond orders assigned and with protonation as expected in physiological pH. Ligand structures were subjected to several quality tests, including cross-checking bond-order assignments against curated SMILES representations of corresponding the PDB HET codes. Those cases where quality tests were passed and where the resulting ligand structures topologically agreed with those curated for the DiffDock benchmarking were kept as a “clean” set for testing conventional docking workflows (the “Clean Test Set”). There were 290 such cases (80% of the total 363 original test cases).

Figure1shows the performance of DiffDock on its Full and Clean Test Sets. The cyan and violet curves correspond to cumulative histograms of the RMS deviations for the the top-scoring pose for the full and clean test sets, respectively. The yellow and green violet curves correspond to cumulative histograms of the RMS deviations for the the best of the top five poses for the full and clean test sets, respectively. There was a relatively marginal improvement on the Clean 290 complex subset, amounting to an increase of roughly five percentage points in success rates at the 2.0Å RMSD threshold, whichfavorsthe DiffDock method compared with the original published Full Test Set. This was presumably due to the automatic quality checks that thegrindpdbmethodology employs (see Data and Methods) and also because two independent curation approaches agreed on the bound ligand structures. In what follows, comparative results will be presented on the Clean Test Set.

SECTION: 2.2DiffDock Performance: Comparison to Conventional Docking Workflows

The original report of DiffDock made comparisons to a number of conventional docking methods, but the methods were employed in an unconventional manner. Rather than providing an explicitly scoped binding site, a so-called “blind” docking procedure was used where docking was performed against entire protein structures. Consequently, whereas mature docking methods in cognate ligand re-docking typically perform with roughly 60–80% success at the 2.0Å RMSD threshold[4]for top-scoring poses, the DiffDock reported performance for GNINA, SMINA, and Glide ranged from 19–23%[1]. In what follows, we will present comparative results for Surflex-Dock[5,6,7,3], Glide[8,9], AutoDock Vina[10,11], and Gnina[12]using conventional cognate-ligand re-docking with a defined binding site. Results for Surflex-Dock, which has a well-studied pocket finding algorithm[13,14], will also be presented in the unknown binding-site condition.

Figure2shows the comparison between pose prediction accuracy for two different testing conditions. At left, Surflex-Dock was run using a conventional docking protocol where the location of the binding site was known. Cumulative histograms of RMSD are shown for DiffDock (violet and green curves) and for Surflex-Dock (cyan and yellow curves). Focusing on the 2Å RMSD threshold, we see a roughly 25–30 percentage point advantage for Surflex-Dock, and an even larger gap at the 1.0Å threshold. The performance difference is both practically and statistically highly significant (pby paired t-test for both Top-1 and Top-5 pose prediction performance).

At right, Surflex-Dock was run using a protocol that lacked a defined binding site, requiring the use of an automated procedure to identify possible binding sites on each protein, docking to up to ten of the highest ranked candidates, and combining results based purely on docking scores. At the 1.0Å threshold, in the unknown binding site condition (so-called “blind docking”), results favored Surflex-Dock by 15–20 percentage points. At 2.0Å, top-scoring pose performance was roughly equivalent but among the top five poses, Surflex-Dock had a roughly 10 percentage point advantage. Statistical comparison of performance on the full 290 complexes was complicated by the presence of extreme outliers for each method, where an incorrect binding site scored better than the correct one. For the 160 cases where both methods had RMSD in the top five condition of 4.0Åor better (the subset seen in the plot of Figure2), Surflex-Dock was statistically superior to DiffDock by paired t-test (pfor the top scoring poses and pfor the top five).

It is important to contextualize the results in the unknown binding site condition. The DiffDock procedure relied on roughly 98% of the PDBBind 2020 set for training, which explicitly identified, in many cases, the known binding site for proteins either identical to or similar to those in the test set. Given, for example, a number of estrogen-receptor protein-ligand complexes as prior knowledge, the binding site is well-defined for a test complex of estrogen-receptor for a new ligand. The procedure used for Surflex-Dock in the “blind docking” condition usedno prior knowledgeof any kind.

In the original DiffDock report, the authors made use of a procedure to run the Glide docking method on the entire protein, rather than making use of a defined binding site, which is the use case for which the method has been developed and optimized. Consequently, the reported results (roughly 22% success at 2.0Å RMSD) were at variance with expectations for cognate-ligand re-docking (closer to 60–80% success[4]).

Here, we report Glide results for 272 complexes whose proteins were correctly processed using an automatic preparation procedure (see Data and Methods). Of these, 260/272 yielded successful dockings, with the remaining 12 being assigned values of 20.0Å for Top-1 and Top-5 pose accuracy. Figure3shows the comparison between pose prediction accuracy for the known binding-site condition. Cumulative histograms of RMSD are shown for DiffDock (violet and green curves) and for Glide (cyan and yellow curves). Focusing on the 2Å RMSD threshold, we see a roughly 25 percentage point advantage for Glide, and an even larger gap at the 1.0Å threshold. The performance difference is both practically and statistically highly significant (pby paired t-test for Top-1 pose prediction and pfor Top-5).

The DiffDock report presented results for Smina and Gnina[15,12], both of which were derived from AutoDock Vina[10,11], a widely used open-source method. Gnina uses a form of deep learning to improve Vina performance, both for pose prediction and for virtual screening applications[12]. Here, we report ligand re-docking results for both Vina and Gnina using the known binding-site condition. Of the full Clean Test Set of 290 complexes, 285 were processed correctly using the standard AutoDock procedures (see Data and Methods) to yield usable protein structures. For both Vina and Gnina, all 285 complexes yielded docking results, with no failures.

Figure4shows the comparison between pose prediction accuracy for the known binding-site condition. Cumulative histograms of RMSD are shown for DiffDock (violet and green curves) and for Vina (left) and Gnina (right), with cyan and yellow curves to depict Top-1 and Top-5 pose prediction performance, respectively. Focusing on the 2Å RMSD threshold, the results for both Vina and Gnina exceeded those of DiffDock by roughly 20–25 percentage points. As previously reported[12], the Gnina method exhibited improved performance for top-ranked pose (right, cyan curve) over both DiffDock (purple curve) and Vina (cyan curve, left-hand plot). Note that neither method approached the success levels of either Glide or Surflex-Dock at the 1.0Å RMSD threshold. As with the previous comparisons, the performance differences, particularly between DiffDock and Gnina, were both practically and statistically highly significant. For Vina Top-1 and Top-5 pose prediction, respectively, p values by paired t-test wereand pfor Top-5). For Gnina, the p values were less thanin both cases.

Note, however, that results for Gnina are difficult to interpret, due to its reliance on extensive training data for its scoring method, which implements a convolutional neural-network that was trained on hundreds of protein-ligand complexes. Rather than this training resulting in a scoring function, as is used by Surflex-Dock, Glide, and Vina, this approach essentially learns characteristics of “native-like” predicted ligand poses from explicit training data. It is possible that the internal representation induced by the Gnina scoring function has, to some extent, memorized binding motifs that are directly represented in the DiffDock test set. This type of effect will be explored directly, with respect to DiffDock performance in what follows.

SECTION: 2.3DiffDock Performance: Effects of Near-Neighbor Training Cases

Recall that DiffDock was trained using a temporal 98/2% split of data, with roughly 17,000 PDB complexes serving as training and in the Clean Set, just under 300 structures for testing. Given the power of deep learning methods to build complex internal representations, it is possible that DiffDock essentially “memorized” many training structures that were helpful for some subset of complexes used for testing.

Figure5shows two test cases on which DiffDock “predicted” well, with bound inhibitors of HIV protease (top) and BACE1 (bottom). At left, we show the test complexes with their cognate ligands, whose poses are to be predicted. At right, we see an alignment oftrainingdata cases that had extremely high binding pocket similarityand2D ligand similarity to the ligands to be predicted. For HIV Protease, the number of near-neighbor cases based on binding pocket similarity was 159, with a subset differing by only a few atoms from the test ligand of 6OXQ. For BACE1, the number of near-neighbor cases based on binding pocket similarity was 37, again with a subset differing by only a few atoms from the test ligand of 6JSN.

Figure6shows two additional test cases on which DiffDock “predicted” well, with bound ligands of BRD2 (top) and ER-(bottom). At left, we show the test complexes with their cognate ligands, whose poses are to be predicted. At right, we see an alignment oftrainingdata cases that had extremely high binding pocket similarityand2D ligand similarity to the ligands to be predicted. For BRD2, the number of near-neighbor cases based on binding pocket similarity was 29, with a subset differing by only a few atoms from the test ligand of 6MOA. For ER-, the number of near-neighbor cases based on binding pocket similarity was 7, again with a subset differing by only a few atoms from the test ligand of 6A6K.

Clearly, having a database of examples that contain nearly identical binding sites and ligands solvesboththe “prediction” problem of where a protein’s binding site isandalmost exactly how the ligand binds. So how large was the effect of near-neighbor training set cases on DiffDock’s reported performance?

We performed a systematic analysis to identify those Clean Test Set cases that had near neighbors within the DiffDock training set, defined as having both high topological similarity between the test and training ligandsandhaving high protein binding site similarity (see Appendices for additional details on the computational procedures). Overall, 191/290 test cases (roughly two-thirds) had a near-neighbor training case and, of these, 24 were termed extreme near-neighbors based on very high topological ligand similarity. Just 99/290 (roughly one-third) of test cases had no identified near-neighbor training cases using this non-exhaustive procedure.

Figure7shows the performance separation of different sets of cases, grouped by their relative challenge in terms of the existence of near-neighbor training cases. Overall, for the 191/290 cases that fell into the near-neighbor set (cyan and orange curves), success rates were 57/65% for Top-1/Top-5, respectively. Results for the 99/290 more challenging cases (violet and green) showed success rates of roughly 21/28% for Top-1/Top-5, respectively. The set of 24 extreme near neighbors (yellow and blue curves), by contrast, exhibited over 90% success for Top-1 and Top-5 pose predictions.

The original DiffDock report’s success rates for the Clean Test Set of 290 complexes were 45/51% for Top-1/Top-5, respectively. Clearly, the apparent level of success was greatly influenced by the presence of a subset of nearly two-thirds of cases where simple table-lookup could produce a good answer, and where DiffDock performed nominally well (57/65%). Note that even in the subset of near-neighbor cases (cyan and orange curves in Figure7), DiffDock’s performance was inferior to competently run, fully automatic docking from the mature commercial methods Surflex-Dock and Glide, whose success rates were 67–68/73–81% (Top-1/Top-5) on the complete Clean Test Set. AutoDock Vina produced performance of roughly 47/73%, clearly better with respect to Top-5 pose prediction performance. Gnina, with the caveat of potentially the same type of train/test set contamination suffered by DiffDock, performed substantially better than DiffDock on the near-neighbor set (58/78%).

On the roughly one-third of cases where automated procedures did not find near neighbors, DiffDock’s performance wasless than halfas good as on the two-thirds of near-neighbor cases. DiffDock’s success or failure was dichotomized by the presence or absence of near-neighbors in the training set: essentially a form of table-lookup.

SECTION: 3Data and Methods

All data was derived from the PDBBind version 2020 database, which also served as the primary source for DiffDock validation using experimentally determined crystal structures. All computational procedures employing the BioPharmics Platform software made use of version 5.173 unless otherwise noted (BioPharmics Division, Optibrium Limited, Cambridge, CB25 9GL, UK). Modules used included the Tools, Docking, and Similarity modules[16,17,5,18,3,13,19]. This included procedures for automatic PDB file processing into proteins and ligands, randomization of ligand conformations, docking, calculation of automorph-corrected RMSD, and ligand/protein similarity calculations to identify near-neighbor training set mates to the Clean Test Set. For Glide docking[8,9], the Schrödinger 2022-3 Suite was employed (Schrödinger LLC, New York, NY, 2023). For docking with Vina[10,11], version 1.1.2 was used, and for Gnina[12], version 1.1 (master:e4cb300+, Built 12-18-2023).

Please refer to the Appendices for additional details.

SECTION: 4Conclusions

It is possible that the DiffDock training procedure has learned an interesting encoding of a large set of structures of protein-ligand complexes. However, what it appears to be doing cannot be considered to be either “docking” or “identification” of ligand binding sites. The reported results are overwhelmingly contaminated with near neighbors to test cases within the extensive training set.

Further, results reported for mature and widely used docking methods presented an extremely misleading baseline of comparison. Surflex-Dock, Glide, Vina, and Gninaallperformed much better than DiffDock on cognate ligand re-docking in the known binding-site condition. Surflex-Dock, which has a mature and automatic method to identify binding sites, also performed much better than DiffDock in the unknown binding-site condition.

The primary reported results for DiffDock were artifactual, and the comparative results for other methods were incorrectly done. We do not mean to suggest that the study’s authors were mendacious in any way. The intention of our report is to be constructive in offering an instructive and carefully done analysis.

There can be value gained from different disciplines applying powerful general methods to new domains. With respect to machine-learning and computer-aided drug design, we would like to offer some observations to consider prior to making strong claims of superiority over pre-existing methods:

The CADD field has a long history, and the significant and challenging problems have changed over time. Cognate ligand re-docking has not been an important problem for well over a decade. Predicting bound poses of ligands withnovel structurescompared to prior known compounds, and into (obviously)non-cognate protein binding sites, is a challenging and important problem. It is important for newcomers to CADD to understand what has been done before and the problems that will make a true difference to the field if solved. It is easy (and tempting) to unintentionally develop artificial benchmarks that do not reflect the ultimate application of a method.

Small molecules are generally not experiments of nature, but are typically designed by people who have biases both with respect to which targets are thought to be interesting and, more importantly, with respect to prior known ligands. So, very often, a compound made and co-crystallized with the protein for which it was designed to bind will have extremely similar prior analogs. Temporal train/test splitting, as was done in the DiffDock study, is the correct idea, but the split cannot be 98/2%, else simple memorization of the training set can dominate results. A more reasonable temporal split for non-cognate docking is 25/75%[3].

Generally speaking, CADD methods are complex, particularly for docking, where aspects of protein preparation and binding site definition exist in addition to the challenges in appropriately representing and manipulating small molecules. Running an unfamilar method according to reasonable practices can be tricky and needs to be done respecting the designed application scenario. When results are obtained that are at large variance to prior published work, as in the DiffDock report with respect to Glide’s performance on cognate ligand re-docking, care should be taken to understand what may have gone wrong.

CADD is not like speech recognition or optical character recognition, where correctly predicting new examples that are very close to prior known data, and which are drawn from the same population as the known data, is the main use-case scenario. The goal in CADD is to make compounds that havedifferentproperties, often with novel scaffolds, than prior known compounds for a target or to design compounds to modulate the activity of a new target. There are areas where predictions on subtle changes to a parent compound are important, for example in affinity prediction, metabolism, toxicity, etc… Pose prediction for minor variants on a parent compound is generally not difficult or challenging, except in cases where a small structural change leads to a large difference in bound pose. But that is exactly the case where ML approaches like DiffDockwill notwork.

The most interesting and challenging problems in CADD arise when data are sparse, not when many thousands of relevant data points exist. Methods in ML that rely on large data sets have interesting, successful, and impactful applications (e.g. learned force-fields[20]), but care must be taken to identify problem areas where the data requirements match the application scenario.

Publication of studies such as the DiffDock report[1]are not cost-free to the CADD field. Magical sounding claims generate interest and take time for groups to investigate and debunk. Many groups must independently test and understand the validity of such claims. This is because most groups, certainly those focused primarily on developing new drugs, do not have the time to publish extensive rebuttals such as this. Therefore their effort in validation/debunking is replicated many fold. The waste of time and effort is substantial, and the process of drug discovery is difficult enough without additional unnecessary challenges.

One cannot make a blanket recommendation to simply ignore all reports from CADD newcomers that make magic claims, because there may be great value in rare cases. However, we can make the strong recommendation that newcomers to CADD heed the foregoing list of five observations.

SECTION: References

SECTION: Appendix: Surflex-Dock Protein Preparation and Docking Scripts

All results reported for Surflex-Dock was done using the BioPharmics Surflex Platform software version 5.173 (BioPharmics Division, Optibrium Limited, Cambridge, CB25 9GL, UK). Given a PDB code, the following script (RunGrind) fetches the protein structure from the RCSB PDB, separates protein, ligands, and water molecules, protonates and selects protomer and tautomer states for the protein and ligands:

Following the raw processing of a particular PDB code from within the PDBBind set, identification of the ligand to be docked along with cross-verification of its structure was done as follows (RunPrep):

As described above, this resulted in 290 clean complexes from the full set of DiffDock’s 363 complex test set. The same procedure resulted in 15,268 clean cases from the DiffDock overall set of roughly 17,000 protein-ligand complexes.

Cognate ligand re-docking was run in an automated fashion, as follows (RunDock):

Docking to proteins without an identified binding site was done as follows (RunFindDock):

SECTION: Appendix: Finding Near-Neighbor Training Cases

The following script was run within on Clean Test Set case, looking for any training set ligands that we highly similar to the cognate test ligand. Those training cases with cognate ligands within the top 1% of Gsim values or whose Gsim value was 0.3 or greater were then aligned to the test case binding site using the PSIM method. All cases where the PSIM score wasand where the training ligand was non-identical to the test ligand were considered “easy” cases (191 total). Of those, when the Gsim value was, these were termed extreme near neighbors (24 of the 191 near-neighbor easy cases).

SECTION: Appendix: Vina Docking Scripts

Docking with Vina version 1.1.2 followed the procedure below (run within each of the Clean Test Set directories):

SECTION: Appendix: Gnina Docking Scripts

Docking with Gnina version 1.1 (master:e4cb300+, Built 12-18-2023) followed the procedure below (run within each of the Clean Test Set directories) after the Vina docking procedure was run to produce the receptor files:

SECTION: Appendix: Glide Protein Preparation and Docking Scripts

All results reported for Glide were generated using the following procedure:

Prepare the proteins:01_protein_prep.py(note: original PDB file copied toprotein.pdb)

Remove the ligands from the prepared proteins:$SCHRODINGER/run 02_extract_ligand_from_mae_protein.py

Build the Glide grids:03_build_glide_grid.py

Dock the molecules:04_run_glide.py

Analyze the data:05_evaluate_docking.ipynb

01_protein_prep.py

02_extract_ligand_from_mae_protein.py

03_build_glide_grid.py

04_run_glide.py

For the Jupyter Notebook05_evaluate_docking.ipynb, please refer to the Data Archive underGlide_Scripts. Alternatively, the following command can be run following completion of the Glide docking script (using v5.190 or higher of the BioPharmics Platform docking module binary):

sf-dock.exe rms_list glide_dock_lib_fix.sdf gold-lig.sdf 5 rmsglide

The resultingrmsglide-sumfile will contain automorph-corrected RMSD values for the top-1 and top-5 poses.

SECTION: Appendix: Data Archive

The data archive can be found at https://www.jainlab.org/downloads/ (DiffDock Archive). The following is the (excerpted) README that describes the data and file structure of the archive: