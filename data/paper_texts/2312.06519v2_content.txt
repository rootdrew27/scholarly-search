SECTION: A GAN Approach for Node Embedding in Heterogeneous Graphs Using Subgraph Sampling

Graph neural networks (GNNs) face significant challenges with class imbalance, leading to biased inference results. To address this issue in heterogeneous graphs, we propose a novel framework that combines Graph Neural Network (GNN) and Generative Adversarial Network (GAN) to enhance classification for underrepresented node classes. The framework incorporates an advanced edge generation and selection module, enabling the simultaneous creation of synthetic nodes and edges through adversarial learning. Unlike previous methods, which predominantly focus on homogeneous graphs due to the difficulty of representing heterogeneous graph structures in matrix form, this approach is specifically designed for heterogeneous data. Existing solutions often rely on pre-trained models to incorporate synthetic nodes, which can lead to optimization inconsistencies and mismatches in data representation. Our framework avoids these pitfalls by generating data that aligns closely with the inherent graph topology and attributes, ensuring a more cohesive integration. Evaluations on multiple real-world datasets demonstrate the method’s superiority over baseline models, particularly in tasks focused on identifying minority node classes, with notable improvements in performance metrics such as F-score and AUC-PRC score. These findings highlight the potential of this approach for addressing key challenges in the field.

SECTION: 1.Introduction

Graph neural networks (GNNs)(Scarselli et al.,2009)are a class of neural networks specifically designed to process graph data. GNNs show remarkable adaptability in managing highly interconnected data of varying scales, making them suitable for a wide range of domains and problems. Graphs are typically categorized as homogeneous or heterogeneous, based on the diversity of their nodes and edges. Extensive research has been conducted on both, with notable examples including the Graph Convolutional Network (GCN)(Kipf and Welling,2016)and the Graph Attention Network (GAT)(Veličković et al.,2018)for homogeneous graphs. Conversely, models like the Heterogeneous Graph Neural Network (HetGNN)(Zhang et al.,2019)and the Heterogeneous Graph Transformer (HGT)(Hu et al.,2020)are specifically developed to address the complexities inherent in heterogeneous graphs(Wang et al.,2021b; Fu et al.,2020).

A prominent research area in Graph Neural Networks (GNNs) focuses on addressing the node class imbalance problem(Shi et al.,2021; Wang et al.,2021a; Zhao et al.,2021; Jing et al.,2023; Wang et al.,2023; Fu et al.,2023), which refers to the situation where the nodes belonging to a particular class constitutes a significant proportion of the total nodes within the graph data. Commonly, oversampling methods(Chawla et al.,2002; Han et al.,2005; Bej et al.,2020)are employed, using specific data sampling techniques to achieve a balanced dataset. These methods can be broadly categorized into (1) Non-generative oversampling methods, including notable examples like GraphSMOTE(Zhao et al.,2021), manipulate existing data points through transformations or interpolations, and (2) generative oversampling methods, such as ImGAGN(Qu et al.,2021), create new data points by sampling from distributions learned from existing data.

Research efforts have explored integrating node attributes and topological information into generative models for graphs(Wang et al.,2017). However, existing methods are often constrained by architectures designed for attribute-less or homogeneous graph data in matrix forms(Li et al.,2023). Some strategies utilize GNN-based encoders to generate node embeddings by leveraging attributes and topology(Zhao et al.,2021), but these methods struggle to create synthetic nodes that seamlessly connect with the original graph dataset, frequently requiring additional edge predictors for establishing necessary connections. On the other hand, some approaches handle data augmentation tasks by using non-graph classifiers like MLPs to bypass the need for edge generation. However, this approach sacrifices crucial edge relations within the dataset and can lead to suboptimal task performance due to the loss of topological context. Additionally, some techniques combine generated and original data using improvised methods, typically involving fixed pre-trained models and will lead to inconsistencies and suboptimal outcomes when addressing new tasks. One strategy simplifies the connection problem by linking artificial nodes to all nodes in the original graph(Qu et al.,2021), allowing the GNN to learn edge weights and identify significant connections during training. However, this method increases the size of augmented graphs, resulting in a sharp rise in computational costs. Another method involves training an edge predictor to determine which nodes in the original graph should connect to synthetic nodes(Zhao et al.,2021). The challenge here is the introduction of a hyperparameter known as the edge threshold, which dictates the connections between generated and other nodes, thereby making the fine-tuning process more time-consuming. These challenges highlight the necessity for a novel, integrated approach that directly incorporates both node and edge data into the generative model’s training process.

We observe that all existing approach that incorporating synthetic nodes into the original graph to mitigate class imbalance actually face a fundamental yet critical challenge: edge imbalance. This issue is particularly pronounced in sparse graphs, where unconnected node pairs (negative edges) vastly outnumber connected pairs (positive edges). Such disparities exacerbate class imbalance during training, often leading to a significant overrepresentation of negative edges, which complicates the effective training of edge predictors for synthetic nodes and usually lead to low-quality synthetic edge construction.

Motivated by these observations, we introduce theFlashGANframework (Framework ofLocalized NodeAugmentation viaSemi-supervised Learning inHeterogeneous Graphs withGenerativeAdversarialNetworks). FlashGAN is a semi-supervised data augmentation framework that learns to generate synthetic data by leveraging both labeled and unlabeled data within graphs. Additionally, FlashGAN features a novel synthetic edge filter designed for simultaneous adversarial learning of both node and edge information. It also employs a subgraph sampling mechanism(Hamilton et al.,2018; Chiang et al.,2019; Zeng et al.,2020), enabling it to train on graph datasets of any scale. At the same time, the use of subgraph sampling mitigates the limitations of traditional edge predictors, effectively addressing the edge imbalance problem. FlashGAN offers several significant advantages (1) It integrates node and edge data directly into the generator’s training process, enhancing the GAN model’s ability to capture both structural and attribute-based features. (2) By leveraging local augmentation, FlashGAN tackles edge imbalance by embedding synthetic nodes within subgraphs. Our experiments demonstrate that models trained with edge sampling within local subgraphs outperform those trained with edge sampling across the entire graph in downstream node classification tasks. (3) The synthetic edge filter simplifies the process of optimizing edge threshold hyperparameters during the data augmentation phase, ensuring the quality of edges generated alongside nodes. In summary, our main contributions are as follows:

We propose a general framework for performing node augmentation with edge awareness in a heterogeneous graph, enabling simultaneous training with both node and edge information.

To the best of our knowledge, we are the first to design a local node augmentation and subgraph sampling method specifically to address the edge imbalance issue. This approach ensures that both node and edge imbalances are effectively managed throughout the augmentation process.

We enhance the GAN framework for graph augmentation tasks by incorporating a synthetic edge filter and modifying the loss function, which streamlines hyperparameter usage and ensures the quality of edges generated alongside nodes.

SECTION: 2.Related Works

The class imbalance problem in structured data has been extensively studied, with various approaches such as weight-adjusting methods(Huang et al.,2016; Lin et al.,2017; Cui et al.,2019)and up-sampling strategies developed based on SMOTE(Chawla et al.,2002,2003; Han et al.,2005; He et al.,2008; Hu et al.,2009). However, these methods often overlook the relational information inherent in graph datasets, leading to suboptimal performance compared to graph-based approaches. Several significant contributions have been made to address the class imbalance problem in graph data. GraphSMOTE(Zhao et al.,2021)combines Graph Neural Networks (GNN) with interpolation to generate synthetic node embeddings. However, it relies on an additional edge predictor to integrate synthetic nodes into the graph, decoupling edge creation from the objective of generating realistic nodes and suffering from edge imbalance when using an entire graph edge sampling strategy. PC-GNN(Liu et al.,2021)utilizes a label-balanced sampler to enhance fraud detection by learning structural information and interactions in the graph. DR-GCN(Shi et al.,2021)addresses multi-class imbalances in graphs by implementing dual regularization and class-conditioned adversarial training. GNN-INCM(Huang et al.,2022)includes embedding clustering and graph reconstruction modules for improved representation and classification. ImGAGN(Qu et al.,2021)employs a generator to simulate minority class attributes and topological structures; however, the assumption that synthetic nodes are connected to all original nodes necessitates the use of a massive matrix to learn the relevance of these nodes. GraphENS(Park et al.,2022)addresses the class imbalance by synthesizing ego networks for minor class nodes and integrating a saliency-based node mixing technique. GraphSHA(Li et al.,2023)addresses class imbalance by enlarging the decision boundaries of minor classes by synthesizing harder minor samples. However, these methods are constrained by their focus on either homogeneous graph data or rule-based approaches for generating minority nodes. They often fail to leverage the full potential of generative models in learning complex data distributions, particularly within heterogeneous graphs. To address these gaps, we propose FlashGAN, a novel framework that overcomes these limitations and introduces a more flexible approach to adversarial learning in heterogeneous graph data. FlashGAN’s ability to integrate both node and edge information within a unified adversarial framework represents a significant advancement in tackling the class imbalance problem.

SECTION: 3.Preliminaries and Definition

SECTION: 3.1.Imbalance Ratio and Imbalanced Graph

We define a heterogeneous attribute graph as, withandas the node and edge type mapping functions, respectively. The node attribute matrix, whereis the number of nodes andis the attribute dimension. Node class labels are denoted by. For a graphwithclasses, the set of node classes is. Letdenote the size of the-th class. The smallest and largest classes are defined asand, respectively. The imbalance ratiomeasures the degree of class imbalance, with a graphconsidered imbalanced ifis significantly smaller than.

SECTION: 3.2.Generative Adversarial Network

Generative Adversarial Networks (GANs)(Goodfellow et al.,2014)are powerful generative models widely used in data augmentation(Radford et al.,2015; Mirza and Osindero,2014; Radford et al.,2016). A typical GAN framework consists of a generatorand a discriminator, which compete in a min-max game to replicate the underlying data distribution. The objective function is:

Through iterative training,aims to generate samples thatcannot distinguish from real data, ultimately making the generated samples indistinguishable from actual data.

SECTION: 3.3.Node Classification on Imbalanced Graph

The node classification task in Graph Neural Networks (GNNs) predicts node labels within a given graph. In the case of imbalanced node classes, where, the task is to learn a classifierthat accurately predicts labels for both majority and minority classes. The classifier’s objective function is:

where,,is the class label of node, andrepresents any performance metric, such as F-score.

SECTION: 4.Methodology

In this section, we present the architecture of FlashGAN, designed to streamline node generation, simplify edge management, and enhance graph augmentation with a focus on minority node connections. FlashGAN operates by embedding synthetic nodes into local subgraphs, determining node placement and features based on the subgraph’s context. For example, in a social network dataset aimed at spam detection(McAuley and Leskovec,2013; Liu et al.,2020b,a), synthetic spam user data is generated and integrated into the training set, balancing the distribution between benign and spam users for improved model performance. The architecture of FlashGAN follows the classical generative adversarial network structure, consisting of a generator and a discriminator. The generator includes a node embedding generator, which produces synthetic nodes for connection within the sampled subgraph, and a synthetic edge filter, which selectively retains or discards synthetic edges based on their potential to deceive the discriminator. The discriminator then distinguishes between real edges in the original subgraph and synthetic edges formed by the insertion of synthetic nodes. As illustrated in Figure1, we use a toy example of a user-user networkwith a single edge typeto clarify each component of FlashGAN. This setup mirrors real-world spam detection scenarios, where spam users, typically minority nodes, are distinguished from benign users, who represent the majority class.

SECTION: 4.1.Feature Extractor

To prevent the raw feature space of nodes from being too sparse and to avoid the challenge of a low dimensionality, which can hinder the generator of FlashGAN from learning the true distribution of the data, we employ a feature extractor to obtain node representations that better reflect node properties and graph topology(Zhao et al.,2021). Letbe a general Heterogeneous Graph Neural Network (HetGNN) model. For a heterogeneous graphwith node feature matrix, we could obtain its node hidden representation matrixby:

where,is the size of the node setandis the size of the hidden representation of nodes.

Any HetGNN model can serve as the backbone for feature extraction in FlashGAN. In this study, we employ the Heterogeneous Graph Transformer (HGT)(Hu et al.,2020)to learn node representations from the training and validation nodes, ensuring that testing node labels are removed to prevent data leakage. FlashGAN is then trained on these new node representations. After training, FlashGAN’s generator augments the graph by adding synthetic nodes to the training portion. Finally, during the downstream testing phase, we reintegrate the testing node labels into the augmented graph and train a node classification model to evaluate the performance on this enhanced dataset.

SECTION: 4.2.Node Embedding Generator

Before generating synthetic nodes to balance the node classes in the original dataset, we perform subgraph sampling from the original graph (e.g., one-hop subgraph sampling)(Park et al.,2022; Li et al.,2023). During model training, these sampled subgraphs are fed into the model in batches. For a sampled subgraph, the node embedding generator creates synthetic nodes and their embeddings for integration. Letsynthetic nodes be generated for integration into a sampled subgraphto accomplish the subgraph augmentation process. We introduce a learnable DCGAN generator(Radford et al.,2016)and modify it to act as a node embedding generator. To generate a synthetic node represented aswith its corresponding node attributes, we concatenate the subgraph representationobtained from global pooling with a-dimensional random noisesampled from a standard normal distributioninto the generator. Thenodes inserted into the subgraphand their respective node attributes are represented byand, respectively. Finally, we let all synthetic nodes be connected to all real nodes in the original subgraph. Since every synthetic node was, by default, connected to all other nodes belonging to the corresponding subgraph. We represent these default-connected edges with the setand refer to them as potential synthetic edges. After adding synthetic nodes to the subgraph, we refer to this subgraph as theaugmented subgraph, denoted by (4):

where.

SECTION: 4.3.Adversarial Synthetic Edge Filter

A synthetic node is successfully added to a subgraph due to the establishment of associated potential edges; however, not all synthetic edges are similar enough to real edges to be retained in the subgraph, so we aim to remove redundant synthetic edges if they are of poor quality. Consequently, the synthetic edge filter is specifically tasked with preserving potential synthetic edges that are sufficiently similar to real edges to deceive the discriminator. Specifically, for each edge type(e.g., ’user-user’ or ’user-product’) in a heterogeneous graph, we introduce a synthetic edge filter.

The synthetic edge filteris modeled with learnable weighted matrix(Hamilton et al.,2018)to compute a probabilityfor an edge, reflecting the likelihood of retaining the edge. The edge embedding, defined as the node embeddings fromand, is learned during the previous feature extraction step described in (3). If the likelihoodfalls below the threshold, the edgeis discarded from the augmented subgraph. Therefore, the potential edge setcan be further divided intoandby the edge threshold value. These two edge sets respectively represent the synthetic edges that are retained and those that are discarded by the edge filter.

Next, we seek to determine the optimal threshold value. Typically, this threshold is treated as a hyperparameter, adjusted through experimentation, but this approach increases computational costs. Given that FlashGAN’s primary goal is to generate synthetic nodes and edges that closely resemble real ones, we propose determiningby using existing edges in subgraphsthat include real nodes from the minority class. The motivation stems from the premise that if an edge filter can effectively retain (or discard) synthetic edges that are sufficiently similar (or dissimilar) to deceive (or not deceive) the discriminator, it should also precisely determine whether node pairs containing real minority nodes form positive or negative edges,i.e., whether the node pairs are connected by edges. Thus, the ideal threshold should differentiate real positive and negative edges precisely. The computation of the optimized thresholdis detailed in (6) and (7), where both positive and negative edges containing real minority nodes are processed through the synthetic edge filter. The resulting probabilities are binarized using a binary step function, with the thresholdthat minimizes loss being selected for filtering synthetic edges.

In implementing the aforementioned loss function, we utilize binary cross-entropy loss, whererepresents the edge embeddings of edge, which includes real minority nodes but excludes synthetic nodes. The edge setsand, as defined in (8), denote the positive and negative edges, respectively, within subgraph, containing real nodes that authentically belong to the minority class.

As defined in (9), the potential synthetic edge setis divided intoandbased on the edge threshold valuederived from (6) and (7).represents the edges that are retained, whileindicates the synthetic edges removed from the augmented subgraph.

In this context, the necessity to convert edge-retaining probabilities that fall within therange into binary values arises from the requirement to use any GNN-based discriminator for training or node classification in the downstream tasks. Specifically, probabilistic edges must be transformed into a binary representation, where connected is indicated by 1 and not connected by 0. After applying edge filterto all the potential edges, we will eventually obtain afiltered augmented subgraph.

where.

SECTION: 4.4.GNN-based Discriminator

To best utilize synthetic edge information, we employ a GNN-based discriminator to distinguish between real edges, which contain real spam users, and synthetic edges, which contain synthetic spam users, within each filtered augmented subgraph. We utilize HGT as the backbone GNN model for the discriminator, denoted as, and apply a learnable weighted matrixthat acts as the edge classifier. The function of the discriminator is mathematically described in (11) and (12), whereandare the node embeddings learned by.

Note that the node embeddings learned by the discriminator are denoted with an asterisk (*) to avoid confusion with the hidden representation matrixlearned in (3).

SECTION: 4.5.Downstream Node Classification Task

After training FlashGAN, the generatorGencan augment the original graph by adding synthetic nodes. The number of synthetic nodes to add is typically determined by the imbalance ratio in the training data. For a graphwith class imbalance, letrepresent the number of nodes in the majority class,the number in the minority class, andthe number inafter augmentation. To achieve an imbalance ratio of, the required number of synthetic nodes is. This augmented graph can then be used for downstream tasks such as node classification.

SECTION: 5.Optimization of FlashGAN

In this section, we define the objective function of FlashGAN and the corresponding loss functions for the generator and discriminator. Unlike GAN-based generative oversampling methods focusing on node-level generation and discrimination, FlashGAN stands out as an edge-aware node augmentation framework. While FlashGAN introduces additional nodes, the optimization goal of its generator is to make the edges created by these synthetic nodes resemble the real edges. On the other hand, the discriminator aims to distinguish between these edges.

SECTION: 5.1.Loss function of Generator

For the-th filtered augmented subgraphwith its previous incarnation, the loss function of generatorGencan be expressed by (13),

The three-term generator loss function of FlashGAN is intricately designed to facilitate the generation of high-quality synthetic nodes and edges. As detailed in (13), the loss function is bifurcated into two principal components: the first term and a combined second and third term. Each component of the loss function serves a distinct role within the FlashGAN framework. Specifically, the first term compels the synthetic edge filter to discern real edges, allowing us to subsequently determine an optimal thresholdto segregate potential synthetic edges in. In contrast, the second and third terms equip the synthetic edge filter with the ability to identify and retain synthetic edges that could deceive the discriminator. Moreover, when the edge filter erroneously retains synthetic edges that fail to deceive the discriminator, the second term imposes a significant penalty. Conversely, if the filter erroneously discards synthetic edges that could have deceived the discriminator, the third term triggers a substantial penalty. Through this mechanism, the synthetic edge filter progressively learns to maintain synthetic edges that can mislead the discriminator, ensuring that only edges analogous to real ones are preserved. This approach advances beyond traditional methods that limit adversarial training to node attribute levels by integrating synthetic edge construction directly into the GAN model’s optimization process, thus enabling the simultaneous generation of synthetic nodes and edges.

SECTION: 5.2.Loss function of Discriminator

As mentioned earlier, the goal of the discriminator is to distinguish between real and synthetic edges. The synthetic edges refer to the set of edges, which corresponds to the subgraphthat has successfully embedded synthetic nodes. On the other hand, the real edges consist of the set of edges within the subgraphthat originally existed, including edges involving minority class nodes, denoted by. The loss function of discriminatorDiscan be expressed by (14),

SECTION: 5.3.Objective Function of FlashGAN

The objective function of FlashGAN can be represented by (15), whereis the edge embedding of the edgebelonging to the corresponding filtered augmented subgraph.andare the edge embedding distributions of the edge sets induced by real minority nodes and synthetic minority nodes, respectively represented byand. The subscript symbolGenrepresents that the formation is affected by the generator.

SECTION: 5.4.Training Procedure

Due to space limitations, the pseudocode for FlashGAN is included in the supplementary materials, with the training pipeline outlined in Algorithm 1. The training process of FlashGAN begins by extracting preliminary node representations via a feature extractor. Sampled subgraphs are then augmented with synthetic nodes and edges, as detailed in Algorithm 2. These augmented subgraphs are then fed into the generator and discriminator, where parameters are iteratively updated using their respective loss functions.

SECTION: 6.Experiments

In this section, we conduct extensive experiments to evaluate FlashGAN’s performance in class-imbalanced node classification, addressing the following research questions:

How does FlashGAN’s performance compare to that of conventional methods and GNN baselines in various metrics?

How does varying the imbalance ratio, specifically the number of synthetic nodes added to the augmented graph, impact minority node classification accuracy?

Does the synthetic edge filter reliably select synthetic edges that optimize node classification performance?

What is the effectiveness of the synthetic nodes and edges generated by FlashGAN on overall node classification, and how significantly do they enhance performance?

SECTION: 6.1.Experimental Setup

We employ FlashGAN to address the class imbalance problem in real-world datasets, Amazon reviews(McAuley and Leskovec,2013)and Yelp reviews(Yelp.Inc.,2015). In the Amazon dataset, we focus on the Musical Instruments category, analyzing text comments, user votes, and product information. In the Yelp dataset, we study Greenwood City owing to its relatively abundant number of products to the number of users. In the Amazon context, users with at least 20 votes are suitable samples; those with over 70% useful votes(Kumar et al.,2018)are benign, and others are spam. In Yelp, users with an average helpful score above 30% are considered benign.

We construct heterogeneous graphs for the Amazon and Yelp review datasets, consisting of two node types (user and product) and three edge types(McAuley and Leskovec,2013). The first edge type, UU, connects users with high similarities, such as similar review texts or shared product purchases. The second edge type, UP, links users and products when a user has rated or reviewed a product. The third edge type, PP, connects products with similar descriptions or categories. These graphs capture various relationships and interactions, comprehensively representing the review data. Table2shows the graph statistics, with additional construction details in the supplementary material.

To validate the proposed FlashGAN framework, we compare it with several representative approaches for addressing class imbalance problems. These include traditional oversampling methods adapted for graphs, such as Oversampling and SMOTE(Chawla et al.,2002), classic loss adjustment techniques like Reweight and Focal Loss(Lin et al.,2017), and various GNN-based oversampling strategies. For Oversampling and SMOTE, we establish connections between the synthetic nodes and the neighboring nodes of the source nodes referenced during their generation. Among the GNN baselines, with the exception of GraphSMOTE, we retain the UU edge and transform the heterogeneous graph dataset into a homogeneous graph containing only user nodes, as this approach yields better experimental results compared to other methods of transforming heterogeneous graphs into homogeneous ones. The technical details of the GNN methods are described as follows:

PC-GNN(Liu et al.,2021)uses a label-balanced sampler to select nodes and edges and a neighborhood sampler to pick neighbor candidates. The model aggregates information from these neighbors to derive the target node’s final representation.

ImGAGN(Qu et al.,2021)introduces the GraphGenerator that simulates attributes and network structure distribution for minority class nodes. However, it is still limited to using GCN layers and only supports homogeneous graph data.

GraphSMOTE(Zhao et al.,2021)extends the capabilities of SMOTE to accommodate homogeneous graphs with only one edge type. We generate spam user nodes and establish connections exclusively on the UU edges within the original graph.

GraphENS(Park et al.,2022)addresses class imbalance by synthesizing ego networks for minor class nodes and integrating a saliency-based node mixing technique.

GraphSHA(Li et al.,2023)addresses class imbalance by enlarging the decision boundaries of minor classes through the synthesis of harder minor samples, featuring a SemiMixup module.

We evaluate our model using AUC-ROC, AUC-PRC, F-Score, and Accuracy. AUC-ROC(Bradley,1997)is widely used, but it can overestimate performance on imbalanced classes. In such cases, AUC-PRC(Davis and Goadrich,2006)is preferred for a more accurate assessment.

SECTION: 6.2.Imbalanced Classification Performance

Table1presents the experimental results of FlashGAN compared to selected baselines. We report the best performance of each baseline based on AUC-PRC metrics. On the Amazon dataset, FlashGAN achieves a 17% improvement in AUC-PRC and a 22% improvement in F-Score compared to the second-best method. On Yelp, it shows a 2.6% improvement in AUC-PRC and a 29% improvement in F-scores. These results highlight FlashGAN’s superiority in key metrics for predicting minority class data, significantly enhancing the detection of minority nodes. In addition to FlashGAN’s default one-hop subgraph sampling, we also tested random walk subgraph sampling, detailed in the supplementary material, to evaluate the impact of subgraph sampling strategies on FlashGAN training. This variant, denoted as FlashGAN-R.Walk in the table, did not result in better classification performance on minority class data compared to the one-hop subgraph approach. These findings support the effectiveness of FlashGAN’s local augmentation strategy and the choice of one-hop subgraph sampling.

SECTION: 6.3.Influence of Imbalance Ratio

In this subsection, we evaluate the performance of FlashGAN and other oversampling-based methods on node classification by embedding varying numbers of synthetic nodes into the graph. The line chart in Figure2shows the AUC-PRC and F-score metrics, starting with the original graph without synthetic nodes (Ori.) and extending to imbalance ratios up to 2.0. We used HGT for node classification and reported average results. FlashGAN consistently excels across various imbalance ratios in both AUC-PRC and F-score. Baseline methods like Reweight and PC-GNN were excluded from this comparison since they are not oversampling techniques and are unaffected by additional data. Additionally, GraphENS, designed to generate ego networks for real minority nodes, is not suitable for comparison by imbalance ratio. ImGAGN and GraphSHA were also excluded due to the requirement that the imbalance ratio be greater than 1 for their implementation.

SECTION: 6.4.Investigation of the Synthetic Edge Filter

We investigate the impact of the synthetic edge filteron improving minority node classification performance in downstream tasks. Specifically, given a synthetic edge filterdesigned for type, which initially selectssynthetic edges based on a threshold, we also assess alternative edge selection strategies. Rather than relying on the threshold, we consider two alternatives: randomly selectingsynthetic edges and choosing thesynthetic edges with the lowest probabilities from the potential set. This methodology allows us to assess the filter’s effectiveness, with results presented in Table1for FlashGAN-LPnE (least probableedges) and FlashGAN-RSSE (randomly sampled synthetic edges). The findings indicate that edges with the highest probabilities markedly improve classification performance and confirm the filter’s role in selecting edges most akin to real edges. Figure 3 illustrates the experimental results of training FlashGAN on the Amazon dataset. Figure 3(a) displays the loss curves for the generator and discriminator as the training progress. Figure 3(b) demonstrates the synthetic edge filter’s ability to learn to differentiate which potential edges can be retained in the subgraph. It shows how these retained and discarded potential edges, on average, are classified by the discriminator as real (1) or fake (0), respectively.

SECTION: 6.5.Effectiveness measure of the synthetic data

Since FlashGAN leverages both node and edge information for co-training, it effectively models the edge behavior of minority nodes. Table3compares FlashGAN with baseline methods that add nodes and edges to the Amazon and Yelp datasets, evaluating the impact of augmented graph size on performance. FlashGAN outperforms other oversampling methods, achieving the highest AUC-PRC scores with an Imbalance Ratio of 0.4 for Amazon and 1.0 for Yelp. Additionally, Table3shows that FlashGAN requires minimal synthetic data to significantly improve AUC-PRC scores, highlighting the efficiency of its data augmentation in enhancing minority class classification.

SECTION: 7.Conclusions

We propose FlashGAN, a novel topology-aware framework designed to generate synthetic nodes and improve predictions for minority classes in graph data. FlashGAN employs subgraph sampling and uses subgraphs as the fundamental training units, ensuring that the framework can effectively handle graph datasets of varying sizes. By conducting node augmentation within local subgraphs, FlashGAN avoids the need to sample from the highly imbalanced positive and negative edges across the entire graph dataset, thereby mitigating the edge imbalance issue. Additionally, the integration of a synthetic edge filter into the conventional GAN framework enhances its capability by directly considering both node attributes and edge information during the adversarial learning process. This approach enables the model to optimally incorporate synthetic nodes into subgraphs, significantly improving downstream node classification tasks. Moreover, the synthetic edge filter eliminates the need for post-experimental tuning of edge threshold hyperparameters, which was previously required to effectively integrate isolated synthetic nodes into the original graph. Experimental results demonstrate FlashGAN’s superiority over baseline models in key metrics for minority class prediction, achieving these improved outcomes with minimal additional data integration compared to other methods.

SECTION: References

SECTION: Appendix APseudocode

The training pipeline of FlashGAN is outlined in Algorithm1. To prevent data leakage, labels of testing nodes are masked throughout the training of FlashGAN. After training, the FlashGAN generator produces synthetic nodes and edges, which are incorporated into the training set for downstream node classification, enhancing the classification performance on minor nodes in the testing data. The training process of FlashGAN begins by extracting preliminary node representations via a feature extractor. Sampled subgraphs are then augmented with synthetic nodes and edges, as detailed in Algorithm 2. These augmented subgraphs are then fed into the generator and discriminator, where parameters are iteratively updated using their respective loss functions.

SECTION: Appendix BDetails of Graph Datasets

SECTION: B.1.Datasets

We employ FlashGAN to address class imbalance in the Amazon and Yelp review datasets. In the Amazon dataset, focused on the Musical Instruments category, we analyze text comments, user votes, and product information, classifying users with over 70% useful votes as benign and others as spam. For the Yelp dataset, centered on Greenwood City, users with an average helpful score above 30% are considered benign. Tables4,5,6, and7detail the features extracted for user and product nodes in both datasets, which were derived from raw data to characterize nodes and construct edges. To avoid information leakage, features related to user labeling, such as spam and helpful votes, were removed and are marked in italics in the tables.

SECTION: B.2.Graph Construction

We construct a heterogeneous graph for the Amazon review dataset and the Yelp review dataset, respectively, which consists of two node types (user and product) and three edge types(McAuley and Leskovec,2013). The first edge type, UU, connects users with high similarities, such as those with similar review texts or who have purchased (reviewed) the same product. For the Amazon dataset, we connect two users if their review similarities measured by TF-IDF exceed 0.6; in the Yelp dataset, users are connected with others whose reviews are among the top 1% in similarity. The second edge type, UP, connects users and products if a user has rated or reviewed a product. The third edge type, PP, connects products that are similar in their product description or belong to the same category. These heterogeneous graphs capture various relationships and interactions among users and products, comprehensively representing these review data.

SECTION: Appendix CImplementation Details

SECTION: C.1.Configurations

To minimize the impact of outliers on the experimental results, we conduct experiments using ten random seeds, ranging from 0 to 9, for each configuration. The average of these results serves as the representative value for each experimental group’s outcome. Our experiments were conducted in an environment with Ubuntu 20.04.5 LTS, CUDA 11.6.2(NVIDIA et al.,2020), torch 2.0.0(Paszke et al.,2019), and dgl 1.0.1+cu117(Wang et al.,2019). The machine is equipped with NVIDIA RTX A6000 GPU.

SECTION: C.2.Details of FlashGAN Training

During the Feature Extraction stage (3), nodes in the graph are divided into training, validation, and test sets in a 7:2:1 ratio. We employ the Heterogeneous Graph Transformer (HGT) to learn new node representations with a dimension of 256, enhancing the original node dimensions of 23 and 32 for the Amazon and Yelp graphs, respectively. Maxpooling is used for graph pooling, while the DCGAN architecture is kept consistent with the original study, with only the final output modified to a dimension of 256. During the synthetic node embedding generation phase, we use different settings for the number of synthetic nodes appended to each subgraph in the Amazon and Yelp datasets. In the Amazon dataset, we add 5 synthetic nodes per subgraph, whereas in the Yelp dataset, we add 3, due to the higher subgraph density in Yelp. The synthetic edge filter and the discriminator’s edge classifier use a weight matrix of 256x256. Before predicting real and synthetic edges, the discriminator’s edge classifier, equipped with a single-layer HGT and a head count of 1, learns new representations for nodes in the filtered augmented subgraph.

SECTION: C.3.Details of Baseline Implementation

For the loss-adjusting approaches, we replaced HGT’s original loss function with reweight and Focal Loss for training, setting the weights in reweight by the reciprocal of a class’s size multiplied by a fixed factor,, and for Focal Loss, settingtoandto. In the implementations of PC-GNN and ImGAGN, we conducted grid searches across several hyperparameters. For PC-GNN, we varied hidden dimensions within, learning rates from, and training epochs from. ImGAGN’s grid search included training epochs within, hidden dimensions, generator’s learning epochs, and learning rates.

For GraphENS and GraphSHA, we fixed the number of layers at 2 and set imb-ratio at 10 , using SAGE as the GNN backbone for its superior performance on Amazon and Yelp datasets. GraphSHA’s grid search also included training epochs within, learning rates, and utilized ’ppr’ (personalized PageRank) for weighted graph generation. The imbalance ratio was set to a default of 100 at the warm-up stage.

For oversampling methods like Oversampling, SMOTE,GraphSMOTE, and FlashGAN, and its variants (Random Walk, LPnE, RSSE), we utilized HGT on the constructed heterogeneous graphs, testing learning rates withinand Adam scheduler’s pct-start within, maintaining other HGT hyperparameters at a hidden dimension of, two layers, and heads set tofor Amazon andfor Yelp.

SECTION: C.4.Random Walk Subgraph Sampling

We explore the use of random walk subgraph sampling as an alternative to FlashGAN’s default one-hop subgraph sampling method. Our goal is to determine whether training FlashGAN with subgraphs sampled via random walks, as opposed to one-hop subgraphs, better facilitates the generation of synthetic nodes that improve minor node classification in downstream tasks. With this motivation, we aim to compare the impact on FlashGAN’s training when the sampled subgraphs contain a similar number of nodes but differ in their spatial distribution within the graph. To this end, we calculated the average size of one-hop subgraphs centered on each user node in the Amazon and Yelp datasets. We found that there are 2,500 valid subgraphs in the Amazon dataset and 4,584 in the Yelp dataset (each containing both user and product nodes), with average sizes of 88 and 253 user nodes, respectively. To ensure that the random walk subgraph sampling yields subgraphs of comparable size to these one-hop subgraphs, we adjusted the corresponding step length to match the average subgraph size between the two methods.