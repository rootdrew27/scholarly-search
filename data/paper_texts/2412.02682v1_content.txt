SECTION: The asymptotic behavior of attention in transformers
A key component of transformers is the attention mechanism orchestrating how each token influences the propagation of every other token through a transformer. In this paper we provide a rigorous, mathematical analysis of the asymptotic properties of attention in transformers. Although we present several results based on different assumptions, all of them point to the same conclusion, all tokens asymptotically converge to each other, a phenomenon that has been empirically reported in the literature. Our findings are carefully compared with existing theoretical results and illustrated by simulations and experimental studies using the GPT-2 model.

SECTION: Introduction
The incorporation of attentionin natural language processing was a significant breakthrough, particularly in the context of sequence-to-sequence models, enabling the creation of transformerswhich revolutionized the field. Even initial transformer models such as GPTor Bertshowed drastic improvements over previous approaches such as the Long Short-Term Memory model.

As practical applications of deep neural networks, such as image recognition, natural language processing, and autonomous driving, continue to advance, our understanding of these networks is struggling to keep pace. This underscores the critical importance of our study, which aims to delve deeper into transformers and their dynamics.
Our understanding of transformers is currently limited by their inherent complexity, making it challenging to comprehensively explain their behavior. However, recent studies have shown the emergence of clusters of tokens empirically and theoretically. These findings suggest that without proper care, large transformers may collapse, a phenomenon where the tokens cluster, limiting the model’s ability to produce different outputs.

Our work was motivated by the paperwhere a mathematical model for attention was proposed, based on prior work on similar models, and investigated. The authors share the vision outlined in, a better understanding of the role and importance of attention mechanisms can be achieved through the study of mathematical models. Our contribution lies in bringing ideas developed by the control community, where the study of asymptotic properties of dynamical and control systems is a central preoccupation, to bear on this problem. While deferring to the next section a more detailed comparison between our results and those available in the literature, we emphasize here that, in contrast with, we do not rely on stochastic and/or mean-field techniques and rather adopt a geometric perspective drawing from control theory, e.g., from consensus dynamics on manifoldssuch as spheresand Input-to-State Stability.

SECTION: Contributions and plan of the paper
The main contribution of this work is to provide a number of results, for a differential equation model of attention, showing that all tokens converge to a single cluster thereby leading to a collapse of the model. We use the termconsensus equilibriato refer to such clusters as is done in the consensus literature. These results hold under different assumptions on the parameters of the model —namely, the query (), key () and value matrices (), as well as the number of heads ()— that are summarized in Table. More specifically, the paper is organized as follows:

In Sectionwe introduce the differential equation model for attention studied in this paper. Since layer normalization is part of the model, tokens will evolve on ellipsoids. As we are mainly concerned with attention, the model does not describe the effect of feedforward layersin a transformer. Yet, we briefly discuss how the model can be extended to accommodate feedforward layers and the challenges brought by such extension.

Sectionis devoted to the single-head case withbeing the identity andbeing time invariant, positive definite, and symmetric. With Theorem, we prove that the dynamics of the transformer is a Riemannian gradient vector field, from which we conclude convergence to an equilibrium point (guaranteed to be of consensus type whenis the identity) for every initial position of the tokens. Although the gradient nature of the dynamics, in this case, was already observed and exploited in, for the benefit of the readers we provide a formal proof of this fact in a slightly more general setting.

In Sectionwe show that tokens converge to a consensus equilibrium whenever their starting positions lie in the interior of some hemisphere of the ellipsoid. This is stated in Theorem, which holds for any number of heads and time varying matrixprovided thatis the identity andis bounded and uniformly continuous as a function of the time. A similar result is reported inunder Lemma 4.2. However, its conclusions hold under the stronger assumptions that bothandare the identity matrix and there is a single attention head.

The previous results hold under no assumptions on the attention matrix other those induced by the assumptions on. In the next sections, we focus on the auto-regressive case, also known as causal attention, where the self-attention matrix is lower triangular.

For the auto-regressive case withbeing the identity, the first token is fixed. In Section, we show that all tokens converge to the position of the first one for almost every initial position of the tokens. In fact, Theoremensures asymptotic stability of this consensus equilibrium. This holds for any number of heads and any time varyingmatrix provided it is bounded. Similar conclusions are reported under Theorem 4.1 inby imposing stronger assumptions: time invariance ofand existence of a single attention head.

To conclude the theoretical part, Sectionextends the previous result to the case whereis a time invariant symmetric matrix and the multiplicity of its largest eigenvalue is one. Therefore, the corresponding eigenspace divides the sphere in two different hemispheres. Theoremensures that the tokens will converge to a consensus equilibrium (moreover, that equilibrium is asymptotically stable) if all the tokens start in one of those hemispheres. We were only able to establish this result for the single-head case although we believe it holds in greater generality. To the best of the author’s knowledge there is no result available in the literature for the case whereis not the identity matrix although this is conjectured, but not proved, in.

In Sectionwe illustrate the theoretical results through simulations of the mathematical model for attention. We do this using a small number of tokens in low dimensions, for better visualization, as well as a number of tokens and dimension comparable to what is used in the GPT-2 model. We also report on several experiments with the GPT-2 model suggesting convergence to a consensus equilibria in more general situations than those captured by our theoretical results thus providing additional confirmation for model collapse.

SECTION: Notations
SECTION: Dynamics of transformers
SECTION: Configuration space
SECTION: Discrete-time attention model
QueryKeyValue

headsmulti-headed self-attention

skip connectionlayer normalizationbatch normalization

SECTION: Continuous-time attention model
SECTION: Transformers as gradient vector fields
SECTION: Riemannian metric on the configuration space
SECTION: Gradient vector field
SECTION: Stability analysis
SECTION: Tokens evolving on an hemisphere
upper Dini derivative

SECTION: Auto-regressive self-attention with identity value matrix
auto-regressive self-attention matrix

SECTION: Symmetric value matrix
SECTION: Simulations
SECTION: Illustration of Theorem
SECTION: Illustration of Theorem
SECTION: Illustration of Theorem
SECTION: Illustration of Theorem
SECTION: GPT-2 Experiments
SECTION: References