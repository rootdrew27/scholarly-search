SECTION: DeforHMR: Vision Transformer with Deformable Cross-Attention for 3D Human Mesh Recovery

Human Mesh Recovery (HMR) is an important yet challenging problem with applications across various domains including motion capture, augmented reality, and biomechanics. Accurately predicting human pose parameters from a single image remains a challenging 3D computer vision task. In this work, we introduceDeforHMR, a novel regression-based monocular HMR framework designed to enhance the prediction of human pose parameters using deformable attention transformers.DeforHMRleverages a novel query-agnostic deformable cross-attention mechanism within the transformer decoder to effectively regress the visual features extracted from a frozen pretrained vision transformer (ViT) encoder. The proposed deformable cross-attention mechanism allows the model to attend to relevant spatial features more flexibly and in a data-dependent manner. Equipped with a transformer decoder capable of spatially-nuanced attention,DeforHMRachieves state-of-the-art performance for single-frame regression-based methods on the widely used 3D HMR benchmarks 3DPW and RICH. By pushing the boundary on the field of 3D human mesh recovery through deformable attention, we introduce an new, effective paradigm for decoding local spatial information from large pretrained vision encoders in computer vision.

1Equal contribution

SECTION: 1Introduction

Motion capture (MoCap) technology has applications in numerous fields such as film, gaming, AR/VR, as well as sports medicine by providing a tool to capture and analyze human pose in 3D. Traditional marker-based MoCap systems utilizing multi-view cameras and marker suits recover highly accurate human pose but suffer from poor accessibility due to the high cost of setting up the adequate laboratory environment[31]. In contrast, a single camera with the correct algorithm can perform 3D Monocular Human Mesh Recovery (HMR), which recovers a mesh of a human body in 3D given an input image or video as a more accessible alternative using deep neural networks[19].

A common parametric approach to 3D HMR leverages the Skinned Multi-Person Linear (SMPL)[29]representation model that regresses joint articulations (often referred to as the pose parameter) and a body shape parameter to generate accurate 3D human body meshes. Current challenges in HMR include occlusion situations[21]and the complexity and variability of human pose, but underlying these issues is simply insufficient spatial understanding in neural networks to output correct pose parameters[26].

More recently, advances in vision transformers[11]have demonstrated versatility and overall impressive performance across a wide range of vision tasks and domains[20], particularly in determining complex spatial relations[14]. In the field of object detection, deformable attention[54][46]has emerged as one promising solution for accurate, space-aware localization, and extending such an approach to HMR requires even greater focus on extracting precise positional semantics[54].

In parallel, issues of data generalization across diverse real world applications for vision models have been diminished by the release of large vision transformer models pretrained on self-supervision tasks on web-scale datasets[33][3]. The ability of these foundation models to generate meaningful features across all data spectra for downstream application has created a new effective learning paradigm, and more recently, works[12]have begun on improving the spatial resolution of these vision foundation model features for ever better results. For HMR,[14]has noted how pretraining with both masked auto-encoding[15]along with 2D keypoint prediction[49][4]has been essential to model convergence, and we build upon along this line of work, investigating how to most effectively decode the features from these large-scale pretrained models.

Integrating the information derived from large, pretrained vision transformer[49]features and deformable attention decoding, we presentDeforHMR, a novel transformer-based HMR framework that significantly improves upon current methods in both accuracy and computation efficiency.

We believeDeforHMRoffers significant benefit through the synergy between the semantically-meaningful spatial features from a pretrained vision transformer and the deformable attention mechanism; in deformable attention the reference and offset locations are floating point values in the feature map coordinate space, and bilinear interpolation is used to extract the relevant key and value information. Hence, the advantage of such deformable attention mechanism is derived mostly from data-dependent spatial flexibility, or the ability to dynamically shift attention to relevant spatial regions based on the characteristics of the input feature. We believe utilizing rich features from the transformer encoder would enable the spatial dynamism of the deformable attention mechanism to be influential by learning better spatial relations.

In summary, our contributions are twofold:

We presentDeforHMR, a regression-based monocular HMR framework that demonstrates SOTA performance on multiple well-known public 3D HMR datasets under the single-frame, regression-based setting.

Inspired by[46], we propose a novel deformable cross-attention mechanism designed to be query-agnostic and spatially flexible.

SECTION: 2Related Work

SECTION: 2.1Monocular HMR

Early work in 3D HMR[34][2]revolved mainly around fitting the SMPL parametric body model to minimize the discrepancy between its reprojected joint locations and 2D keypoint predictions on the 2D image. End-to-end 3D human mesh recovery, not relying upon intermediate 2D keypoints or joints, was first proposed by Kanazawa et al.[19]. This was achieved by leveraging novel deep learning advancements at the time and regressing the SMPL parameters along with a camera model to derive the 3D human meshes. Ever since then, various neural network-based HMR methodologies have been proposed. In[25], the authors aim to resolve the discrepancy between plausible human meshes and accurate 3D key-point predictions through a hybrid inverse kinematics solution involving twist-and-swing decomposition. Li et al.[26]proposes to mitigate the loss of global positional information after cropping the human body through utilizing more holistic features containing global location-aware information to ultimately regress the global orientation better, and, Goel et al.[14]would contribute to this through implementing a vision transformer architecture using a single query token fed into the decoder for SMPL parameter predictions. HMR-2.0 established a new competitive baseline on single human mesh recovery, and in particular, they show how their transformer network can encode and decode complex human pose due to their ability to better capture difficult spatial relations.

In[25], Li et al. notes how an alternative approach to direct regression of the SMPL parameters is optimization-based HMR[8][48][47][55], which estimates the body pose and shape through an iterative fitting process. For instance, PLIKS[39]fits a linearized formulation of the SMPL model on region-based 2D pixel-alignment, and ReFit[45]iteratively projects 2D keypoints in order to effectively generate accurate meshes. However, optimization-based HMR at inference-time does not have any runtime guarantees and often struggles from large runtime due to the iterative refinement process, and thus can be difficult to integrate into real-time application settings.

As computational capacity has increased over recent years, the ability to use complete sequences of video frames for human mesh recovery has recently found success. Within this area of temporal HMR[10][30][7][24], Kocabas et al.[22]have proposed adversarial training with a temporal network architecture to learn to discriminate between real and fake pose sequences. Moreover,[52]proposes to mitigate the effects of occlusions in video sequences through infilling occluded humans with a deep generative motion infiller, and[50]utilizes a temporal motion prior[36]to effectively decouple the human motion and camera motion given a video sequence. More recently, Shin et al.[40]have incorporated motion encoding and SLAM[42]approximation, along with model scale in order to achieve obtain state of the art performance for multi-frame inputs.

While this line of work is promising for integrating complete video information in human mesh recovery, we restrict our focus to single-frame inputs so that our method generalizes to individual images.

SECTION: 2.2Deformable Attention

The Deformable Transformer[54]architecture, first proposed in end-to-end object detection[5], has demonstrated comparable performance to other SOTA methods without needing any hand-designed components commonly used in object detection[35][13]. The deformable attention module is designed for efficiency and complex relational parameterization, having the keys and values be sparsely sampled learned offsets from a reference location determined by a given query. Zhu et al.[54]show that this increases model training and inference speed while also incorporating inductive biases for precise spatial modeling beneficial for object detection.

Yoshiyasu[51]extends this notion of deformable attention to optimization-based non-parametric 3D HMR with the DeFormer architecture, using the joint and shape query tokens at each layer to generate reference points and offsets on multi-scale maps to be used in the attention computation. DeFormer works directly with positional information without the SMPL parameterization for dense mesh reconstruction, and it improves upon previous baselines of similar model size.

In[46], Xia et al., show how previous works for deformable attention, in fact, function more like a deformable convolution[9]without attention interactions between all queries and all keys. They then propose the Deformable Attention Transformer (DAT), a vision transformer backbone using true deformable self-attention. DAT demonstrates its advantage of deformable self-attention for localization-based tasks such as object detection and instance segmentation, outperforming shifted window full self-attention methods[28]on COCO[27]. In their analysis, they suggest that deformable attention consistently attends to more important and relevant areas of the image and feature map compared to full self-attention, confirming that the true deformable attention interactions between queries and keys result in realized performance and interpretable improvements.

SECTION: 3Methodology

In this section, we delve into the methodologies of each component ofDeforHMR. More specifically, we discuss using a frozen ViT pretrained on pose estimation as a feature encoder and our novel deformable cross-attention mechanism. Lastly, we touch upon model training specifications.

SECTION: 3.1Generating Feature Maps

We use the ViT-Pose from Goel et al.[14]as our initial feature encoder. This is a ViT-H with patch size 16 and input size 256 by 192 that is pretrained with masked autoencoding on ImageNet and 2D pose estimation on COCO[49]. We freeze all the weights during training and pass the input image through to generate the features maps. That is, given an input imageand a patch size of 16, we represent the spatial output tokens of the encoderasforand.

SECTION: 3.2Deformable Attention Decoder

Our approach can be thought of as using query tokens for SMPL parameters. These are learnable tokens, representing 24 pose tokens and 1 shape token, which further incorporate information from the image features through the decoder blocks.

Following the standard paradigm of the transformer decoder[43], each layer of our deformable cross-attention decoder is compromised of a self-attention, a deformable cross-attention, and a feed-forward network.
We further elaborate on our deformable cross-attention mechanism in the subsequent section.

After passing the queries through the decoder, we learn linear projectionsandto get the desired outputs of pose parameters, and shape parameters. For the pose rotation angles in the SMPL parameters, we use the common 6D representation proposed by Zhou et al. (2020)[53]for a more continuous loss-landscape, converting to the actual pitch/roll/yaw and rotation matrix afterwards. Moreover, we use one round of iterative error feedback[6], starting with the mean SMPL values from Humans 3.6M[18]to condition our predictions better. These are thus finally passed into the SMPL model to generate our 3D meshes.

We propose a novel, query-agnostic deformable cross-attention mechanism designed to capture fine-grained spatial details as shown in3. The deformable aspect introduces learnable offsets that allow the attention to adaptively select key-value pairs from the feature map, as opposed to uniform attention to all spatial locations. Our method is inspired by the self-attention mechanism proposed in[46].

For some layer, let the input tokens to this layer be, for batch size, the 25 SMPL tokens, and model dimension. The first part of our decoder block is multi-head self-attention with residual on the query tokens, so let the output from the self-attention beLet the spatial features from the encoder be. We will refer to these spatial features as thecontextfor our decoder.

We consider a base set of reference points, which are initialized as grid coordinates normalized to the range. These reference points provide an initial uniform bias for the positions of the keys in the feature map:

whereanddenote the indices over height and width, respectively.

We then computeunique raw offsets for each reference point by processing the context features through a series of grouped convolutions, non-linear activations (GELU)[16], and normalization layers:

These raw offsets are then passed through hyperbolic tangent and scaled by, restricting the offset to withinmultiplied by the grid spacing. Hence, the final offsetsare

The resulting offsetsindicate the amount by which each reference point in the grid of dimensionshould be shifted, allowing the model to focus on different parts of the feature map depending on the input context. This mechanism enables the attention to be more flexible and context-aware.

The final sampling positions are hence the sum of the reference point positions and offsets.

With these positions, we sample the context, employing bilinear interpolation to extract precise embedding values at these adjusted positions. The sampled contextcan finally be projected into keysand values. And likewise, we project the input tokens into the queries.

We calculate cross-attention scores by first taking the dot product of the querieswith the keys, then summing this term with an attention bias term, which is computed through sampling a learnable relative positional embedding tensorvia bilinear interpolationusingas sampling location:

whereis the dimension of the keys, used to scale the scores and stabilize training.

More specifically, the relative positional embedding is a unique learned grid for each query position. This is similar to both DAT[46]and the original relative position encoding[37]; the keys have actual positions, so it uses the grid formulation as in DAT, but the queries do not have positions, so we simply index and learn the relative position embedding separately for each query.

We lastly multiply the attention coefficients by the values and add the residual to get the cross attention output. Passing this through a 2-layer feed-forward network, we get the layer’s final output

We want to emphasize that our deformable cross-attention differs greatly from DeformableDETR[54]-style cross-attention proposed by previous works. Unlike typical deformable attention methods where offsets are conditioned on the queries, our model computes offsets directly from the context, meaning they are query-agnostic. This design choice is inspired by the Deformable Attention Transformer[46](DAT) paper; however, their focus on encoder architectures means their deformable self-attention models do not fully decouple relations between queries and key-values. By having query-agnostic cross-attention here, we can
ensure that the shifts in receptive fields and sampling clusters via deformable attention are consistent and coordinated across all queries, capturing global information more effectively.

SECTION: 3.3Training Details

Following[26], we train with reconstruction loss on the SMPL parameters, the 3D joint positions, the 3D mesh vertices, and the projected 2D joint positions, all using mean square error. The relative loss weight for SMPL parameters is, 2D and 3D joint positions is, and mesh vertices. For all training runs, we freeze the ViT-Pose to explore efficient decoding methods.

We train all models on real world datasets, two with 3D SMPL ground truth derived from motion capture—3DPW[44]and MPI-INF-3DHP[32]—and three psuedo-labeled from 2D pose ground truth using the CLIFF-annotator[26]: COCO[27], MPII[1], and Humans3.6M[18]. We train for 100 epochs. The evaluation is performed on the test split of 3DPW and RICH, and we use mean per-joint position error (MPJPE), procrustes analysis MPJPE (PA-MPJPE), and per-vertex error (PVE), all in millimeters (mm) to determine how well the model recovers accurate human pose in 3D.

SECTION: 4Results

We compare various model architectures and approaches using our evaluation metrics in Table1. Note that since we are interested in single-frame inputs and inference in real-time applications, we exclude multi-frame temporal approaches and optimization-based approaches.

Upon comparing HMR evaluation metrics with several state-of-the-art regression-based HMR methodologies, we demonstrate thatDeforHMRestablishes a new state-of-the-art benchmark on both 3DPW[44]and RICH[17]datasets by a considerable margin.

SECTION: 4.1Analysis

Our HMR model exhibits a robust capability in capturing the general body pose and proportions of individuals across various scenarios, as seen in the visualizations on Figure4. Upon rendering the recovered meshes on four distinct images from the test set of the 3DPW[44]and RICH[17]dataset, we confirm our model comprised of the ViT-Pose transformer encoder and the transformer decoder using deformable cross-attention generalizes well across various in-the-wild image examples. Our model demonstrates accurate, plausible, and realistic meshes for humans in various scenarios such as but not limited to executing a fencing motion, walking while conversing sideways, sitting at a table, crouching downwards, etc. In particular, compared to pre-existing HMR models, namely HMR2.0[14], we show strengths in accuracy of upper body articulation and orientation, as well as feet and hand position.

In table2, we decouple some of the main differences between HMR2.0andDeforHMR: multi-query decoder and deformable cross-attention. To do so, we evaluate all four combinations ofmulti-query versus single-query, anddeformable cross-attention versus regular cross-attention on the test set of 3DPW[44]. The ablation results clearly indicate that both the use of multiple queries and the deformable cross-attention mechanism inDeforHMRcontribute significantly to performance improvements across all three metrics. Specifically, models incorporating these components consistently achieve lower error rates, indicating the effectiveness of each design choice, and furthermore, the performance increase going from single to multi-query for deformable cross-attention is much larger than regular cross-attention (4.0mm PVE decrease versus 3.1). This suggests true synergy between the multi-query formulation and deformable cross-attention, enabling superior 3D HMR performance.

In Figure5, we visualize what the first attention head of the first (left) and last (right) layer inDeforHMRdecoder attends towards. The maroon and red square dots are context positions where the attention value sum over the queries is over 0.25, with bright red corresponding to the largest values.DeforHMRis able to incorporate specific information of each individual’s limb positions well through the deformable cross-attention, and we can see that the most important positions correspond well with the attention values and locations. Specifically, in the second and third row the model is able to attend towards uncommon arm and leg positions accurately.

SECTION: 5Conclusion

Through this work, we push the boundaries of HMR by combining a pretrained vision transformer encoder with novel deformable cross attention. We have two main contributions. First, we introduceDeforHMR, a regression-based framework for monocular HMR that demonstrates SOTA performance on popular 3D HMR datasets such as 3DPW[44]and RICH[17]. Second, inspired by the self-attention mechanism proposed in the Deformable Attention Transformer (DAT)[46], we extend their method with an innovative deformable cross-attention transformer decoder. This mechanism is both query-agnostic and spatially adaptive, enabling the model to dynamically shift focus on relevant spatial features. We show our decoder performs well without additional encoder fine-tuning, allowing for this method to be applicable for API-based large scale models as well.

Despite these advancements, our work possesses some limitations. While our approach demonstrates significant improvements, there is always room for enhancing the model’s robustness, particularly towards examples that are inherently more challenging due occlusions and varying lighting conditions in real-world in-the-wild scenarios. Notably, occlusion from obstacles as well as self-occlusion presents a challenge for the model, particularly noticeable in scenarios where one limb occludes another, such as arms or legs during walking motions. These situations often result in inaccurate limb positioning.

Our work reveals several promising future directions, both within HMR and in other applications. Given the effectiveness of deformable cross-attention for decoding information from spatial features, we believe this method can easily be applied to lower-level tasks such as object detection, instance segmentation, keypoint detection, and pose estimation. Moreover, a potential avenue for advancement is applying our deformable attention towards video data and temporal HMR, dynamically attending towards relevant temporal frames as needed. All things considered,DeforHMRprovides a new effective form of decoding spatial features, a paramount necessity in future applications for large pretrained vision models.

SECTION: References

Supplementary Material

SECTION: ATraining and Evaluation Details

We train on two NVIDIA TITAN-RTX GPUs with DDP and global batch size 200. We use AdamW optimizer with learning rateand weight decay. For both training and evaluation, in each provided scene, we crop the bounding box of each person and resize it to 256 by 192.

SECTION: BAdditional Ablation Studies

SECTION: B.1Effect of positional encoding type