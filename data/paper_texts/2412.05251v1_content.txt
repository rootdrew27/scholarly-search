SECTION: Uncertainty Quantification for Transformer Models for Dark-Pattern Detection

The opaque nature of transformer-based models, particularly in applications susceptible to unethical practices such as dark-patterns in user interfaces, requires models that integrate uncertainty quantification to enhance trust in predictions. This study focuses on dark-pattern detection, deceptive design choices that manipulate user decisions, undermining autonomy and consent. We propose a differential fine-tuning approach implemented at the final classification head via uncertainty quantification with transformer-based pre-trained models.
Employing a dense neural network (DNN) head architecture as a baseline, we examine two methods capable of quantifying uncertainty: Spectral-normalized Neural Gaussian Processes (SNGPs) and Bayesian Neural Networks (BNNs). These methods are evaluated on a set of open-source foundational models across multiple dimensions: model performance, variance in certainty of predictions and environmental impact during training and inference phases. Results demonstrate that integrating uncertainty quantification maintains performance while providing insights into challenging instances within the models. Moreover, the study reveals that the environmental impact does not uniformly increase with the incorporation of uncertainty quantification techniques. The study’s findings demonstrate that uncertainty quantification enhances transparency and provides measurable confidence in predictions, improving the explainability and clarity of black-box models. This facilitates informed decision-making and mitigates the influence of dark-patterns on user interfaces. These results highlight the importance of incorporating uncertainty quantification techniques in developing machine learning models, particularly in domains where interpretability and trustworthiness are critical.

123

SECTION: 1Introduction

The field of NLP was revolutionized with the arrival of transformer models, a groundbreaking architecture introduced by Vaswaniet al.in their seminal work, "Attention is All You Need"[42]. Prior to this, NLP relied heavily on Convolutional Neural Networks (CNN), which were useful in analyzing the spatial features of the data but lacked semantic awareness and nuances. Later, Recurrent Neural Networks (RNN) were used, which processed data sequentially and struggled but faced issues with long-range dependencies within text and stability during training due to the vanishing gradient.

To address this, gated RNNs like Long Short-Term Memory (LSTM) were introduced, which mitigated the vanishing gradient problem but were not parallelizable and required high computational demand for training[12]. With their unique self-attention mechanism, transformers enabled parallel processing of entire data sequences, offering a substantial leap in efficiency and effectiveness. This architecture’s ability to capture complex relationships across distant parts of a text significantly enhanced performance across a myriad of NLP tasks, setting new benchmarks in machine translation, sentiment analysis, and beyond. Subsequent iterations, such as BERT[8]and the GPT series[6,34], further refined and extended the transformer’s capabilities, embedding it as the cornerstone of modern NLP research and applications. The transformative impact of these models is not just limited to their superior performance; they have also democratized access to high-quality NLP tools, fostering innovation and expanding the field’s frontiers[22].

Specifically, transformer models have been widely used for sequence classification tasks, from text sentiment analysis[46]to DNA classification[13]. Despite all their advantages, transformers suffer from similar limitations to other neural network-based models,i.e.their black-box nature that makes their understanding difficult[39]. This aspect can be critical in tasks such as autonomous driving[9]or medical diagnosis[49], where there is a need to obtain a measure of certainty in the model predictions before committing to any action.

Given the black-box nature of transformer models and the critical importance of reliable predictions in high-stakes applications, integrating uncertainty quantification into these models becomes paramount[39]. However, transformer architectures’ complexity and pre-trained nature present significant challenges in modifying their internal components to accommodate uncertainty measures. As a result, focusing on the final classification head offers a practical and effective approach to introduce uncertainty quantification[38].

The interpretability and reliability of transformer-based models can be improved by integrating classification heads with predictions and measures of confidence or uncertainty. This is particularly important in applications where errors can have high costs, and understanding the model’s confidence can lead to better decision-making processes[1,27]. One example of a pervasive issue requiring such knowledge is the use of dark-patterns in user interfaces. These deceptive design strategies compromise user autonomy and challenge the ethical integrity of digital services. Therefore, understanding a model’s confidence can help prevent such issues and promote fair and transparent digital practices.

In this paper, to address the inherent opacity of neural networks and meet the growing demand for more transparent and trustworthy AI systems, three approaches are explored to improve the interpretability and reliability of transformer-based models for dark-pattern detection: (1) dense neural networks (DNNs), (2) Bayesian neural networks (BNNs), and (3) spectral-normalized neural Gaussian Processes (SNGPs) classification heads. We examine the performance of BNNs and SNGPs in quantifying uncertainty across various deep learning models, analyze the impact of these uncertainty quantification techniques on the performance and environmental sustainability of AI models during both training and inference phases, and explore the practical implications of uncertainty modelling through real-world examples of high and low uncertainty cases.

This research enhances the detection of dark-patterns by integrating uncertainty quantification techniques in transformer models to improve the transparency and reliability of predictions, which is crucial for applications where deceptive design practices compromise user autonomy. By providing a quantitative analysis of how these models perform, the research highlights the enhanced ability to identify dark-patterns while addressing potential impacts on environmental sustainability. The goal is to develop AI systems that are both ethically responsible and environmentally considerate, aligning with the growing demand for transparency and trust in AI applications.

SECTION: 1.1Classification Heads

Dense layers, also known as fully connected layers, are the most basic form of a neural network layer, where each input neuron is connected to every neuron in the next layer[21]. In classification tasks, a dense layer typically serves as the final layer that maps the learned representations to the target classes. The primary advantage of dense layers lies in their simplicity and effectiveness in learning complex patterns through these direct connections. However, they do not inherently provide measures of uncertainty in their predictions, treating all inputs with equal certainty.

Bayesian dense layers[19]extend the concept of dense layers by incorporating Bayesian inference into the network’s architecture. Unlike traditional dense layers with fixed weights after training, Bayesian dense layers treat weights as distributions, This approach allows the network to simulate multiple possible models of parameterswith an associated probability distribution, enhancing the model’s ability to express and quantify uncertainty in its predictions.
This is crucial for critical applications like drug discovery and fair AI systems, where decision-making relies heavily on the reliability of the model’s output[1,27].

During training, BNNs utilize a prior distribution for weights instead of fixed values, reflecting initial beliefs which are updated via a likelihood function assessing the model’s fit to the data. Bayesian inference computes a posterior distribution combining these elements, often approximated through variational inference for practical implementation. After training, the BNN generates multiple predictions by randomly sampling different sets of weight values from the posterior distribution of weights. This process offers insights into into their uncertainty and impacting the predictive uncertainty. By comparing these multiple predictions, the degree of uncertainty can be assessed, where low variability among predictions indicates low uncertainty, while high variability suggests greater uncertainty. This is uncertainty quantified using multiple forward passes to generate a distribution of predictions.

However, the major challenge with Bayesian dense layers is their computational complexity and the need for more sophisticated training techniques to manage the probabilistic nature of the weights.

Spectral-normalized Neural Gaussian Process (SNGP)[25]is a relatively recent approach that combines the ideas of spectral normalization and Gaussian Processes (GPs) with deep learning to enhance a deep classifier’s capacity to measure the distance between the test example and the training data. Spectral normalization[30]is a technique used to stabilize the training of neural networks by normalizing the weight matrices, ensuring that the Lipschitz constant of the function (represented by the network) is constrained. This helps maintain the model’s generalization ability. On the other hand, GPs provide a principled, probabilistic approach to learning in kernel machines, offering a powerful tool for uncertainty quantification. By integrating GPs with deep learning, SNGP layers aim to preserve the deep neural network’s capacity for feature extraction and representation learning while enhancing the model’s ability to provide meaningful uncertainty estimates for its predictions. This makes SNGP particularly appealing for tasks requiring a careful balance between performance and interpretability, such as safety-critical applications.

dark-patterns, defined as deceptive user interface designs that manipulate online users into unintended actions have emerged as significant threats to privacy, fairness, and transparency in digital environments. These designs exploit psychological vulnerabilities, leveraging mechanisms such as obfuscation, false urgency, and social proof to influence user decisions in favour of the companies implementing them[5]. The importance of detecting these manipulative strategies cannot be overstated, as they compromise user autonomy and undermine the ethical foundation of digital services[16,43]. Machine learning and deep learning algorithms have demonstrated great accuracy in identifying deceptive practices in digital ecosystems[47]. This offers a solution to enhance user protection and promote transparency and fairness. However, to ensure effective detection, it is important to utilize state-of-the-art architectures and incorporate certainty in the detection process[10]. Thus, combating dark-patterns is crucial, aligning with the growing demand for digital accountability and protection of consumers in complex online landscapes. This relaionship highlights the ongoing efforts and significant impact of research in tackling unethical practices in digital user interfaces.

SECTION: 2Related work

Transformers have been widely used for text classification tasks, showcasing their versatility and efficacy across a wide array of applications. One notable application is in the domain of customer feedback analysis[31], where the authors demonstrate transformers’ ability to handle complex multi-label classification tasks. This research highlights how transformers, with their deep contextual understanding, can effectively categorize customer reviews into multiple relevant categories, thus providing valuable insights into customer sentiment and preferences.

The ability of transformer models to handle noisy data is crucial for maintaining their performance in real-world applications. A study titled "Transferable Post-hoc Calibration on Pretrained Transformers in Noisy Text Classification"[50]proposes post-hoc calibration techniques to fine-tune pretrained transformer models, enabling them to classify texts accurately even in the presence of noise and variability. This study demonstrates the adaptability of transformers to diverse and challenging datasets and states the importance of managing certainty in the predictions of black-box deep learning models.

Another study[33]further illustrates the effectiveness of transformer-based approaches in sentiment analysis. This study explores the nuanced capabilities of transformer models in detecting sentiment, leveraging their deep learning architecture and attention mechanism to comprehend and interpret the nuances of human emotions expressed in text. The research suggests that transformer models can accurately classify texts that express a clear opinion. However, they struggle with ambiguous and ambivalent linguistic patterns. Therefore, identifying the data presented in such situations is crucial for improving the model’s performance.

Additionally, text classification can be challenging due to the lack of labelled data, which can significantly reduce model performance. Zhanget al.[40]have proposed a solution to this problem by incorporating knowledge graphs with transformational models. The research emphasises the importance of data quality. In data-scarce scenarios, it is essential to have diverse and challenging data to help the model learn and improve further. There is a need for future research in this area, particularly regarding the application of certainty to identify points where data can be strengthened.

Together, these studies paint a comprehensive picture of the state-of-the-art in the application of transformer models to text classification, highlighting their flexibility, efficiency, and effectiveness across varied contexts and challenges.

In the development of Bayesian Neural Networks (BNNs), significant strides have been made across various domains, demonstrating their versatility and effectiveness in enhancing classification tasks through uncertainty estimation. A notable advancement is presented by Bensenet al.[2], where a hierarchical structure within BNNs is tailored for convolutional networks. This approach capitalizes on the inherent uncertainty estimation capabilities of BNNs to improve classification outcomes, particularly in complex visual tasks.
Similarly, Milaneset al.[29]showcase the application of BNNs in the biomedical field. Here, BNNs are leveraged to classify motor imagery tasks, proving particularly adept at managing the inherent noise in electroencephalogram (EEG) data, thus underscoring the robustness of BNNs in handling data with significant variability.
In the realm of image classification, the effectiveness of BNNs is further highlighted in[3].
This research emphasizes how uncertainty estimation intrinsic to BNNs can bolster prediction confidence, thereby enhancing the reliability of image classification systems. Extending this integration of Bayesian methods into convolutional neural networks (CNNs), Ferranteet al.[11]explore the incorporation of uncertainty directly into the network architecture. This integration not only improves the performance in image-based classification tasks but also provides a clearer understanding of the model’s decision-making process, making it more interpretable.

Gaussian Processes (GPs) are a powerful, flexible and widely used Bayesian non-parametric framework for modeling and inference in a wide range of domains, from machine learning and computer science to physics, engineering, and the natural sciences[35]. GPs are particularly well-suited for modeling complex, nonlinear, and multi-modal systems, as well as for handling small sample sizes and high-dimensional input spaces. At a high level, GPs provide a prior distribution over functions, which can be combined with data to obtain a posterior distribution over functions. This posterior distribution is also a GP, which can be used for a variety of tasks, such as function interpolation, extrapolation, optimization, and uncertainty quantification. One of the key advantages of GPs is that they provide a principled way to quantify and propagate uncertainty, which is especially important in applications where the data is noisy or the underlying system is not well understood.Wanget al.[44]use GPs to focus on distinguishing educational content from non-educational materials, using word embeddings generated with Word2Vec[28]. Jayashreeet al.[17]use GPs with convolutional kernels to benchmark the performance of GPs in text classification tasks for different datasets.Yeet al.[48]use SNGPs combined with focal loss for reliable dialog response retrieval.

Complementing the focus on certainty quantification, the principles of GreenAI underscore the need for environmentally sustainable practices in AI research and development[15,37]. As models grow in complexity and size, substantial computational resources are required, leading to significant energy consumption and environmental impact. Aligning with the objectives of GreenAI, the ability to assess the certainty of model outputs accurately allows for more efficient allocation of computational resources. Resources can be optimized when the model exhibits high confidence, reducing unnecessary energy consumption. Conversely, in cases of high uncertainty, the model requires additional processing or data.

The integration of certainty quantification techniques, such as BNNs or SNGPs, can enhance the interpretability and reliability of AI systems. By accurately quantifying uncertainty, these techniques can identify instances where the model is highly confident, minimizing the need for excessive computational resources and associated energy consumption. These techniques will ensure that the integration of sustainability into AI research and development aligns with pressing environmental objectives while also fostering the development of AI systems that are both reliable and energy-efficient.

In this context, the ability to quantify uncertainty in AI predictions becomes an invaluable asset. Accurately assessing certainty can optimize computational resources, reduce energy consumption, and contribute to realizing Green AI principles, enhancing the interpretability and trustworthiness of AI systems and promoting sustainable practices in AI development.

SECTION: 3Methodology

This section describes the components of the dark-pattern classification with uncertainty quantification task. We begin by describing the dark-patterns dataset used for this study. Then, we provide an overview of the model and head selection for the task.

SECTION: 3.1Data

The dataset used is the dark-patterns dataset developed by Yadaet al.[47], containing 2356 examples scraped from different websites.

It consist of a binary problem equally balanced for English language. No pre-processing is applied maintaining the case of raw text. The texts consists of examples of dark-patterns and normal patters with maximum length of 857 characters, median length 463 and min length 1 characters, with currency, Han and Hiragana symbols <1% each.

SECTION: 3.2Models

The choice of models for this comparison is grounded in their innovative contributions and varied approaches to NLP and machine learning challenges:

Dolphin-Llama2-7B-AWQ111https://huggingface.co/cognitivecomputations/dolphin-llama2-7b: An advanced model originating from the LLaMA2 architecture[41], renowned for its natural language understanding and generation capabilities, especially in conversational contexts. It is enhanced by training on the Dolphin dataset222https://huggingface.co/datasets/cognitivecomputations/dolphinto eliminate bias and alignment issues, making it particularly effective for dark-pattern detection. This model incorporates AWQ technology[24]for 4-bit weight quantization, optimizing efficiency and speed without sacrificing accuracy, highlighting its potential for rapid and precise analysis in identifying manipulative digital interfaces.

bert-large-uncased: BERT[8]is a foundational model that significantly advanced the understanding of context in language, as the first model to successfully apply Transformers at scale. The selection of its largest variant, for our study serves a dual purpose: to benchmark the evolution of model architectures over time and to ensure a comprehensive analysis by employing the most capable version.

Mistral-7B-OpenOrca-AWQ333https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-AWQ: Mistral model version quantized with AWQ method trained on Q&A OpenOrca Microsoft Dataset[23]augmented with GPT-4 and GPT-3.5. The foundational Mistral[18]model focuses on balancing computational efficiency with performance, suitable for diverse application scenarios.

mamba-370m444https://huggingface.co/state-spaces/mamba-370m-hf: Mamba[13]leverages new architecture based on selective state-space models (SSMs). Unlike Transformers, Mamba selectively retains or discards information based on the current token without attention, significantly reducing complexity from quadratic to linear with sequence length. This architectural innovation is especially relevant for analyzing complex webpage elements in dark-pattern detection, offering a promising approach.

nomic-embed-text-v1555https://huggingface.co/nomic-ai/nomic-embed-text-v1: Nomic Embed[32]innovates in embedding techniques to provide more dynamic, context-aware representations, surpassing leading models as of February 2024. Its top 25 ranking on the MTEB leaderboard666https://huggingface.co/spaces/mteb/leaderboardfor tasks critical to our project, like Semantic Search and Summarization, underscores its suitability for analyzing web content. With a compact size, a low memory usage footprint and advanced training methods, Nomic Embed efficiently processes up to 8192 tokens, making it ideal for identifying manipulative elements in extensive online materials.

It is important to note that all these models are open-source, contributing to the democratization of advanced artificial intelligence tools and fostering innovation across the field.

The quantized LLM models are quantized with Activation-aware Weight Quantization (AWQ)[24]. AWQ focuses on low-bit weight quantization (INT3/4) recognizing that all weights are not equally important. AWQ’s selective quantization preserves essential model performance while ensuring that models remain lightweight and fast, making it a key technology for the effective and efficient application of advanced AI in real-world scenarios.

Our model development strategy employs the "pre-train and fine-tune" paradigm[7,14,20], utilizing pre-trained models for fine-tuning. This stage is crucial for enhancing the models’ performance and certainty in the downstream task of identifying dark-patterns. Through this methodology, we aim to deepen our understanding of these models’ capabilities in specific real-world scenarios, focusing on the critical intersection of performance and certainty in detecting dark-patterns.

BNNs use different methods for variance reduction, two of them being the reparametrization[4]and flipout methods[45]. The flipout method has emerged as a preferable variance reduction technique over the reparameterization trick. While reparameterization effectively reduces variance for models with continuous latent variables by transforming stochastic variables into deterministic functions, its application is limited to tractable distributions. Flipout, on the other hand, introduces random perturbations to gradients within mini-batches, mimicking the effects of larger batches to stabilize training without additional computational costs. This approach not only broadens its applicability, including to discrete variables and complex distributions but also reduces intra-batch interference, making it particularly suitable for the vast and varied parameter spaces of LLMs.

In BNNs, weights are represented by probability distributions, which encode beliefs about the possible values those weights can take based on the data and prior information. To make a prediction, one must sample from these distributions, resulting in a different set of weights for each prediction. These varying sets of weights lead to a range of possible outputs for a given input, reflecting the model’s uncertainty about the most appropriate weights to use. For this reason, it is necessary to compute multiple predictions for the same inputs in order to obtain a confidence interval.

The flowchart for the methodology of this work is shown in Figure1. The DNN, SNGP and BNN classification heads are added to each model for the dark-patterns binary classification tasks. The DNN head outputs single-point predictions while the SNGP head outputs a probability distribution and the BNN head outputs multiple predictions which form a confidence interval.

SECTION: 4Results and Discussion

This study extensively explores the trade-off between performance, certainty, and sustainability of Transformer models, focusing on Dense Neural Networks (DNNs) as a baseline, Bayesian Neural Networks (BNNs), and Spectral-normalized Gaussian Processes (SNGPs) for uncertainty quantification.

For the experiments, every model is fine-tuned on the dark-patterns dataset with the three different classification heads: DNN, BNN and SNGP. We compare the model size, accuracy, F1, inference time and train and test carbon emissions measured with Codecarbon[26]. For the model tuning, 20% of the data is reserved as test, and the remaining 80% is divided into 20% validation and 80% train. The training parameters are defined in Table2.All experiments are conducted in a cloud server with an Nvidia RTX 5000 GPU with 16G VRAM.

Even though the Mistral and Llama 2 models are quantized, they remain frozen during fine-tuning since the GPU does not have enough VRAM to fit all the weights. For the remaining models, all weights are fine-tuned. Regarding the hyperparameters of the classification heads, the SNGP head has 1024 inducing points, based on the default value on the original paper[25]. For fairness of results, the DNN and BNN classification heads have a hidden layer of 1024 units, with the number of output neurons of the base model as inputs and one output neuron for the binary classification logits. For the BNN, the weight initialization parameters areandand the number of predictions used for obtaining the confidence interval is 10.

It is important to note that the main objective of this paper is not to improve the baselines on dark-pattern detection, but to use the dark-pattern classification task as a way to compare different classification heads for uncertainty quantification in transformer models.

SECTION: 4.1Trade-off Performance Analysis

After evaluating the performance of different types of models used in this study, we discovered that each model has its unique advantages and limitations based on specific accuracy and inference time metrics (see Table3). These performance metrics significantly impact determining the suitability of each model type for various practical applications.

DNNs displayed consistent accuracy across all tests, making them a reliable choice for applications where stable and predictable performance is needed. Due to their simpler architecture, DNNs also had the fastest inference times among the models tested. This makes them particularly valuable in scenarios where rapid decision-making is critical, such as in real-time systems where delay can result in inefficiencies or safety concerns.

BNNs, while offering the advantage of quantifying uncertainty, showed variability in performance, particularly in accuracy. This variability originates from their probabilistic nature, which, while providing a deeper insight into the model’s confidence levels, can lead to less predictability in outcomes. The inference times for BNNs were significantly longer than those for DNNs due to the computational overhead of managing probabilistic weights and multiple sampling to estimate uncertainty. This model type is best suited for applications where prediction confidence is more critical than prediction speed, such as in strategic decision-making environments where incorrect decisions could have high consequences.

In the case of DNNs vs BNNs, DNNs have half the number of parameters of BNNs, as BNNs require a mean and standard deviation for each weight, and can be trained easily with a cross entropy loss, while probabilistic models usually use Kullback-Leibler divergence, which is not as straigthforward. In the case of SNGPs, they use a simpler Laplace approximation to the original Gaussian process, as exact and even approximate GPs suffer from a high computational cost as they need to invert the covariance matrix to obtain the predictive Gaussian distribution. DNNs are also a good choice when the inference time requirements don’t allow for the multiple predictions of BNNs or the slightly slower predictions of SNGPs.

SNGPs were designed to balance accuracy and the quantification of uncertainty. However, their integration with larger models like Mistral resulted in reduced performance and higher variability in accuracy, as indicated in Figure3. Despite this, SNGPs maintained moderate inference times and offered valuable insights into the uncertainty of predictions, making them suitable for use in environments where reliability and detailed probabilistic understanding are necessary but where the extreme computational demands of BNNs are prohibitive.

One of the most significant findings from the performance analysis was the impact of model size on accuracy and inference times. Larger models such as Llama and Mistral had slower inference times and also showed decreased accuracy in the case of Mistral using SNGP. This result indicates the greater challenge of modelling certainty in larger models. This aspect is crucial when selecting a model for practical applications, as the benefits of larger, more complex models must be weighed against the increased resource demands and potential decrease in performance.

SECTION: 4.2Sustainability and Environmental Impact

The study demonstrates a direct correlation between the size of the Transformer models and their carbon emissions, even when LLMs have frozen layers except for the head, such as Mistral and Llama. These larger models require significantly more computational power, translating to higher energy consumption and increased carbon emissions, as depicted in Figure3. This relationship is crucial for organizations that balance performance with sustainability, as opting for smaller, more efficient models like Nomic could significantly reduce their environmental impact.

The environmental impact varies significantly across different model types during the training and inference phases. Traditional DNNs, while less computationally intensive than BNNs or SNGPs, do not offer uncertainty quantification, which might necessitate retraining or additional computational overhead in uncertain scenarios. Although BNNs provide valuable insights into model certainty, they also have a much higher energy cost due to the need to process multiple samples to estimate uncertainty. This is evident from the study’s findings, where BNNs consumed up to ten times more energy than DNNs under similar conditions.

SECTION: 4.3Comparing Uncertainty Quantification Approaches

Through the detailed review of model performance, stability, and energy consumption, we can better understand the strengths and limitations of each BNN and SNGP approach.

As shown in Table4, the analysis reveals significant differences in certainty and accuracy between SNGP and BNN models. For instance, BERT and Llama models equipped with SNGP heads showed exceptional stability with identical scores of 95.745 for the top and bottom 10% of predictions, accompanied by a near-zero mean variance (0.005). This indicates highly stable predictions across the dataset, suggesting that SNGP models maintain consistent performance even in varying data conditions. In contrast, the Mistral model with an SNGP head displayed less stability, with a notable discrepancy between the highest and lowest 10% of predictions (68.085 vs 57.447) and a significant mean variance (0.931).

When coupled with an SNGP, Nomic showcased high certainty, as both the top and bottom 10% of predictions scored very high (97.872), coupled with low variance (0.005). This reflects a robust model architecture or particularly effective training data, emphasizing the potential of SNGP to provide reliable and consistent outputs.

In general, SNGP models tend to show lower mean variances than their BNN counterparts for the same set of models, indicating more stable predictions. While BNNs may occasionally reach higher peaks of certainty or confidence in certain predictions, their performance across the dataset is comparatively less stable, marked by higher mean variances. This variability could influence the choice between SNGP and BNN depending on the need for stability in application outputs.

From an energy consumption perspective, the SNGP models generally incur similar costs to traditional models without uncertainty quantification heads, whereas BNNs can be significantly more energy-intensive. For example, the BNN classification head’s test emissions were ten times higher than those of DNNs. This higher energy demand is primarily because BNNs require multiple samples to measure certainty effectively. Note that BNN models could require even more computational resources if more points are needed to enhance certainty and reliability, potentially affecting their scalability and practical application in energy-sensitive environments.

The use of an SNGP head implies an additional energy cost of approximately 1.2% on average, which is minimal compared to the substantial ten times increase associated with BNN heads. This analysis suggests that while SNGP provides a balance between performance uncertainty stability and energy efficiency, BNNs pose challenges regarding higher operational costs and environmental impact.

SECTION: 4.4Practical Implications

One of the study’s objectives is the model’s ability to quantify uncertainty in its predictions. Table5presents high and low uncertainty instances from the Nomic SNGP model. The semantic clarity of the text examples is evident in the low uncertainty cases, where the model’s predictions are made with confidence. These examples are straightforward, containing complete phrases that convey a clear message, related to consumer behaviour or stock levels, such as "Only a few more left!" or "Hurry! Limited Quantity Available.".

On the other hand, the high uncertainty cases consist of single words like "Arthritis Aids" or "Irwin", which are semantically ambiguous without further context. This ambiguity translates into a higher variance in the model’s confidence. The presence of these high-variance cases in the table serves a critical function; it underscores the capability of the Nomic SNGP model to introspect and evaluate its certainty.

This model’s "self-awareness" has practical implications that add a layer of interpretability to the AI’s decision-making process compared to the traditional dense head layer model counterparts. This interpretability is crucial for end-users, enabling them to discern when a model’s output is reliable and when it should be treated with scepticism. This discernment is critical when considering the identification of dark-patterns. The certainty measure allows the model to signal which cases are clear-cut and ambiguous, thus avoiding the pitfall of overgeneralizing or making unwarranted assumptions based on uncertain predictions, a real-world relevance that should resonate with data scientists and AI developers.

It is important to note that the ability to measure certainty is a tool that end-users could use, and it also plays a crucial role in guiding the development of the model itself. As discussed in a recent study by Schmarjeet al.[36], having high-quality data and addressing label ambiguity is crucial for data scientists to identify gaps in the training data or areas where the model require further improvement. Knowing the level of certainty in the model’s predictions can significantly improve the learning process by analyzing instances of high uncertainty that indicate the model’s difficulties and which predictions need reevaluation or additional context.

SECTION: 5Conclusions and Future Work

This research paper focuses on enhancing the interpretability of transformer models by integrating uncertainty quantification, aimed specifically at detecting dark-patterns in user interfaces as an example of risky situations where certainty is valuable. We demonstrate that this approach can make AI systems more trustworthy without significantly compromising performance. Our study uses dense layers, Bayesian dense layers, and spectral-normalized neural Gaussian processes to achieve this goal. The evaluations across various metrics—model size, accuracy, inference time, and environmental impact—indicate that while there are trade-offs, particularly regarding computational demand and carbon footprint, the benefits of increased reliability and accountability in model predictions are profound.
While we have made progress, there are still areas to explore, particularly in detecting and mitigating bias in text-based AI applications, where dark-patterns can skew outcomes unfavourably. We suggest that uncertainty quantification methods can be adapted to identify and correct biases in training data or model predictions. Additionally, we propose using conformal prediction and distance awareness to establish confidence intervals around predictions, providing a clear statistical guarantee about their accuracy. Applying these methods to transformer models can further enhance their usability in risk-sensitive environments.

The funding for this work was provided by Funditec.

SECTION: References