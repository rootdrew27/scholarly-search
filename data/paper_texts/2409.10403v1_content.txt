SECTION: A Knowledge-Enhanced Disease Diagnosis Method Based on Prompt Learning and BERT Integration

This paper proposes a knowledge-enhanced disease diagnosis method based on a prompt learning framework. The method retrieves structured knowledge from external knowledge graphs related to clinical cases, encodes it, and injects it into the prompt templates to enhance the language model’s understanding and reasoning capabilities for the task. We conducted experiments on three public datasets: CHIP-CTC, IMCS-V2-NER, and KUAKE-QTR. The results show that the proposed method significantly outperforms existing models across multiple evaluation metrics, with an F1 score improvement of 2.4% on the CHIP-CTC dataset, 3.1% on the IMCS-V2-NER dataset, and 4.2% on the KUAKE-QTR dataset. Additionally, ablation studies confirmed the critical role of the knowledge injection module, as the removal of this module resulted in a significant drop in F1 score. The experimental results demonstrate that the proposed method not only effectively improves the accuracy of disease diagnosis but also enhances the interpretability of the predictions, providing more reliable support and evidence for clinical diagnosis.

Keywords:Knowledge Enhancement; Disease Diagnosis; Prompt Learning; BERT; Knowledge Graph

SECTION: 1Introduction

Disease diagnosis is the process of systematically identifying and confirming a patient’s illness. It involves analyzing the patient’s symptoms, medical history, physical examination results, and data obtained from various medical tests, such as laboratory tests and imaging studies. The purpose of disease diagnosis is to determine the cause of the illness, thereby guiding physicians in selecting appropriate treatment methods and increasing the patient’s chances of recovery. Disease diagnosis holds critical importance in medicine. It serves as the foundation for formulating treatment plans, allowing doctors to prescribe targeted treatment regimens and avoiding misdiagnosis and over-medication. Moreover, accurate diagnosis helps prevent further deterioration and spread of diseases, particularly in the control of infectious diseases. On the other hand, disease diagnosis can enhance the efficiency of medical resource utilization, reduce unnecessary tests and treatments, and lower healthcare costs.

Initially, people primarily relied on doctors’ experience and knowledge for diagnosis, using methods such as patient interviews, physical examinations, and laboratory tests. However, this simple and mechanical approach clearly could not produce optimal diagnostic results. To address issues of subjectivity, time consumption, and risks of misdiagnosis or missed diagnosis, knowledge engineering methods were later adopted[14]. These methods utilize rule-based matching techniques to determine text categories.However, in recent years, with the rapid development of technologies such as big data and cloud computing, the internet has experienced an explosive growth in information. The vast amount of textual data has posed significant challenges to traditional classification methods. In response to this, efforts have been made to develop new algorithms and models that integrate multi-source information, including clinical data, medical imaging, and biomarkers, to enhance the accuracy and reliability of diagnoses.For example, Luo et al.[8]proposed a new multi-modal heterogeneous graph model to represent medical data, which effectively addresses the challenge of label allocation within the same cluster. This model enables more precise diagnostic targeting, improving the accuracy of diagnosis.

In addition, with the rapid advancement of artificial intelligence and natural language processing technologies, the use of deep learning models for text classification to assist in disease diagnosis has become a significant area of research.Among them, the BERT model, as a powerful natural language processing tool, has demonstrated outstanding performance in many text classification tasks.Onan et al.[12]proposed an innovative hierarchical graph-based text classification framework, which captures the complex relationships between nodes through a dynamic fusion mechanism of contextual node embeddings and the BERT model.Although these methods have shown good performance in terms of classification accuracy and stability, they still face many challenges in practical applications. The main issues are related to poor structured reasoning and low efficiency in medical data annotation.

To address this issue, this paper proposes an integrated approach that combines pre-trained language models with knowledge graphs for extracting and processing structured knowledge from clinical texts, which is then applied to disease prediction tasks. Experimental results show that the proposed method achieves superior performance on public datasets for disease diagnosis.In addition, experiments have demonstrated that knowledge injection based on prompt learning effectively guides language models in acquiring relevant medical knowledge, thereby enhancing reasoning performance. Through this method, we achieved the conversion from raw text to high-quality semantic vectors, and mapped these vector representations to specific vocabulary using a function.This method not only enhances the semantic representation capabilities of the text but also provides rich contextual information and structured knowledge support for disease prediction, thereby improving the model’s performance and reliability in practical applications. The following sections of this paper will provide a detailed explanation of the implementation steps and experimental results.

SECTION: 2Related work

The continuous advancement of information technology has driven the development of intelligent clinical decision support systems. The widespread application of machine learning models has significantly improved the effectiveness of these systems, particularly in the field of disease prediction. The evolution of these models has gone through several stages: from the early expert rule-based models, to models based on statistical analysis and case-based reasoning, and finally to the current advanced models utilizing machine learning and deep learning techniques[6].At the same time, breakthroughs in the field of natural language processing (NLP) have introduced innovative tools and techniques, opening up new perspectives for disease diagnosis and providing unprecedented possibilities.

SECTION: 2.1Disease Diagnosis

In these early disease diagnosis methods, rule-based research primarily relied on the analysis of medical literature and case data. Expert rule-based disease diagnosis methods involved the collection of expert diagnostic experiences to form disease diagnosis pathways, thus creating expert systems. A typical example of such an expert system is the MYCIN expert system[18], developed by Shortliffe in 1976, which became a foundational model for many subsequent expert systems in the medical field.However, many techniques are unable to explain the processes involved in disease monitoring and data inference. To address this, Aami and Sarfraz[1]proposed a fuzzy rule-based diabetes classification system, which combines fuzzy logic with the cosine amplitude method. They developed two fuzzy classifiers, and the proposed model demonstrated high predictive accuracy.Sanz et al.[16]further developed a new approach based on the Fuzzy Rule-Based Classification System (FRBCS) by integrating it with Interval-Valued Fuzzy Sets (IV-FRBCS). They validated the applicability and effectiveness of this method in medical diagnostic classification problems, demonstrating its potential in improving diagnostic accuracy in complex medical scenarios.With the accumulation of large-scale clinical data, statistical analysis has become an important method for disease diagnosis. Researchers use statistical analysis to uncover potential correlations between patient characteristics and medical indicators, thereby providing new perspectives and approaches for disease prediction and diagnosis. As a result, statistically based disease diagnosis methods have become increasingly significant in medical research and practice.Lv et al.[10]developed a multi-classification and disease diagnosis model based on methods such as multinational logistic regression and discriminant analysis, focusing on factors like sleep quality. This classification model can be utilized during the early stages of disease diagnosis to categorize different diseases, aiding in the initial diagnostic process.Yadav et al.[4]discussed statistical modeling and prediction techniques for various infectious diseases, addressing the issue of single-variable data related to infectious diseases. They proposed fitting time series models and making predictions based on the best-fit model, offering a more accurate approach to forecasting the spread and development of infectious diseases.

Recently, neural network-based disease diagnosis methods have emerged and gradually become a hot topic of research. Researchers are focused on developing new algorithms and models that integrate multi-source information, such as clinical data, medical imaging, and biomarkers, to enhance the accuracy and reliability of diagnoses. These advancements aim to improve diagnostic capabilities by leveraging the power of deep learning in processing complex medical data.Wang and Li employed machine learning methods, combined with large-scale clinical databases, to develop a statistical-based disease prediction model. This model successfully achieved diagnostic predictions for multiple diseases, offering new insights into personalized disease diagnosis. Their approach represents a significant advancement in tailoring diagnoses to individual patient data.On the other hand, to address the challenge of assigning specific stages in the diagnosis of clinical diseases with long courses and staging characteristics, Ma et al.[11]proposed a gout staging diagnosis method based on deep reinforcement learning. They first used a candidate binary classification model library to accurately diagnose gout, and then refined the binary classification results by applying predefined medical rules for gout diagnosis. Additionally, some scholars have integrated diagnostic rules into deep neural networks for bladder cancer staging, significantly mitigating the drawback of cancer staging methods based on deep convolutional neural networks that tend to overlook clinicians’ domain knowledge and expertise.Additionally, to address the impact of imbalanced medical record samples on the training and predictive performance of disease prediction models, the academic community has proposed various solutions for training models on small and imbalanced datasets [11-12]. These methods aim to improve model robustness and accuracy when faced with limited or skewed data distributions, which is a common challenge in medical data analysis.

However, despite the numerous advantages brought by neural network-based disease diagnosis methods, there are still some shortcomings. First, the aforementioned methods often require large amounts of labeled data for training, and annotating medical data typically demands significant time and effort from expert physicians, making it costly and time-consuming. Second, due to the complexity and diversity of medical data, existing machine learning models may struggle to adapt well to various diseases and clinical scenarios, leading to insufficient generalization capability of the models.Through further reading and research, it was found that many scholars have proposed different improvement methods to address this challenge.For example, Luo et al.[9]first proposed a new multi-modal heterogeneous graph model to represent medical data, which helps address the label allocation challenges within the same cluster, enabling more precise targeting of desired medical information.At the same time, R.D. et al. proposed a domain knowledge-enhanced multi-label classification (DKEC) method for electronic health records, addressing the issue of previous work neglecting the incorporation of domain knowledge from medical guidelines. They introduced a label attention mechanism and a simple yet effective group training method based on label similarity[3].This method greatly improves applicability to minority (tail) class label distributions.

SECTION: 2.2Application of the BERT Model in Disease Diagnosis

BERT (Bidirectional Encoder Representations from Transformers), proposed by Google in 2018, is an advanced pre-trained natural language processing (NLP) model. This model has achieved state-of-the-art results in various NLP tasks, including but not limited to text classification, named entity recognition (NER), and question-answering systems (QA).Recently, the research community has begun exploring the potential of the BERT model in the medical field. By integrating multi-source information such as clinical texts and medical literature, the BERT model can absorb and learn rich medical knowledge, thereby providing more precise auxiliary information in the disease diagnosis process. The bidirectional contextual modeling capability of the BERT model gives it a significant advantage in understanding the complex contexts of medical texts, offering comprehensive informational support for disease diagnosis.

In the research on medical named entity recognition algorithms, Tian et al.[19]first proposed a method based on pre-trained language models. This method utilizes the BERT model to generate sentence-level feature vector representations of short text data and combines it with recurrent neural networks (RNN) and transfer learning models to classify medical short texts.Onan et al.[13]proposed an innovative hierarchical graph-based text classification framework to enhance the performance of text classification tasks. This framework effectively captures the complex relationships between nodes in the hierarchical graph through a dynamic fusion mechanism of contextual node embeddings and the BERT model.Xu et al.[20]developed a medical text classification model (CMNN) that combines BERT, convolutional neural networks (CNN), and bidirectional long short-term memory networks (BiLSTM) to address the efficiency and accuracy challenges in medical text classification. This model showed improvements in evaluation metrics such as accuracy, precision, recall, and F1 score compared to traditional deep learning models.

As advanced NLP technologies gradually penetrate the medical field, the BioBERT model has emerged. This model is specifically optimized for the biomedical domain, demonstrating significant advantages in the understanding and processing of medical texts.Sharaf et al.[17]provided a detailed analysis and overview of a systematic BioBERT fine-tuning method aimed at meeting the unique needs of the healthcare field. This approach includes annotating data for medical entity recognition and classification tasks, as well as applying specialized preprocessing techniques to handle the complexity of biomedical texts.

Recently, pre-trained language models have achieved significant success in many question-answering tasks[19-20]. However, while these models encompass a wide range of knowledge, they perform poorly in structured reasoning. Additionally, considering the previously mentioned issues, such as the low efficiency of medical data annotation, these limitations pose further challenges.This paper aims to enhance structured reasoning using knowledge graphs and leverage the BERT model’s outstanding performance in text classification tasks. The primary focus of this research is on how to effectively utilize language models and knowledge graphs for reasoning, thereby achieving the goal of significantly improving disease prediction accuracy.

SECTION: 3Task formulation

Given clinical information and electronic medical records of a patient, whererepresents the-th word in the text andrepresents the total number of words, this research aims to predict the disease typethat the patient has based on the content of.Therefore, this task can be represented as learning a modelwith parameters.Given an inputoutputs a predicted result. Here,is the set of labels for all candidate disease types.

This paper utilizes prompt learning to accomplish the task, converting it from a classification problem into a language modeling problem. The original classification problem is formulated as fitting. The transformed language modeling problem is formulated as fitting, whererepresents the prompt template used to encapsulate the original text into a new input.

SECTION: 4Methodology

SECTION: 4.1Knowledge Retrieval

Clinical texts contain many conceptual and structured forms of knowledge, such as symptoms, diagnoses, and treatment plans. Knowledge retrieval is capable of identifying and extracting key medical concepts and relationships.For example, “Type 2 diabetes” is a common chronic disease, characterized primarily by persistently elevated blood glucose levels. The diagnostic criteria include fasting blood glucose and HbA1c levels. Common symptoms include excessive thirst, hunger, frequent urination, weight loss, and fatigue.Through knowledge retrieval, it can extract structured knowledge such as causes, related complications, risk factors, and more. This type of knowledge plays an important auxiliary role in disease prediction. Therefore, this paper utilizes the following methods to achieve knowledge retrieval from the text:

Given the clinical textof a patient, we processby performing tasks such as tokenization and part-of-speech tagging, resulting in a set of vocabulary. First, we apply the named entity recognition (NER) modelto identify the set of entities.

Let the knowledge graph be, where the entity set is. V may include entities such as “Type 2 diabetes,” “hyperglycemia,” “insulin,” “weight gain,” etc. The edge set E may include relationships such as “is symptom,” “used for treatment,” “causes,” and others.Next, for each entityextracted from the text, we find the corresponding entityin the knowledge graph. The key to this matching process is determining whetherandrepresent the same or similar concepts. This process can be implemented using a similarity function. The entitiesandare considered a match whenreaches a certain threshold. When the conditionsandare satisfied, they are considered to represent the same or related entities.

Next, for each pair of matched entities, we search for all possible reasoning pathsin the knowledge graph. Relationships in the knowledge graph can be represented in the form of triples, whereare entities, andis the relationship. The goal of inference is to find possible paths P from entityto.

For each pair of matched entities, collect all possible reasoning paths, thereby obtaining structured knowledge between entities extracted from clinical texts. This knowledge is then utilized for further analysis and applications, such as disease prediction and relationship inference.For example, the entity extracted from the text“type 2 diabetes”, and the next step is to find the matching entity in the knowledge graph“Type 2 Diabetes”. If the similarity function, they are considered a match.Finding the path P: (“Type 2 Diabetes”, “causes”, “High Blood Sugar”)(“High Blood Sugar”, “leads to”, “Kidney Disease”). Next, collect and record the path, constructing the reasoning chain from “Type 2 Diabetes” to “Kidney Disease”.

Next, we will represent the structured knowledge along these reasoning paths.

SECTION: 4.2Knowledge Representation

The reasoning paths from the collection are concatenated into a single text sequence. This concatenated text is then represented as a vectorusing a model.

(1) Reasoning Path Representation

Each reasoning pathis converted into a readable text sequence, with each path consisting of a series of triples. These triples can be transformed into text, where each triple is expressed in the form of “Entity 1 reaches Entity 2 through Relationship.”For example, given a reasoning path, it can be converted into the text: “Entityreaches Entitythrough Relationship, andreaches Entitythrough Relationship. ”Assuming the knowledge graph contains the following triples: (“Type 2 Diabetes”, “causes”, “High Blood Sugar”), (“High Blood Sugar” , “leads to” , “Kidney Disease”), the conversion to text would be: “Type 2 Diabetes reaches High Blood Sugar through causes, and reaches Kidney Disease through leads to.”

After converting the path into text, the pre-trained BERT modelis used to transform the textinto a vector representation.

(2) Text Vectorization Model Selection (BERT)

The input textis first tokenized by BERT, splitting the input text into smaller subword units. For example, the input text “Type 2 diabetes passed causes to high blood sugar, pass leads to Kidney disease” might be tokenized into subwords like “Type 2”, “diabetes”, “pass”, “causes”, “to”, “Kidney disease”, “to”, and so on.Subsequently, each token is converted into an ID from BERT’s vocabulary. After tokenization, the next step involves encoding, where the token ID sequence is transformed into embedding vectors. These embedding vectors are then fed into BERT’s multi-layer Transformer encoder, effectively capturing the contextual relationships and dependencies between the tokens. The formula for this process is as follows:

Where:

is the input text sequence.

is the text vector representation output by the BERT model.

The functiontransforms the text sequenceinto its vector representation. Here,contains the semantic information of the input text.

The BERT model outputs the vector representation of the textas:

Here,is the vector representation obtained by processing the input text sequencethrough the BERT model. It captures the semantic information of the input text.M is an additional mapping function that takesas input and further processes it.is the final text vector representation obtained after processing by the mapping function M.

(3) Output Vector Extraction

In the text vectorization model selection mentioned earlier, we used the BERT model to convert the input text sequenceinto the vector representation. During this process, we typically choose to use the special token[CLS], and its corresponding vector is used as the overall representation of the entire input text. This comprehensive representation is denoted as.

SECTION: 4.3Prompt Template

(1) Construction of Prompt Template

To effectively utilize the BERT model for text vectorization, we first need to preprocess the original input text x to construct a proper prompt template Templatethat meets the input format requirements of the BERT model.This prompt template will generate a textthat contains special tokens and specific vocabulary.The textcontains special tokens such as [MASK], soft tokens, and other relevant vocabulary. These elements help the model focus on specific parts of the input and guide the prediction process, enhancing the performance of tasks like classification or inference.

For example:

Original input text: “Type 2 diabetes is a chronic metabolic disease.”

Prompt template Template: “Type 2 diabetes is a[MASK]type[SOFT]disease.”

Generated preprocessed text: “Type 2 diabetes is a[MASK]type[SOFT]disease.”

(2)Expression of Template Conversion

Here,represents either words from the original text or other words added based on the context. Suppose we want to convert the original text x into the template. This can be achieved by inserting special tokens and soft labels. A specific example is as follows:

Original input text: “Type 2 diabetes is a chronic metabolic disease.”

Converted preprocessed text: “Type 2 diabetes is a[MASK]type[SOFT]disease.”

It can be represented as:

(3) Vector Representation and Processing

Using the pre-trained model M , the preprocessed textis transformed into a set of vectors. Each vectorcorresponds to a word or token in the preprocessed text, where:

is the vector corresponding to the soft token.

is the vector corresponding to the mask token.

For example, for the original input text: “Type 2 diabetes is a chronic metabolic disease.”, the generated vector set can be explained as follows:

: The vector corresponding to the first word in the original text, “Type 2 diabetes”.

: The vector corresponding to the soft token.

: The vector corresponding to the second word in the original text, “is a”.

: The vector corresponding to the mask token.

: The vector corresponding to the third word in the original text, “disease”.

Other vectors follow similarly, with each vector representing the respective words or tokens in the preprocessed text.

(4) Vector Processing

The formula for replacing the originalvector with the average ofand other related vectorsis as follows:

Where:is the updatedvector,

is the original soft token vector,

represents related vectors (e.g., corresponding to related tokens or words),

is the number of related vectors used in the calculation.

Finally, after replacing the originalwith, we obtain the updated vector set:, which will be used in the next steps of the model processing.

SECTION: 4.4Prediction

The textcontaining the mask and the new soft token is input into a pre-trained language model for forward inference. This produces representations for each token. The representation at the mask position is then used for a verbalizer prediction.

Given the modified input, it is formalized as:

Feedinto the pre-trained language model, and perform forward inference to obtain the vector representations for each token:

Extract the representationof the mask token from the model output.

Use a verbalizer functionto mapto a specific vocabulary term. The Verbalizeris a mapping from the vector space to the vocabulary, typically used to convert the model’s predicted vector into a specific word.

The specific steps are as follows:

(1) Calculate the similarity betweenand the embedding vectors of each word in the vocabulary to obtain a probability distribution. For example, if the embedding vector for each word in the vocabulary is, then the probability distribution is given by:

(2) Based on the computed similarities, select the word with the highest probability as the prediction result.

SECTION: 5Experimental settings

SECTION: 5.1Datasets

In this paper, experiments were conducted on the CHIP-CTC, IMCS-V2-NER, and KUAKE-QTR datasets. CHIP-CTC, one of the experimental datasets, originates from a bench-marking task released at the CHIP2019 conference. All text data is sourced from real clinical trials, including 22,962 entries in the training set, 7,682 entries in the validation set, and 10,000 entries in the test set. The dataset is available athttps://github.com/zonghui0228/chip2019task3.

The IMCS-V2-NER dataset, used as another experimental dataset, comes from the Named Entity Recognition task in the IMCS2 dataset developed by the School of Data Science at Fudan University. It includes 2,472 entries in the training set, 833 entries in the validation set, and 811 entries in the test set. The dataset is available athttps://github.com/lemuria-wchen/imcs21.

The KUAKE-QTR dataset, used as another experimental dataset, includes 24,174 entries in the training set, 2,913 entries in the validation set, and 5,465 entries in the test set. The dataset is available athttps://tianchi.aliyun.com/dataset/95414.

SECTION: 5.2Baseline

When evaluating our proposed method, we established a comprehensive set of baseline models to ensure rigorous and fair comparison. These baselines were selected to represent robust benchmarks in the field, including both traditional machine learning algorithms and advanced deep learning techniques.

SVM: Support Vector Machines (SVM) differentiate between classes by finding the optimal hyperplane, effectively handling both linearly separable and non-linearly separable problems. This classifier uses kernel techniques to process text data, enabling efficient identification and classification of complex text patterns in tasks such as sentiment analysis and spam detection.

CNN: Convolutional Neural Networks (CNNs) extract local features from text data using convolutional layers in text classification tasks. These local features capture key semantic information within sentences or documents, helping the model understand and classify the text content. This ability of CNNs is particularly well-suited for scenarios where quick and effective identification of key information from text is required.

RNN: Recurrent Neural Networks (RNNs) are particularly well-suited for text classification tasks due to their ability to process sequential text data and capture long-term dependencies between words. This allows RNNs to understand contextual information within the text, leading to more accurate predictions of text categories, such as sentiment or topic classification.

BiLSTM: Bidirectional Long Short-Term Memory (BiLSTM) networks combine both forward and backward LSTMs, allowing them to consider both preceding and succeeding contextual information in the text. This structure makes BiLSTMs particularly effective for text classification tasks as they can capture temporal dependencies in the text data comprehensively. By doing so, BiLSTMs provide a deeper semantic understanding, resulting in higher accuracy across various text classification scenarios.

Attention: In text classification tasks, the Attention mechanism enables the model to focus on key parts of the text, enhancing its understanding of the overall content’s importance. By assigning different weights to various words or phrases, it emphasizes the information most influential for the classification task.

BiRNN: Bidirectional Recurrent Neural Networks (BiRNN) combine both forward and backward RNNs, allowing them to consider contextual information from both directions in a text sequence. This structure enables BiRNNs to better capture long-term dependencies in text data, thereby enhancing classification performance. It is particularly well-suited for tasks that require a comprehensive understanding of text semantics, such as sentiment analysis and topic classification.

BERT: BERT (Bidirectional Encoder Representations from Transformers) plays a crucial role in text classification tasks. By utilizing pre-trained models to understand the semantics and context of text, and through fine-tuning techniques, BERT can capture rich textual features and improve classification accuracy. Its bidirectional encoding and context-aware capabilities enable BERT to excel across various text classification scenarios.

The inclusion of these baselines aims to provide a clear reference point for evaluating the performance of our new method. By comparing with these established models, we aim to demonstrate the advancements of our approach relative to existing solutions. Bench-marking against these well-established models helps highlight scenarios where our method offers superior performance, thereby validating its effectiveness and efficiency in addressing the problem at hand.

SECTION: 6Results and Analysis

This section presents our experimental results and provides an analysis of these results. To ensure the reliability of the experiments, we repeated each experiment three times and used the average values as the final results.

SECTION: 6.1Comparison Experiments

The results indicate that as the complexity of the models increases, the accuracy improves accordingly across different datasets. SVM, as a classical machine learning model, can handle linearly separable data, but it struggles to capture more complex relationships when datasets have intricate features and patterns. CNN performs better than SVM on the CHIP-CTC, IMCS-V2-NER, and KUAKE-QTR datasets, as it excels at extracting local key information compared to SVM.The experimental results indicate that although the accuracy improves compared to SVM, the overall performance remains relatively low. CNNs show limited effectiveness in handling natural language tasks, particularly in capturing long-range dependencies. The Attention mechanism proves to be more effective at focusing on crucial parts of the input sequence, thereby enhancing the model’s performance with sequential data. Compared to CNNs, Attention provides greater flexibility in highlighting key aspects of the data, resulting in improved accuracy. Additionally, BiRNNs exhibit significantly improved performance across various datasets. Unlike traditional RNNs, BiRNNs utilize two separate RNNs—forward RNN and backward RNN—to process the input sequence, which contributes to their superior performance.BiRNN addresses the limitation of traditional RNNs, which can only utilize past context and cannot leverage future context. By incorporating bidirectional processing, BiRNNs offer enhanced context understanding and feature extraction capabilities. The bidirectional feature extraction enables BiRNNs to better capture complex patterns and semantic relationships within sequences, thus improving the model’s predictive performance.

In comparison, BiLSTM further enhances these capabilities by employing LSTM units with gating mechanisms that effectively mitigate the vanishing gradient problem. This allows BiLSTM to capture long-range dependencies and bidirectional context more effectively. Experimental results demonstrate that BiLSTM achieves higher accuracy than BiRNN, highlighting its advantages in handling complex sequence processing tasks.Compared to BiLSTM and other models, BERT leverages the self-attention mechanism in the Transformer architecture to simultaneously consider both left and right context information. Trained on extensive corpora and fine-tuned for specific tasks, BERT’s multi-layer self-attention mechanism captures deeper and more nuanced features. This results in superior feature extraction capabilities compared to RNNs, BiRNNs, and BiLSTMs, making BERT more effective in understanding and processing complex textual data.

From the last row of the table, it is evident that our method achieved the best performance across all four datasets. This demonstrates the feasibility and effectiveness of our approach. Furthermore, it highlights the significant impact of integrating medical knowledge into the model through the prompt-learning framework, which notably enhances model performance.

SECTION: 6.2Ablation Study

This section aims to analyze the effectiveness of each module in our proposed method. To achieve this, we first remove the knowledge representation component from our method.As seen in the first row of Table 3, removing the knowledge representation results in a significant drop of 0.2 in the F1 score. This is because knowledge representation is a core component of our method, which injects structured knowledge obtained from the knowledge graph into the language model, thereby enhancing the model’s understanding of medical domain knowledge and improving performance on medical diagnostic tasks.

Subsequently, we modified the prompt template, changing it to: “The characteristics of Type 2 Diabetes include [MASK] nature, and it is also manifested as [SOFT] disease.”From rows 2 to 4 of Table 3, it can be observed that the prompt template used in our method achieves the best performance, while other prompt templates do not perform as well. We believe this may be due to the presence of excessive or irrelevant words in the prompt templates, which could introduce noise into the language model’s reasoning process.

Finally, we experimented with the impact of different prediction words on the model’s inference results. As shown in the last three rows of the table, inappropriate prediction words can severely mislead the model’s reasoning, leading to a significant decrease in the final disease diagnosis results.

SECTION: 6.3Model Interpretability Study

This section aims to analyze the interpretability of the model. Unlike general text classification tasks, in disease diagnosis tasks, users are additionally concerned with the interpretability of the results. An unexplainable prediction result is unacceptable to users. Therefore, we conduct an analysis of the model’s interpretability, using clinical case examples from Table X.It can be observed that during the knowledge retrieval phase, the following knowledge was extracted from the clinical case: “Type 2 diabetes is a common chronic disease,” “its main feature is persistently elevated blood sugar levels,” “polydipsia, polyphagia, and polyuria are common symptoms,” and “high blood sugar can lead to kidney disease.” From this knowledge, we can identify the key pieces of information from the clinical case that are related to the final prediction result. In other words, we can clearly understand which pieces of knowledge led the model to make the current prediction.

SECTION: 7Conclusion

In this paper, we propose a knowledge-enhanced disease diagnosis method based on a prompt learning framework. This method leverages an external knowledge graph to retrieve relevant knowledge from clinical cases and then encodes this structured knowledge into prompt templates. By incorporating this encoded knowledge, the language model’s understanding of the task is improved, resulting in more accurate disease diagnosis outcomes. Experimental results demonstrate that the proposed method effectively enhances the performance of language models in disease diagnosis tasks. Additionally, the model exhibits strong interpretability, providing users with supporting evidence related to the diagnostic results.

In the future, we will explore additional methods for knowledge injection. Additionally, we plan to investigate more advanced knowledge editing techniques to integrate medical knowledge into the reasoning process of language models.

SECTION: References