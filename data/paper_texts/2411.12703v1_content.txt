SECTION: Strengthening Fake News Detection: Leveraging SVM and Sophisticated Text Vectorization Techniques. Defying BERT?
The rapid spread of misinformation, particularly through online platforms, underscores the urgent need for reliable detection systems. This study explores the utilization of machine learning and natural language processing, specifically Support Vector Machines (SVM) and BERT, to detect news that are fake. We employ three distinct text vectorization methods for SVM: Term Frequency Inverse Document Frequency (TF-IDF), Word2Vec, and Bag of Words (BoW), evaluating their effectiveness in distinguishing between genuine and fake news. Additionally, we compare these methods against the transformer large language model, BERT. Our comprehensive approach includes detailed preprocessing steps, rigorous model implementation, and thorough evaluation to determine the most effective techniques. The results demonstrate that while BERT achieves superior accuracy with 99.98% and an F1-score of 0.9998, the SVM model with a linear kernel and BoW vectorization also performs exceptionally well, achieving 99.81% accuracy and an F1-score of 0.9980. These findings highlight that, despite BERT’s superior performance, SVM models with BoW and TF-IDF vectorization methods come remarkably close, offering highly competitive performance with the advantage of lower computational requirements.

SECTION: 
The expansion of fake news has emerged as a significant threat to public opinion and societal stability. Rapid dissemination of misinformation via social media and online platforms has led to crises affecting political decisions, public health, and social harmony. Research indicates that fake news spreads significantly faster and more widely than true news, often leading to real-world consequences. For example, during the 2016 U.S. Presidential Election, fake news stories were more engaging on Facebook than real news. According to the Pew Research Center, 64% of Americans feel false news has seriously misled them about current affairs. The massive amount of data produced daily renders conventional hand fact-checking methods insufficient. Apart from confusing the people, false news can motivate violence and compromise the democratic framework. Fake news spread over WhatsApp has been linked to political unrest and societal instability in Bangladesh as well as mob violence and lynchings in India. During the COVID-19 pandemic, false information about the virus and vaccinations seriously threatened public health. These events show how crucial it is to have trustworthy mechanisms in place to spot false news.
Consequently, automated systems that can identify bogus news utilizing artificial intelligence (AI) and machine learning (ML) techniques are in more demand. By fast analyzing large datasets and spotting trends implying the inclusion of false information, artificial intelligence (AI) tools can dramatically increase the accuracy and effectiveness of spotting fake news.

This work provides a fresh natural language processing method based on Support Vector Machines (SVM) combined with three text vectorizing techniques and a transformer LLM model—more especially, BERT-base—that precisely labels news items as either fake or real. By means of Term Frequency-Inverse Document Frequency (TF-IDF), Word2Vec, and Bag of Words (BoW) approaches, the paper aims to raise the accuracy and dependability of false news detection systems. By contrasting these vectorizing techniques with BERT and proving the effectiveness of SVM classifiers, this study aids to the preservation of information integrity and the fight against false information in the digital world.

The research presents significant contributions and originality in the following aspects:

It compares BoW, TF-IDF, and Word2Vec vectorization methods for SVM classifiers, providing insights into their performance for fake news detection.

It analyzes the marginal benefits of the RBF kernel over the linear kernel in SVM for fake news detection. The research utilizes a preprocessed version of the ISOT fake news dataset, ensuring the practical relevance and applicability of the findings.

The paper contrasts SVM with the BERT base model, highlighting their respective advantages and limitations, particularly in resource-constrained environments.

SECTION: 
Several studies have explored various approaches for fake news detection. Shu et al.reviewed data mining methods and challenges involved in detecting fake news, emphasizing the need for comprehensive techniques. Zhang et al.investigated linguistic features for fake news detection and proposed a composite model combining LSTM and CNN, achieving an accuracy of 92.4%. Rashkin et al.utilized linguistic cues, showing that stylistic features can indicate deceptive content. Pérez-Rosas et al.explored lexical, syntactic, and semantic features, reporting an F1 score of 0.78, highlighting the importance of feature diversity. Wangintroduced the LIAR dataset and developed several machine learning models, with logistic regression achieving an accuracy of 24.7%, underscoring the challenges in fake news identification. Ruchansky et al.presented the CSI model, integrating user behavior analysis with content analysis, achieving an accuracy of 89.2%. Dhruv Khattar et al.developed a multi-modal variational autoencoder integrating text and image features, resulting in an accuracy of 94.1%. Bhardwaj et al.proposed a composite approach combining CNN and BiLSTM, achieving an accuracy of 96.7%. Shu and Bin Wuintroduced a time-evolving graph neural network, capturing the dynamic nature of news dissemination, with notable performance improvements. Alnabhan and Brancotested deep learning models for cross-domain false news identification. Karim et al.explore the effectiveness of larger BERT-based models in streamlining the classification of ADHD-related concerns through knowledge distillation, demonstrating the potential of simplified models without sacrificing performance. Their work highlights the adaptability of BERT for nuanced tasks, which could inform advancements in domains like fake news detection by leveraging sophisticated model architectures and text representations. Combining word embeddings with linguistic features, Verma et al.presented WELFake, which greatly raised accuracy and F1-score. By means of a benchmark for assessing the efficacy of several approaches, Kanavos et al.performed a comparative analysis of machine learning algorithms and text vectorizing techniques for false news identification. Using SVM classifiers with various text vectorizing techniques—including TF-IDF, Word2Vec, and BoW—this work expands on these foundations to identify the best efficient text representation method for spotting bogus news.

By using SVM classifiers with various text vectorizing techniques, including TF-IDF, Word2Vec, and BoW, this work builds on these foundations to ascertain the most efficient text representation method for spotting bogus news. This thorough comparison seeks to improve the dependability and accuracy of false news detecting systems.

SECTION: 
This part covers in great detail the approaches to create the false news detecting mechanism. Data collecting, preprocessing, text vectorizing, and Support Vector Machine (SVM) and BERT based categorization is part of the method.

SECTION: 
Designed by the Information Security and Object Technology (ISOT) Research Group at the University of Victoria, Kaggle’s ”Fake and Real News Dataset”provides the dataset used in this research effort. Labeled as either true or false, this dataset comprises thorough news items of political news, world news, government news, Middle-Eastern news, US news. While the bogus news items came from many websites highlighted by Politifact and Wikipedia, the actual news items were gathered from Reuters.com. Our preprocessed data contains 21,477 real news items and almost 23,421 false news items. Fake news items are denoted with a ”0,” while actual news items have a ”1.” The dataset offers a large base for study since it covers a wide spectrum of news subjects. To guarantee the best quality for later study, the data is carefully cleaned and preprocessed. The distribution of real and false news items in both the training and test datasets is ”Train Fake” with 18,796 articles, ”Train Real” with 17,121 pieces, ”Test Fake” with 4,624 posts, and ”Test Real” with 4,356 articles.

Preprocessing involves several steps to clean and prepare the text data for analysis. Initially, the text is cleaned by removing stopwords and unwanted characters and then tokenized. The process utilizes thelibrary for tokenization, which breaks down the text into individual words while removing stopwords and words shorter than three characters. Specifically, the steps include removing stopwords using the NLTK library’s predefined list of English stopwords and tokenizing the text with thefunction that converts the text into a list of lowercase tokens and removes punctuations and words shorter than three characters, and finally reassembles the tokenized lists back into cleaned text strings. For BERT training, the dataset was tokenized using BertTokenizer from pre trained BERT-base-uncased model.

The 3D t-SNE plot presented inoffers a more nuanced and comprehensive view of the dataset by adding an additional dimension to the visualization. This three-dimensional perspective enhances our ability to perceive the complex relationships and interactions between data points that might not be as apparent in the 2D plot. The colors in this 3D plot represent different clusters, each potentially corresponding to distinct groups of fake or real news articles. This clustering is indicative of the inherent structure within the dataset and supports the hypothesis that the features used in our model are effective in capturing the distinctions between the two classes. By examining these clusters, we can gain deeper insights into the data’s distribution and the model’s potential to classify new, unseen data accurately. The 3D t-SNE plot thus not only validates the separability observed in the 2D plot but also highlights the complex, multi-dimensional nature of the data, reinforcing the robustness of our chosen vectorization and classification techniques.

SECTION: 
Text vectorization transforms textual data into numerical vectors. This study employs three techniques: TF-IDF, Word2Vec and BoW.

This reflects the importance of terms within a document relative to a corpus. This technique mitigates the limitations of BoW by considering the inverse document frequency, thus reducing the weight of common terms and highlighting unique terms. The mathematical representation is:

whererepresents the total number of documents, anddenotes the number of documents that include the term.

Word2Vec generates dense vector representations for words by training on a large corpus. This technique captures semantic relationships, allowing words with similar meanings to have similar vector representations. This study uses the continuous bag-of-words, also known as CBOW architecture which is used to predict words within a context window, thus embedding semantic meaning into the vectors.

BoW converts text into a matrix of token counts. This method, while straightforward, is effective for capturing word frequency, though it may miss context and semantics. We apply this method to establish a baseline for comparison with more sophisticated techniques.

SECTION: 
Support Vector Machines (SVMs) are utilized for news article classification using both linear and radial basis function (RBF) kernels.

The linear kernel is employed to classify the vectorized text. The objective of the SVM is to identify a hyperplane that optimally separates data points into different classes. Considering a set of training examples, whererepresents the feature vector anddenotes the class label, the SVM solves the optimization problem:

whereis the weight vector,is the bias,is the regularization parameter, anddictates the number of training examples.

Additionally utilized for vectorized text classification is the RBF kernel. Formulated as: the Radial Basis Function (RBF) kernel is:

where the influence of a single training example is determined by.

Figureshows the method for creating a fictitious news detecting system. Data collecting from several sources comes first, then data cleansing to eliminate extraneous material. Then preprocessing uses stemming, stopword elimination, and tokenizing to change the text. Bag of Words (BoW), TF-IDF, or Word2Vec help to vectorize the text into numerical representations. Training and testing sets separate the data; models including Support Vector Machines (SVM) and BERT are trained to categorize bogus and legitimate news items. To guarantee the efficiency of the detection system, model evaluation is lastly conducted evaluating accuracy, precision, recall, and F1 score.

SECTION: 
The results show that the SVM classifiers—especially with BoW and TF-IDF vectorizing methods—perform remarkably well in spotting bogus news. Demonstrating its efficiency for this work, the BoW method with a linear kernel produced the best accuracy and F1 score. Lastly, the training and analysis of BERT base are also demonstrated.

SECTION: 
The TF-IDF vectorizing method performed attaching an accuracy of 99.52% and an F1 score of 0.9949, the SVM classifier running a linear kernel These findings, shown in Table, show that TF-IDF is likewise a useful technique for text vectorizing in false news identification.

: The confusion matrix shows how well the classifier properly and erroneously classified events.

SECTION: 
The Word2Vec vectorization technique resulted in a lower performance compared to BoW and TF-IDF. The SVM classifier with a linear kernel attained an accuracy of 96.54% and an F1-score of 0.9644. Although Word2Vec captures semantic meanings of words, it appears that for this particular task, BoW and TF-IDF are more effective. The outcomes are detailed in Table.

SECTION: 
Employing the Bag of Words (BoW) vectorization technique, the SVM classifier with a linear kernel attained an accuracy of 99.81% and an F1-score of 0.9980. These high metrics demonstrate that the model excelled in differentiating between fake and real news articles. The detailed results are presented in Table.

In, we can observe how the plots illustrate the support vectors identified by the SVM with a linear kernel. These vectors are critical as they define the optimal hyperplane that separates the fake news from the real news in the feature space. Moreover, in, the ROC curve shows the true positive rate against the false positive rate for different threshold values. The area under the curve (AUC) of 1.0 indicates perfect classification by the SVM model with a linear kernel.

SECTION: 
Additionally, the SVM classifier with a radial basis function (RBF) kernel was evaluated for all three vectorization techniques. The results show a slight improvement over the linear kernel in some cases.

The SVM classifier utilizing the BoW technique combined with an RBF kernel attained an accuracy of 99.62% and an F1-score of 0.9961, as illustrated in Table. These results highlight the effectiveness of the RBF kernel in improving the model’s performance. The slight improvement in accuracy and F1-score indicates that the RBF kernel can better capture the non-linear relationships in the data, enhancing the classifier’s ability to distinguish between fake and real news articles.

Tableshows the SVM classifier employing TF-IDF vectorization with an RBF kernel obtaining an accuracy of 99.31% and an F1-score of 0.9928. This shows a great degree of recall and accuracy, therefore proving the strength of the TF-IDF and RBF kernel mix. The findings imply that this method provides a reliable means of differentiating between fake and legitimate news sources since it successfully catches the subtleties in the textual data.

As shown in Table, ultimately the SVM classifier using Word2Vec with an RBF kernel achieved an accuracy of 97.75% and an F1-score of 0.9767. This shows that although Word2Vec efficiently detects semantic links, its performance is somewhat lower than BoW and TF-IDF, suggesting that the choice of vectorizing algorithm greatly influences the capacity of the model to distinguish between fake and true news.

Our results imply that, when combined with strong classifiers such SVM, simpler vectorizing strategies like BoW and TF-IDF can beat more sophisticated algorithms like Word2Vec in fake news identification. This is probably related to the need of word frequency in spotting false news. Though its computing cost may not be justified in many situations, the RBF kernel shown modest gains over the linear kernel in some cases, suggesting its usefulness in addressing non-linear data relationships. For identifying false news, SVM classifiers mixed with BoW or TF-IDF proved generally quite successful. Future work might investigate hybrid models using several vectorizing techniques.

SECTION: 
We matched the results of our SVM model with a BERT base model educated on the same fake news dataset in order to assess its performance even more. In three epochs, the BERT basic model attained outstanding performance with 99% accuracy and f1 score.

The remarkable performance of the confusion matrix in textbfFigurefor the BERT base model in fake news identification reveals With only a few misclassifications, the matrix indicates an incredibly high number of accurate classifications for both actual and fake news events. With just 7 false positives and 19 false negatives, the model specifically successfully finds 4689 actual news items and 4265 erroneous news items. This great degree of accuracy emphasizes how strong and dependable the BERT base model is for separating real from fraudulent news. The low number of misclassifications suggests that the model is well-tuned and able to manage the complexity and subtleties in the dataset.

This analysis highlights generally how well SVM and BERT models address the problem of fake news detection. While the BERT model gives great accuracy and robustness at the expense of higher processing needs, the SVM model offers a lightweight and fast alternative. Future studies might investigate hybrid techniques combining the advantages of several models to improve detection accuracy and robustness even more.

SECTION: 
The widespread problem of false news seriously jeopardizes the integrity of knowledge sharing and society’s stability. This research has explored the efficacy of Support Vector Machines (SVM) coupled with various text vectorization methods—namely Bag of Words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), and Word2Vec—in the detection of fake news. Our comprehensive experiments demonstrate that SVM classifiers, particularly those utilizing BoW and TF-IDF, deliver outstanding performance in distinguishing fake news from genuine news. The SVM classifier with a linear kernel and BoW vectorization achieved the highest accuracy of 99.81% and an F1-score of 0.9980, illustrating its robustness and effectiveness. The TF-IDF vectorization method also performed exceptionally well, achieving an accuracy of 99.52% and an F1-score of 0.9949. Although the Word2Vec technique, which captures semantic relationships, showed slightly lower performance (96.54% accuracy and 0.9644 F1-score), it remains a valuable tool in the text analysis domain.

The application of the Radial Basis Function (RBF) kernel provided marginal improvements over the linear kernel for some vectorization techniques, indicating its potential for capturing non-linear relationships in the data. However, the computational complexity associated with the RBF kernel may not be justified given the minimal performance gains observed.

Furthermore, this study compared the performance of SVM models with the BERT base model, which exhibited slightly superior performance, particularly in terms of precision and recall, achieving perfect scores across all metrics. Despite the BERT base model’s high computational requirements, its exceptional performance underscores the potential of advanced deep learning techniques in fake news detection.

Overall, this study highlights the efficacy of straightforward vectorization techniques such as BoW and TF-IDF when paired with SVM classifiers for fake news detection. The findings suggest that robust and efficient models can be developed using these methods, contributing significantly to the automated identification of false information in news articles. Future research could investigate hybrid models that integrate the strengths of SVM and deep learning approaches, as well as additional features such as user behavior and network analysis, to further enhance the accuracy and reliability of fake news detection systems. By advancing the methodologies for detecting fake news, this study aims to support efforts in maintaining the credibility of information and curbing the spread of misinformation in the digital age.

SECTION: References