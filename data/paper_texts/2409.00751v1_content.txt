SECTION: Self-Supervised Vision Transformers for Writer Retrieval
While methods based on Vision Transformers (ViT) have achieved state-of-the-art performance in many domains, they have not yet been applied successfully in the domain of writer retrieval. The field is dominated by methods using handcrafted features or features extracted from Convolutional Neural Networks. In this work, we bridge this gap and present a novel method that extracts features from a ViT and aggregates them using VLAD encoding. The model is trained in a self-supervised fashion without any need for labels. We show that extracting local foreground features is superior to using the ViT’s class token in the context of writer retrieval. We evaluate our method on two historical document collections. We set a new state-at-of-art performance on the Historical-WI dataset (83.1% mAP), and the HisIR19 dataset (95.0% mAP). Additionally, we demonstrate that our ViT feature extractor can be directly applied to modern datasets such as the CVL database (98.6% mAP) without any fine-tuning.

SECTION: Introduction
Writer retrieval involves systematically extracting documents, written by the same author as a queried document, from a large corpus of handwritten texts with unidentified authors. Closely related to this, writer identification aims to identify the writer of a queried document by consulting a corpus of labeled documents. In historical research, these processes are crucial for categorizing and examining manuscripts based on authorship, particularly when manuscripts lack signatures. In the forensic sciences, accurately identifying authors in handwritten notes or documents is essential in criminal investigations, such as verifying ransom notes, anonymous threatening letters, or fraudulent documents.

Recently, methods employing Vision Transformers (ViT)have achieved state-of-the-art performance in various computer vision tasks, including handwritten text recognition.
However, ViTs are prone to overfitting even on large datasets, making them challenging to train.
Self-supervised learning can help to address this challenge, as no annotations are required for training and, even when annotations are available, self-supervised training followed by supervised fine-tuning can outperform fully supervised training.
Popular self-supervised learning paradigms include contrastive learning, self-distillation, masked image modelingand similarity maximization; or a combination of these.

Current writer retrieval and identification methods still rely on local features extracted from Convolutional Neural Network (CNN) activationsor handcrafted methods. While there have been efforts to employ ViTs for writer retrieval, the results were not competitive.
Motivated by the recent success of pre-training a ViT for handwritten text recognition using masked image modelingon only IAM, we revisit the use of self-supervised ViTs in writer retrieval.

In this work, we introduce a fully self-supervised approach that employs a ViT as a feature extractor for writer retrieval and identification (see). Our method requires no labels and outperforms existing methods in historical document benchmarks. We train the ViT using a combination of Masked Image Modelingand self-distillation, utilizing a student-teacher network.
The teacher’s self-attention is used to guide the masking process which in turn the student has to reconstruct in feature space.
Contrary to previous attempts of utilizing a ViT as a feature extractor for writer retrieval, we do not use the class token of the ViT as feature representation. Instead, we extract foreground patch tokens from the ViT’s output sequence, i.e., tokens corresponding to input patches with a sufficient amount of foreground pixels. Our evaluation demonstrates that encoding these features with a Vector of Locally Aggregated Descriptors (VLAD)to obtain a global page descriptor enhances performance notably compared to using the class token with either sum-pooling or VLAD. However, even when using sum-pooling as encoding, our method surpasses previous methods, underscoring the quality of the extracted features.

We make the following contributions:

We successfully apply a Vision Transformer to the task of writer retrieval through self-supervised learning.

We demonstrate that features extracted from our ViT outperform both handcrafted and CNN-based features.

We show that encoding foreground tokens using VLAD is superior to encoding class tokens.

We show that our method learns robust features directly applicable to historical and modern handwriting without the need for model fine-tuning.

The remainder of this paper is organized as follows. First, we cover related work in the field of writer retrieval in. Next, we outline our proposed method in more detail in. Then, we introduce the evaluation protocol for our experiments in. The conducted experiments are detailed in. Finally, we summarize our findings and give an outlook on future work in.

SECTION: Related Work
Writer retrieval methods commonly follow the same pipeline of local feature extraction, page-level aggregation, and distance-based retrieval with optional reranking.

SECTION: Feature Extraction
Local features are generally divided into handcrafted features and deep learning features. Examples of handcrafted features are SURF, Zernike momentsor a combination of SIFT and Pathlet features. In contrast, deep learning features were first introduced by Fiel and Sablatnig, who used a supervised CNN as a feature extractor. Christleinpropose an unsupervised method for CNN training for historical documents, in which pseudo labels are derived from clustering SIFT descriptors.
Inan ImageNet pre-trained ViT is fine-tuned for document images in a self-supervised student-teacher approach operating on differently augmented views, using the ViT’s class tokens as feature representation. However, the method underperforms considerably compared to CNN-based or handcrafted methods.

SECTION: Aggregation
Methods to aggregate local features into global page descriptors are generally categorized into codebook-free and codebook-based methods. Codebook-free methods include sum-pooling and generalized max-pooling. Codebook-based methods used in writer retrieval/identification include Fisher Vectors, GMM supervectorsand VLAD-based methods, where several VLAD encodings are computed and jointly decorrelated using PCA with whitening. The VLAD encoding is sometimes incorporated into the network architecture using NetVLAD. As an additional refinement step, Christleintrain exemplar SVMs for each query, using the query as the only positive example and all training pages as negatives, exploiting the writer-disjointness of training and test sets. We find that a single VLAD encoding is sufficient with our features.

SECTION: Retrieval and Reranking
Retrieval is commonly done using a distance measure, like cosine distance. Other metrics such as thedistance,distance, or the Canberra distance have also been explored. An optional step that has shown to be beneficial in retrieval tasks is reranking, which refines the retrieval list by exploiting the information within it. The authors ofuse areciprocal nearest neighbor Query Expansion by averaging each descriptor with itsreciprocal nearest neighbors. The E-SVM approach of Christleinis extended into a Pair/Triple SVM approach in, where similar documents from the test set are included as additionalexamples. In(Similarity) Graph Reranking is explored. Here, a parameter-free graph network is constructed and used to obtain updated global descriptors using message propagation.

SECTION: Method
Our method follows the common framework of local feature extraction, aggregation, distance-based retrieval and reranking used in previous work.
Our method operates on binary images, obtained in a preprocessing step, if necessary.
An illustration of the method is given in. As the computational complexity of a ViT grows quadratically with the sequence length, our ViT uses a fixed input image size of, and operates on windows extracted from the document.
The ViT is trained in a self-supervised fashion (see) and used to extract patch tokens corresponding to handwriting (see). The extracted features are aggregated into a global page descriptor using a VLAD encoding (see). Finally, the global page descriptors are compared using cosine distance and optionally reranked (see).

SECTION: Self-supervised Training
ViTs feature a large number of trainable parameters and are prone to overfit, requiring extensive amounts of annotated data to train a ViT successfully with traditional, supervised methods.
Thus, utilizing self-supervised training for ViTs is a logical conclusion.
We chose AttMaskfor self-supervised training. The method is an adaptation of iBOT, which incorporates Masked Image Modeling (MIM) into the DINOframework.

DINOemploys self-distillation, a special form of knowledge distillation, i.e., training a student network to reproduce the output of a teacher network. In self-distillation, the teacher network is defined as an exponential moving average of the student.
Student and teacher receive differently augmented views of the input, forcing the student to learn an invariance to the applied augmentations. Additionally, the method samples global and local views. While all views are shown to the student, only the global views are shown to the teacher, thus training a local-to-global correspondence and disentangling objects in feature space.

iBOTintegrates the MIM objective into DINO’s framework by masking a randomized selection of input patches from the student’s view while still showing them to the teacher.
Afterwards, the student has to predict the teacher’s output for the masked patches.

Finally, AttMaskintroduces a novel masking strategy that increases the complexity of feature reconstruction compared to the original iBOT masking, aiming to generate a more robust feature space. The final self-attention map generated by the teacher is used to mask the most attended input patches from the student, forcing the student to develop a deeper understanding of the input by masking the most important regions.

SECTION: Local Feature Extraction
We directly use the self-supervised Vision Transformer (ViT)as a local feature extractor without any additional fine-tuning. A document imageis cut intowindowsin a regular grid. The ViT further cuts each windowinto a sequence of flattened patches, where each patchis then of length.
A learnable class token [CLS] is prepended, forming the input for the ViT as.
Thus, the output of the ViTis given as the token sequence

We find that for aggregating local features using VLAD, retaining the local information of the patch tokens is crucial. However, handwriting images are often sparse due to the horizontal and vertical spacing between words. As a result, many ViT patches may only contain background information, contributing little to the analysis of handwriting characteristics. To address this, we filter out patch tokens that lack sufficient foreground pixels.
The set of foreground tokensis then given as:

whereis the-th pixel in the flattened patch anda threshold on the number of contained foreground pixels.

SECTION: Aggregation
To construct the VLAD codebook, we cut all training documents into windows of size 224 x 224 with stride 224, i.e., non-overlapping, and gather all foreground tokens from the entire training set. These are jointly clustered using minibatch-Meanswithcentroids, which are used as the VLAD codebook.
During inference, the test documents are cut into windows with an adjustable stride of. For each test documentthe set of foreground tokensis gathered and encoded by assigning each tokento the closest centroid and aggregating the residuals between the centroids and their assigned features.
For a centroid, this yields

whereis the nearest centroid toin codebook.
The resulting VLAD encodingof a documentis the concatenation of all such residuals:

We apply power normalization with power 0.5 followed by-normalization. Finally, principal component analysis (PCA) with whitening is used for decorrelation and dimensionality reduction todimensions, resulting in the global page descriptorfor document. The parameters of the PCA are fitted on the training set.

SECTION: Retrieval and Reranking
For retrieval, we use the cosine distance measure.
A low distance indicates that documents are similar.
The cosine distancebetween two global page descriptorsandextracted from documentsandis given as

We evaluate different reranking strategies from previous methodsin conjunction with our method, i.e.,RNN, Graph reranking and SGR.

SECTION: Evaluation Protocol
In this section, we first introduce the metrics for our evaluation in.
Next, we describe the utilized datasets in.
Lastly, we outline the hyperparameters of our baseline implementation in.

SECTION: Metrics
The evaluation is done in a leave-one-out fashion, i.e., every document in the test set is used once as a query image. The remaining documents are ranked by their distance to the query such that the documents with the lowest distance rank highest. The relevant documents, i.e., documents written by the same author, should ideally be at the top of this ranking. A common measure to describe the quality of a retrieval list is the mean average precision (mAP).
To assess the writer identification performance, the Top1 accuracy is commonly considered, i.e., the percentage of query images for which the highest ranking result is a relevant document.

SECTION: Datasets
We evaluate our method on two benchmark datasets of historical document images.

The Historical-WI dataset, a collection of historical document images, was released for the. This dataset is available in both binarized and color image formats. It includes a predefined train-test split: the training set comprises 1,182 document images authored by 394 writers, with each writer contributing three documents. The test set is more extensive, containing 3,600 document images from 720 writers, each contributing five documents. Spanning from the 13th to the 20th century, these documents feature texts in German, Latin, and French.shows three examples of contained document images.

The HisIR19 dataset was released for the. Contrary to the Historical-WI dataset, there is no predefined training split. The authors of the challenge suggest using the Historical-WI test dataset for training. Additionally, a validation dataset is included containing 1200 images of 520 writers, with 300 writers contributing a single page, 100 writers contributing three pages, and 120 writers contributing five pages. The test set is considerably larger than the Historical-WI dataset and contains a total of 20000 documents authored by 10068 writers. 7500 writers contributed one page each, while the others contributed three to five documents. The dataset contains images from manuscript books from the European Middle Ages (9to 15century), letters from the 17and 18centuries, as well as charters and legal documents.shows two examples of contained document images.

SECTION: Implementation Details
We use the following parameters as baseline implementation unless explicitly stated differently. Our model is a ViT-small/16with an input image size of 224. For HisIR19 the document images are only available in color format, thus we binarize them as a preprocessing step using Sauvola Binarizationwith a window size of 51. For our evaluations on the HisIR19 dataset, we directly use the ViT trained on the Historical-WI training set and do not perform any additional fine-tuning. We use the HisIR19 validation set only to construct the VLAD codebook.

We generate training data by sampling windows of size 256 in a regular grid with stride 32 from the Historical-WI training set, resulting in roughly 1.75 million training windows. On these, we train the model for 20 epochs. We apply a cosine learning rate schedule with a linear warmup during the first two epochs and a peak learning rate of 0.005. The last layer is frozen during the first epoch.
We use a MultiCrop augmentationwith two global crops (size 224, scale) and eight local crops (size 96, scale).
Since we operate on binary images, all color-related augmentations are dropped.
Instead, following Peer,
we apply Dilation and Erosion with random kernels to all crops independently with 50% probability.

During feature extraction, we extract windows with strideand apply a foreground threshold ofpixels for extracting patch tokens. To save computation, we only use input windows with more than 2.5% foreground pixels during inference and discard the rest.
For aggregation, we usecentroids in the VLAD codebook and reduce the final dimensionality of our VLAD encodings todimensions.

SECTION: Experiments
We evaluate the different parts of our method separately. First, in, we investigate the performance of our foreground tokens in conjunction with different aggregation methods. Second, in, we compare different training paradigms to train our ViT feature extractor. Third, in, we evaluate different reranking algorithms. Fourth, in, we investigate the effect of the feature extraction and aggregation parameters. Finally, in, we compare our results with previous work.

SECTION: Feature Extraction and Aggregation
In our proposed method, we extract all foreground patch tokens as features for a given window instead of using the class token. To evaluate this strategy, we compare the performance when using the class tokens as features versus using our foreground patch tokens at various threshold values.
For aggregating all local features extracted from a document, we consider sum-pooling and VLAD.

The results given inshow that using class tokens works well with sum-pooling, whereas a significant drop in performance is observed with VLAD.
In contrast, when using all patch tokens (VLAD outperforms sum-pooling.
A likely explanation for this is the low number of class features extracted compared to patch tokens.
Filtering empty ViT patches () improves the performance of both encodings compared to using all tokens. Again, VLAD yields better results than sum-pooling.
Importantly, it also yields better results than sum-pooling of the class tokens on both datasets.
While further increasingharms performance on the Historical-WI dataset, we observe a peak aton the HisIR19 dataset. This is likely caused by noise in the automated binarization process which is not present in the curated binarized version of Historical-WI.

SECTION: Vision Transformer Training
For feature extraction, we train a ViT in a self-supervised approach using AttMask. In this section, we evaluate other self-supervised training approaches, as well as supervised approaches.

In this section, we evaluate other related self-supervised training approaches.
We compare AttMaskto its predecessors, DINOand iBOT, and evaluate different masking strategies.
Both iBOT and AttMask allow to configure the masking process.
Choosingrand, the ViT’s input patches are masked randomly, whereas when choosingblockpatches for masking are selected, such that they form consecutive block shapes in the original image.
In the case of AttMask, we evaluate the masking strategieshighandhint.
The masking strategyhighmasks the most highly attended patches in the input image, whilehintreveals some highly attended patches again. We use thehighmasking strategy as default option in the remaining experiments.shows that DINO, iBOT and AttMask slightly improve upon each other.
For all methods, the best results are obtained from encoding our foreground tokens using VLAD, with AttMask achieving slightly higher mAP and Top1 than the others.

Given the availability of writer identities in our training dataset, a straightforward training approach for the ViT is to use the writer identity as the classification target. We also experiment with using the page id as a classification target. As illustrated in, both supervised training strategies underperform when compared to self-supervised methods. Interestingly, contrary to our findings in, the foreground tokens yield better performance with sum-pooling than the class tokens.

SECTION: Reranking
In this section, we evaluate the impact of several reranking methods on the performance of our baseline implementation.
We evaluate the kRNN reranking used in, Graph reranking (Graph), and Similarity Graph Reranking (SGR).
We evaluate the impact of reranking in combination with both the class tokens and foreground tokens (), and both sum-pooling and VLAD as encoding.
To save computation, we don’t optimize the reranking hyperparameters for each combination on the training set but use fixed values which we found to work well across all combinations. ForRNN we set. For Graph-reranking we setfollowing.
For SGR we use. Thesuggested inheavily reduced our results, likely due to our higher baseline performance.

shows that all reranking methods increase mAP at the cost of Top1 accuracy. On the Historical-WI dataset, sum-pooling of class tokens still produces better results (80.9% mAP) than foreground tokens (78.7% mAP). On the HisIR19 dataset, both the class tokens and the foreground tokens achieve equal mAP of 93.1%, closing the slight gap in the un-reranked results (see).
Even with reranking, VLAD computed on the class tokens still heavily underperforms compared to other combinations. The best results overall are still achieved with VLAD on the foreground tokens (82.0% on Historical-WI, 94.2% on HisIR19). While all reranking approaches yield similar mAP results,RNN produces slightly better results on both datasets, likely due to retaining the best Top1 accuracy.

SECTION: Parameter Evaluation
In this section, we evaluate the remaining parameters of our pipeline on the Historical-WI dataset. We do not consider encoding class tokens using VLAD as the previous experiments have shown this combination to not yield competitive results.

During inference, we sample windows in a regular grid with stride. In the previous experiments, we used a baseline value of.shows that reducing the stride enhances performance in all cases. Loweringto 56 improves the performance of VLAD on the foreground tokens to 82.6% mAP (+1.5%). We did not evaluate smaller strides for computation reasons as halving the stride produces four times more input windows.

Our baseline constructs a codebook of sizefor the VLAD encoding.shows that both mAP and Top1 are relatively stable regardless of the number of clusters. Even with as few as 10 clusters performance only deteriorates slightly, and still considerably improves on sum-pooling.

After aggregation, we use principal component analysis with whitening and dimensionality reduction todimensions. Asshows, retrieval performance with VLAD peaks atdimensions, whereas Top1 accuracy peaks at. With sum-pooling, the dimensionality of the final page descriptor is equal to the ViT’s embedding dimensionality, in our case 384. As such, larger values can not be evaluated. For both class tokens and foreground tokens, mAP and Top1 peak around 256 dimensions but still fall short compared to VLAD.

SECTION: Comparison with State-of-the-Art
For our comparison with other methods, we distinguish between the performance without additional reranking steps and the performance when reranking is applied. We use the baseline parameters outlined in, with the exception of reducing the evaluation stride, i.e., setting. The results are given in.

On the Historical-WI dataset, our method surpasses existing methods considerably in terms of mAP. We achieve 82.6% mAP and 91.9% Top1 score without reranking, beating the best previous method of Lai(77.1% mAP, 90.1% mAP) by 5.5% mAP and 0.8% Top1 score. Notably, our method also exceeds previous methods when using sum-pooling as an encoding in conjunction with the ViTs class token. We achieve 79.4% mAP with this configuration, beating previous methods by more than 2% mAP. This is especially noteworthy as methods based on CNN-based features perform much worse with sum-pooling: Christleinreport a drop of over 30% mAP compared to theirVLAD encoding.
When applying additional reranking, our method still beats previous methods. We achieve 83.1% mAP with reranking, improving over the method of Peer(80.6% mAP)by 2.5% mAP. Still, even the sum-pooling of class tokens outperforms previous methods slightly.

On the HisIR19 dataset, our method also outperforms previous methods considerably. Without reranking, we achieve 94.4% mAP and 97.8% Top1 accuracy, beating the previously best method (92.8% mAP, 97.4% Top1) by 1.6% mAP and 0.4% Top1 accuracy. When also applying reranking, our method achieves 95.0% mAP and 97.6% Top1 accuracy, improving over the best previous method by 1.8% mAP and 0.9% Top1 accuracy. Similar to our findings for the Historical-WI dataset, the sum-pooled class tokens achieve competitive performance, with and without reranking.

Additionally, we evaluate our method on the CVL database, a dataset containing modern handwriting. We directly use the ViT feature extractor trained on the Historical-WI dataset without any fine-tuning and construct the VLAD codebook from the training set of the CVL dataset. With reranking, we achieve 98.6% mAP and 99.4% Top1 accuracy, matching the results of previous methods. Even without reranking, our method achieve a mAP of 97.1%, highlighting the robustness of the extracted features, despite only training on a relatively small set of historical documents.

SECTION: Conclusion
In this work, we presented a novel method using a Vision Transformer as a feature extractor. The model is trained in an unsupervised fashion. Patch tokens containing foreground are extracted as local features and subsequently encoded with VLAD. Retrieval is done using the cosine distance, with optional reranking. Our method achieved a new state-of-the-art performance on the historical benchmark datasets Historical-WI and HisIR19, improving over previous methods by 2.5% mAP and 1.8% mAP respectively. We additionally showed that our method is versatile and also works well on modern datasets, achieving 98.6% mAP on the CVL database without requiring any fine-tuning of the ViT.

Further research could be done to evaluate other SSL methods for model training and different model architectures. In terms of SSL methods, DINOv2introduced several improvements to the iBOT framework which might be interesting for writer retrieval, e.g. the KoLeo regularizer. Regarding architectures, Swin-Transformershave shown promising results in domains with limited data, which might help to boost performance and training time.

Moreover, we showed that only considering patch tokens containing sufficient foreground information is beneficial. Here, future research could investigate other strategies for filtering out patch tokens, for instance by utilizing the self-attention of the ViT to identify relevant patches.

SECTION: References