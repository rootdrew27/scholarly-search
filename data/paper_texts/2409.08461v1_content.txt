SECTION: VistaFormer: Scalable Vision Transformers for Satellite Image Time Series Segmentation
We introduce VistaFormer, a lightweight Transformer-based model architecture for the semantic segmentation of remote-sensing images. This model uses a multi-scale Transformer-based encoder with a lightweight decoder that aggregates global and local attention captured in the encoder blocks. VistaFormer uses position-free self-attention layers which simplifies the model architecture and removes the need to interpolate temporal and spatial codes, which can reduce model performance when training and testing image resolutions differ. We investigate simple techniques for filtering noisy input signals like clouds and demonstrate that improved model scalability can be achieved by substituting Multi-Head Self-Attention (MHSA) with Neighbourhood Attention (NA). Experiments on the PASTIS and MTLCC crop type segmentation benchmarks show that VistaFormer achieves better performance than comparable models and requires only 8% of the floating point operations using MHSA and 11% using NA while also using fewer trainable parameters. VistaFormer with MHSA improves on state-of-the-art mIoU scores by 0.1% on the PASTIS benchmark and 3% on the MTLCC benchmark while VistaFormer with NA improves on the MTLCC benchmark by 3.7%.

SECTION: Introduction
Semantic segmentation is a foundational task in computer vision that predicts a class category for each pixel, rather than an image-level prediction. This computer vision task is useful in remote sensing, especially for satellite image time series (SITS) data, where it is necessary to analyze temporal patterns and changes in specific geographical regions. An important application of SITS data is in identifying crop types, since crops undergo phenological events throughout their growth cycle that can be captured in remote sensing imagery. Accurately identifying crop types has profound for tasks including estimating agricultural yields, monitoring crop health, understanding food security vulnerabilities, creating climate adaptation strategies, and more.

While including additional samples increases the breadth of information in a model’s input, it can dramatically increase the dimensions of input data. Since the earth’s surface is covered by more than 60% clouds, many of these additional inputs may be partially or completely obstructed by cloud coverage. The most performant models applied to crop-type segmentation benchmarks are Transformer-based models that apply self-attention along the temporal dimensionor both the temporal and spatial dimension.

This paper introduces VistaFormer, an encoder-decoder model architecture that applies self-attention along the spatial dimension and uses gated convolutionsto enable downsampling the temporal dimension while rendering profound multi-scale representations. We show that Multi-Head Self-Attention (MHSA) can be substituted with Neighbourhood Attention (NA)which dramatically reduces the number of floating point operations required to achieve optimal performance. To verify the performance of VistaFormer, we use two time-series crop-type segmentation benchmarks, namely the MTLCC and PASTIS benchmarks, both of which include few predicted classes and use Sentinel-2 data as inputs. We find that VistaFormer achieves improved overall Accuracy (oA) and mean-Intersection-over-Union (mIoU) scores relative to state-of-the-art performance while using only 8% of the floating point operations when using MHSA, and 11% with NA, and also using fewer trainable parameters. These advantages are provided by a model that does not require additional position code interpolation which can reduce performance when resolution in train and test datasets differsand make preparing model inputs simpler than in other proposed Transformer-based architectures.

Given the ease of use of the model and its low computational requirements, VistaFormer offers a valuable contribution to advancing time-series deep learning models in a domain aimed at solving some of Earth’s most urgent challenges. The code for implementing VistaFormer can be found, and the repository includes links to all data necessary to reproduce the experiments reported in this paper. This paper expands on the research presented in.

SECTION: Related Work
This section reviews research on SITS crop identification, highlighting the progression from traditional machine learning models to advanced neural network approaches, explores the impact of attention mechanisms and Transformer architectures on vision tasks, and discusses the adaptation of these techniques for SITS, comparing our approach to notable models like U-TAEand TSViT.

SECTION: SITS Crop Identification
While some research has been done to use SITS data for identifying general land classes, crop type classification has been an especially active area of research. Some of the first models to identify crops using SITS data rely on machine learning models such as support vector machines and random forest classifiers, which struggle to learn complex non-linear relationships and often require thoughtfully engineered input features. More recent research has demonstrated that models that include neural network layers like RNNs, LSTMs, and convolutions surpass these traditional models in performance.demonstrate improved crop prediction performance using LSTM-based architectures on Sentinel data whilefinds that integrating both recurrent and CNN layers improves performance over pure recurrent layer-based architectures.uses CGRUand CLSTMlayers to extract relevant features from raw optical SITS data and shows that these layers are capable of filtering out clouds from inputs.show that variations of 3D U-Netshave comparable performance for crop segmentation to models that integrate 2D U-Netsand recurrent layers.

SECTION: Attention & Transformers in Vision
While recurrent layers excel at learning deep representations of sequences, they struggle to process data in parallel and are challenged with learning long-range dependencies. The introduction of attentionand the subsequent introduction of the Transformerarchitecture, improved on this layer by introducing self-attention mechanisms that enable parallel processing of global sequences and capturing long-range dependencies more effectively, enhancing both computational efficiency and the ability to understand complex patterns in data. While self-attention is highly parallelizable, its computational complexity scales quadratically with the size of the input, making encoding images as a raw sequence of pixels prohibitive for most images. ViTintroduced the first pure Transformer-based model that achieved state-of-the-art performance in image classification. This model reduced the computational complexity of applying the Transformer to vision by encoding images into patches and treating each patch as a sequence of tokens.

For dense prediction tasks, PVTintroduced a pyramid structure-based pure self-attention backbone that outperformed comparable CNN-based architectures. PVT was then improved on by models like Swin, Twins, and CoaTthat removed fixed size position embeddings to enhance local feature representations and improve model results on dense prediction.introduced SegFormer, a more efficient alternative, that among other things introduced a purely data-driven position encoding layer usingdepth-wise convolutions in the MLP layer of the Transformer. While more recent model architectures like Mask2Formerand I-JEPAshare structural similarities with the original Transformer architecture, such as employing self-attention mechanisms to process and compare different parts of the input data, the simplicity and effectiveness of the self-attention layer in the Transformer makes it ideal for constructing models that minimize floating point operations and parameter complexity.

SECTION: SITS for Transformers
Previous work introduced U-TAEwhich uses a U-Net architecture with a temporal attention mask that is only computed for the lowest resolution layer and is then upsampled to higher resolution embeddings. These masks are used to collapse the temporal dimension along with a 1D convolution to produce a single map per resolution. We differ from U-TAEmost notably by downsampling both spatial and temporal dimensions after the first encoder layer to reduce floating point operations, and by computing spatial attention in each encoder block.

TSViTproposes an architecture inspired by ViT, that uses input dates to encode temporal positions and uses separate self-attention Transformer layers for computing attention weights along temporal and spatial dimensions. This architecture is effective but computationally expensive in terms of floating point operations since it does not downsample inputs and computes attention on time and space sequences separately. TSViToptimized model performance by encoding temporal positions using dates of the model by encoding also encodes temporal positions using dates, which does not accommodate integrating additional data sources like radar and requires additional data pre-processing for inputs.

Most recently,introduced a model architecture that computes the similarity between a temporal context cluster and temporal input features. The temporal module is used to wrap a 2D segmentation model, allowing for enhanced model flexibility. The pre-trained model in their experiments holds state-of-the-art performance in terms of mIoU for crop-class segmentation on the PASTIS and MTLCC benchmarks used for our experiments. We compare our model’s performance instead to models that have a similar number of trainable parameters, achieve their performance from randomized weights, and report on both mIoU and oA scores as these benchmarks have significant class imbalances.

SECTION: Methods
In SITS semantic segmentation tasks, we are given an inputand outputwheredenotes input channels,andindicate input dimensions,the number of samples, anddefines predicted classes.

Here we introduce VistaFormer, a Transformer-based model designed to take a careful view from a distance, using a simple model architecture to output a dense prediction. We propose a three-layer encoder-decoder architecture, as shown in Figure, where each encoder block downsamples inputs using gated convolutions and computes self-attention on each of the downsampled inputs. Each of the Transformer layers uses a lightweight depth-wise convolution to encode position information similar to the Transformer blocks used in SegFormer. The decoder blocks apply trilinear interpolation to increase the dimensions of each multi-scale representation and use a 1D convolution to collapse the temporal dimension and unify the embedding dimension of each encoder block.

SECTION: Encoder
Each encoder layer, as can be seen in Figure, is structured such that inputs are downsampled using 3D convolution layers, reshaped to treat each pixel as a token in a sequence, and then fed through Transformer blocks.

We apply non-overlapping 3D convolutions to each input at each encoder layer and find that using a simplified variation of gated convolutionsprovides modest performance improvements. VistaFormer uses a gated convolution on inputwheredenotes convolution anddenotes the sigmoid function:

This deviates from the gated convolution presented inin that we do not apply an activation function on, opting to useto scale convolution outputs and allow for increased variability in input data. This convolution mechanism was introduced for image in-painting to ignore irrelevant pixels in an input image while computing convolution outputs. We find this downsampling architecture is similarly suitable for cases where input images may contain visual obstructions based on the merits of this model’s performance while using gated convolutions that have not been pre-trained to mask out clouds and other atmospheric distortions as seen in Table. Given that the applied context for this model is for datasets where individual pixels account for a considerable area, we carefully downsample inputs along spatial dimensions and do not downsamplefor the first layer of the encoder block.

The main computational bottleneck of the encoder is the self-attention Transformer layer. In Multi-Head Self-Attention (MHSA), each of the heads,, andhave the dimensions, whereis the length of the sequence and the self-attention is computed as:

The computational complexity of this process iswhich presents computational challenges when the input dimensions increase or as the number ofsamples increases. By substituting MHSA with 2D NA, which computes attention in a localized neighbourhood around each token of size, we can improve the scalability of VistaFormer by using a neighbourhoodsmaller than. We provide an implementation of VistaFormer which uses MHSA and a separate implementation using NA that scales better as the input dimensions increase.

Models like ViT and TSViT use positional encoding (PE) to introduce location information, which fixes the resolution of positional encodings. This can result in reduced performance when the test resolution differs from the training resolution. To address this issue and simplify the model implementation, we use a 33 depth-wise convolution directly in the feed-forward network (FFN) which was shown to be sufficient to provide positional information for Transformers in. To compute the output from the FFN layer we have:

SECTION: Decoder
SECTION: Experiments
SECTION: Datasets
SECTION: Implementation Details
SECTION: Results
SECTION: Ablations
SECTION: Model Scalability
SECTION: Conclusion
SECTION: References