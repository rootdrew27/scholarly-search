SECTION: LMLT: Low-to-high Multi-Level Vision Transformer for Image Super-Resolution
Recent Vision Transformer (ViT)-based methods for Image Super-Resolution have demonstrated impressive performance. However, they suffer from significant complexity, resulting in high inference times and memory usage. Additionally, ViT models using Window Self-Attention (WSA) face challenges in processing regions outside their windows. To address these issues, we propose the Low-to-high Multi-Level Transformer (LMLT), which employs attention with varying feature sizes for each head. LMLT divides image features along the channel dimension, gradually reduces spatial size for lower heads, and applies self-attention to each head. This approach effectively captures both local and global information. By integrating the results from lower heads into higher heads, LMLT overcomes the window boundary issues in self-attention. Extensive experiments show that our model significantly reduces inference time and GPU memory usage while maintaining or even surpassing the performance of state-of-the-art ViT-based Image Super-Resolution methods. Our codes are availiable at.

SECTION: Introduction
Single Image Super-Resolution (SISR) is a technique that converts low-resolution images into high-resolution ones and has been actively researched in the field of computer vision. Traditional methods, such as nearest neighbor interpolation and bilinear interpolation, were used in the past, but recent super-resolution research has seen significant performance improvements, particularly through CNN-based methodsand Vision Transformer (ViT)-based methods.

Since the introduction of SRCNN, CNN-based image super-resolution architectures have advanced by utilizing multiple convolutional layers to understand contexts at various scales. These architectures deliver this understanding through residual and/or dense connections.

However, super-resolution using CNNs faces several issues in terms of performance and efficiency. Firstly, CNN-based models can become excessively complex and deep to improve performance, leading to increased model size and memory usage. To mitigate this, several models share parameters between modules, but this approach does not guarantee efficiency during inference. SAFMNaddresses the balance between accuracy and complexity by partitioning image features using a multi-head approachand implementing non-local feature relationships at various scales. However, it struggles to capture long-range dependencies due to limited kernel sizes.

ViT-based models have shown superior performance compared to CNN-based models by effectively modeling global context interactions. For example, SwinIRutilized the Swin Transformerfor image super-resolution, demonstrating the effectiveness of the transformer architecture. Subsequently, hybrid models combining ViT and CNN have been proposed, achieving significant performance increases.

However, ViT models face quadratically increasing computational costs as input size grows. To address this, Window Self-Attention (WSA) has been developed, which perform self-attention by dividing the image into windows. Despite this, WSA suffers from quality degradation at window boundaries and lacks interaction between windows. Additionally, conventional ViT-based models stack self-attention layers in series, which significantly increases computational load and inference time.

In this paper, we propose LMLT (Low-to-high Multi-Level Transformer) to improve efficiency during inference while maintaining performance. Similar to SAFMN, our approach uses a multi-head methodto split image features and apply pooling to each feature. Each head applies the self-attention mechanism. Unlike conventional self-attention blocks, which stack self-attention layers in series (Figure(a)), we stack them in parallel to reduce computation (Figure(b)). This means we integrate the number of heads and layers (depth) into a single mechanism. Note that we call the head with the most pooling the lower head, and the number of pooling applications decreases as we move to the upper heads.

Since the window size is the same for all heads, the upper heads focus on smaller areas and effectively capture local context. In contrast, the lower heads focus on larger areas and learn more global information. This approach allows us to dynamically capture both local and global information. Additionally, we introduce a residual connectionto pass global information from the more pooled lower heads to the less pooled upper heads. This enables the windows of the upper heads to view a wider area, thereby resolving the cross-window communication problem.

Trained on DIV2Kand Flickr2K, our extensive experiments demonstrate that ViT-based models can effectively achieve a balance between model complexity and accuracy. Compared to other state-of-the-art results, our approach significantly reduces memory usage and inference time while enhancing performance. Specifically, our base model with 60 channels and large model with 84 channels decrease memory usage to 38% and 54%, respectively, and inference time to 22% and 19% compared to ViT-based super-resolution models like NGswinand SwinIR-lightat scalescale. Moreover, our models achieve an average performance increase of 0.076db and 0.152db across all benchmark datasets.

SECTION: Related Works
is one of the most popular deep learning-based methods for enhancing image resolution. Since SRCNNintroduced a method to restore high-resolution (HR) images using three end-to-end layers, advancements like VDSRand DRCNhave leveraged deeper neural network structures. These methods introduced recursive neural network structures to produce higher quality results. ESPCNsignificantly improved the speed of super-resolution by replacing bicubic-filter upsampling with sub-pixel convolution, a technique adopted in several subsequent works. To address the limited receptive field of CNNs, some researchers incorporated attention mechanisms into super-resolution models to capture larger areas. RCANapplied channel attention to adaptively readjust the features of each channel, while SANused a second-order attention mechanism to capture more long-distance spatial contextual information. CSFMdynamically modulated channel-wise and spatial attention, allowing the model to selectively emphasize various global and local features of the image. We use the same window size, similar to CNN kernels, but vary the spatial size of each feature. This allows our model to dynamically capture both local and global information by obtaining global information from smaller spatial sizes and local information from larger spatial sizes.

has surpassed the performance of CNN-based models by efficiently modeling long-range dependencies and capturing global interactions between contexts. After the success of ViTin various fields such as classification, object detection, and semantic segmentation, several models have aimed to use it for low-level vision tasks. IPTconstructed a Transformer-based large-scale pre-trained model for image processing. However, the complexity of ViT grows quadratically with input size. To mitigate this, many approaches have aimed to reduce computational load while capturing both local and global information. For example, SwinIRused the Swin-Transformermodel for image reconstruction. Restormerorganized self-attention in the channel direction to maintain global information and achieve high performance in image denoising. HATcombined self-attention, which captures representative information, with channel attention, which holds global information. To combine local and global information without adding extra complexity, we add features from lower heads, which contain global information, to upper heads, which contain local information. This enables the windows to see beyond their own area and cover a larger region.

research focuses on making super-resolution models more efficient. The CNN-based model FSRCNNimproved on SRCNNby removing the bicubic interpolation pre-processing and increasing the scale through deconvolution, greatly speeding up computation. CARNreused features at various stages through cascading residual blocks connected in a multi-stage manner. IMDNprogressively refined features passing through the network. However, improving performance often requires stacking many convolution layers, leading to increased computational load and memory usage.

In contrast, the ViT-based model ELANaimed to enhance spatial adaptability by using various window sizes in self-attention. HNCTintegrated CNN and Transformer structures to extract local features with global dependencies. NGswinaddressed the cross-window communication problem of the original Swin Transformerby applying Ngram. Despite these advances, the considerable computational load of overly deep stacked self-attention mechanisms still constrains the efficiency of ViT-based super-resolution models. To address computational load, we connect self-attention layers in parallel, integrating multi-head and depth (number of layers) to lighten the computation. This, along with reducing the spatial size of features, makes our model more efficient.

Additionally, efforts to lighten networks through methods such as knowledge distillation, model quantization, or pruninghave been made. Some approaches differentiate between classical and lightweight image super-resolution models by using the same architecture but varying hyperparameters, such as the number of network blocks or feature channels.

SECTION: Proposed Method
First, we use aconvolution to extract shallow-level features from the image. Next, we stack multiple LHS Blocks (Low-to-High Self-attention Blocks) to extract deep-level features. In each LHS Block, the features go through Layer Normalization (LN), our proposed LMLT (Low-to-high Multi-Level Transformer), LN again, and the CCM. Residual connections are also employed. Finally, we use aconvolution filter and a pixel-shuffle layerto reconstruct high-quality images. For more details on CCM, refer to Appendix.

LMLT operates within the LHS Block. After features pass through the first LN, we divide them intoheads using a Multi-Head approachand pool each split feature to a specified size. Specifically, the feature for the uppermost head is not pooled, and as we move to lower heads, the pooling becomes stronger, with the height and width halved for each subsequent head. Each feature then undergoes a self-attention mechanism. The output of the self-attention is interpolated to the size of the upper head’s feature and added element-wise (called a low-to-high connection). The upper head then undergoes the self-attention process again, continuing up to the topmost head. Finally, the self-attention outputs of all heads are restored to their original size, concatenated, and merged through aconvolution before being multiplied with the original feature.

In each attention layer, the feature is divided intonon-overlapping windows. The dot product of the query and key is calculated, followed by the dot product with the value. LePEis used as the Positional Encoding and added to the value. The output is upscaled by a factor of 2 and sequentially passed to the upper head until it reaches the topmost head.

Our proposed LMLT effectively captures both local and global regions. As seen in Figure, even if the window size is the same, the viewing area changes with different spatial sizes of the feature. Specifically, when self-attention is applied to smaller features, global information can be obtained. As the spatial size increases, the red window in the larger feature can utilize information beyond its own limits for self-attention calculation because it has already acquired information from other regions in the previous stage. This combination of lower heads capturing global context and upper heads capturing local context secures cross-window communication. Figurevisualizes the type of information each head captures. From Figure(a) to(d), the features extracted from each head whenis assumed to be 4 are visualized by averaging them along the channel dimension. The first head ((a)) captures relatively local patterns, while fourth head ((d)) captures global patterns. In Figure(e), these local and global patterns are combined to provide a comprehensive representation. By merging this with the original feature ((f)), it emphasizes the parts that are important for super-resolution.

We improve the model’s efficiency by connecting self-attention layers in parallel and reducing spatial size. In the proposed model, given a featureand a fixed window size of, the number of windows in our LMLT is reduced by one-fourth as we move to lower heads, by halving the spatial size of the feature map. Additionally, since each head replaces depth, the channel count is also reduced to. Therefore, the total computation for each head is given by Equation. Here,meansth head.

On the other hand, in WSA, the self-attention layers are stacked in series, and the spatial size and channel of the feature do not decrease, so the number of windows remains, and the channel count stays at. Therefore, the total computation amount is shown in Equation.

Therefore, in our proposed model, if the number of heads is greater than 1, both the number of windows and channels decrease compared to WSA, resulting in reduced computational load. The more heads there are, the greater the reduction in computational load.

SECTION: Experiments
Following previous studies, we use DIV2K, consisting of 800 images, and Flickr2K, consisting of 2,650 images, as training datasets. For testing, we use the Set5, Set14, BSD100, Urban100, and Manga109datasets.

We categorize our model into four types: a Tiny model with 36 channels, a Small model with 36 channels and 12 blocks, a Base model with 60 channels, and a Large model with 84 channels. First, the low-resolution (LR) images used as training inputs are cropped intopatches. Rotation and horizontal flip augmentations are applied to this training data. The number of blocks, heads, and growth ratio are set to 8 (except for the Small model), 4, and 2, respectively. We use the Adam Optimizerwithand, running for 500,000 iterations. The initial learning rate is set toand is reduced to at leastusing the cosine annealing scheme. To accelerate the speed of our experiments, we settoandtofor the 36-channel model. To account for potential variability in the results due to this setting, we conduct three separate experiments with the LMLT-Tiny model and reported the average of these results. All other experiments are conducted only once.

The quality of the recovered high-resolution images is evaluated using Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index (SSIM). These metrics are calculated on the Y channel of the YCbCr color space. To test the efficiency of our model, we follow the method of SAFMN, measuring GPU memory consumption (#GPU Mem) and inference time (#AVG Time) for scaling a total of 50 images across various models. #GPU Mem, obtained through PyTorch’s, represents the maximum memory consumption during inference, and #AVG Time is the average time per image for inferring a total of 50 LR images at,, andscales. The results for,, andscaling are based on upscaling random images of sizes,, and, respectively.

SECTION: Comparisons with State-of-the-Art Methods
To evaluate the performance of the proposed model, we compare our models with other state-of-the-art efficient and lightweight SR models at different scaling factors. PSNR, SSIM, the number of parameters, and FLOPs are used as the main performance evaluation metrics. Note that FLOPs refer to the computational amount required to create an image with a resolution of 1280720.

We first compare the LMLT-Base with IMDN, LatticeNet, RFDN-L, SRPN-Lite, HNCT, FMEN, and NGswin. Tableshows that our LMLT-Base achieves the best or second-best performance on most benchmark datasets. Notably, we observe a significant performance increase on the Manga109 dataset, while our model uses up to 30% fewer parameters compared to the next highest performing model, NGswin, while the PSNR increases by 0.27dB, 0.29dB, and 0.29dB, respectively, at all scales.

Next, we compare the LMLT-Large model with other SR models. The comparison group includes ESRT, SwinIR-light, ELAN, and SRFormer-light. As shown in Table, our large model ranks first or second in performance on most datasets. Among the five test datasets, LMLT-Large shows the best performance for all scales on the Manga109dataset compared to others, and also the best performance on the Set14dataset. Specifically, compared to SRFormer-light, which showed the highest performance on Urban100among the comparison group, our model shows performance gains of 0.13dB, 0.24dB, and 0.15dB at each scale on the Manga109dataset. In addition to this, we demonstrate that our model has a significant advantage in inference time and GPU memory occupancy at next paragraph. The comparison results of LMLT-Tiny and LMLT-Small with other state-of-the-art models can be found in Appendix.

To test the efficiency of the proposed model, we compare the performance of our LMLT model against other ViT-based state-of-the-art super-resolution models at different scales. We evaluate LMLT-Base against NGswinand HNCT, and LMLT-Large against SwinIR-lightand SRFormer-light. The results are shown in Table.

We observe that LMLT-Base and LMLT-Large is quite efficient in terms of inference speed and memory usage compared to other ViT-based SR models. Specifically, compared to NGswin, our LMLT-Base maintains similar performance while reducing memory usage by 61%, 62%, and 61% for,, andscales, respectively, and decreasing inference time by an average of 78%, 76%, and 78%. Similarly, when comparing SwinIRand our LMLT-Large, despite maintaining similar performance, memory usage decreases by 44%, 43%, and 46% for each scale, respectively, and inference time decreases by an average of 87%, 80%, and 81%. This demonstrates both the efficiency and effectiveness of the proposed model.

Tableshows the time consumption for module in the proposed method and SwinIR-light, specifically detailing the time from the first Layer Normalization (LN)to the LHSB and WSA. Our LMLT significantly reduces the time required for the self-attention mechanism. Especially, LMLT-Large achieves time reductions of 94% at thescale, 90% at thescale, and 88% at thescale compared to SwinIR. Given the similar performance between SwinIR and our method, this represents a significant increase in efficiency. Note that the inference time for a total of 50 random images is measured. The time measurements are conducted using’s record and, and due to hardware access latency during log output, the time of the modules might be longer than the time report in Table. Tables comparing the memory usage and inference speed of our LMLT with other models can be found in Appendix.

Figureillustrates the differences on the Manga109dataset between our model and other models. As shown, our LMLT successfully reconstructs areas with continuous stripes better than other models. Additionally, we include more comparison images, and further compare our proposed models LMLT-Tiny with CARN, EDSR, PAN, ShuffleMixerand SAFMNon the Urban100dataset atscale. Detailed results can be seen in Appendix.

SECTION: Ablation Study
We examine the effects of the low-to-high element-wise sum (low-to-high connection) and downsizing elements of our proposed model. As shown in Table, the low-to-high connection yields significant results. Specifically, on the Urban100dataset, PSNR increases by 0.04 dB to 0.05 dB across all scales, and SSIMincreases by nearly 0.0011 at thescale, demonstrating the benefits of including the low-to-high connection. Appendixvisualizes the differences in features on Urban100with and without the low-to-high connection, showing that it significantly reduces the boundary lines between windows. Additionally, experiments adding the proposed low-to-high connection to SAFMNfor,, andscales are also provided in Appendix.

Tablevalidates the effectiveness of using multiple scales by experimenting with cases where pooling is not applied to any head. Specifically, we compare our proposed LMLT with cases where pooling and the low-to-high connection are not applied, as well as cases where merging is also not performed. The results show that performance is lower in all cases compared to the proposed model. Appendixdemonstrates that when pooling is not applied, the lack of information connection between windows hinders the proper capture of informative features, even though spatial information is retained.

We analyze the impact of LHSB and CCMand their interplay. Following the approach in SAFMN, we examine performance by individually removing LHSB and CCM. Results are shown in the ‘Module’ row of Table. Removing LHSB reduces the number of parameters by nearly 10%, decreases memory usage to nearly 74%, and drops PSNR by 0.59 dB on the Urban100dataset. Conversely, removing CCM reduces the number of parameters by nearly 90% and PSNR by 1.83 dB. Adding an MLP after the self-attention module, as done in traditional Transformers, reduces parameters by about 69% and PSNR by approximately 0.85 dB. To maintain the same number of layers as in the previous two experiments (8 layers), we conducted another experiment with only 4 blocks, resulting in a 49% reduction in parameters and only a 0.55 dB drop in PSNR, indicating the least performance loss. This suggests that the combination of LHSB and CCM effectively extracts features. Additionally, incorporating FMBConvreduces parameters by nearly 58% and PSNR by 0.28 dB, while memory usage remains similar.

We analyze the impact of aggregating features from each head using aconvolution or applying activation before multiplying with the original input. Results are shown in the ‘Act / Aggr’ row of Table. Without aggregation, PSNR decreases by 0.10 dB on the Urban100dataset. If features are directly output without applying activation and without multiplying with the original input, PSNR decreases by 0.12 dB. Omitting both steps leads to an even greater decrease of 0.22 dB, indicating that including both aggregation and activation is more efficient. Conversely, multiplying features directly to the original feature without the activation function improves performance by 0.1 dB. Detailed experimental results are discussed in Appendix.

Lastly, we examine the role of Positional Encoding (PE) in performance improvement. Results are shown in the ‘PE’ row of Table. Removing PE results in decreased performance across all benchmark datasets, notably with a PSNR drop of 0.06 dB and an SSIM decrease of 0.0006 on the Urban100dataset. Using RPEresults in a maximum PSNR increase of 0.03 dB on the Set14dataset, but has little effect on other datasets. Additionally, parameters and GPU memory increase by 5K and 45M, respectively.

SECTION: Conclusion
In this paper, we introduced the Low-to-high Multi-Level Transformer (LMLT) for efficient image super-resolution. By combining multi-head and depth reduction, our model addresses the excessive computational load and memory usage of traditional ViT models. In addition to this, LMLT applies self-attention to features at various scales, aggregating lower head outputs to inform higher heads, thus solving the cross-window communication issue. Our extensive experiments demonstrate that LMLT achieves a favorable balance between model complexity and performance, significantly reducing memory usage and inference time while maintaining or improving image reconstruction quality. This makes the proposed LMLT a highly efficient solution for image super resolution tasks, suitable for deployment on resource-constrained devices.

SECTION: References
SECTION: Impact of number of Blocks, Channels, Heads and Depths
SECTION: Effects of Low-to-high connection and Pooling
SECTION: Impact of Activation function
SECTION: LAM and ERF Comparisons
SECTION: CCM : Convolutional Channel Mixer
SECTION: Comparisons on LMLT with Other Methods