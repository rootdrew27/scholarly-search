[["ti:\"vision transformers\"", [{"title": "LayerShuffle: Enhancing Robustness in Vision Transformers by Randomizing\n  Layer Execution Order", "authors": ["Matthias Freiberger", "Peter Kun", "Anders Sundnes L\u00f8vlie", "Sebastian Risi"], "published_date": "2024-07-05T13:54:15Z", "summary": "Due to their architecture and how they are trained, artificial neural\nnetworks are typically not robust toward pruning or shuffling layers at test\ntime. However, such properties would be desirable for different applications,\nsuch as distributed neural network architectures where the order of execution\ncannot be guaranteed or parts of the network can fail during inference. In this\nwork, we address these issues through a number of training approaches for\nvision transformers whose most important component is randomizing the execution\norder of attention modules at training time. With our proposed approaches,\nvision transformers are capable to adapt to arbitrary layer execution orders at\ntest time assuming one tolerates a reduction (about 20\\%) in accuracy at the\nsame model size. We analyse the feature representations of our trained models\nas well as how each layer contributes to the models prediction based on its\nposition during inference. Our analysis shows that layers learn to contribute\ndifferently based on their position in the network. Finally, we layer-prune our\nmodels at test time and find that their performance declines gracefully. Code\navailable at https://github.com/matfrei/layershuffle.", "arxiv_id": "2407.04513v2", "html_link": "https://arxiv.org/html/2407.04513v2", "search_term": "ti:\"vision transformers\"", "html_content": "SECTION: LayerShuffle:\u00a0Enhancing Robustness in Vision Transformers by Randomizing LayerExecution Order\n\nDue to their architecture and how they are trained, artificial neural networks are typically not robust toward pruning or shuffling layers at test time. However, such properties would be desirable for different applications, such as distributed neural network architectures where the order of execution cannot be guaranteed or parts of the network can fail during inference. In this work, we address these issues through a number of training approaches for vision transformers whose most important component is randomizing the execution order of attention modules at training time. With our proposed approaches, vision transformers are capable to adapt to arbitrary layer execution orders at test time assuming one tolerates a reduction (about 20%) in accuracy at the same model size. We analyse the feature representations of our trained models as well as how each layer contributes to the models prediction based on its position during inference. Our analysis shows that layers learn to contribute differently based on their position in the network.\nFinally, we layer-prune our models at test time and find that their performance declines gracefully. Code available athttps://github.com/matfrei/layershuffle.\n\nSECTION: 1Introduction\n\nWhile demonstrating impressive performance in many domains(Krizhevsky et\u00a0al.,2012; Vaswani et\u00a0al.,2017; Radford et\u00a0al.,2021; Rombach et\u00a0al.,2022), deep learning systems demand both extensive computational resources and tight integration of their parts. For applications at scale, they therefore increasingly require the construction of large data centers with thousands of dedicated hardware accelerators. A paradigm shift from central to decentral model inference, where loosely coupled neural networks are distributed over a number of edge devices that share the computational load of the model(Gacoin et\u00a0al.,2019)therefore seems ultimately desirable.\nUnfortunately, current deep learning models lack the robustness necessary for such a paradigm shift.\n\nIn general, artificial neural networks (ANNs) are not robust toward pruning or replacing network layers during deployment.\nSimilarly, changing the order of execution in-between layers without further training usually results in catastrophic losses in accuracy. Nevertheless, these properties would be desirable e.g.\u00a0in distributed setups as described above, where a model is executed on a number of shared nodes in a network.\nThis way, overloaded or malfunctioning nodes could simply be skipped in favor of other available nodes.\n\nAugmenting models with these properties has historically been challenging. Due to the structure of the most common types of ANNs and how they are trained through backpropagation(Linnainmaa,1970; Werbos,1982; Rumelhart et\u00a0al.,1986), each neuron can only function by adapting to both its connected input and output neurons as well as the overall desired output of the network at training time.\nFurthermore, the hierarchical organization of explanatory factors is usually considered a necessary prior in deep learning, i.e.\u00a0one assumes that subsequent layers extract increasingly high-level features(Bengio et\u00a0al.,2013). Therefore, switching the execution orders of layers implies that layers would need to adapt and extract either low-level or high-level features depending on their position in the network. Unfortunately, network layers adapting in such a way to a changed order of execution appears to be infeasible for most known network architectures. The above prior is therefore violated and the overall performance of the network suffers beyond the point where the network successfully executes the task it has been trained for.\n\nThe more recently discovered transformer architecture(Vaswani et\u00a0al.,2017)has been shown to be more flexible. Transformers, when trained accordingly, can be layer-pruned at test-time(Fan et\u00a0al.,2019), and recent work merges similar transformer-based language models(Akiba et\u00a0al.,2024), all with only moderate reduction or even an improvement in performance. We hypothesize that the reason for the high adaptability of transformers can be found in self-attention modules being able to adapt their output based on the received input. Thus it should be possible to train a transformer network to not only adapt to the variation of its input features based on the overall network input but also the variations caused by receiving input from different layers during test time.\n\nWe propose and evaluate three training approaches for vision transformers to address the robustness issues laid out above. The most important component common to all approaches is randomizing the execution order of the vision transformer\u2019s stacked self-attention-and-feed-forward modules at training time (Figure1(a)). More precisely, the main contributions in this paper are:\n\nWith LayerShuffle, the layers of a vision transformer(Dosovitskiy et\u00a0al.,2020)are capable of adapting to an arbitrary execution orderat test time, assuming one tolerates a moderate reduction in performance. Providing each layer additionally with its current position in the network improves performance only slightly compared to a model without it, suggesting that each attention layer is already capable of determining its role based on the incoming data alone.\n\nAn analysis reveals that layers of models trained with LayerShuffle adjust their output depending on which position they hold in the network.\n\nTrained models can be layer-pruned at test time similar to the models trained with the techniques proposed inFan et\u00a0al. (2019), where their performance declines gracefully, i.e.\u00a0models with reduced amounts of layers still remain functional.\n\nSECTION: 2Related work\n\nZhu et\u00a0al. (2020)find that for particular subsets of inputs, transformers perform better when changing the execution order of layers to an input-specific sequence. They optimize the execution order per sample in order to maximize the performance of the model for natural language processing tasks.\n\nWhile the goal in their work is to find a layer sequence of a pre-trained model that is optimal for a given input, our approach aims to make the model robust to any sequence of execution, where layers might even be missing.\n\nIn parallel to our work on vision transformers, two groups have conducted similar experiments with the aim to understand how language models (LLMs) process data.Lad et\u00a0al. (2024)found that LLMs are very robust to changing the positions of adjacent layers or ablating single layers from the model.Sun et\u00a0al. (2024)perform similar experiments, and find that transformers improve iteratively upon their predictive output by subsequently refining the internal representation of the presented input. The main difference to our work is that the authors of these works do not perform any refinement on the models and switch and ablate layers locally with the aim of better understanding the inner workings of LLMs. Here we focus on methods and training approaches to increase this innate robustness of the transformer architecture to a point where models at test time function regardless of their layer execution order, and respond gracefully to the ablation of several layers in any position of the network.\n\nAnother related work is LayerDrop(Fan et\u00a0al.,2019), where the authors focus on robust scalability for models on edge devices. They propose dropping whole transformer layers during training and show that this training approach allows models to still deliver acceptable (if somewhat reduced) performance upon pruning layers at test time (e.g.\u00a0for balancing computational load). The main difference to our approach is that we randomly change the execution order during training, and, contrary to LayerDrop, do not remove any layers. Also, LayerDrop focuses on entirely on load balancing in compute-limited production systems while our main focus is on arbitrary execution order and the possibility to replace defective nodes by others on top of these issues in case of overloaded or malfunctioning nodes in distributed systems.\n\nWork on introducing permutation invariance into neural networks has been conducted byLee et\u00a0al. (2019),Tang & Ha (2021)as well asPedersen & Risi (2022). The corresponding former two approaches exploit the permutation equivariance of attention, i.e.\u00a0the fact that the order in which a sequence of vectors gets presented to the attention module does not change its result, but merely shuffles the sequence of output vectors. This equivariance is achieved by using a fixed-seed query vector in order to obtain an permutation invariant latent code. This latent code stays the same no matter in which order input tokens/patches are presented to the module. The main contrast to our work here is that we exploit permutation invariance in the order of layer executions rather than the order of tokens and patch embeddings and can therefore not make use of permutation equivariance of the attention operation, as it does not apply to switching inputs and outputs.\n\nFinally, the work ofGacoin et\u00a0al. (2019), not unlike our own, is motivated by the observation that a paradigm of distributed model inference over a number of loosely coupled compute nodes, edge devices or swarm agents promises a positive impact on the ecological and economical footprint of deep learning solutions. The authors propose a graph-theory-based framework to optimize the distribution of model parts to individual devices and optimize the overall energy consumption of the network. While our work sets out from the same motivation, it complements the approach ofGacoin et\u00a0al. (2019)as the the authors do not address robustness to adverse conditions in such distributed setups while it is the entire focus of this paper. The exact distribution of our models on the other hand, is beyond the scope of our work but combining our models with the approaches in(Gacoin et\u00a0al.,2019)seems a promising direction of future research.\n\nSECTION: 3Methods\n\nWe investigate three approaches for arbitrary layer execution order in vision transformers(ViT; Dosovitskiy et\u00a0al.,2020): First, we simply permute the order of layers randomly during training, such that every training batch is presented to the network\u2019s layers in a different random order (Section3.1). Second, while randomly permuting the layer order as in the previous approach, we use an layer-depth encoding inspired by learned word embedding approaches (Section3.2) to test if this additional information would further improve performance. Third, while randomly permuting layer order as in the previous approaches, we try to predict from the output of every layer at which position the layer is currently located in the network using a small layer position prediction network for every layer (Section3.3). A detailed overview on ViTs can be found in the appendix.\n\nSECTION: 3.1Randomly permuting layer order during forward pass\n\nDuring each forward pass, i.e.\u00a0for each batch presented to the ViT, we randomly permute the execution order of layers during training. The intention here is to teach the layers to not only extract meaningful intermediate representations when receiving input from a particular layer, but to be able to process and encode information from and for all possible layers in the network. In terms of training, exchanging the order of layers does not require any changes in the basic error backpropagation algorithm. For the forward path, the order how weight matrices are multiplied and activation and attention functions applied changes for every batch and forward pass. This needs to be accounted in the backward pass by propagating the gradients in the precise reverse order that has been set in the forward pass, i.e.\u00a0 multiplying the computed per-layer gradient matrices in the correct order. As we use Pytorch(Paszke et\u00a0al.,2019)in all our experiments, this aspect is taken care of the framework\u2019s autogradient feature. We refer to this model asLayerShuffle. To further illustrate the approach, a pseudo-code listing is given in Algorithm1.\n\nSECTION: 3.2Layer position encoding\n\nIn the second approach,LayerShuffle-position, we provide each layer with its current position in the network. Through this variation we aim to test if each layer can already adapt sufficiently by itself to information coming from different layers during test time or if giving it the current position can help further. In more detail, jointly with permuting the layer execution order, each layer learns a vector embeddingfor each possible index positionof the layer during training, where L is the number of layers andis our chosen embedding dimension. The layer\u2019s current indexin the network is presented together with the input to the layer(Figure2). The layer fetches the embedding vectorassociated with the passed indexand concatenates it to the input vector:N is the number of patches extracted form the input image, the functionsconcatandrepeatrespectively concatenate and repeat tensors along their last (most varying) dimension.\nA projection network, which consists of a LayerNorm (LN)(Ba et\u00a0al.,2016)module, a single linear layer, a GELU(Hendrycks & Gimpel,2016)activation function as well as a Dropout(Srivastava et\u00a0al.,2014)module, is then used to combine input and embedding and reduce it again to the used latent dimensionof the transformer. To ensure gradient flow during training, a residual connection is added as well:\n\nThe resulting outputis passed on to a regular multi-head-attention-and-feed-forward structure as described in Equations1and2.\n\nSECTION: 3.3Predicting current layer position\n\nTo determine if the incoming information to each attention layer is indeed sufficient for it to figure out its role, we specifically test for this ability with theLayerShuffle-predictvariant. We equip each layer of the network with a simple position prediction module that takes the current layer output as an input and seeks to predict the current position of the layer in the network (Figure3). The module consists of a single linear layerreceiving layer-normalized (LN)input..\n\nEach of these layer order prediction modules optimizes a cross-entropy loss where then the overall network optimizes the lossHere,is the regular cross-entropy loss of the output layer, andis the layer position prediction loss of layer i, which is also a cross-entropy loss:\n\nwhereis the number of layers in the network,is the-dimensional output of the position prediction network of layer, anddenotes the-th dimension of the vector.is the output logit denoting the network\u2019s predicted confidence that the layer currently is deployed at its actual position with index.\n\nSECTION: 4Experiments\n\nWe conduct our experiments on the ILSVRC2012 dataset(Russakovsky et\u00a0al.,2015), more commonly termed ImageNet2012, as well as the CIFAR-100 datasetKrizhevsky et\u00a0al.. We use the originalViT-B/16(Dosovitskiy et\u00a0al.,2020)vision transformer, as well theDeiT-Bdistilled data-efficient image transformerTouvron et\u00a0al. (2021). Pre-trained weights for ImageNet2012 are publicly available for both models(Dosovitskiy et\u00a0al.,2020; Wu et\u00a0al.,2020; Touvron et\u00a0al.,2021).\n\nTheViT-B/16has been pre-trained on ImageNet21k(Deng et\u00a0al.,2009; Ridnik et\u00a0al.,2021)at an 224224 input image resolution and refined on ImageNet2012 at the same resolution.DeiT-Bhas the same architecture asViT-B/16, but uses an additional destillation token during training, which is used to distill the inductive bias of a large convolutional network into the transformer in order to require less training data. It is pre-trained exclusively on ImageNet2012.\n\nBoth models are again refined on both ImageNet2012 and CIFAR-100 at the same resolution, but using the training processes as described in Section3. That is, layer execution order is randomly permuted while refining the model. To establish a baseline, on ImageNet2012, we refine the original models for one more epoch on without changing the layer order. Any longer training was found unlikely to bring additional improvement in preliminary experiments since our networks are already pretrained on ImageNet. For CIFAR-100, we refine our baseline for 20 epochs. For each approach, including the baselines, we trainnetworks and compare their average validation accuracy.\n\nAll models are refined using Adam(Kingma & Ba,2014)(,,), where an initial learning rate ofwas empirically found to work best. In terms of batch size, we evaluate training batch sizes of 640 images, which is the maximum multiple of 8 that can fit in the video memory of our used GPU, as well as 128 images for models that benefit from a smaller batch size. Even smaller batch sizes do not yield any improvement in performance for our models. Inspecting training curves shows that for ImageNet2012 the performance of models plateaus at 20 epochs the latest, which is therefore set as the maximum number of training epochs. For CIFAR-100 we use 100 epochs since the models were pretrained on a different dataset, i.e.\u00a0ImageNet. We use a form of early stopping by evaluating the model achieving the lowest crossentropy loss on the validation set after the maximum amount of training epochs. All models have been trained on a single NVIDIA H100 Tensor Core GPU with 80GB of memory. Training a single model on ImageNet2012 for 20 epochs takes about 7 hours whereas CIFAR-100 training times are significantly shorter due to the smaller train set.\n\nSECTION: 4.1Sequential vs.\u00a0arbitrary execution order\n\nThe average accuracy for all approach\non all vision transformer architectures and datasets is shown in Table1.\nWe make the following observations across all models and datasets:\n\nOn both the CIFAR-100 and the ImageNet2012 dataset, our baselines refined from pre-trainedViT-B/16andDeiT-Bmodels perform very much as expected. For a classic sequential execution order of the model layers, on ImageNet2012 the trained models achieve an average validation accuracy very close to the performance of the respective original pretrained models(Dosovitskiy et\u00a0al.,2020; Touvron et\u00a0al.,2021). Our refined baselineViT-B/16obtains an average accuracy ofwith a standard deviation of. TheDeiT-Bmodel attains a slightly lower accuracy ofwith a standard deviation of. Baseline results CIFAR-100 look similar withViT-B/16andDeiT-Bachieving(standard deviation:) and(standard deviation:) respectively.\nNot surprisingly, for an arbitrary layer execution order, the average model accuracy declines catastrophically to belowfor all trained models on both datasets. Our original assertion that in general, ANNs are not robust to changing the execution order of their layers, is in line with these results.\n\nOur LayerShuffle approaches show lower performance than the baselines when executing layers in their original order. On ImageNet2012, our trainedViT-B/16models obtain average accuracies of,, andrespectively for ourLayerShuffle,LayerShuffle-positionandLayerShuffle-predictapproaches. Average accuracies forDeiT-Bmodels are in a similar range with,, andfor our respectiveLayerShuffle,LayerShuffle-positionandLayerShuffle-predictapproaches. On CIFAR-100 on the other hand the gap between baseline and LayerShuffle models is somewhat larger for the trainedViT-B/16models. These models merely achieve,andwith slightly higher standard deviations forLayerShuffle,LayerShuffle-positionandLayerShuffle-predictapproaches. ForDeiT-Bmodels on the other hand, these approaches perform similarly well to the models trained on ImageNet with scores of,andfor the above mentioned techniques. A possible explanation for these discrepancies could be found inViT-B/16models requiring more and more diverse training data compared toDeiT-Bmodels, which has been pre-trained with the aim to reduce the amount of required training data.\n\nDespite being outperformed by the baseline in a sequential execution order setting, all models improve dramatically over their corresponding baseline models in an arbitrary execution order setting. Taking a closer look at LayerShuffle model performance in that setting, we find that the simplest approach performs very well across both architectures and datasets. For both ImageNet2012 as well as CIFAR-100 validation setsDeiT-Btrained on LayerShuffle yields the best performance with average accuracies ofandrespectively, narrowly outperformingLayerShuffle-position, which receives information about the layer position.LayerShuffle-positionachieves scores ofandon these datasets. ForViT-B/16models on the other hand,LayerShuffle-positionoutperformsLayerShuffle. The former achieves scores ofandof ImageNet2012 and CIFAR-100, with the latter performing only slightly worse withand. The most likely explanation for models of theViT-B/16architecture achieving significantly lower accuracies on CIFAR-100 thanDeiT-Bmodels can again be found in the former requiring less training data than the latter.\n\nWe find that the position prediction approach,LayerShuffle-predictis outperformed by both our remaining approaches on all datasets and architectures. On ImageNet2012, refinedViT-B/16models achieve average accuracies ofwhereasDeiT-Bmodels attain. On CIFAR-100 the former score, the latter. A possible explanation might be that due to optimization of multiple objectives (fitting both the output labels as well as predicting the current position of the layer) this approach requires more careful hyperparameter tuning.\n\nA further interesting observation is to be made when comparing the performance for sequential and arbitrary execution order for each approach respectively. For all approaches, using the original layer order for sequential execution still performs better than an arbitrary order. This is most likely a consequence of fine-tuning from a sequentially trained model.\n\nFor the layer position prediction approach, we measure the average accuracy of layer position predictions over all five trainedLayerShuffle-predictmodels, and find that the layer position is predicted correctly inof all cases. These results demonstrate that each layer has enough information coming from its inputs alone to predict where it is in the network, providing the basis to adapt to its current position. We investigate this further when analyzing intermediate network representations in Section4.3.\nIn conclusion, refining a pre-trained model while randomly permuting the execution order of the network layers can make a model more robust towards such arbitrary execution orders at test time. On the other hand, Dropout and LayerNorm by themselves do not have the same effect and fail to produce networks robust against layer shuffling.\n\nSECTION: 4.2Removing layers during test time\n\nTo determine how neural networks trained with LayerShuffle would perform when several devices in a (distributed) model become unavailable, we further investigate the effect of pruning an increasing amount of layers during test time. We evaluate its average validation accuracy over 5 models when only using 3, 6, or 9 layers. In addition, we refine the originalViT-B/16transformer using LayerDrop(Fan et\u00a0al.,2019)with a drop probability of 0.2 (as recommended by the authors) and compare it as a baseline to our approach under identical conditions. Note that whenever we evaluate the accuracy of our proposed approaches as well as the baseline, we do so two times: Once, for the original \u201dsequential\u201d layer order as originally intended and trained for theViT-B/16transformer, and once with arbitrary layer execution order where we change the order randomly for every forward path.\n\nFor sequential execution (Figures1(b)), LayerDrop with a drop rate of 0.2 behaves similarly to LayerShuffle, with the exception that our approach performs better for a small number (3) of layers with an average accuracy of approximatelyvs.\u00a0close tofor LayerDrop.\nWhile for 6 layers, both approaches are roughly on par, for 9 layers LayerShuffle is slightly outperformed by LayerDrop as both approaches show an average accuracy in therange. At the full amount of 12 layers, this gap in average accuracy stays roughly the same as the LayerDrop-refined model closes in on the full accuracy of the original model, while our LayerShuffle approach achieves slightly lower accuracies (see also Table1). For comparison, we also visualize models where we refined a reduced number of 3, 6, and 9 layers: while delivering similar performance as LayerDrop for 9 and 12 layers, these models perform significantly better than the previously discussed approaches at lower numbers, i.e.\u00a03 and 6 layers. They do however, bear the drawback that for each specific amount of layers a new model must be refined from the original model, whereas for both LayerDrop and our LayerShuffle approach, only a single full-size model needs to be refined and the number of layers can be configured at will at test time.\n\nFor arbitrary execution (Figure1(c)), LayerShuffle is the only approach that succeeds, with the average accuracy improving as the number of layers is increased. LayerDrop does not perform well regardless of the number of layers in the model.\n\nA noteworthy detail is the comparable high average accuracy of the fully retrained baseline with 3 layers. Given the low performance of the refined models with 6 and 9 layers, as well as that there are only 6 possible permutations for 3 layers, the most likely explanation is that one of the 5 random permutations evaluated for the model was the original layer execution order the model has been trained for, i.e.\u00a0 [1,2,3] therefore skewing the achieved accuracy in this case. In conclusion, we find that our proposed approach has similar test-time scaling capabilities as LayerDrop, while still ensuring robustness towards arbitrary layer execution orders.\n\nSECTION: 4.3Analysis of intermediate network representations\n\nTo gain a deeper insight into how information is encoded in the models, we conduct two experiments. First, we compute Uniform Manifold Approximation and Projection (UMAP)(McInnes et\u00a0al.,2018)embeddings of the entire output of a particular attention module (i.e. combined self-attention and feed-forward layers), where we color-code all output vectors based on the position the module held in the network when producing this output. In more detail, we concatenate all patch tokens of a single image together with the class token as a single vector, and use this representation as a single state vector in our compression. To extract a sufficient number of these state vectors, we present 1,000 randomly sampled images from the ImageNet2012 validation set to a LayerShuffle-trained model. While we use an evaluation batch size of 1 image and record all outputs of a single, previously selected layer, we randomly permute the execution order of layers such that the selected layer changes position in the network during every forward path. After the layer\u2019s output vectors for all 1,000 images have been recorded, a UMAP reduction of the output space to 2D is performed.\n\nSecond, in order to investigate how much each layer contributes to the final classifier output when deployed in different positions within the model, we compute the L2-Norm of the class-token of each layer output. We correct for the contribution of previous layers by subtracting the class token of the previous layer before computing the token\u2019s norm. That way, we consider solely the additive contribution of the layer to the class prediction of the model. We collect these token norms for all network layers as we shuffle their position while presenting 1000 randomly sampled input images in an identical manner as in the previous experiment.\n\nFinally, to establish a baseline, we extract both representations for the originalViT-B/16weightsDosovitskiy et\u00a0al. (2020)as well.\nFigure4(b)shows the obtained visualizations for the originalViT-B/16model acting as a baseline as well as our model refined withLayerShuffle.\nIn more detail, Figures4(a)and4(b)show the UMAP embeddings of a single layer\u2019s output for both the baseline and our model. The current position of the layer in the network when producing a given output is color-coded from dark (position close to the input) to light (position close to the output). Note that this information about the layer position has not been presented to the UMAP algorithm. Apart from rough trends, no clear ordering of the space is visible for the baseline (Figure4(a)). For LayerShuffle, while there is no sharp separation between outputs generated at different positions in the network, the layer clearly adapts to its current position and extracts different features for different positions in the network (Figure4(b)).\n\nA further interesting observation is the very distinct collection of points for layer positions close to the input, which are detached from the remaining manifold of points. This results suggests that extracting low-level features, requires special treatment.\n\nFigures4(c)and4(d)show the distributions on the normalized L2-norm of additive contributions to class tokens for different layer positions in the transformer for both the baseline and our model. Each x-axis in the plot corresponds to a single layer of the network, where position of the layer in the network is color-coded again from close to the input (dark) to close to the output (light). x-axes are also ordered corresponding to the layer\u2019s original position in the pre-trained model, where the order of layers is top to bottom.\nWe can see that for the baseline model norms are basically spread out over the whole range. This implies that layers in the baseline model overall contribute evenly to the predictive output of the model, regardless of their current position in the network.\n\nOn the other hand, the ridge plot gathered from layer outputs of the model refined with our method paints a different picture. The norm of attention modules output and therefore it\u2019s contribution to the model\u2019s prediction varies based on the distance to it\u2019s original position in the networks. Modules which were originally closer to the input (x-axes on top of the plot) often show larger contributions to the predictive output of the model when on positioned closer to the input and vice versa. This indicates that our refinement of the model conditions its layers to contribute to the overall predictive output if the received input lies within the layers learned distributions of inputs (i.e.\u00a0the layer is close at a position assigned to it in the original pre-trained network), and withhold or reduce their output otherwise. This is also in line with recent work conducted in parallelSun et\u00a0al. (2024), which frames transformer layers as incrementally refining a rough sketch of the model\u2019s output, an iterative process which is enabled by the transformer\u2019s extensive utilization of skip-connections.\n\nIn conclusion, our analysis indicates that refining networks withLayerShufflemakes vision transformers robust to arbitrary execution orders as it trains the layers to solely add to the models contribution if the layer input is in-distribution and reduce their output otherwise, in which case the model\u2019s skip-connection forwards the out-of-distribution output to the subsequent layer.\n\nSECTION: 5Discussion and Future Work\n\nThis paper presented a new approach called LayerShuffle, which enabled vision transformers to be robust to arbitrary order execution as well as pruning at test time.\nFor sequential execution, LayerShuffle performs on average\nonly slightly worse than the LayerDrop approach but is the only method that works when the layer execution is arbitrary.\n\nOur analysis confirmed that layers of models trained with LayerShuffle adjust their output depending on which position they hold in the network. Furthermore, our results indictate that refining networks withLayerShuffletrains the layers to only contribute to the model\u2019s class prediction if the layer input is in-distribution and reduce their output otherwise, in which case the layer\u2019s skip-connection forwards the barely modified out-of-distribution embedding to the subsequent layer.\n\nIn the future, these properties could makeLayerShuffle-trained models ideal candidates to be distributed over a number of very loosely coupled compute nodes to share the computational load of model inference. Given the enormous engineering, financial and logistical effort as well as the environmental impact(Strubell et\u00a0al.,2020)of building and maintaining datacenters for state-of-the-art deep learning approaches on the one hand, as well as the large amount of available, but scattered compute through existing smartphones, laptop computers, smart appliances and other edge devices on the other hand, approaches that allow distributed neural networks to perform inference could be of great impact. We therefore consider the deployment and orchestration of our trained models onto an actual set of edge devices and the practical implementation of the inference process on a network of such devices, likely by combining our approach with previously proposed frameworks to address this issue(Gacoin et\u00a0al.,2019), a very promising direction of future research.\n\nSECTION: Acknowledgements\n\nWe would like to thank Prof.\u00a0Christian Igel for his insightful feedback on the manuscript. Furthermore we thank the members of the REAL and Creative AI lab for fruitful discussions. This work was supported by a research grant (40575) from VILLUM FONDEN.\n\nSECTION: References\n\nSECTION: Vision Transformers\n\nDosovitskiy et\u00a0al. (2020)have successfully adapted the transformer architecture to computer vision by introducing a preprocessing step that converts images to suitable sequences.\nThey do so by splitting an imageinto a sequence of N flattened patches, and then pass each patch through a linear embedding layer,andare here the height, width and number of channels of the image respectively andis the patch size.is the internal latent dimension of the transformer which remains constant throughout the network and can be set as a hyperparameter.\n\nAfter converting the image into a sequence that can be processed by a transformer encoder, inspired by BERT(Devlin et\u00a0al.,2018), the authors prepend aclasstoken toin which the class information of the input image can be aggregated by the transformer. To encode position information into the embedding, a positional embedding tensoris added. Both theclasstoken as well as the positional embeddings are learnable embeddings, which are trained jointly with the rest of the network. The resulting input sequence presented to the transformer network can be expressed as\n\nThis sequence is presented to a standard transformer architecture of stacked attention modules. Each attention module consists of a multi-head self-attention (MSA) layer and a feedforward layer or multilayer perceptron (MLP) layer. MSA layers utilize self-attention (SA)(Vaswani et\u00a0al.,2017), a powerful concept that allows transformers to relate and combine its feature embeddings with each other. Self-attention extracts features from the input sequence, which in turn preforms a transformation of the input vector sequence.\n\nSpecifically, self-attention extracts query, key and value sequences,andfrom the input sequence using a linear projection:Theandsequences are then used to compute a Softmax-normalized transformation matrixindicating how to incorporate information of the whole sequence (i.e.\u00a0in our case all image patches) for every single vector of the sequence:Scaling the dot-product product byhere ensures a balanced distribution of the Softmax output. After obtaining, the output of SA is computed as\n\nA multi-head self-attention (MSA) layer(Vaswani et\u00a0al.,2017)performs several attention operations in parallel, concatenates the result and projects it back to the internally used latent dimension of the transformer:\n\nIn an attention module the multi-head self-attention layer is followed by a multi-layer-perceptron (MLP) layer transforming the recently combined embeddings to extract new feature representations. Before presentingto each layer in the module, the embeddings are normalized using LayerNorm(Ba et\u00a0al.,2016). To ensure consistent gradient flow during training, residual connections(He et\u00a0al.,2016)are behind both the MSA and the MLP layers(Wang et\u00a0al.,2019). Furthermore, as a regularization measure, Dropout(Srivastava et\u00a0al.,2014)is applied after every MSA and MLP layer.\nIn summary, given the sequencefrom a previous attention module as input, we first compute the intermediate representation\n\nwhich is the presented to the MLP layer to compute the final output of the module\n\nFinally, after N attention modules, the first vector of the sequence (corresponding to theclass-token in the preprocessed input) is handed to a linear layerto predict the final class of the image:denotes the number of classes.", "text_file": "data\\paper_texts\\2407.04513v2_content.txt"}, {"title": "Power Plant Detection for Energy Estimation using GIS with Remote\n  Sensing, CNN & Vision Transformers", "authors": ["Blessing Austin-Gabriel", "Cristian Noriega Monsalve", "Aparna S. Varde"], "published_date": "2024-12-06T12:15:11Z", "summary": "In this research, we propose a hybrid model for power plant detection to\nassist energy estimation applications, by pipelining GIS (Geographical\nInformation Systems) having Remote Sensing capabilities with CNN (Convolutional\nNeural Networks) and ViT (Vision Transformers). Our proposed approach enables\nreal-time analysis with multiple data types on a common map via the GIS,\nentails feature-extraction abilities due to the CNN, and captures long-range\ndependencies through the ViT. This hybrid approach is found to enhance\nclassification, thus helping in the monitoring and operational management of\npower plants; hence assisting energy estimation and sustainable energy planning\nin the future. It exemplifies adequate deployment of machine learning methods\nin conjunction with domain-specific approaches to enhance performance.", "arxiv_id": "2412.04986v1", "html_link": "https://arxiv.org/html/2412.04986v1", "search_term": "ti:\"vision transformers\"", "html_error": "Error processing HTML: HTTP Error 404: Not Found"}, {"title": "Slicing Vision Transformer for Flexible Inference", "authors": ["Yitian Zhang", "Huseyin Coskun", "Xu Ma", "Huan Wang", "Ke Ma", " Xi", " Chen", "Derek Hao Hu", "Yun Fu"], "published_date": "2024-12-06T05:31:42Z", "summary": "Vision Transformers (ViT) is known for its scalability. In this work, we\ntarget to scale down a ViT to fit in an environment with dynamic-changing\nresource constraints. We observe that smaller ViTs are intrinsically the\nsub-networks of a larger ViT with different widths. Thus, we propose a general\nframework, named Scala, to enable a single network to represent multiple\nsmaller ViTs with flexible inference capability, which aligns with the inherent\ndesign of ViT to vary from widths. Concretely, Scala activates several subnets\nduring training, introduces Isolated Activation to disentangle the smallest\nsub-network from other subnets, and leverages Scale Coordination to ensure each\nsub-network receives simplified, steady, and accurate learning objectives.\nComprehensive empirical validations on different tasks demonstrate that with\nonly one-shot training, Scala learns slimmable representation without modifying\nthe original ViT structure and matches the performance of Separate Training.\nCompared with the prior art, Scala achieves an average improvement of 1.6% on\nImageNet-1K with fewer parameters.", "arxiv_id": "2412.04786v1", "html_link": "https://arxiv.org/html/2412.04786v1", "search_term": "ti:\"vision transformers\"", "html_content": "SECTION: Slicing Vision Transformer for Flexible Inference\n\nVision Transformers (ViT) is known for its scalability.\nIn this work, we target to scale down a ViT to fit in an environment with dynamic-changing resource constraints.\nWe observe that smaller ViTs are intrinsically the sub-networks of a larger ViT with different widths.\nThus, we propose a general framework, namedScala, to enable a single network to represent multiple smaller ViTs with flexible inference capability, which aligns with the inherent design of ViT to vary from widths.\nConcretely, Scala activates several subnets during training, introduces Isolated Activation to disentangle the smallest sub-network from other subnets, and leverages Scale Coordination to ensure each sub-network receives simplified, steady, and accurate learning objectives.\nComprehensive empirical validations on different tasks demonstrate that with only one-shot training, Scala learns slimmable representation without modifying the original ViT structure and matches the performance of Separate Training.\nCompared with the prior art, Scala achieves\nan average improvement of 1.6% on ImageNet-1K with fewer parameters.\nCode is available athere.\n\nSECTION: 1Introduction\n\nVision Transformers (ViTs)[9]are renowned for its scalability and various avenues[41,5,7]have been explored to scale up ViT models.\nTo tailor ViTs to run on devices with limited resources,\nsome recent progress[36,35]utilize knowledge distillation[13]to scale down ViT.\nParticularly,\nDeiT[29]introduces two smaller variants of DeiT-B: DeiT-Ti and DeiT-S\nwhich have been widely used in resource-limited applications.\nAlthough these small ViTs exhibit enhanced efficiency,\nthey lack the flexibility to implement customized adjustments that accommodate dynamically changing resource constraints in real-world scenarios,\ne.g., the computation budget of mobile phones depends on the energy level (low-power mode) and number of running apps.\nConsequently, the standard Separate Training (ST) protocol trains models with different sizes separately to provide a spectrum of options with diversified performance and computation.\nST requires repetitive training procedures to produce multiple model choices,\nand the challenge is amplified for\nfoundation models[26,24,17].\nFrom users\u2019 perspective, they are only offered limited model choices,\nthat might not cater to all scenarios.\n\nAnalyzing the architectures of ViT-Ti/S/B,\nwe observe that these ViTs\nare the same architecture with the only difference in the number of embedding dimensions\n(we ignore the difference in the number of heads as it does not impact the overall model size),\nindicating smaller ViTs are intrinsically the sub-networks of larger model with different widths (see Fig.2).\nThis suggests that a large ViT can be transformed to represent small models by uniformly slicing the weight matrix at each layer.\nGiven a width ratio, we adjust the size of the network by this single hyperparameter,\nallowing a single ViT to represent multiple small variants with the weights of those sub-networks shared in a nested nature,\ne.g., ViT-B (=0.25) equals ViT-Ti and ViT-B (=0.5) corresponds to ViT-S.\nIn this manner, we empower ViTs for flexible inference capability,\nand we aim to slice a ViT within a broad slicing bound and fine-grained slicing granularity so that the diversity and number of sub-networks can be ensured for higher flexibility.\nThis problem is non-trivial as fully training all the sub-networks within a constrained budget is nearly infeasible. Consequently, it is quite challenging for these subnets to match the performance of separate training.\n\nAlthough various approaches have delved into slicing deep networks for flexible inference, the problem we target to resolve,\ni.e., uniformly slicing ViTs within a large slicing bound and fine-grained slicing granularity,\nis intrinsically different from others in three perspectives:\n(1) slicing strategy: as shown in Fig.2(i), the supernet training techniques[2,38,4]in NAS usually slice through multiple dimensions with a small slicing bound, resulting in irregularities in model architectures\nand a minor computational adjustment space.\n(2) slicing granularity: recent width slicing approaches[14,18]either slice specific portions of the network or utilize a considerably large slicing granularity, leading to a limited number of models produced.\n(3) network architecture: US-Net[37]shares a similar vision with us but it has only demonstrated success in the CNN architecture.\n\nIt is crucial to note the fundamental differences between slicing CNN and ViT:\n(1) vanilla small ViTs such as ViT-Ti/S/B, are inherently designed to vary based on widths, aligning with our approach.\nConversely, many CNNs are structured to vary from depths, like ResNet-18/34[12], and slicing them by width brings unconventional architectures.\n(2) slimmable CNN necessitates calibration[39,37]for each sub-network pre-inference due to Batch Normalization[15],\nunlike slimmable ViTs that can be directly utilized for evaluation.\n(3) the transformer architecture[33]has wider applications than CNN in this era, e.g., MAE[11], CLIP[26], DINOv2[24], LLMs[31,32,1].\nNevertheless, ViTs have much less image-specific inductive bias than CNN and\ntheir slimmable ability remains unclear.\nAs shown in Fig.2, we empirically implement US-Net on ViT-S and observe substantial performance gaps at most width ratios compared to ST, indicating that the available solution of uniform slicing does not work well on the transformer architecture.\n\nTo investigate the underlying causes of this phenomenon, we conduct analyses in Sec.3which are briefly summarized in two folds:\n(1) ViTs display minimal interpolation ability, indicating that the optimization of intermediate subnets falls notably short compared to Separate Training (ST);\n(2) sustained activation of the smallest sub-network poses a negative effect on other subnets, which affects the overall performance as their weights are shared in a nested nature.\nTo resolve these issues, we propose a general framework, namedScala, to enforce ViTs to learn slimmable representation.\nSpecifically,\nwe propose Isolated Activation to disentangle the representation of the smallest sub-network from other subnets while still preserving the lower bound performance.\nBesides, we present Scale Coordination to ensure each subnet receives simplified, steady, and accurate learning objectives.\nIn this manner, the slimmable ViT can be transformed into multiple smaller variants during inference and match the performance of ST.\n\nCompared to ST which trains all the subnets individually, Scala reduces the storage and training costs remarkably since the weights of smaller ViTs are shared with the full model and we only need one-shot training without extending the training duration.\nFurther, Scala has a very large slicing bound and fine-grained slicing granularity, enabling diverse sub-network choices during evaluation.\nIn this way, the delivered system can make tailored adjustments that accommodate dynamically changing resource constraints in real-world scenarios, promising the application on edge devices.\nCompared with the prior art SN-Net[25]which supports flexible inference on ViTs, Scala clearly outperforms it under different computation with fewer parameters.\nMoreover, Scala matches the performance of ST on various tasks without modifying the network architecture,\ndemonstrating its generalizability and potential to replace ST as a new training paradigm.\nThe contributions are summarized as follows:\n\nAlthough slicing ViTs exhibits multiple advantages, we provide detailed analysis and practical insights into the slimmable ability between different architectures (Sec.3and Tab.2) and find slicing the ViT architecture to be the most challenging problem.\n\nWe propose a general frameworkScalato enable ViTs to learn slimmable representation for flexible inference.\nWe present Isolated Activation to disentangle the representation of the smallest subnet and Scale Coordination to ensure each subnet receives simplified, steady, and accurate signals.\n\nComprehensive experiments on different tasks demonstrate that Scala, requiring only one-shot training, outperforms prior art and matches the performance of ST, substantially reduces the memory requirements of storing multiple models.\n\nSECTION: 2Related Work\n\nScaling Up ViTs.Like Transfromer[33]in NLP, scalability and performance improvements in ViTs[9]have been a central focus of recent research.\nSpecifically,\nstrategies have been explored to scale the depth of ViT[46,30]and it is scaled to even larger sizes with almost 2 billion parameters and reaches new state-of-the-art results[41].\nAfterward, ViTs have been scaled up to 4 billion[5]and 22 billion[7]parameters with extraordinary performance and enormous costs.\n\nScaling Down ViTs.The advent of ViTs has also sparked interest in scaling down these models.\nTechniques such as knowledge distillation[13]have been explored to reduce the ViT model size[36,35].\nFor example, DeiT[29]presents smaller ViTs with 5M parameters.\nAdditionally, researchers have explored quantization methods[22,21]to further compress ViTs for deployment on edge devices.\nUnfortunately, these static models cannot make customized adjustments for resource-changing environments in real scenarios.\n\nSlimmable Neural Network.The derivation of multiple smaller models from a single network has been previously explored but most works focus on the CNN structure.\nSlimmable Networks[39,37]and its variants[34,3,10]train a shared network which adapts the width to accommodate the resource constraints during inference.\nLater, this idea is adapted into two-stage NAS methods[2,38,4]for supernet training.\nThe supernet is scaled at multiple dimensions with a small computation change, in contrast to our work where we only scale the width dimension with a large slicing bound.\nSN-Net[25]is a recently proposed method that constructs a supernet with several pre-trained models and inserts linear layers to build dynamic routes for flexible inference.\nRecently, several of these techniques have been extended to Transformer architecture[14,18], while they either scale part of the network or the slicing granularity is large which means they could only deliver very few models in the end.\nDiffering from the previous works, our method is the first work to scale the ViT structure with large slicing bound and small slicing granularity which is intrinsically a more challenging problem.\n\nSECTION: 3Revisiting Slicing in Vision Transformer\n\nDue to the excessive costs of\nconstantly activating all the sub-networks during training,\nthe sandwich rule is proposed in US-Net[37]to train the slimmable network at the smallest width, largest width, and 2 random intermediate widths in each iteration so that the performance of the lower bound and upper bound are guaranteed.\nAlthough the intermediate sub-networks are optimized less frequently compared to Separate Training (ST), US-Net manages to achieve comparable performance with ST on the CNN architecture.\nTo have a better understanding of the distinction between CNN and ViT,\nwe apply US-Net to MobileNetV2[28], a CNN, and DeiT-S[29], a ViT, but constantly activate four sub-networks with the width ratio ofat each iteration.\nSubsequently, we evaluate the pre-trained models at both inboundand outboundunseen width ratios to evaluate their interpolation and extrapolation abilities, respectively.\nShown in Fig.4,\nCNN exhibits moderate interpolation and extrapolation capabilities by achieving acceptable performance at previously unobserved widths during training.\nIn stark contrast, ViT fails entirely at unseen widths, suggesting that optimizing larger sub-networks does not directly benefit the performance of smaller ViTs, even though their weights are shared in a nested nature.\n\nWe analyze the results from the expected training epochs for each sub-network.\nLetrepresent the total number of networks to be delivered,\nwhereinintermediate sub-networks are included, and according to the sandwich rule, two of these are randomly sampled during each iteration.\nFormally, the expected training epochs for the intermediate networkscan be expressed as:\n\nwhereis the number of training epochs for the full model\nand it suggests that the optimization of most sub-networks falls notably short compared to ST.\nAs ViT has demonstrated minimal interpolation ability at unseen widths compared to CNN,\neach sub-network within the slimmable ViT requires optimal utilization of every training iteration to achieve satisfactory performance.\nNevertheless, the smallest subnet is constantly activated during training according to the sandwich rule and we hypothesize that the over-emphasis of the smallest sub-network, often exhibits the worse performance, may increase the training difficulty of other subnets as their weights are shared in a nested nature.\nTo validate it, we implement US-Net[37]on DeiT-S without constantly activating the smallest sub-network.\nFig.4verifies our hypothesis showing an accuracy drop at the smallest subnet but a significant performance improvement at other width ratios.\n\nSECTION: 4Scala\n\nWe first introduce the training and inference paradigms of Scala.\nThen, we describe Isolated Activation which disentangles the smallest subnet from other sub-networks while maintaining the lower bound performance.\nFurther, we present Scale Coordination to ensure each subnet receives simplified, accurate, and steady learning objectives.\nWithout any modification to the architecture, we deliver a general framework Scala which could be easily built on existing methods.\n\nSECTION: 4.1Framework\n\nOur goal is to build a general framework that makes a ViTslimmable, i.e., the delivered network can be transformed into different small variants for flexible inference.\nFirst, we introduce a hyperparameterto denote the width ratio of the sub-network.\nBased on our analysis in Fig.4, ViTs have minimal interpolation ability, which suggests that all subnets have to be individually optimized to achieve decent performance.\nFollowing the sandwich rule[37], we sample the smallest, largest(), and 2 random intermediate width ratios,at each iteration during training.\nThe corresponding sub-networks are:,,and.\nand we accumulate the gradients of those subnets at each iteration.\nAt the inference stage, the networkis evaluated at an arbitrary width ratio that has been optimized during training by adjusting.\n\nSECTION: 4.2Isolated Activation\n\nIllustrated in Sec.3, constant activation of the smallest sub-networkensures its own accuracy at the cost of other subnets\u2019 performance.\nThis is a dilemma as there is a significant accuracy drop ofif we do not constantly activate it (see Fig.4), otherwise, the performance of other sub-networks are severely limited.\nTo alleviate this issue, we propose Isolated Activation to disentangle the representation offrom other sub-networks while still constantly activating it.\nIt not only ensures the performance of the lower bound but facilitates the optimization of other subnets as well.\n\nFormally, given the learnable weightof a random layer in ViT (stands for the output, input channel number,represents the height and width of the convolution kernel,for fully connected layers), the weight ofwhereis selected as:\n\nIn contrast, the smallest sub-networkis activated as:\n\nwhere we slice the weights in a reverse direction so that we disentangle the representation offrom other sub-networks.\nWith this simple but critical design, we not only ensure the performance ofwith constant activation, but also alleviate the negative effects it brings.\n\nSECTION: 4.3Scale Coordination\n\nWe present the training strategy of Scala in this section.\nWe follow the setting of DeiT[29]to train the full modelwith knowledge distillation and introduce a distillation token for knowledge transfer between sub-networks.\nAs our goal is to scale down a given networkto multiple smaller variants, we simply choose the pre-trained model itselfas the external teacher for the full networkto facilitate training.\nTo optimize the sub-networks at different scales, we present the Scale Coordination training strategy,\nwhich is composed of three techniques: Progressive Knowledge Transfer, Stable Sampling, and Noisy Calibration, to ensure that each subnet receives simplified, accurate, and steady learning objectives.\n\nProgressive Knowledge Transfer.Given an input image, the activated sub-networkproduces two predictions:\n\nwhereanddenote the prediction generated by the classification and distillation head, respectively.\nAs we activate multiple sub-networks:,,andat each iteration during training, our idea is to utilize the predictions of the larger network to facilitate the optimization of smaller subnets.\n\nGiven the sorted width ratio list, we utilize the KL divergence[19]loss to progressively distill the knowledge of the larger network into the smaller one:\n\nwhererepresents the number of classes,anddenotes the index ofin.\nInstead of usingas the optimization target for all smaller networks, we ensure each subnet receives simplified learning objective as small subnet have large capacity gap compared to(e.g.,is almost 16 times larger thanif) and minimizing their KL loss complicates the optimization process and leads to inferior performance.\nWith Progressive Knowledge Transfer, we simplify the optimization objective for small sub-networks by utilizingandas the teacher assistants to fill the gap betweenandand train them in a one-shot manner.\n\nStable Sampling.As the knowledge is gradually transferred from the larger network to the smaller one, the two intermediate networks serve as the bridge to connectand, asis the student ofandis the teacher of.\nTherefore, we need to carefully control the width ratiosandto prevent the obvious model capacity variation.\n\nConcretely, we introduce the slicing granularityand\nthe number of networkswe can deliver (including the full model) with a single ViT is denoted as:\n\nThen, we divide the slicing boundinto two smaller ones:\n\nwhereand,will be the random integer sampled from the uniform distribution,, respectively. Thus,andare defined as:\n\nand we ensure the model capacity gap between the four networks is stable and secure the learning objective for each subnet is steady.\n\nNoise Calibration.Although all the subnets receive guidance from larger networks, a notable issue is that the predictions from the teacher are not always accurate, sometimes even noisy, especially at the early training stage.\nTo avoid the noisy signal dominating the optimization direction, we first calculate the Cross-Entropy loss by:\n\nwhererepresents the one-hot label for class. Then, we calibrate the noise by combining the KL divergence loss and Cross-Entropy loss:\n\nwhereis a hyperparameter used to balance the two losses and we empirically letin our implementations.\nBy doing so, we mitigate the negative effects brought by the noisy predictions of the teacher model and ensure each subnet is guided by the accurate learning objectives.\n\nSECTION: 5Experiments\n\nWe validate Scala with the plain ViT structure DeiT[29].\nWe first analyze of the main property of Scala\nand compare our method with the state-of-the-art method SN-Net[25]and Separate Training (ST) at a larger scale.\nMoreover, we examine the transferability of Scala and its application on Semantic Segmentation.\nFinally, we provide ablations to validate the efficacy of our designs.\n\nSECTION: 5.1Experiment Settings\n\nAll the object recognition experiments are carried out on ImageNet-1K[8].\nWe follow the training recipe of DeiT[29]and conduct the experiments on 4 V100 GPUs.\nFor Scala, we set,, andso that we could enable a single ViT to represent 13 different networks () with a large slicing bound (i.e.,is almost 16 times larger than).\n\nSECTION: 5.2Proof-of-Concept\n\nIn this part, we conduct experiments over DeiT-S[29]for 100-epoch training to prove the concept.\n\nComparison with scaling baselines.We compare Scala with multiple scaling baselines, including:\n(1) AutoFormer[4]: we apply this ViT-based supernet training method into our setting to scale through width;\n(2) US-Net[37]: the prior work that obtains similar performance with ST over the CNN structure;\n(3) Separate Training (ST): we repetitively train the model with different widths from scratch and evaluate them individually.\nTab.1shows that AutoFormer lags behind Scala remarkably as we target to scale in a wider range.\nUS-Net shows significantly worse performance compared to ST which indicates that scaling down ViT is a more challenging problem compared to the CNN architecture.\nNevertheless, Scala achieves better performance compared to ST at all width ratios with one-shot training, reducing the storage costs of saving multiple models observably.\n\nSlicing Granularity and Bound.Fig.6shows the results of various slicing granularity.\nFirst, Scala outperforms ST with differentand the advantage at small width ratios is more obvious, which promises its application on edge devices.\nMoreover, it is shown that less fine-grained granularityresults in better overall performance with the same slicing bound as the expected training epochsfor intermediate subnets increase correspondingly.\nWe further conduct experiments with differentwhile fixingandto study the effect of the slicing bound.\nFig.6shows that smaller bounds lead to markedly better performance and it further verifies that slicing through a large bound is intrinsically more difficult, which distinguishes Scala from the supernet training methods[2,38,4]in NAS.\n\nApplication on Hybrid Structures.We experiment Scala on the CNN-ViT hybrid architecture Uniformer-S[20].\nAs Uniformer contains Batch Normalization (BN)[15]which cannot be directly evaluated after slicing because of normalization shifting[39], we calibrate the statistics of BN before inference following[37].\nShown in Fig.8, Uniformer-S is scaled down to 13 different variants with better performance compared to ST, demonstrating the generalization ability of Scala.\nHowever, performing BN calibration at each width ratio requires considerable extra effort.\nThis highlights the benefit of ViT, as Layer Normalization (LN) allows direct evaluation without additional operations.\n\nApplication on Lightweight Structures.We further validate Scala on lightweight structure Uniformer-XS[20]which integrates the design of token pruning and train these methods for 150 epochs. Shown in Fig.8, Scala still matches the performance of ST and exhibits a significant advantage at small width ratios, which promises its application on edge devices with a limited budget.\n\nFast Interpolation of Slimmable Representation.Training models with different slicing granularityfrom scratch is time-consuming and here we show that the slimmable representation of certain granularity can be scaled to others with a small amount of training epochs.\nSpecifically, we train the model with the originalfor 70 epochs and decrease the value ofin the last 30 epochs to deliver more sub-networks for higher inference flexibility.\nFig.10shows the results of fast interpolation are similar to those trained from scratch and the newly appeared sub-networks are quickly interpolated to achieve decent performance.\nWe further increasefor sub-networks with higher performance and the phenomenon shown in Fig.10is similar to down interpolation.\nBesides, we observe that the accuracy of abandoned sub-networks gradually decreases but they maintain the performance to a great extent.\n\nSlimmable Ability across Architectures.We examine the slimmable ability of different architectures in Tab.2by applying Scala on different architectures and evaluating these networks at unseen width ratios to explore the interpolation ability.\nCNN exhibits very strong interpolation ability as the performance at unseen widths lies in the range of trained width ratios.\nIn contrast, CNN-ViT and ViT suffer from remarkable performance decreases to different extents\nand ViT achieves almost zero accuracy which further validates that the problem we target to solve, i.e., slicing ViT, is the most challenging one.\n\nSECTION: 5.3Comparisons over Extended Training\n\nIn this section, we perform training over DeiT-B[29]for 300-epoch training following the standard protocol on ImageNet-1K[8]to compare with the state-of-the-art.\n\nComparisons with state-of-the-art.SN-Net[25]is state-of-the-art work that supports flexible inference on ViT.\nSpecifically, it utilizes several pre-trained models (e.g., DeiT-Ti/S/B) to construct a supernet and inserts additional layers to build dynamic routes for flexible inference.\nShown in Tab.5.2, we empirically compare Scala with SN-Net over DeiT-B following the standard 300-epoch training protocol[29].\nScala obtains similar performance with SN-Net at large width ratios and clearly outperforms it at small computational budgets.\nBesides, SN-Net has to preserve the parameters of multiple models and additional layers, while Scala only needs to keep the weights of the full network.\nWhen adopting the stronger teacher network[27]as SN-Net does, Scala outperforms SN-Net with an average improvement of 1.6across all width ratios.\n\nComparisons with Separate Training.In Tab.5.2, we compare with ST on DeiT-B[29]with longer training process, i.e., 300-epoch training, wherecorresponds to DeiT-Ti, DeiT-S and DeiT-B, respectively.\nScala exhibits a clear advantage atand matches the performance of ST exceptdue to significantly less training time.\nWhen, we can achieve similar performance atwith 40training epochs of ST.\nFurther reducingto 4, resulting in the constant activation of the two intermediate networks, allows us to consistently outperform ST at all width ratios. This substantiates the effectiveness of Scala and the slimmable representation.\n\nSECTION: 5.4Transferability\n\nTo assess the transferability of Scala, we employ DeiT-B[29]as the backbone for a 300-epoch pre-training on ImageNet-1K and leverage the foundation model DINOv2-B[24]as the teacher network to inherit good behaviors.\nOur study aims to address two key questions:\n\nWhether the slimmable representation can be transferred to downstream tasks?As depicted in Fig.11(a), Scala consistently outperforms Separate Training (ST) across all width ratios, despite the intermediate sub-networks being trained for approximately 55 epochs.\nAfter that, we conduct linear probing on video recognition dataset UCF101 with 8 evenly sampled frames and average their features for the final prediction.\nFor the classification head added on Scala, we make it slimmable to fit the features with various dimensions and follow the same training protocol as in object recognition. In Fig.11(b), two notable observations emerge:\n(1)\nScala consistently outperforms ST across different width ratios on the UCF101 dataset, implying the great transferability of the slimmable representation;\n(2) Scala retains its slimmable ability when applied to a new task and exhibits promising performance across a wide slicing range (10141 GFLOPs), promising its application on other downstream tasks.\n\nWhether the generalization ability can be maintained in the slimmable representation?Inspired by the work[43]which replicates the success of vision foundation models on ImageNet-1K, we remove all the Cross-Entropy losses during training to alleviate the dataset bias issue and inherit the strong generalization ability of the teacher network DINOv2. Then we conduct linear probing on 12 fine-grained classification datasets following the setup in DINOv2. Tab.5shows that Scala significantly outperforms DeiT variants on the average performance of fine-grained classification which suggests that Scala indeed inherits the fruitful knowledge from DINOv2 with remarkable improvement in its generalization ability. Moreover, the improvement over DeiT does not decrease when we scale down the width ratios during inference and it indicates that Scala maintains the flexible inference capability very well even though it contains more knowledge than before.\n\nAircraft\n\nCal101\n\nCars\n\nC10\n\nC100\n\nDTD\n\nFlowers\n\nFood\n\nPets\n\nSUN\n\nVOC\n\nCUB\n\nAverage\n\nSECTION: 5.5Dense Prediction\n\nIn previous sections, we have validated the effectiveness of Scala on classification tasks, we further examine whether the slimmable representation could be transferred for dense prediction task like semantic segmentation. We utilize the pre-trained model Uniformer-S[20]drawn from Fig.8, which has a hierarchical design and is obtained by 100-epoch training (our results lag behind official results where the backbone is trained for 300 epochs), and equip it with Semantic FPN[16].\nTo compare with Separate Training (ST), we extract four subnets from Scala (Uniformer-S) and train them separately.\nShown in Tab.5.6, Scala outperforms ST at all widths which verifies the slimmable representation benefits the downstream tasks.\nNote that we do not scale the decoder as it involves extra designs and is out of the scope of this work.\nHowever, we show that the slimmable representation can be generalized to semantic segmentation as feature extractors because the feature maps are spatially intact,\npromising its application as an end-to-end slimmable framework on dense prediction tasks.\n\nSECTION: 5.6Ablation Study\n\nWe conduct ablation to examine the effectiveness of our designs in Tab.5.6.\nFirst, we build Scala without Isolated Activation so that the smallest sub-network will entangle with others and it shows an obvious performance drop at all width ratios.\nThen, we remove Progressive Knowledge Transfer (PKT) and pass the knowledge fromto smaller subnets through classification token following US-Net[37].\nIt shows much worse performance, especially at small ratios, which proves the strength of PKT as it implicitly introduces some teacher assistants to simplify the optimization objective for small sub-networks.\nFurther, we random sample the width ratios ofandbetweenand compare it with Stable Sampling (SS).\nThe results are slightly inferior to SS which suggests SS is helpful in securing the steady learning objective for each sub-network.\nFinally, we remove Noise Calibration (NC) from Scala and only use the predictions from larger networks to guide the small subnets.\nIt shows remarkable performance drops at small width ratios, where the noise from the teacher network is most obvious, demonstrating the effectiveness of NC in calibrating the noise and providing accurate signals for sub-networks.\n\nSECTION: 6Conclusion and Limitations\n\nIn this paper, we observed that smaller ViTs are intrinsically the sub-networks of a large ViT with different width ratios.\nHowever, slicing ViT is very challenging due to its poor interpolation ability.\nTo address this issue, we proposed Scala to enable a single network to represent multiple smaller variants with flexible inference capability.\nSpecifically, we proposed Isolated Activation to disentangle the representation of the smallest subnet from others and presented Scale Coordination to ensure the sub-network receives simplified, steady, and accurate learning objectives.\nExtensive experiments on different tasks prove that Scala, requiring only one-shot training,\noutperforms the state-of-the-art method under different computations\nand matches the performance of Separate Training with significantly fewer parameters,\npromising the potential as a new training paradigm.\n\nOne limitation of Scala is the longer training time compared to\nconventional supervised learning of a single model, attributable to the activation of multiple subnets during training.\nNevertheless, our training time is obviously less than separately training all the sub-networks.\nIn the future, we aim to enhance the training efficiency of Scala.\n\nSECTION: References\n\nSECTION: Appendix AAppendix\n\nSECTION: A.1Implementation Details\n\nFor Separate Training (ST), we follow the exact training strategy of the official DeiT[29]and Uniformer[20]setting.\nWe use random horizontal flipping, random erasing[44], Mixup[42], CutMix[40], and RandAugment[6]for data augmentation. AdamW[23]is utilized as the optimizer with a momentum of 0.9 and\na weight decay of 0.05. We set the learning rate to 1e-3 and decay with a cosine shape.\nThe models are trained on 4 V100 and 8 A100 GPUs with a total batch size of 1024.\nWe adopt Exponential Moving Average (EMA) following the official setting.\n\nWhile we utilize the pre-trained ST model () as the teacher forto facilitate training as mentioned in the Scale Coordination section in the main paper, we adopt a larger learning rate (2e-3) and mild data augmentation (reduce the magnitude for RandAugment[6]to 1 and turn off repeated augmentation) because Scale Coordination already regularizes the network training strongly.\nAt every training iteration, we activate four sub-networks separately based on Stable Sampling and accumulate their gradients for backpropagation.\nThe rest hyperparameters are set as the same as those in Separate Training.\n\nSECTION: A.2Slimmable Ability of Vanilla Representation\n\nTab.1shows that ViT is not slimmable if we directly evaluate the vanilla pre-trained model at other widths.\nHere, we further explore the slimmable ability of vanilla representation and fine-tune the vanilla pre-trained model with Scala.\nTab.8shows that fine-tuning obtains obviously worse performance at small width ratios compared to training from scratch, which denotes that the vanilla representation is not slimmable and is essentially different from the slimmable representation.\n\nSECTION: A.3Larger Slicing Bound\n\nAs discussed in the main text, the slicing bound has a huge impact on the performance of Scala.\nHere we further expand the slicing bound fromto.\nAs shown in Fig.12, Scala suffers from an obvious performance drop atas it is not constantly activated in the new setting.\nNevertheless, our method still manages to outperform ST at all width ratios and shows a significant advantage at the smallest ratio.\n\nSECTION: A.4Longer Training Process\n\nPrevious experiments validate that Scala achieves achieves performance comparable to that of Separate Training (ST) on DeiT-B[29]in the 300-epoch training setting, even though the intermediate sub-networks training time is much less.\nWe further extend the training process to 400 epochs in this section and the results are shown in Tab.9.\nThe overall performance at various width ratios is improved with longer training and Scala () outperforms ST at all widths even though the expected training epochs for intermediate sub-networks are still much less than ST.\n\nSECTION: A.5More Ablation Studies on Activation Method\n\nWe validated the effectiveness of Isolated Activation by removing this component and we further conduct more ablation studies on the activation methods where the designs are illustrated in Fig.13.\nAs shown in Tab.10, choice (b) leads to slightly better performance at, but the performance drops atsignificantly as it is entangled withand the over-emphasize ofadversely affect its performance.\nOn the other hand, choice (c) results in a similar performance at, but the accuracy decreases significantly atwhich further verifies our hypothesis thatshould be isolated to reduce its negative impact on other sub-networks.\n\nSECTION: A.6Verification of Slimmable Ability\n\nIn the main text, we found that ViT has the minimal interpolation ability compared to the CNN structure.\nThis suggests that optimizing larger sub-networks does not directly contribute to the performance improvement of smaller variants, even though their weights are shared in a nested nature.\nA further question is, whether ViT can still maintain the slimmable ability for the unseen width ratios during training.\n\nTo verify this point, we respectively fix the width ratios ofto 0.8125 and 0.4375 during training, so that only one sub-network is optimized at each range.\nFig.14shows that the accuracy of unseen sub-networks is very low due to the lack of interpolation ability.\nNevertheless, the performance at other width ratios remains similar to the default setting even though their weights are shared with each other.\nThis indicates that the correlation between sub-networks in ViT is weak and further highlights how challenging this problem is.\n\nSECTION: A.7Comparisons with Distillation Baselines\n\nWhile we have shown that Scala outperforms baseline methods US-Net[37]and Separate Training (ST), we further compare Scala with much stronger baselines, by adding an external teacher to their full network during training.\nSpecifically, we adopt the pre-trained full model from ST as the teacher and conduct knowledge distillation for models with different widths separately.\nTab.11shows that ST+KD exhibits similar performance at larger width ratios with Scala, despite that Scala clearly outperforms ST+KD at smaller widths, promising its application on edge devices.\nAlthough obtaining a better full model, US-Net+KD exhibits worse performance at smaller width ratios because it utilizes the full network as the teacher for all subnets and this phenomenon verifies our motivation of proposing Progressive Knowledge Transfer.\n\nSECTION: A.8Comparisons in Training Time\n\nAssuming to deliver 13 models in the end, we compare the training time (100 Epoch) of Scala with US-Net[37], Separate Training on 8 A100 GPUs. The difference between US-Net and Scala is not large as the transformer architecture has been well-optimized on GPU and we do observe a significant time gap between Scala and Separate Training as they have to train 13 models iteratively. Moreover, Scala can be configured to deliver 25 models without an increase in training time as we sample 4 networks at each iteration in all scenarios which further highlights our strengths.\n\nSECTION: A.9Comparisons with MatFormer\n\nMatFormer[18]only slices the FFN block in the transformer architecture so it offers a minor computational adjustment space and we adapt their method on DeiT-S to compare with Scala. Fig.15shows that Scala achieves comparable performance with it (better in most cases) whenwith a larger adjustment scope.\n\nSECTION: NeurIPS Paper Checklist\n\nClaims\n\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope?\n\nAnswer:[Yes]\n\nJustification: Our contribution is to enable ViT to become slimmable during inference which is described in the abstract and introduction.\n\nGuidelines:\n\nThe answer NA means that the abstract and introduction do not include the claims made in the paper.\n\nThe abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.\n\nThe claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.\n\nIt is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.\n\nLimitations\n\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\n\nAnswer:[Yes]\n\nJustification: See conclusion.\n\nGuidelines:\n\nThe answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.\n\nThe authors are encouraged to create a separate \"Limitations\" section in their paper.\n\nThe paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.\n\nThe authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.\n\nThe authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.\n\nThe authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.\n\nIf applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.\n\nWhile the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.\n\nTheory Assumptions and Proofs\n\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?\n\nAnswer:[N/A]\n\nJustification: We do not have theory included.\n\nGuidelines:\n\nThe answer NA means that the paper does not include theoretical results.\n\nAll the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.\n\nAll assumptions should be clearly stated or referenced in the statement of any theorems.\n\nThe proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.\n\nInversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.\n\nTheorems and Lemmas that the proof relies upon should be properly referenced.\n\nExperimental Result Reproducibility\n\nQuestion: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?\n\nAnswer:[Yes]\n\nJustification: We have included the implementation details in the main text and appendix.\n\nGuidelines:\n\nThe answer NA means that the paper does not include experiments.\n\nIf the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.\n\nIf the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.\n\nDepending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.\n\nWhile NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example\n\nIf the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.\n\nIf the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.\n\nIf the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).\n\nWe recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.\n\nOpen access to data and code\n\nQuestion: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?\n\nAnswer:[Yes]\n\nJustification: We have included the details and will release the code soon.\n\nGuidelines:\n\nThe answer NA means that paper does not include experiments requiring code.\n\nPlease see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n\nWhile we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).\n\nThe instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n\nThe authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.\n\nThe authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.\n\nAt submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).\n\nProviding as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.\n\nExperimental Setting/Details\n\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?\n\nAnswer:[Yes]\n\nJustification: We conduct experiments following the standard protocol and include the details.\n\nGuidelines:\n\nThe answer NA means that the paper does not include experiments.\n\nThe experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.\n\nThe full details can be provided either with the code, in appendix, or as supplemental material.\n\nExperiment Statistical Significance\n\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?\n\nAnswer:[No]\n\nJustification: Repeating experiments on ImageNet requires lots of resources.\n\nGuidelines:\n\nThe answer NA means that the paper does not include experiments.\n\nThe authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.\n\nThe factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).\n\nThe method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)\n\nThe assumptions made should be given (e.g., Normally distributed errors).\n\nIt should be clear whether the error bar is the standard deviation or the standard error of the mean.\n\nIt is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.\n\nFor asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).\n\nIf error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.\n\nExperiments Compute Resources\n\nQuestion: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?\n\nAnswer:[Yes]\n\nJustification: We have provided details of our computing resources in the appendix.\n\nGuidelines:\n\nThe answer NA means that the paper does not include experiments.\n\nThe paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.\n\nThe paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.\n\nThe paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper).\n\nCode Of Ethics\n\nQuestion: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethicshttps://neurips.cc/public/EthicsGuidelines?\n\nAnswer:[Yes]\n\nJustification: We have read and follow the rules.\n\nGuidelines:\n\nThe answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n\nIf the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.\n\nThe authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).\n\nBroader Impacts\n\nQuestion: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?\n\nAnswer:[N/A]\n\nJustification: There is no societal impact.\n\nGuidelines:\n\nThe answer NA means that there is no societal impact of the work performed.\n\nIf the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.\n\nExamples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.\n\nThe conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.\n\nThe authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.\n\nIf there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).\n\nSafeguards\n\nQuestion: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?\n\nAnswer:[N/A]\n\nJustification: Our work has no such risks.\n\nGuidelines:\n\nThe answer NA means that the paper poses no such risks.\n\nReleased models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.\n\nDatasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.\n\nWe recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.\n\nLicenses for existing assets\n\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?\n\nAnswer:[Yes]\n\nJustification: We have cited the papers that created the datasets.\n\nGuidelines:\n\nThe answer NA means that the paper does not use existing assets.\n\nThe authors should cite the original paper that produced the code package or dataset.\n\nThe authors should state which version of the asset is used and, if possible, include a URL.\n\nThe name of the license (e.g., CC-BY 4.0) should be included for each asset.\n\nFor scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.\n\nIf assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets,paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.\n\nFor existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.\n\nIf this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators.\n\nNew Assets\n\nQuestion: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?\n\nAnswer:[N/A]\n\nJustification: We do not release new assets.\n\nGuidelines:\n\nThe answer NA means that the paper does not release new assets.\n\nResearchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.\n\nThe paper should discuss whether and how consent was obtained from people whose asset is used.\n\nAt submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.\n\nCrowdsourcing and Research with Human Subjects\n\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?\n\nAnswer:[N/A]\n\nJustification: Our paper does not involve crowdsourcing nor research with human subjects.\n\nGuidelines:\n\nThe answer NA means that the paper does not involve crowdsourcing nor research with human subjects.\n\nIncluding this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.\n\nAccording to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.\n\nInstitutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects\n\nQuestion: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?\n\nAnswer:[N/A]\n\nJustification: Our paper does not involve crowdsourcing nor research with human subjects.\n\nGuidelines:\n\nThe answer NA means that the paper does not involve crowdsourcing nor research with human subjects.\n\nDepending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.\n\nWe recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.\n\nFor initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.", "text_file": "data\\paper_texts\\2412.04786v1_content.txt"}, {"title": "Megatron: Evasive Clean-Label Backdoor Attacks against Vision\n  Transformer", "authors": ["Xueluan Gong", "Bowei Tian", "Meng Xue", "Shuike Li", "Yanjiao Chen", "Qian Wang"], "published_date": "2024-12-06T04:39:41Z", "summary": "Vision transformers have achieved impressive performance in various\nvision-related tasks, but their vulnerability to backdoor attacks is\nunder-explored. A handful of existing works focus on dirty-label attacks with\nwrongly-labeled poisoned training samples, which may fail if a benign model\ntrainer corrects the labels. In this paper, we propose Megatron, an evasive\nclean-label backdoor attack against vision transformers, where the attacker\ninjects the backdoor without manipulating the data-labeling process. To\ngenerate an effective trigger, we customize two loss terms based on the\nattention mechanism used in transformer networks, i.e., latent loss and\nattention diffusion loss. The latent loss aligns the last attention layer\nbetween triggered samples and clean samples of the target label. The attention\ndiffusion loss emphasizes the attention diffusion area that encompasses the\ntrigger. A theoretical analysis is provided to underpin the rationale behind\nthe attention diffusion loss. Extensive experiments on CIFAR-10, GTSRB,\nCIFAR-100, and Tiny ImageNet demonstrate the effectiveness of Megatron.\nMegatron can achieve attack success rates of over 90% even when the position of\nthe trigger is slightly shifted during testing. Furthermore, Megatron achieves\nbetter evasiveness than baselines regarding both human visual inspection and\ndefense strategies (i.e., DBAVT, BAVT, Beatrix, TeCo, and SAGE).", "arxiv_id": "2412.04776v1", "html_link": "https://arxiv.org/html/2412.04776v1", "search_term": "ti:\"vision transformers\"", "html_error": "Error processing HTML: HTTP Error 404: Not Found"}, {"title": "Superpixel Tokenization for Vision Transformers: Preserving Semantic\n  Integrity in Visual Tokens", "authors": ["Jaihyun Lew", "Soohyuk Jang", "Jaehoon Lee", "Seungryong Yoo", "Eunji Kim", "Saehyung Lee", "Jisoo Mok", "Siwon Kim", "Sungroh Yoon"], "published_date": "2024-12-06T00:38:36Z", "summary": "Transformers, a groundbreaking architecture proposed for Natural Language\nProcessing (NLP), have also achieved remarkable success in Computer Vision. A\ncornerstone of their success lies in the attention mechanism, which models\nrelationships among tokens. While the tokenization process in NLP inherently\nensures that a single token does not contain multiple semantics, the\ntokenization of Vision Transformer (ViT) utilizes tokens from uniformly\npartitioned square image patches, which may result in an arbitrary mixing of\nvisual concepts in a token. In this work, we propose to substitute the\ngrid-based tokenization in ViT with superpixel tokenization, which employs\nsuperpixels to generate a token that encapsulates a sole visual concept.\nUnfortunately, the diverse shapes, sizes, and locations of superpixels make\nintegrating superpixels into ViT tokenization rather challenging. Our\ntokenization pipeline, comprised of pre-aggregate extraction and\nsuperpixel-aware aggregation, overcomes the challenges that arise in superpixel\ntokenization. Extensive experiments demonstrate that our approach, which\nexhibits strong compatibility with existing frameworks, enhances the accuracy\nand robustness of ViT on various downstream tasks.", "arxiv_id": "2412.04680v1", "html_link": "https://arxiv.org/html/2412.04680v1", "search_term": "ti:\"vision transformers\"", "html_content": "SECTION: Superpixel Tokenization for Vision Transformers:Preserving Semantic Integrity in Visual Tokens\n\nTransformers, a groundbreaking architecture proposed for Natural Language Processing (NLP), have also achieved remarkable success in Computer Vision.\nA cornerstone of their success lies in the attention mechanism, which models relationships among tokens.\nWhile the tokenization process in NLP inherently ensures that a single token does not contain multiple semantics, the tokenization of Vision Transformer (ViT) utilizes tokens from uniformly partitioned square image patches, which may result in an arbitrary mixing of visual concepts in a token.\nIn this work, we propose to substitute the grid-based tokenization in ViT with superpixel tokenization, which employs superpixels to generate a token that encapsulates a sole visual concept.\nUnfortunately, the diverse shapes, sizes, and locations of superpixels make integrating superpixels into ViT tokenization rather challenging.\nOur tokenization pipeline, comprised of pre-aggregate extraction and superpixel-aware aggregation, overcomes the challenges that arise in superpixel tokenization.\nExtensive experiments demonstrate that our approach, which exhibits strong compatibility with existing frameworks, enhances the accuracy and robustness of ViT on various downstream tasks.\nThe source codes are available at:https://github.com/jangsoohyuk/SuiT\n\nSECTION: 1Introduction\n\nTransformers[51], originally proposed for Natural Language Processing (NLP), have demonstrated exceptional performance across diverse tasks such as machine translation[55,19,5], natural language understanding[27,34], and text generation[41,31].\nThe transformative power of Transformers was again attested by the release of large language models[7], which continue to revolutionize NLP as we know it.\nSimilarly, the Vision Transformer (ViT)[14], which extended the Transformer architecture to image recognition, changed the landscape of Computer Vision research.\nThe subsequent research efforts to broaden the usage of ViTs to various visual tasks, including detection[45,8,65], segmentation[32,63,48], generation[15,38,60], and multi-modal applications[42], have made the ViT the dominant architecture in Computer Vision.\n\nIn NLP, the success of Transformers is commonly attributed to the attention mechanism, which captures contextual dependencies by modeling the relationship among input texts, which are expressed as tokens.\nWidely adopted tokenization methods in NLP, such as Byte Pair Encoding[46],\nWordPiece[47], and SentencePiece[29], first split each sentence into words, which are then broken down into more granular tokens.\nFor instance, when tokenizing \u201cAttention is All You Need,\u201d each word is separated into: [\u201cAttention,\u201d \u201cis,\u201d \u201cAll,\u201d \u201cYou,\u201d \u201cNeed\u201d].\nBy definition, this initial word-level splitting of sentences prevents multiple words with different meanings from being fused into a single token.\n\nIn contrast, fixed-size grid patches form the basis of tokenization in ViT and its variants[14,50,35].\nAs visualized in Figure1, when na\u00efvely sliced square image patches are used as tokens, a single token can simultaneously contain two or more visual concepts,e.g.,eye and fur.\nConsequently, this approach in ViT diverges from the core principle of tokenization, which is to convert plain inputs into smaller yet more structured subunits for further processing.\n\nIn this work, we employ the concept of superpixels to design a tokenization process that yields semantically preserving tokens that contain a single, isolated visual concept.\nA superpixel is a cluster of connected pixels in an image that share similar properties, such as color, texture, or position, and it is often used as a fundamental unit in image processing[2,11,33].\nFigure1illustrates how leveraging superpixels alters tokenization of ViT.\nGrouping pixels into superpixels divides an image into cohesive regions that capture similar semantics, thereby preventing the mixture of unrelated semantic information in each token.\n\nDespite the advantage of superpixels in tokenization, the different shapes and sizes of each superpixel present significant challenges in utilizing them as ViT tokens[25,59,62].\nIn ViTs, tokens are generated from fixed-size square patches at predefined locations through simple flattening and linear projection.\nBecause superpixels consist of varying numbers of pixels, de-facto flattening and linear projection operations in ViT are incompatible with superpixels.\nMoreover, the location of each superpixel changes dynamically depending on the image.\nThe variability of superpixel locations makes it infeasible to directly apply the positional embeddings of ViT, which are designed for fixed locations.\nThe inadequacy of original ViT operations necessitates novel techniques for addressing irregular geometry and composition to reap the inherent benefits of superpixels.\n\nWe thus propose a novel tokenization pipeline that overcomes the aforementioned problems and enables the effective incorporation of superpixels into ViT tokenization.\nThe proposed pipeline consists of two major technical components:pre-aggregate feature extractionandsuperpixel-aware feature aggregation.\nThe first stage prepares pre-aggregate features for the subsequent superpixel-aware aggregation, which utilizes pooling operations to remove the irregularity and variability of superpixels.\nApplying pooling operations directly on the input image may result in a severe loss of information.\nBy training the pre-aggregate feature extractor and applying it prior to our feature aggregation, we effectively sidestep this potential information loss.\nCollectively, this two-stage process allows the proposed tokenization approach to leverage the benefits of superpixels\nwhile preserving important details of an image.\n\nWe demonstrate the superiority of ourSuperpixel-Tokenized VisionTransformer (SuiT) compared to the baselines. SuiT outperforms in a variety of tasks including ImageNet-1K classification[12], segmentation[56], transfer learning[66], and self-supervised learning[9]. We further analyze the properties that emerge using our tokenization pipeline by conducting extra examinations.\nFurthermore, we visualize that our proposed tokenization method indeed results in image tokens that preserve semantics more effectively than the grid-based tokenization method.\n\nOur contribution can be summarized as follows:\n\nWe propose an effective superpixel-based tokenization method that overcomes the challenges associated with the use of superpixels.\n\nWe empirically show that our tokenization method produces tokens that better preserve semantic information than grid-based tokens.\n\nWe demonstrate the effectiveness of our approach through experiments on image classification, transfer learning, self-supervised learning, and zero-shot segmentation.\n\nSECTION: 2Related Work\n\nSuperpixels, which are perceptually meaningful clusters of pixels, have been a foundational concept in computer vision[2,11,33].\nPrevious studies have attempted to use superpixels in several tasks such as segmentation[30,37]and object detection[18], but these works did not demonstrate the broad applicability of superpixels across a range of vision tasks.\n\nAnother recent work, STViT[23], applies superpixel-like clustering across all layers via supertoken attention, but operates at the token level, resulting in a coarser granularity than superpixels.\nCoC[36], borrows the concept of superpixels within neural networks but lacks further exploration of superpixel tokenization.\nSimilarly, SPFormer[37]utilizes superpixel-based representations but requires specialized attention modules making it difficult to integrate with existing ViT architectures in a plug-and-play manner.\n\nIn contrast to these approaches, our research explores a simple yet effective superpixel tokenization method that can be easily employed in a transformer architecture for a range of general-purpose tasks.\n\nWhile most ViT variants focus on improving backbone architectures and attention mechanisms using naive square-shaped patches, there have been several attempts in exploration of advanced tokenization strategies tailored for vision tasks. Quadformer[44]and MSViT[16]both propose adaptive tokenization strategies that dynamically adjust token resolutions based on image content.\n\nThe studies SPiT[1]and sViT[28]are the most similar to our work in that they target only the tokenizing module while keeping the backbone intact and share similar research motivations. SPiT[1]focused on analyzing this tokenization from the explainable AI (XAI) perspective but did not explore its generalizability across various tasks. Also, SPiT[1]suffers from performance degradation without gradient features, as they lack of abundant features due to their learning free feature extraction. Moreover, its applicability to other learning paradigms, such as self-supervised learning[9], was not investigated.\n\nsViT[28]shared a similar motivation in aiming for semantically preserving tokenization, but it still divides tokens based on a na\u00efve bounding box, failing to create truly semantically preserving tokens. Additionally, resizing these tokens into square shapes compromised the visual structure of objects within the tokens. These shortcomings in design leads leads to limited performance in various tasks.\n\nAs described in the previous section, we propose a semantically preserving tokenization through superpixels in vision and investigate the performance of our method across various tasks and explore its adaptability to self-supervised frameworks.\n\nSECTION: 3Preliminaries\n\nIn this section, we introduce the key concepts and notations that are integral to our work, which aims to construct semantically preserving ViT tokens with superpixels.\nIn Section3.1, we describe the mechanism of superpixel algorithms, and in Section3.2, we go over the traditional ViT tokenization.\nSubsequently, we discuss the research challenges in adopting superpixels as tokens in ViTs.\n\nSECTION: 3.1Superpixel Algorithms\n\nSuperpixels are clusters of image pixels grouped together based on color, position, or other similarity measures.\nThey provide a compact and meaningful representation of the image by grouping pixels into regions that align with its underlying structure.\n\nGiven an input image, a superpixel algorithmproduces a superpixel index map:\n\nwhereis the number of superpixels, andis a parameter that controls the trade-off between color and positional distance, i.e., compactness in SLIC[2].\n\nThe superpixel index of at coordinatecan be defined as, where.\nA-th superpixelcan then be defined as:\n\nwhereis the pixel at. Each superpixelconsists of varying numbers of pixels with different shapes and sizes.\n\nSECTION: 3.2Tokenization in Vision Transformers\n\nIn grid-based tokenization of conventional ViT and its variants[50,35,42], the input imageis divided intonon-overlapping patches of size, each represented as, wherecorresponds to the index of each patch. Each patchis flattened into a vectorand linearly projected using an embedding matrix, whereis the dimension of tokens.\nIn addition to this, positional embeddingof its location is injected to form the input token.\nThe set of tokenswhich originates from patches as above, is given to the ViT models along with a learnable classification tokenfor classification.\n\nThe tokenization process in ViT requires square-shaped and fixed-sized patches, acquired at uniform locations.\nThese constraints on image patches make a na\u00efve application of superpixels to ViT tokenization challenging.\nFirst, the straightforward flatten-and-project operation is incompatible with superpixels that come in varying sizes and shapes[25,59,62].\nFlattening of superpixels with diverse shapes and varying numbers of pixels yields vectors of varying lengths, which hinder a simple linear projection.\nSecond, unlike grid patches with fixed positions, superpixels are located at different positions depending on the image.\nConsequently, the standard positional embedding, which relies on fixed positions, is inappropriate for superpixels.\n\nSECTION: 4Method\n\nTo address the aforementioned issues in Section3, we propose a novel superpixel tokenization pipeline that effectively embeds each superpixel into a single token.\nThe overview of superpixel tokenization is illustrated in Figure2.\nOur proposed pipeline incorporates two components: pre-aggregate feature extraction (Section4.1) and superpixel-aware feature aggregation (Section4.2).\nOur aggregation is based on pooling operations to address the inconsistent size and shape of superpixels.\nHowever, na\u00efve application of such aggregation on the RGB space may lead to critical loss of information.\nTo sidestep this potential risk, we train a pre-aggregate feature extractor and use its output as input for the superpixel-aware aggregation.\nThese two processes of the proposed tokenization method together exploit the benefits of superpixel-based tokenization while conserving critical image information.\n\nSECTION: 4.1Pre-aggregate Feature Extraction\n\nWe start by extracting local features from the input image using a simple convolutional block.\nWe adopt the design of the initial convolutional block in ResNet[17],\nwhich consists of aconvolutional layer, a Batch Normalization layer[24], and a ReLU activation function[3].\nWe make a slight modification by replacing ReLU with GELU[20], to align with recent advances in the field[14,35].\nGiven an input imageand the convolutional block, we extract local features.\n\nPositional features play a crucial role in transformers, as they are unaware of the order of input tokens.\nWhile the original Transformer proposed to use sinusoidal positional encodings with pre-defined frequencies[51], ViTs opted to use learnable positional embeddings for each pre-defined locations[14,50].\n\nSuperpixels have complex positional information of high granularity. Using the learnable positional embeddings as in ViT is inadequate, as it would require excessive number of parameters,i.e.,learnable embeddings each of-dimensions.\nTo keep it parameter-efficient, we adopt the sinusoidal positional encoding with learnable frequencies, which is known to capture high frequency information efficiently[49].\nTo our knowledge, this positional encoding scheme has not been applied to the Transformer literature before, and is a peculiar choice to adapt to our design of using superpixels of high-granularity.\n\nConsequently, we acquire the value of-th dimension in-dimensional positional featureat each spatial coordinateas below:\n\nwhereandeach denote the frequency of horizontal and vertical axis in the-th dimension.\n\nGiven the local featuresand the positional features,\nwe integrate the two to obtain our pre-aggregate features.\nThe two feature mapsandare concatenated and passed to a linear projection layer:\n\nwheredenotes concatenation in the channel dimension.\nThe resulting pre-aggregate feature mapis used in the subsequent aggregation stage.\n\nSECTION: 4.2Superpixel-aware Feature Aggregation\n\nTo aggregate the extracted pre-aggregate features within each superpixel into a representative embedding,\nwe employ average and max pooling, which effectively handles features of varying numbers.\nThe pre-computed superpixel index mapof an input imagexis used as the guideline for determining which features belong to which superpixels.\n\nGiven the-dimensional pre-aggregate features, we perform average pooling and max pooling within each superpixel cluster.\nThis results in two feature vectors per superpixel: average featuresand max features, each of-dimensions:\n\nwhereanddenote the number of pixels within superpixeland the pre-aggregate feature at coordinate, respectively.\n\nAverage pooling and max pooling extract complementary features from each superpixel. Average pooling captures the overall characteristics common in the superpixel, while max pooling identifies the most salient features that may be obscured by averaging alone.\nWe concatenate-dimensional vectors from each pooling method along the channel dimension, yielding a-dimensional token:\n\nThis approach effectively combines both general and prominent information, creating a more comprehensive representation of each superpixel.\n\nSECTION: 5Experiments\n\nSizeModel# Params.GMACs# tokensTop-1TinyDeiT5.7M1.2619672.2SuiT5.5M0.7910072.21.2617175.31.4419675.7SmallDeiT22.1M4.6119679.8SuiT21.7M3.6413279.84.6017280.55.2019680.9BaseDeiT86.8M17.619681.8SuiT86.0M16.416081.817.617382.019.719682.1\n\nSECTION: 5.1Experimental Settings\n\nWe validate our tokenization method by applying it to ViTs, dubbed as SuiT.\nWe experiment on three model scales, Tiny (), Small (), and Base (). The valuesandare both set to.\n\nWe employ FastSLIC[4]as our superpixel algorithm due to its high\nspeed.\nWe use a stride of 2 in the convolutional layer of local feature extraction, which produces feature maps ofsmaller size than the input. The positional feature mapand superpixel index mapare scaled to the same size accordingly.\n\nThis design choice was based on the observation that using a stride of 1 offered no notable performance improvements while significantly increasing computational costs.\nFurther details are described in Appendix.\n\nSECTION: 5.2Comparison to Baseline\n\nAs our approach focuses on tokenization without modifying the backbone network, we focus on comparison with our baseline, DeiT[50]on ImageNet-1K classification[12].\nWe mostly follow the experimental settings from DeiT, with minimal changes for fair comparison.\n\nTable1compares the classification performance of SuiT to that of DeiT. For each model size, the table provides a detailed comparison across key aspects: classification accuracy, computational cost measured in GMACs, and the number of tokens processed.\nThis adaptive inference capability arises from the flexibility to adjust the number of superpixel tokens, enabling users to customize the configuration based on specific needs.\nWhen achieving comparable performance, SuiT exhibits substantially higher efficiency than DeiT. At equivalent GMACs, SuiT delivers superior accuracy. Maintaining an identical number of tokens further boosts its accuracy,\n\nSECTION: 5.3Comparison to Existing Tokenization Methods\n\nWe evaluate our tokenization method against existing approaches under two scenarios used in existing tokenization studies. For a fair comparison, we follow the setup of[44,16], which fine-tunes a ViT pretrained on ImageNet-21K[43]. Additionally, we include SPiT[1], a concurrent work leveraging superpixels for tokenization, which trains its model from scratch on ImageNet-1K. Both experiments are conducted on ImageNet-1K classification tasks.\n\nAs reported inTable2, our method consistently outperforms existing tokenization methods across all scales of model size, showing its efficacy.\nOn fine-tuning experiments following[44,16], SuiT outperforms existing tokenization methods for fine-grained tokens, such as Quadformer[44]and MSViT[16].\n\nIn experiments for training from scratch following[1], SuiT consistently outperforms SPiT, another work on superpixel-based tokenization, by a noticeable margin.\nThese results verify the excellence of our tokenization pipeline compared to prior work.\n\nSECTION: 5.4Transfer Learning\n\nWe verify the generalization ability of SuiT when transferred to various downstream tasks. We follow the standard fine-tuning protocol and default settings of DeiT[50]for fair comparison, by fine-tuning ImageNet-1K pre-trained SuiT to each downstream task specifically. We test our models on iNaturalist[22], Flowers102[39], and StanfordCars[10]. Further details can be found in the Appendix.\n\nTable3presents a comparison of the classification accuracy between DeiT and SuiT when fine-tuned on various downstream tasks.\nAcross various tasks and model scales, \u00a0SuiT consistently achieves higher accuracy than DeiT. Notably, on iNaturalist2018, SuiT-Small outperforms DeiT-Small by 0.8%p, demonstrating its superior ability to effectively transfer learned features, particularly when ample data is available.\n\nThese results indicate that SuiT shows improved generalizability than DeiT. This positions SuiT a strong choice for transfer learning across diverse domains. The consistently superior performance of SuiT, regardless of model size or dataset complexity, further underscores its suitability for real-world applications.\n\nSECTION: 5.5Self-supervised Learning\n\nWe further study the applicability of SuiT in self-supervised scenario. Specifically, we train SuiT on ImageNet-1K with DINO[9], a well-known self-supervised learning approach, chosen for its proven effectiveness and popularity.\n\nAfter pre-training SuiT with DINO, we evaluate its performance when transferred to downstream tasks to confirm the quality of learned features.\nAs shown in the lower section of Table3, DINO-SuiT consistently surpasses DINO-ViT across all downstream tasks. This demonstrates that SuiT effectively supports self-supervised scenarios, further establishing its strength as a versatile pipeline for a wide range of vision tasks.\n\nIt is well known that the attention map of ViTs trained with DINO effectively attends to object regions without explicit supervision[9]. As shown in Figure3, while DINO-ViT\u2019s self-attention maps are limited to square patch-level details, those of DINO-SuiT even capture finer structures of salient objects.\nTo demonstrate that this fine-grained property synergistically enhances the salient object segmentation capabilities of DINO[9], we measure the segmentation performance on three benchmark datasets, ECSSD[57], DUTS[54]and DUT-OMRON[58]following the works ofWang et\u00a0al.[56],Aasan et\u00a0al.[1].\nWe conduct our experiments on the TokenCut framework[56], which is a normalized graph cut algorithm that extracts saliency mask from attention map.\n\nSuperpixel tokenization significantly improves DINO-ViT[9]performance across most metrics without post-processing, achieving results comparable to DINO-ViT with post-processing.\n\nNotably, as shown in Table4, our model exhibits minimal performance differences between post-processed and non-post-processed cases, unlike DINO-ViT.\nCompared to SPiT[1], SuiT demonstrates competitive performance across three datasets. However, SPiT[1]does not show its applicability to self-supervised methods such as DINO and is trained with larger resolution images.\nQualitative comparisons in Figure4highlight that DINO-SuiT detects all salient objects, even in multi-object scenarios, whereas DINO-ViT shows limitations such as rectangular masks without post-processing and incomplete object detection.\n\nSECTION: 6Analysis\n\nSECTION: 6.1Semantic Integrity of Superpixel Tokens\n\nTo verify the improved semantic preservation of our superpixel tokens, we perform K-means clustering[26]on tokens embedding space. We compare the clustering results of superpixel tokens with grid-based tokens.\n\nAs shown in Figure5, our superpixel tokens are grouped based on semantic similarity rather than spatial proximity, unlike the patch tokens from DeiT, which lack consistency within clusters. For example, in the first row, tokens corresponding to the regions of ducklings are grouped together, whereas clusters from DeiT\u2019s patch tokens fail to convey clear meaning. This analysis suggests that superpixel tokens are better at preserving semantics. Additional results are provided in the Appendix.\n\nSECTION: 6.2Class Identifiability Analysis\n\nVilas et\u00a0al.[52]examined class-specific encoding in supervised ViTs by projecting intermediate representations through the trained classifier head into a class-specific embedding space, similar to the logit lens method in NLP[40,6]. Building on this, we evaluate class-specific encoding across SuiT\u2019s layers by projecting token representations through the classifier head and measuring the proportion of tokens correctly predicting the target class.\n\nFigure6shows that SuiT\u2019s classification token shows higher class identifiability in the earlier layers compared to DeiT. This can be attributed to the semantic preserving nature of superpixel tokenization, which aids in aggregating class-specific encoding in early layers. In the deeper layers, DeiT\u2019s patch tokens exhibit class identifiability levels slightly lower than its classification token, with a marginal gap. This observation aligns with findings from previous studies[52,53], which report that patch tokens in supervised ViTs tend to encode increasingly global information as layers deepen. In contrast, SuiT\u2019s patch tokens maintain a lower proportion of class-identifiable features in the later layers, suggesting a reduced emphasis on global feature encoding. This divergence alludes that SuiT and DeiT adopt distinctive strategies to aggregate class-specific features at different network depths.\n\nSECTION: 6.3Enhanced Robustness\n\nWe examine the robustness and out-of-distribution (OOD) generalization ability of SuiT in ImageNet variants, ImageNet-A[13]and ImageNet-O[21]. ImageNet-A is a subset of ImageNet-1K curated to mislead classifiers and test model robustness, while ImageNet-O, composed of OOD samples not in ImageNet-1K, evaluates the OOD detection capabilities of models.\n\nIn Table5, SuiT consistently outperforms DeiT on both datasets across model variants.\n\nThese findings indicate that our superpixel tokenization improves model robustness in challenging scenarios by mitigating the model\u2019s reliance on features that are easily learned but lack generalizability.\n\nSECTION: 6.4Ablation Studies\n\nThe following ablation studies on our proposed components are conducted using SuiT-Tiny.\nTable6highlights the importance of components in pre-aggregate feature extraction. First, using raw 3-channel RGB features leads to a significant performance drop, confirming the need for higher-dimensional local features. Second, removing positional information reduces performance but retains partial class prediction through local cues. Experiments also reveal that predefined positional frequencies under-perform compared to learned frequencies, emphasizing the need for vision-specific customization. Lastly, our results show that the combination of concatenation and projection more effectively integrate local and positional features than simple concatenation or addition used in prior studies[51,14,50].\n\nTable7presents the analysis of our aggregation method, which employs cardinality-independent operations. Removing max pooling results in a notable performance drop, and removing average pooling causes a slight decrease. We also tested standard deviation as an alternative to max pooling, but it proved numerically unstable and unsuitable for training. Softmax pooling, which computes a weighted sum of values within each superpixel, showed promising results but slightly underperformed compared to max pooling. This may be due to the softmax operation suppressing prominent features crucial for recognition, leading to less discriminative representations.\n\nSECTION: 7Conclusion\n\nIn this work, we proposed a novel superpixel-based tokenization method for ViTs as an attempt to improve the visual tokenization process.\nBy leveraging superpixels, the proposed tokenization obtains semantic-preserving tokens, overcoming the issue of mixed semantics in grid-based patches.\nTo incorporate superpixels that are irregular in shape, size, and location, we proposed a two-stage pipeline consisting of pre-aggregate feature extraction and superpixel-aware feature aggregation.\nExtensive experiments demonstrated the effectiveness of our method on diverse tasks,e.g.,supervised classification, transfer learning, and self-supervised learning. Additional analytical findings provided meaningful insights into the mechanism and emergent properties of superpixel tokenization.\nWe hope our promising results inspire further exploration of token design in ViTs, fostering advancements in both model performance and interpretability.\n\nSECTION: References\n\nSupplementary Material\n\nSECTION: A1Implementation Details\n\nSECTION: A1.1ImageNet-1K classification\n\nThe settings for experiments from scratch is presented in TableA2. We mostly follow the settings of DeiT[50]with minimal changes.\nThe main difference to the DeiT setting is as follows:\n1) to align with our main concept of using superpixels, we disable\nMixUp[61].\n2) In random erasing[64],\nthe randomly erased region is filled with per-channel random color instead of per-pixel random color. 3) We use gradient clipping, which was known to be helpful in ViT[14], but later removed in DeiT[50].\nOther than that, we slightly tweak hyper-parameters of learning rate, weight decay and stochastic depth.\nFor models of all three scales, Tiny, Small and Base, we use a base learning rate of, which is scaled proportionally to the batch size, by. We use different weight decay and stochastic depth values for each scale. Most notably, our model uses a larger weight decay and drop rate for stochastic depth, especially in Base scale. This is because we found that our model tends to overfit quickly, compared to the baseline DeiT. We thus opt to use a stronger regularization term. Similar observations were made in Transfer Learning, which is also discussed in the SectionA1.2.\n\nThe settings for fine-tuning is presented in TableA2. We mostly follow the settings of Ronen et al.[44]. We initialize our models with the weights pre-trained on ImageNet-21K and fine-tuned on Imagenet-1K, as in[44], and the tokenization part is randomly initialized only. We use the pre-trained weights provided by thetimmlibrary,vit_{size}_patch16_224.augreg_in21k_ft_in1k, where{size}denotes the model size,e.g., Tiny, Small, Base. We train our models for 130 epochs. In this experiment, stronger weight decay did not lead to notable difference, but the drop rate of stochastic depth was significant in prevention of overfitting, as in the experiments from scratch.\n\nSECTION: A1.2Transfer Learning\n\nHere, we list the training settings used for transfer learning. For both ImageNet-1K[12]classification models and DINO[9]pre-trained models, we follow the default training settings of DeiT[50]and DINO[9]. The details for each experiment can be found at TableA4and TableA4.\nOur experiments, especially on smaller datasets, required stronger regularization terms. A very high drop rate for stochastic depth of 0.5 was essential in Flowers and StandfordCars, to avoid overfitting.", "text_file": "data\\paper_texts\\2412.04680v1_content.txt"}, {"title": "TransAdapter: Vision Transformer for Feature-Centric Unsupervised Domain\n  Adaptation", "authors": ["A. Enes Doruk", "Erhan Oztop", "Hasan F. Ates"], "published_date": "2024-12-05T11:11:39Z", "summary": "Unsupervised Domain Adaptation (UDA) aims to utilize labeled data from a\nsource domain to solve tasks in an unlabeled target domain, often hindered by\nsignificant domain gaps. Traditional CNN-based methods struggle to fully\ncapture complex domain relationships, motivating the shift to vision\ntransformers like the Swin Transformer, which excel in modeling both local and\nglobal dependencies. In this work, we propose a novel UDA approach leveraging\nthe Swin Transformer with three key modules. A Graph Domain Discriminator\nenhances domain alignment by capturing inter-pixel correlations through graph\nconvolutions and entropy-based attention differentiation. An Adaptive Double\nAttention module combines Windows and Shifted Windows attention with dynamic\nreweighting to align long-range and local features effectively. Finally, a\nCross-Feature Transform modifies Swin Transformer blocks to improve\ngeneralization across domains. Extensive benchmarks confirm the\nstate-of-the-art performance of our versatile method, which requires no\ntask-specific alignment modules, establishing its adaptability to diverse\napplications.", "arxiv_id": "2412.04073v1", "html_link": "https://arxiv.org/html/2412.04073v1", "search_term": "ti:\"vision transformers\"", "html_content": "SECTION: TransAdapter: Vision Transformer for Feature-Centric Unsupervised Domain Adaptation\n\nUnsupervised Domain Adaptation (UDA) aims to utilize labeled data from a source domain to solve tasks in an unlabeled target domain, often hindered by significant domain gaps. Traditional CNN-based methods struggle to fully capture complex domain relationships, motivating the shift to vision transformers like the Swin Transformer, which excel in modeling both local and global dependencies. In this work, we propose a novel UDA approach leveraging the Swin Transformer with three key modules. A Graph Domain Discriminator enhances domain alignment by capturing inter-pixel correlations through graph convolutions and entropy-based attention differentiation. An Adaptive Double Attention module combines Windows and Shifted Windows attention with dynamic reweighting to align long-range and local features effectively. Finally, a Cross-Feature Transform modifies Swin Transformer blocks to improve generalization across domains. Extensive benchmarks confirm the state-of-the-art performance of our versatile method, which requires no task-specific alignment modules, establishing its adaptability to diverse applications. our code available atenesdoruk/TransAdapter.\n\nKeywordsDomain AdaptationUnsupervised learningTransformer\n\nSECTION: 1Introduction\n\nDeep neural networks (DNNs) have significantly advanced computer vision, excelling in diverse tasks[Wang et\u00a0al.(2022b),Qian et\u00a0al.(2021),Jiang et\u00a0al.(2022),Tan et\u00a0al.(2019),Chen et\u00a0al.(2021),Jiang et\u00a0al.(2021)]. However, their dependence on large labeled datasets imposes high costs and time constraints[Csurka(2017),Zhao et\u00a0al.(2020),Zhang et\u00a0al.(2020),Oza et\u00a0al.(2021)]. Unsupervised Domain Adaptation (UDA) addresses this challenge by enabling knowledge transfer from labeled source domains to unlabeled target domains, tackling domain shifts[Bousmalis et\u00a0al.(2017),Kuroki et\u00a0al.(2019),Wilson & Cook(2020)Wilson and Cook,VS et\u00a0al.(2021)].\n\nTraditional UDA methods, relying on Convolutional Neural Networks (CNNs), learn domain-invariant features to reduce domain discrepancies through adversarial training and feature normalization[Kang et\u00a0al.(2019),Zhang et\u00a0al.(2019),Jiang et\u00a0al.(2020),Li et\u00a0al.(2021)]. However, CNNs struggle with complex domain shifts and long-range dependencies, limiting cross-domain generalization[Morerio et\u00a0al.(2020),Jiang et\u00a0al.(2020)].\n\nTransformers, widely adopted in NLP[Vaswani et\u00a0al.(2017),Devlin et\u00a0al.(2018)]and computer vision[Dosovitskiy et\u00a0al.(2020),Han et\u00a0al.(2020),He et\u00a0al.(2021),Khan et\u00a0al.(2021)], offer revolutionary feature learning capabilities. The Swin Transformer[Liu et\u00a0al.(2021b)], known for its hierarchical structure and shift-window mechanism, achieves success but struggles with long-range dependencies due to its localized attention. This limitation, critical for UDA, along with its reliance on large-scale pretraining and fixed partitioning, hampers generalization to domain-specific nuances and significant shifts.\n\nTo overcome these limitations, this work introduces TransAdapter, a novel framework for UDA that enhances the Swin Transformer by integrating three innovative modules: Graph Domain Discriminator, Adaptive Double Attention module, and Cross Feature Transform module. These modules address traditional limitations by improving feature alignment and enhancing generalization across domains.\n\nContributions of this paper are summarized as follows:\n\nOur Graph Domain Discriminator: Unlike CNNs, which focus on local spatial correlations, this discriminator uses a Graph Convolutional Network (GCN) to model non-Euclidean relationships between features. By employing an adjacency matrix based on cosine similarity, it captures both shallow and deep feature dependencies in a scale-invariant manner. This enables holistic domain alignment at individual feature and relational levels, improving feature transferability across domains.\n\nThe Adaptive Double Attention module simultaneously processes window and shifted window attention features, effectively capturing long-range dependencies crucial for robust domain adaptation. An attention reweighting mechanism emphasizes significant features, while an entropy matrix generated using Graph Domain Discriminator features guides domain alignment. This matrix highlights domain-invariant patterns, enhancing feature transferability.\n\nThe Cross-Feature Transform module applies dynamic, bidirectional feature transformations between source and target domains using gating attention. By balancing directional contributions through cross-attention and gating mechanisms, it bridges domain gaps effectively. Pairwise feature distances combined with gating outputs ensure adaptive feature alignment and improved generalization across datasets.\n\nWe utilize CutMix and MixUp as pixelwise feature transform strategies on source data, guided by high-confidence pseudo-labels generated from a Swin-Base model.\n\nIntegrating these modules within the Swin Transformer, TransAdapter effectively addresses UDA challenges by leveraging transformers to handle domain shifts, long-range dependencies, and domain-specific nuances, setting a new standard for domain adaptation in vision tasks.\n\nSECTION: 2Related Work\n\nSECTION: 2.1Unsupervised Domain Adaptation (UDA) and Transfer Learning\n\nUnsupervised Domain Adaptation (UDA) within transfer learning focuses on learning transferable knowledge that generalizes across domains with varying data distributions. The primary challenge is addressing domain shift\u2014the discrepancy in probability distributions between source and target domains. Early UDA methods, such as Deep Domain Confusion (DDC), minimized the maximum mean discrepancy (MMD) to learn domain-invariant characteristics[Tzeng et\u00a0al.(2014a)Tzeng, Hoffman, Saenko, and Darrell]. Long et al.[Long et\u00a0al.(2015b)Long, Cao, Wang, and Jordan]enhanced this by embedding hidden representations in a reproducing kernel Hilbert space (RKHS) and applying a multiple-kernel variant of MMD for more effective domain distance measurement. Hidden representations refer to the activations within layers of a neural network, capturing hierarchical features of input data.Long et al.[Long et\u00a0al.(2017)Long, Zhu, Wang, and Jordan]later aligned the joint distributions of multiple domain-specific layers across domains using a joint maximum mean discrepancy (JMMD) metric. Adversarial learning methods, inspired by GANs, also gained popularity.\n\nSECTION: 2.2UDA with Vision Transformers\n\nHerath et al.[Herath et\u00a0al.(2023)Herath, Wang, and Huang]proposed an energy-based self-training and normalization approach for UDA, leveraging energy-based learning to improve instance selection on unlabeled target domains. Their method aligns and normalizes energy scores to learn domain-invariant representations, achieving superior performance on benchmarks like DomainNet, Office-Home, and VISDA2017. Sanyal et al.[Sanyal et\u00a0al.(2023)Sanyal, Gupta, and Roy]introduced Domain-Specificity Inducing Transformers for Source-Free Domain Adaptation, using vision transformers in privacy-oriented source-free settings. Their approach, leveraging Domain-Representative Inputs and novel domain tokens, achieves state-of-the-art performance across single-source, multi-source, and multi-target benchmarks. Alijani et al.[Alijani et\u00a0al.(2024)Alijani, Zhou, and Wang]categorized vision transformers\u2019 role in domain adaptation and generalization into feature-level, instance-level, model-level, and hybrid adaptations, highlighting their robustness to distribution shifts. Du et al.[Du et\u00a0al.(2024)Du, Li, and Zhao]introduced Domain-Agnostic Mutual Prompting for UDA, leveraging pre-trained vision-language models to address limitations of traditional UDA methods by utilizing rich semantic knowledge and handling complex domain shifts effectively. These studies highlight the growing importance of vision transformers in UDA, offering innovative solutions for domain shifts and enhancing generalization across domains.\n\nSECTION: 3Method\n\nBefore demonstrating how our method reduces the domain gap in domain-adaptive vision transformer, we first outline the problem formulation. Letrepresent a set oflabeled samples in the source domain, whereare the input samples andare their corresponding class labels. In the target domain, we have, consisting ofunlabeled samples,, with no labels. The objective in this unsupervised domain adaptation task is to develop a classifier that generalizes across domains using bothand.\n\nSECTION: 3.1Adaptive Double Attention\n\nThe Adaptive Double Attention (ADA) module, shown in Figure2, introduces an entropy-guided mechanism to address domain alignment challenges in unsupervised domain adaptation (UDA), particularly for long-range dependencies. While existing architectures, such as vanilla ViTs and Swin Transformers, model local and global dependencies, they lack effective mechanisms for domain alignment. ADA resolves this by integrating feature correction, double attention mechanisms, and entropy-guided reweighting, dynamically aligning source and target domain representations.\n\nA key feature of ADA is entropy-guided reweighting, integrated directly into the attention process. The entropy, calculated from outputs of a graph domain discriminator, prioritizes transferable features while suppressing domain-specific ones:\n\nThe graph domain discriminator processes key and shifted key features, generating outputsand. Lower entropy indicates better alignment, while higher entropy signals domain-specific noise. Entropy values dynamically reweight the attention scores:\n\nThe reweighted scores are concatenated, normalized with the softmax function, and combined with the value vectors:\n\nTo further minimize domain discrepancies, ADA employs a feature correction step before attention. Inspired by prior work, this correction block modifies target features by incorporating a correction term, implemented using two fully connected layers with ReLU activations. This ensures harmonized inputs for attention mechanisms. Window attention captures fine-grained spatial details within local regions, while shifted window attention models global dependencies. ADA integrates these mechanisms through cross-attention, where queries from window attention interact with keys from shifted attention, unifying local and global dependencies.\n\nThe final output of the adaptive attention mechanism,, is computed as follows:\n\nHere,is the input to the transformer block,is the adaptive attention output, andis the final block output. Residual connections and layer normalization stabilize learning and ensure efficient gradient flow.\n\nBy combining entropy-guided reweighting with dual attention mechanisms, the ADA module prioritizes transferable features, aligning long-range dependencies effectively. This robust approach addresses the limitations of existing architectures like Swin Transformers, enhancing domain alignment and improving generalization across diverse domains.\n\nSECTION: 3.2Graph Domain Discriminator\n\nGraph convolutions in the domain discriminator explicitly model inter-sample relationships, critical for domain alignment. Unlike methods like DANN[Ganin & Lempitsky(2015)Ganin and Lempitsky], which process samples independently, graph convolutions operate on an adjacency matrix encoding pairwise relationships between source and target samples. This allows theGraph Domain Discriminator (GDD)to leverage global and local topological dependencies, enabling a nuanced understanding of domain shifts.\n\nThe adjacency matrix, central to GDD, is constructed using cosine similarities between learnable projections of sample features:\n\nwhereandare the projected features of samplesand. This matrix enables GDD to capture inter-sample dependencies across domains, which are essential for modeling domain shifts that often involve subtle feature variations. By propagating relational information across samples, the adjacency matrix allows GDD to consider both individual domain-specific characteristics and structural interactions, fostering a more comprehensive domain alignment.\n\nThe GDD employs three graph convolutional layers with ReLU activation to aggregate information from sample neighbors, enriching domain-shared feature representations. A pooling operation after the first layer reduces dimensionality while emphasizing salient features. By leveraging local features from the-th transformer block and global features from the-th block, GDD achieves hierarchical alignment of domain representations.\n\nTo promote domain invariance, a Gradient Reversal Layer (GRL) is incorporated after the graph convolutional layers, facilitating a min-max optimization process. This setup enables the domain discriminator to minimize domain-specific biases while guiding the feature extractor to generate domain-invariant features. By simultaneously modeling global feature distributions and fine-grained inter-domain relationships, GDD achieves robust domain alignment, improving the adaptability of the shared feature space.\n\nSECTION: 3.3Cross Feature Transform\n\nThe proposed Cross Feature Transform (CFT) module enhances domain adaptation within the Transformer architecture by facilitating effective feature alignment between source and target domains. Unlike static methods, the CFT module is applied dynamically after a randomly selected transformer block in each iteration, providing a robust feature transformation approach and reducing the likelihood of overfitting[Sun et\u00a0al.(2022)Sun, Lu, Zhang, and Ling]. The general architecture of the CFT module is illustrated in Figure4.\n\nCentral to the CFT module are bidirectional cross-attention mechanisms, which optimize feature transferability between domains, enabling implicit mixing of features. This enhances the model\u2019s ability to learn domain-invariant representations, thereby improving generalization to the target domain[Wang et\u00a0al.(2022a)Wang, Guo, and Zhang]. The computation of source-to-target attention featuresand target-to-source attention featuresis performed as follows:\n\nTo refine feature alignment, the CFT module incorporates a gating mechanism using a learnable parameter, balancing contributions from both directions:\n\nwhereis the sigmoid function. This adaptive formulation allows prioritization of source-to-target or target-to-source transformations based on data context.\n\nThe pairwise distance between features is computed and combined with the gating attention output:\n\nHere,represents the pairwise distance,the gating attention output, andis the target feature added as a shortcut.\n\nSECTION: 3.4Pixel-Wise Feature Transform with Pseudo Labeling\n\nWe employ CutMix[Yun et\u00a0al.(2019)Yun, Han, Oh, Chun, Choe, and Yoo]and MixUp[Zhang(2017)]as pixel-wise transformation strategies on raw images to improve feature transferability between domains. Although these methods generally necessitate labeled data, our unsupervised domain adaptation task operates without ground truth labels in the target domain. To tackle this issue, we generate pseudo-labels for the target data using a Swin-Base model trained on the source domain. To reduce noise in these pseudo-labels, we implement a confidence threshold based on the model\u2019s accuracy, retaining only predictions that exceed this threshold for the transformation operations. These transformation are applied solely to the source data, as our network incorporates a Cross Feature Transform (CFT) module that enhances feature transferability between domains, thus diminishing the necessity for direct transformation on the target data. The pixel-wise CutMix and MixUp operations, guided by high-confidence pseudo-labels, are shown in Figure1.\n\nSECTION: 4Experiments\n\nSECTION: 4.1Datasets\n\nTheOffice-31dataset[Saenko et\u00a0al.(2010)Saenko, Kulis, Fritz, and Darrell]contains 4,652 images across 31 categories from three domains: Amazon (A), DSLR (D), and Webcam (W). Images were sourced from Amazon.com or captured in office settings using a DSLR or webcam.\n\nTheOffice-Homedataset[Venkateswara et\u00a0al.(2017)Venkateswara, Eusebio, Chakraborty, and Panchanathan]includes four domains: Artistic (Ar), Clip Art (Cl), Product (Pr), and Real-World (Rw), with 65 categories per domain, offering diverse evaluation scenarios.\n\nTheVisDA-2017dataset[Peng et\u00a0al.(2017)Peng, Usman, Kaushik, Hoffman, Wang, and Saenko], designed for synthesis-to-real tasks, includes 12 categories. The source domain contains 152,397 synthetic renderings, while the target domain has 55,388 real-world images.\n\nTheDomainNetdataset[Peng et\u00a0al.(2019)Peng, Bai, Xia, Huang, Saenko, and Wang], the largest UDA benchmark, comprises approximately 0.6 million images from six domains: Clipart (Clp), Infograph (Inf), Painting (Pnt), Quickdraw (Qdr), Real (Rel), and Sketch (Skt), covering 345 categories for challenging multi-source and single-source adaptation tasks.\n\nSECTION: 4.2Implementation Details\n\nFor all domain adaptation (DA) tasks, we utilize the Swin model, pretrained on the ImageNet dataset[Deng et\u00a0al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei], as the backbone network in our proposed TransAdapter framework. Additionally, we construct two model variants:TransAdapter-SandTransAdapter-B, derived respectively from Swin-S and Swin-B backbones, integrating 12 dual transformer blocks from their corresponding Swin architectures within the TransAdapter framework. The models are optimized using the Stochastic Gradient Descent (SGD) algorithm[Bottou(2010)], with a momentum of 0.9 and a weight decay parameter of. We employ a base learning rate offor the Office-31 and Office-Home datasets, while a lower learning rate ofis applied for the VisDA-2017 dataset. The learning rate follows a warmup cosine scheduler, gradually increasing during the initial training phase and subsequently decaying throughout the remaining iterations. Across all datasets, the batch size is consistently set to 32, and the models are trained overiterations. The hyperparametersandin the TransAdapter method are set toand, respectively, for all DA tasks, as shown in Equation11.\n\nSECTION: 4.3Objective Function\n\nThe domain adaptive model optimizes a combined objective function comprising cross-entropy loss for classification, local adaptation loss (strong alignment), and global adaptation loss (weak alignment). The classification loss for the labeled source domain is:\n\nwheredenotes last transformer block output,is the ground truth for the source domain, andCErepresents cross-entropy loss.\n\nFor adaptation, local and global losses are computed as averages over source and target domains:\n\nwhereand.denotes the output of the second transformer block for local alignment and the output of the last transformer block for global alignment.represents the focal loss function designed to address class imbalance.\n\nThe total loss is:\n\nwhereandare weighting coefficients.\n\nSECTION: 4.4Ablation Study\n\nTable3presents the ablation study results, demonstrating the impact of each proposed module in our model. Adding the Graph Domain Discriminator (GDA) improves domain alignment by modeling complex feature relationships, resulting in a performance boost across all datasets, with notable gains on VisDA-2017 () and DomainNet (). Introducing Pixelwise Transform further enhances performance by leveraging high-confidence pseudo-labels for effective feature augmentation, yielding an additional improvement oftoacross datasets. The Cross-Feature Transform (CFT) module significantly bridges domain gaps through dynamic, bidirectional feature transformations, leading to remarkable gains, particularly on VisDA-2017 () and DomainNet ().\n\nCNN vs GCN Based Discriminator.The results in Table3demonstrate the advantage of GDA, which employs a Graph Convolutional Network (GCN) to model complex, non-Euclidean relationships using an adjacency matrix based on cosine similarity. Unlike CNNs, which focus on local spatial dependencies, GDA captures global context and non-local feature correlations for comprehensive domain alignment. This approach achieves notable improvements on challenging datasets like VisDA-2017 (+3.6%) and DomainNet (+3.4%), highlighting GDA\u2019s effectiveness in addressing significant domain shifts in diverse and complex scenarios.\n\nVisualization.Figure5shows t-SNE visualizations of domain discrepancies using final transformer block features from TransAdapter-B. Adding the GDD on Swin-Base model reduces gaps by modeling complex feature relationships, while Pixelwise Transform enhances alignment through pseudo-label-guided augmentation. The CFT dynamically bridges domain gaps, resulting in more cohesive feature distributions. The complete TransAdapter-B after adding ADA achieves the most compact and well-aligned clusters, effectively minimizing domain discrepancies.\n\nSECTION: 4.5External Comparison\n\nVisDA-2017.Table4summarizes the performance of methods on the VisDA-2017 dataset across ResNet, DeiT, Swin, and ViT backbones. TransAdapter demonstrates competitive results, often outperforming state-of-the-art methods. With the Swin-S backbone, TransAdapter-S achieves 87.1%, surpassing Swin-S (70.8%) and Swin-B (73.9%) while approaching BCAT-B (89.2%). With Swin-B, TransAdapter-B achieves 91.2%, outperforming BCAT-B (89.2%) and PMTrans-B (88.0%). TransAdapter-B also achieves leading accuracy in categories such as bicycle (94.1%), house (98.9%), motorcycle (98.1%), person (87.1%), plant (96.8%), and truck (67.6%).\n\nOffice-Home.Table1highlights TransAdapter\u2019s state-of-the-art performance on the Office-Home dataset. With the Swin-S backbone, TransAdapter-S achieves 86.3%, significantly outperforming Swin-S (76.1%) and approaching BCAT (86.6%). With Swin-B, TransAdapter-B achieves 89.4%, surpassing BCAT (86.6%) and PMTrans-B (89.0%), while leading in categories such as \"AC\" (93.5%), \"PC\" (94.3%), \"AP\" (91.3%), \"RR\" (92.8%), and \"CR\" (81.1%).\n\nOffice-31.Table5presents the results on the Office-31 dataset. With the Swin-S backbone, TransAdapter-S achieves 90.2%, significantly surpassing Swin-S (86.1%) and achieving competitive performance against CDTrans-S (90.4%). With Swin-B, TransAdapter-B achieves a state-of-the-art average accuracy of 95.5%, outperforming BCAT-B (95.0%) and PMTrans-B (95.3%). It secures the highest accuracy in tasks such as(99.9%),(88.3%), and(87.2%), while achieving 100% accuracy in several tasks, demonstrating its adaptability and robustness.\n\nDomainNet.Table2compares TransAdapter with state-of-the-art methods on the DomainNet dataset across six domains: clipart (clp), infograph (inf), painting (pnt), quickdraw (qdr), real (rel), and sketch (skt). TransAdapter achieves the highest average accuracy of 53.7%, significantly surpassing CDTrans (45.2%) and SSRT (45.2%). It excels in challenging domain pairs, such as(63.9%),(62.5%), and(42.2%), and achieves leading results in individual domains such as clipart (55.8%) and real (54.4%). These results demonstrate TransAdapter\u2019s ability to handle diverse domain shifts, establishing it as a state-of-the-art UDA solution.\n\nSECTION: 5Conclusion\n\nIn this paper, we introduce TransAdapter, a novel framework that leverages the Swin Transformer for Unsupervised Domain Adaptation (UDA). Our approach features three specialized modules: a graph domain discriminator, adaptive double attention, and cross-feature transform, which enhance the Swin Transformer\u2019s ability to capture both shallow and deep features while improving long-range dependency modeling. Experimental results on standard UDA benchmarks show that TransAdapter significantly outperforms existing methods and demonstrates robustness against domain shifts. However, the combined use of window and shifted window attention may increase computational complexity, and our current implementation lacks task-specific adaptation mechanisms for detection and segmentation. Future work will focus on extending the model for these applications and exploring ways to reduce computational complexity while maintaining long-range dependency modeling.\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.04073v1_content.txt"}, {"title": "Automated LaTeX Code Generation from Handwritten Math Expressions Using\n  Vision Transformer", "authors": ["Jayaprakash Sundararaj", "Akhil Vyas", "Benjamin Gonzalez-Maldonado"], "published_date": "2024-12-05T03:58:13Z", "summary": "Converting mathematical expressions into LaTeX is challenging. In this paper,\nwe explore using newer transformer based architectures for addressing the\nproblem of converting handwritten/digital mathematical expression images into\nequivalent LaTeX code. We use the current state of the art CNN encoder and RNN\ndecoder as a baseline for our experiments. We also investigate improvements to\nCNN-RNN architecture by replacing the CNN encoder with the ResNet50 model. Our\nexperiments show that transformer architectures achieve a higher overall\naccuracy and BLEU scores along with lower Levenschtein scores compared to the\nbaseline CNN/RNN architecture with room to achieve even better results with\nappropriate fine-tuning of model parameters.", "arxiv_id": "2412.03853v1", "html_link": "https://arxiv.org/html/2412.03853v1", "search_term": "ti:\"vision transformers\"", "html_content": "SECTION: Automated LaTeX Code Generation from Handwritten Mathematical ExpressionsCategory: Computer Vision\n\nConverting mathematical expressions into LaTeX is challenging. In this paper, we explore using newer transformer based architectures for addressing the problem of converting handwritten/digital mathematical expression images into equivalent LaTeX code. We use the current state of the art CNN encoder and RNN decoder as a baseline for our experiments. We also investigate improvements to CNN-RNN architecture by replacing the CNN encoder with the ResNet50 model. Our experiments show that transformer architectures achieve a higher overall accuracy and BLEU scores along with lower Levenschtein scores compared to the baseline CNN/RNN architecture with room to achieve even better results with appropriate fine-tuning of model parameters.\n\nSECTION: 1Github Link\n\nhttps://github.com/osjayaprakash/deeplearning/tree/main\n\nSECTION: 2Introduction\n\nConverting handwritten mathematical expressions into digital formats is time consuming, specifically LaTeX code. Our goal is to train a ML model that is capable of encoding handwritten notes and converting to the source code seamlessly. The input to our algorithm is an image of a handwritten mathematical expression and the output is a sequence of tokens representing a LaTeX sequence. The challenge of our project requires the use of both computer vision and NLP techniques. We utilize an encoder-decoder architecture to encode the input image and decode it into a sequence.\n\nSECTION: 3Related work\n\nSchechter et\u00a0al. [2017]investigated a variety of methods like neural networks, CNNs, Random Forests, SVMs, OCR, CGrp, and SA. However, most state of the art methods utilize encoder-decoder architectures involving CNNs and LSTM architectures likeGenthial and Sauvestre [2017a]. In recent works likeBian et\u00a0al. [2022], both left-to-right and right-to-left decoders are utilized. The CNN-RNN architecture will serve as a baseline for our work.\n\nTransformer architectures (Vaswani et\u00a0al. [2023]) are currently achieving the best results for NLP tasks .Dosovitskiy et\u00a0al. [2021]introduced vision transformers which uses sequences of image patches to replace convolutions. We will leverage a vision transformer encoder and transformer decoder architecture and compare it to the baseline.\n\nSECTION: 4Dataset and Features\n\nWe will use the datasets from two main repositories:Im2latex-100k(Kanervisto [2016]) andIm2latex-230k(Gervais et\u00a0al. [2024]). TheIm2latex-100k(Kanervisto [2016]) dataset, available atZenodo, contains 100,000 image-formula pairs. TheIm2latex-230k(Gervais et\u00a0al. [2024])\ndataset, also known asIm2latexv2, contains 230,000 samples. It includes both OpenAI-generated and handwritten examples, further enhancing the diversity of the data. This dataset is available atIm2markup. The training data format is<image file name> <formula id>.\n\nThe dataset disk size is 849 MB. The images are gray scales with 50x200 pixels. The numbers of symbols (Figure\u00a01) in the latex formulas vary from range varies from 1 to 150 symbols. Voabulary contains 540 symbols, referFigure\u00a07andFigure\u00a08for the list of popular and least occurring symbols with their frequency.\n\nSECTION: 5Methods\n\nSECTION: 5.1CNN encoder and GRU/LSTM\n\nAs a baseline, We use the CNN Encoder to encode the image input of resized image (50x200) with 1 channel (greyscale). We use 3x3 convolutional filter followed by 2x2 max pooling layer. This previous block is repeated three times and followed fully connected layer.\n\nDuring decoding, We compute the embedding for formula tokens and concatenated with image encoded embedding. The concatenation of image and token embedding fed into LSTM/GRU units, followed by fully connected network. The activation is softmax. Overall model architecture is:Input ImageCNN (Encoder)LSTM/GRU (Decoder)Output LaTeX Sequence\n\nSECTION: 5.2LSTM with funetuning with pretrained Resnet50\n\nHere we use the pretained ResNet50 model as a encoder (98Mb disk size). However, ResNet50 expects the image with fixed size 254x254 and 3 channels. Our input images are grey scale. So, we transform the input image to the ResNet50 input usingtf.keras.layers.Lambda(lambda x: tf.image.grayscale_to_rgb(x).\n\nSECTION: 5.3Vision transformer encoder and transformer decoder\n\nThe vision encoder leverages patches of the image. We create patches of 10 X 10. Since our images are of size 50 X 200, we have a total of 100 patches per image.\n\nIn the vision transformer encoder, these patches are taken and embedded linearly and added to the positional embeddings. That is fed into a standard transformer layer. We use 8 transformer layers for our architecture that have 4 attention heads and 2 layer multi-layer perceptron with 2048 and 1024 units.\n\nWe use the standard transformer block for the decoder which uses cross-attention to find parts of the image to focus on and self-attention for the sequence generation. Our configuration uses 4 attention layers with 8 heads for both the self-attention and cross-attention components in each layer.\n\nSECTION: 6Experiments\n\nSECTION: 6.1Setup/Hyperparameters/Metrics\n\nWe use a single AWS G6.xlarge instance to train the models on 50,000 data points. The training time varies between 1 hr 30 mins and 2 hrs. We use early stopping with a patience of 10. We had a batch size of 128 for the CNN-RNN and ResNet-RNN, and a batch size of 64 for the transformer architecture which was primarily motivated by GPU memory constraints for the AWS instance. We used the default learning rate of 0.001 for the CNN-RNN and ResNet-RNN with the Adam optimizer, and a learning rate that decayed from 1e-4 to 1e-6 for the transformer architecture with the AdamW optimizer based on experimentation with different learning rates.\n\nWe compute the following metrics to compare the baseline and other methods:\n\nWe measure the \u2018sparse categorical loss\u2018 and accuracy which measures the loss/accuracy accross all tokens.\n\nWe measure the masked loss and accuracy to measure the accuracy for the non-padded tokens (we pad our tokens to length 151 which is the max and this will only check the loss/accuracy for the tokens that are part of the label sequence)\n\nWe measure the Levenshtein distance and BLEU-4 score for predicted sequences of a subset of the training set. These metrics were chosen in order to quantify closeness/correctness between sequences beyond a simple binary score that relies on exact matching.\n\nSECTION: 6.2CNN-RNN baseline\n\nWe explored using both LSTM/GRU for the decoder and the difference between the two were negligible. Here are the training curves with CNN - LSTM/GRU architectures:\n\nBoth models had \u00a085% accuracy with GRU being slightly worse off. Due to this, we used the numbers from the LSTM decoder to compare against the other models.\n\nSECTION: 6.3Results\n\nWe can see that the transformer architecture gets significantly lower loss and Levenshtein score, and higher accuracy and BLEU score compared to the baseline CNN-RNN model and ResNet-RNN model.\n\nSECTION: 7Conclusion/Future Work\n\nWe can see that overall the transformer architecture outperformed the vanilla CNN-RNN architecture on all measured metrics. We surmise that this is due to the combination of the algorithm utilizing positional embeddings along with attention mechanisms for both the encoder and decoder. Given more time, we would have mainly focused on experimenting with various transformer architecture configurations (by changing number of layers, attention heads, patch size for the vision transformer encoder etc.). We also would have looked at training more complex models with more data. For this paper, we used 50,000 data points with appropriate model sizes due to GPU memory constraints on the AWS instances we could use.\n\nSECTION: 8Contributions\n\nJayaprakash Sundararaj: Initial report, researching the dataset and existing methods. Implementing the full CNN and LSTM as a baseline. Extending to pre-trained ResNet50 model with finetuning.\n\nAkhil: Ideation, AWS/GPU setup, Extending to CNN + GRU as a baseline, vision transformer encoder + transformer decoder model, masked loss and accuracy.\n\nBen: Looked into potential final accuracy metrics, Implementing the Levenshtein and BLEU-4 metrics specific to models based on prediction outputs.\n\nSECTION: References\n\nSECTION: 9Appendix", "text_file": "data\\paper_texts\\2412.03853v1_content.txt"}, {"title": "UniForm: A Reuse Attention Mechanism Optimized for Efficient Vision\n  Transformers on Edge Devices", "authors": ["Seul-Ki Yeom", "Tae-Ho Kim"], "published_date": "2024-12-03T10:04:15Z", "summary": "Transformer-based architectures have demonstrated remarkable success across\nvarious domains, but their deployment on edge devices remains challenging due\nto high memory and computational demands. In this paper, we introduce a novel\nReuse Attention mechanism, tailored for efficient memory access and\ncomputational optimization, enabling seamless operation on resource-constrained\nplatforms without compromising performance. Unlike traditional multi-head\nattention (MHA), which redundantly computes separate attention matrices for\neach head, Reuse Attention consolidates these computations into a shared\nattention matrix, significantly reducing memory overhead and computational\ncomplexity. Comprehensive experiments on ImageNet-1K and downstream tasks show\nthat the proposed UniForm models leveraging Reuse Attention achieve\nstate-of-the-art imagenet classification accuracy while outperforming existing\nattention mechanisms, such as Linear Attention and Flash Attention, in\ninference speed and memory scalability. Notably, UniForm-l achieves a 76.7%\nTop-1 accuracy on ImageNet-1K with 21.8ms inference time on edge devices like\nthe Jetson AGX Orin, representing up to a 5x speedup over competing benchmark\nmethods. These results demonstrate the versatility of Reuse Attention across\nhigh-performance GPUs and edge platforms, paving the way for broader real-time\napplications", "arxiv_id": "2412.02344v1", "html_link": "https://arxiv.org/html/2412.02344v1", "search_term": "ti:\"vision transformers\"", "html_content": "SECTION: UniForm: A Reuse Attention Mechanism for Efficient Transformers on Resource-Constrained Edge Devices\n\nTransformer-based architectures have demonstrated remarkable success across various domains, but their deployment on edge devices remains challenging due to high memory and computational demands. In this paper, we introduce a novel Reuse Attention mechanism, tailored for efficient memory access and computational optimization, enabling seamless operation on resource-constrained platforms without compromising performance. Unlike traditional multi-head attention (MHA), which redundantly computes separate attention matrices for each head, Reuse Attention consolidates these computations into a shared attention matrix, significantly reducing memory overhead and computational complexity. Comprehensive experiments on ImageNet-1K and downstream tasks show that the proposed UniForm models leveraging Reuse Attention achieve state-of-the-art imagenet classification accuracy while outperforming existing attention mechanisms, such as Linear Attention and Flash Attention, in inference speed and memory scalability. Notably, UniForm-l achieves a 76.7% Top-1 accuracy on ImageNet-1K with 21.8ms inference time on edge devices like the Jetson AGX Orin, representing up to a 5x speedup over competing benchmark methods. These results demonstrate the versatility of Reuse Attention across high-performance GPUs and edge platforms, paving the way for broader real-time applications.\n\nSECTION: 1Introduction\n\nRecent advances in neural network architectures have been dominated by Transformers, which excel in capturing intricate dependencies across various domains, including natural language processing (NLP), computer vision, and speech recognition. Transformers have proven their worth in large-scale tasks such as image classification, object detection, and language modeling. Vision Transformers (ViTs), in particular, have significantly advanced computer vision, taking inspiration from the breakthroughs seen in large language models (LLMs) such as GPT and Llama.\nHowever, the impressive performance of these models comes at the cost of increased computational and memory demands, which pose significant challenges for latency-sensitive and real-time applications, particularly on edge devices with limited resources.\n\nEdge deployment, such as on devices like Raspberry Pi and Jetson Nano, introduces unique constraints, including limited memory, low energy consumption, and strict real-time processing requirements. The high computational and memory overhead of ViTs and other Transformer-based models makes it difficult to meet these constraints. Even though optimized GPU systems like NVIDIA A100 provide high throughput, the resource disparity between high-end GPUs and edge devices results in a significant performance gap during inference.\n\nTo address these challenges, several optimization approaches such as model pruning, quantization, and Neural Architecture Search (NAS) have been explored to optimize models for edge deployment\nHowever, these techniques often fall short of addressing the non-linear relationship between model complexity and inference time on resource-constrained platforms. This non-linearity has been well-documented in studies such as MCUNet and Neural Architecture Search for Efficient Edge Inference, which highlight that models designed for GPUs often fail to scale efficiently to edge environments[30,9]. This gap emphasizes the need for models designed specifically for edge hardware.\n\nIn response to these challenges, we propose a simple yet efficient reuse attention mechanism that is specifically designed to address the constraints of edge devices while maintaining the scalability and accuracy of ViTs. Unlike traditional attention mechanisms, which calculate the attention matrix for each head, our method reuses the attention matrix across multiple attention heads within a layer. This significantly reduces redundant computations and minimizes memory accesses, a crucial improvement for memory-bound edge devices. This approach is particularly advantageous in real-time, latency-sensitive applications, offering higher inference speeds and reduced memory usage. Despite the optimizations, our reuse attention mechanism can also maintain competitive performance on tasks like object detection and segmentation, ensuring its applicability across different downstream tasks.\n\nIn summary, our contributions are threefold:\n\nEfficient Attention MechanismWe propose a simple yet efficient attention mechanism that calculates the attention matrix once per layer, significantly reducing redundant calculations and memory access. This innovation enables more efficient processing on memory-constrained edge devices while maintaining computational rigor, thus addressing the growing demand for real-time, low-latency AI applications.\n\nImproved Throughput on Diverse HardwareOur method demonstrates significant improvements in throughput across diverse hardwares, ranging from high-performance GPUs to resource-constrained edge devices. Unlike conventional attention mechanisms, our approach scales gracefully without compromising on Top-1 accuracy, outperforming many state-of-the-art (SOTA) models.\n\nVersatility in Downstream TasksWe extend the applicability of Reuse Attention to downstream tasks like object detection and segmentation. This versatility shows its practical utility in real-world applications while maintaining computational efficiency.\n\nSECTION: 2Backgrounds\n\nDeploying Transformers on edge devices presents unique challenges due to the limited computational and memory resources available in these environments. This section highlights three major bottlenecks: (1) memory bottlenecks from multi-head attention (MHA), (2) computational inefficiencies and redundancy in traditional attention modules, and (3) the performance gap between high-performance GPUs and resource-constrained edge devices. These challenges have a significant impact on the scalability and real-time deployment of ViTs, underscoring the need for optimized transformer architectures that can effectively operate in low-resource environments while maintaining performance.\n\nSECTION: 2.1Memory Bottlenecks\n\nAttention mechanisms, particularly multi-head attention (MHA), are foundational to the success of Transformer-based models, including Vision Transformers (ViTs) as well as Large Language Models, enabling them to capture long-range dependencies and global relationships in input data[37]. However, MHA introduces significant memory bottlenecks, especially in resource-constrained environments like edge devices[18]. The core issue lies in the memory-intensive operations involved in attention modules, such as softmax calculations and matrix multiplications, which require frequent memory access to intermediate results and output storage, placing high demands on memory bandwidth[16].\nThe frequent data movement between memory and compute units significantly increases memory traffic as the input sequence size grows, which leads to slower inference times for real-time applications.\n\nHigh-performance GPUs, such as the NVIDIA H100 with 3.35 TB/s bandwidth, are well-equipped to handle the memory demands of transformer models, particularly multi-head attention (MHA). In contrast, edge devices like the Raspberry Pi 3B, with about 197 times less bandwidth (17 GB/s), struggle significantly as shown in Fig.2. The low memory bandwidth in edge devices forces frequent memory I/O operations, increasing memory traffic and leading to performance bottlenecks like cache misses, especially during repetitive MHA operations. This negatively impacts inference speed as the processor repeatedly accesses slower memory[17,16]. Previous studies have shown that these memory inefficiencies can be mitigated through optimizations such as FlashAttention, which reduces unnecessary memory transfers, minimizing latency and improving throughput on low-resource hardware[10,7]\n\nSECTION: 2.2Computation and Parameter Optimization\n\nComputational inefficiencies in traditional MHA modules, particularly in resource-constrained environments, present a significant challenge. Each attention head computes its attention map independently, leading to redundant and resource-intensive operations as the number of heads increases. Studies have shown that many attention maps across different heads exhibit substantial similarity, suggesting that not all computations are necessary[21]. Furthermore, Michel et al.[27]demonstrate that transformer models often rely on only a subset of heads, with many contributing minimally to the final output. This underscores the diminishing returns of additional heads and highlights the potential benefits of simplified architectures, such as single-head attention or reduced multi-head configurations, which strike a better balance between computational efficiency and performance.\n\nMoreover, parameter efficiency is equally critical, particularly for edge device deployments. Research on ViT pruning[39]reveals redundancy in the Query and Key components of the attention module across layers, while the Value component consistently retains essential information. This finding suggests that selectively pruning or simplifying the Query and Key components while preserving the Value can significantly reduce parameter overhead. Such optimizations lead to improved memory and computational efficiency without compromising performance, enabling more practical and efficient transformer designs for deployment in low-power environments.\n\nSECTION: 2.3Performance Gap between High-performance GPUs vs. Edge Devices\n\nThe increasing complexity of Transformer-based models, such as Vision Transformers (ViTs), highlights the growing need for more efficient mechanisms, especially when deploying models across a variety of hardware platforms. A distinct performance gap exists between high-performance GPUs and Edge-devices, primarily due to the computational and memory demands of conventional attention mechanisms. While high-performance GPUs can manage these demands, Edge-devices face significant challenges due to limited computational power and memory bandwidth, making efficient deployment difficult.\n\nHigh-performance GPUs, equipped with High Bandwidth Memory (HBM), can efficiently handle these demands, processing large sequences with minimal latency. For example, the H100 offers a memory bandwidth of 3.35 TB/s, which is approximately 197 times greater than the Raspberry Pi 3B, allowing it to smoothly manage increasing token counts. In contrast, Edge-devices, such as the Jetson Nano with LPDDR memory (25.6 GB/s), are heavily constrained by lower memory bandwidth and compute capacity (see Figure2). As the number of tokens increases, memory access in conventional attention mechanisms grows exponentially, overwhelming these devices. This results in frequent cache misses, increased latency, and overall inefficiency when handling multi-head attention (MHA) operations, which require frequent memory input/output (I/O) actions.\n\nOur experiments clearly validated this performance gap as shown in Figure3. By evaluating the memory access and arithmetic intensity of attention modules across devices\u2014from high-end GPUs such as the H100 and A100 to Edge-devices like the Jetson Nano and Raspberry Pi\u2014we observed that when token counts reached 1024 or higher, memory access became the primary bottleneck for Edge-devices. While high-performance GPUs continued to perform well under these conditions, Edge-devices experienced significant slowdowns due to their limited resources. Additionally, the increased number of memory accesses led to frequent cache misses, further degrading performance, underscoring the need for specialized optimizations in attention mechanisms. Without addressing these memory bandwidth constraints, deploying Transformer-based models on Edge-devices for real-time or large-scale input processing remains a major challenge.\n\nSECTION: 2.4Related Works: Comparative Analysis of Attention Mechanisms\n\nBuilding on the identified three primary bottlenecks, we evaluate popular attention mechanisms (i.e., Original Attention, Linear Attention, Cascaded Group Attention, and Flash Attention) alongside our proposed Reuse Attention approach, focusing on four key metrics: model performance, GPU efficiency, edge efficiency, and memory scalability (see Table3).\n\nConventional attention mechanisms, though delivering robust model performance, impose substantial computational and memory demands. These demands scale sharply with increased token counts, model depth, and the number of heads, ultimately constraining efficiency on both GPUs and edge devices. This limitation has been a well-documented obstacle in scaling Transformer models to edge environments[37,33,7].\n\nLinear Attention improves GPU and edge efficiency by reducing computational complexity. However, this simplification often sacrifices accuracy, as it struggles to capture complex dependencies required for tasks like language modeling. Consequently, Linear Attention may be less suitable for high-precision applications where performance is critical[15,32,13,25].\n\nCascaded Group Attention achieves high accuracy, but its grouped computations reduce efficiency on edge devices, where limited parallelism and bandwidth lead to significant performance drops. Prior work has highlighted the resource constraints that prevent grouped attention mechanisms from reaching optimal efficiency on hardware-limited platforms[21].\n\nFlash Attention enhances GPU efficiency by employing block-wise computation to reduce memory I/O, but it still encounters training instability and demands significant memory bandwidth, particularly at higher embedding dimensions, which limits its practicality on SRAM-limited edge devices[7,11].\n\nIn contrast, our proposed Reuse Attention significantly reduces memory and computational demands by reusing the attention matrix across heads and employing multi-scale value processing. This design achieves high accuracy while enhancing efficiency on both GPUs and edge devices, making it a viable option for real-world edge deployments. Preliminary results show that Reuse Attention retains accuracy comparable to conventional attention but with a reduced resource footprint, aligning with findings in recent efforts to optimize attention mechanisms for constrained environments.\n\nSECTION: 3Proposed Method\n\nIn this section, we introduce the Reuse Attention Mechanism with Multi-Scale Value Processing, a simple yet efficient approach designed to mitigate the computational and memory bottlenecks of the conventional Multi-Head Attention (MHA) mechanism in Transformers. Our method focuses on reducing redundant computations and minimizing memory access overhead, which are critical factors affecting inference speed and efficiency on edge devices. By reusing the attention matrix across all heads and incorporating multi-scale processing in the value projections, our approach enhances both computational efficiency and representational diversity.\n\nSECTION: 3.1Redundancy in Query and Key Components\n\nStudies have observed that attention maps exhibit high similarities across different heads, leading to computational redundancy[21]. This redundancy suggests that computing separate attention matrices for each head may be unnecessary. Additionally, Mehta et al. demonstrated that using synthetic or fixed attention patterns can maintain or even improve performance, aligning with our motivation to reuse the attention matrix[26].\n\nSECTION: 3.2Importance of Value Projections\n\nRecent research exploring the pruning of various structural components of Vision Transformers indicates that reducing the dimensionality of the Value projections leads to greater performance degradation compared to reducing the dimensions of the Query and Key projections[39]. This suggests that the Value projections potentially encode more crucial information for the model\u2019s performance. Therefore, we maintain the original dimensionality of the Value projections while reusing the attention matrix to enhance efficiency without compromising expressiveness.\n\nSECTION: 3.3Enhancing Representational Diversity with Multi-Scale Processing\n\nOur multi-scale Value processing is inspired by the effectiveness of multi-scale convolutional strategies, such as MixConv[36]and Inception modules[35]. Each Value projection undergoes depthwise convolution with a unique kernel size, enabling each head to capture features at different receptive fields. This design improves the model\u2019s ability to learn rich and diverse representations essential for complex tasks, particularly in edge environments where efficiency and flexibility are crucial.\n\nSECTION: 3.4Reuse Attention: Overview and Computational Steps\n\nOur Reuse Attention Mechanism with Multi-Scale Value Processing is a streamlined approach designed to address the inefficiencies in conventional attention architectures. By reusing a single attention matrix across all heads and incorporating multi-scale processing within Value projections, our method reduces computational redundancy and minimizes memory overhead, enhancing efficiency on edge devices\n\nFigure4illustrates the architectural differences between prominent attention mechanisms, highlighting how Queries, Keys, and Values are handled across architectures compared to our proposed method.\n\nMulti-Head Attention (DeiT):\nThis configuration employs independent Query, Key, and Value projections for each head, which allows diverse representations but introduces high memory and computational demands, especially with increased heads or token counts. Recent studies highlight that the growing number of heads in Multi-Head Attention exacerbates memory and compute constraints, limiting its scalability[5]\n\nGrouped-Query Attention (Llama 3):\nIn this approach, Queries are grouped, with each group sharing a representative Key-Value pair. While this setup reduces resource requirements, it may sacrifice specificity within groups, as each Key-Value pair must represent a range of Queries. This trade-off has been explored as a means to improve efficiency without entirely sacrificing representational power, although some group configurations can hinder task-specific performance[1].\n\nMulti-Query Attention: This method goes further by consolidating queries with a single Key-Value pair to improve efficiency, but its capacity to capture diverse structures is restricted. These methods face inherent trade-offs between performance and efficiency, and they lack scalability, especially in edge environments with limited resources, particularly when dealing with high-dimensional embeddings[2].\n\nTo address the limitations of existing attention mechanisms, we proposeReuse Attentionwith Multi-Scale Value Processing, which reduces redundant computations and minimizes memory access overhead while preserving the expressive power of MHA. Our Reuse Attention mechanism leverages a unified attention matrix shared across all heads and introduces multi-scale processing in the value projections, thereby maintaining efficiency and enhancing the model\u2019s representational capacity, especially for edge deployment.\n\nOur Reuse Attention mechanism maximizes computational efficiency by creating a single attention matrix shared across all heads. Given an input, we compute the shared Query and Key projections as follows:\n\nThe shared attention matrixis then calculated using:\n\nwherecaptures the global attention across the entire dimension.\n\nThough the matrix multiplication complexity remains, the proposed method significantly reduces memory I/O operations by eliminating the need to separately compute attention matrices for each head. In traditional multi-head attention, each head computes its own attention matrix, resulting in redundant memory accesses and increased I/O overhead. By reusing a single attention matrixacross all heads, our method reduces the total amount of data that needs to be read from and written to memory, thereby improving memory efficiency.\n\nRecent research demonstrated that minimizing memory transfers between compute units and memory banks can drastically improve efficiency on memory-bound hardware[7]. By reusingacross all heads, our approach parallels the memory efficiency strategies of Flash Attention, which minimizes bandwidth requirements through reduced memory transfers, effectively lowering memory traffic, and computational overhead per head. This design addresses a key limitation in transformer architectures by significantly decreasing memory I/O without increasing computational complexity, thereby enhancing throughput on memory-constrained devices[28,21,43].\n\nEach head in the Reuse Attention Mechanism processes its Value projection using depthwise convolutions with unique kernel sizes, enabling multi-scale feature extraction:\n\nBy applying depthwise convolutions with varying kernel sizesacross heads, our method captures multi-scale contextual information. Smaller kernels focus on fine-grained details, while larger kernels encompass broader context. This approach allows each head to extract features at different scales without increasing the memory I/O, as the convolution operations are performed locally within the Value projections. Inspired by studies[36,39], which demonstrated the effectiveness of multi-scale convolutions in capturing spatial hierarchies, our approach further improves representational diversity across scales. This multi-scale processing enables a richer representation without requiring additional memory bandwidth, aligning well with insights on optimizing model expressiveness through hierarchical representation without inflating parameter size or memory requirements.\n\nBy reusing the unified attention matrixacross all heads, the Reuse Attention Mechanism achieves a significant reduction in redundant memory operations:\n\nUnlike conventional attention, which calculates separate attention matrices per head, our method reuses the samefor all heads, thus avoiding repeated memory access patterns and reducing the total memory bandwidth required. This reuse strategy is akin to techniques in recent research like that of Ribar et al.[31]in LLMs and Shim et al.[34]in speech recognition, which illustrate how minimizing repeated memory accesses can lead to substantial performance gains in constrained environments. Our approach ensures that the high memory bandwidth demand seen in conventional MHA is alleviated, aligning with recent findings on optimizing memory efficiency in transformers.\n\nFinally, the outputs from each head are concatenated to form the final output:\n\nThis aggregation leverages the multi-scale, memory-efficient representations from each head, preserving high expressiveness without overburdening memory resources. By reducing memory transfers and managing bandwidth effectively, our method provides significant improvements in memory efficiency, which is crucial for edge applications with limited memory resources. This aligns well with insights from existing studies on transformer efficiency for edge applications.\n\nSECTION: 3.5UniForm Network Architectures\n\nThe UniForm Network is constructed around the concept of Reuse Attention, with its architecture illustrated in Fig.5. Similar to previous Like previous hierarchical backbones[22,21,38,14,12], UniForm follows a progressive design that operates in three stages. Across these stages, the channel dimensions, the depth, and the number of attention headsare incrementally increased to accommodate different levels of feature abstraction.\n\nIn the first stage, we introduce overlapping patch embedding to transform 1616 input patches into tokens of dimension. This method enhances the model\u2019s capacity for low-level visual representation learning, capturing finer details with minimal computational overhead. The UniForm Network then builds upon these tokens through a series of UniForm Blocks, stacking them within ach stage while reducing the token count by a factor of 4 via downsampling layers. The downsampling in resolution is inspired by the efficiency of hierarchical architectures which maintain spatial relationships while progressively reducing computational complexity[22,21]\n\nWe employ depthwise convolution (DWConv) and feedforward network (FFN) layers sequentially before and after attention modules to balance local feature extraction and global context understanding efficiently. This combination reduces computational complexity while capturing both low-level and high-level representations. The use of DWConv and FFN layers around attention is a proven technique from prior researches, enhancing model performance by optimizing the flow of information without the high cost of full self-attention[21,19,4]. The architecture is highly scalable, supporting Tiny, Small, Medium, and Large variants (as shown in Table4), with each variant adjusting the number of channels, attention heads, and depth to meet varying task complexities and computational constraints. UniForm also introduces Reuse Attention blocks, which reuse intermediate features across stages, reducing computational costs without sacrificing accuracy. This modular design enhances flexibility, allowing the network to adapt seamlessly to different patch sizes and resolutions.\n\nSECTION: 4Experimental Results\n\nSECTION: 4.1Implementation Details\n\nWe conduct image classification experiments on ImageNet-1K[8]. The models are buit with PyTorch 2.3.0[29]and MMPreTrain 1.2.0[6], and trained from scratch for 300 epochs on 2 NVIDIA A100 GPUs using AdamW optimizer[24]and cosine annealing learning rate scheduler[23]. We set the total batchsize as 512. The input images are resized and randomly cropped into 224224. The initial learning rate is 0.001 with weight decay of 0.025. We include some augmentation and regularization strategies in training, including Mixup[41], Cutmix[40], and\nrandom erasing[44].\n\nAdditionally, we evaluate the throughput across different hardware plateforms.\n\nFor GPU, throughput is measured on an NVIDIA A100, with a batch size of 2048 for a fair comparison across models.\n\nFor CPU, we measure the runtime on an Intel Xeon Gold 6426Y @ 2.50 GHz processor using a batch size of 16 and single-thread execution following the methodology based on[12].\n\nIn contrast to prior works, we also extensively emphasize the evaluation on the inference performance on various edge devices. These include popular multiple versions of theJetson(Nano, Xavier, Tx2, Nx, and AGX Orin) and multiple versions of theRaspberry Pi(2B, 3B, 3B Plus, 4B, and 5). All models are tested with a batch size of 16 and run in single-thread mode to maintain consistency. This evaluation demonstrates the practicality of each model in edge computing environments, where resource constraints are significantly more stringent than on server-grade hardware.\n\nMoreover, we evaluate the transferability of the UniForm model to downstream tasks. For image classification, we fine-tune the models for 300 epochs following[42]with similar hyperparameter settings. For instance segmentation on the COCO dataset[20], we use Mask RCNN and train for 12 epochs (1schedule) with the same settings as[22]using the MMDetection framework[3].\n\nSECTION: 4.2Image Classification\n\nThe UniForm models (Tiny, Small, Medium, and Large) consistently outperform state-of-the-art (SOTA) models across a range of sizes, delivering both higher accuracy and superior throughput in Fig.1and Tab.5. When compared to traditional CNN-based models like ShuffleNetV2 and MobileNetV3, as well as modern Transformer-based models like EfficientViT and EdgeNeXt, UniForm-s achieves 70.1% Top-1 accuracy, significantly outperforming MobileNetV3-small (67.4%), EfficientViT-M1 (68.4%), MobileViT-XXS (69.0%), and ShuffleNetV2 1.0x (69.4%) while also offering a higher throughput on CPU (231 images/s) as well as GPU (50,582 images/s vs. 41,965 images/s for MobileNetV3-small and 47,045 images/s for EfficientViT-M1). Furthermore, UniForm outperforms models from other architecture families, including MLP-based models like Mixer-B/16 and fusion-based models like EdgeNeXt. UniForm-l, for example, achieves 76.7% accuracy with a throughput of 25,356 images/s on GPU, significantly faster than Mixer-L/16 (688 images/s) and ViG-Ti (1,406 images/s), all while delivering higher accuracy than both.\n\nThis trend is evident across all UniForm variants, demonstrating that UniForm not only provides better accuracy but also achieves faster throughput on both GPU and CPU compared to other models of similar sizes, making it highly efficient for both large-scale and edge-device environments.\n\nThe results presented in Table6showcase the inference speed of UniForm variants compared to state-of-the-art CNN and ViT models across a wide range of edge devices. The key takeaway from these findings is UniForm\u2019s exceptional performance, consistently achieving faster inference times while maintaining competitive accuracy across different edge-devices.\n\nAcross all edge devices, UniForm significantly outperforms its counterparts in terms of inference speed. For example, UniForm-t demonstrates a 5x improvement in speed on the Jetson-Nano (11.9ms) compared to EfficientViT-M0 (56.8ms) while also providing a higher Top-1 accuracy (66.0% vs. 63.2%). This trend continues with UniForm-s and UniForm-m, both showing notable improvements in inference times across all Raspberry Pi versions and Jetson devices when compared to models like MobileNetV3 and EdgeNeXt. On the RaspberryPi4B, UniForm-t (19.4ms) is significantly faster than MobileNetV3-small (33.3ms) and EfficientViT-M1 (400.3ms). This highlights its versatility for deployment in resource-constrained environments where power efficiency and speed are critical, providing a viable solution for deploying advanced vision models in real-world edge scenarios..\n\nUniForm excels at balancing accuracy and speed on edge devices, outperforming both CNN-based models like MobileNetV3 and ShuffleNetV2, as well as Transformer-based models like EfficientViT and DeiT-Tiny. For instance, UniForm-l achieves the highest accuracy (76.7%) with significantly faster inference times on devices like the Jetson AGX Orin (2.4ms) compared to other Transformer models.\n\nSECTION: 4.3Downstream Tasks\n\nWe validated the effectiveness of UniForm models on several downstream tasks, focusing on image classification and instance segmentation to showcase the model\u2019s adaptability and competitive edge over state-of-the-art architectures.\n\nWe evaluate UniForm across several image classification benchmarks, including CIFAR-10, CIFAR-100, Flowers-102, and Oxford-IIIT Pet. UniForm consistently demonstrates competitive top-1 accuracy across these datasets, maintaining a balance between inference speed and performance. The results show that UniForm efficiently handles different dataset scales, particularly excelling in datasets like Flowers-102 and Oxford-IIIT Pet. This suggests that UniForm transfers well to smaller, fine-grained classification tasks while preserving throughput on both edge devices and traditional GPUs.\n\nFor instance segmentation, UniForm is tested on object detection tasks using the COCO dataset. When paired with Mask R-CNN, UniForm demonstrates robust performance, achieving competitive segmentation results while maintaining efficient inference on edge devices. This evaluation is essential in environments where high-resolution dense prediction tasks must operate under constrained resources.\n\nInference Speed Improvement: Achieved up to a 30% reduction in inference time on edge devices like Raspberry Pi and Jetson Nano compared to the traditional attention mechanism.\n\nMemory Usage Reduction: Observed a substantial decrease in memory consumption, enabling deployment on devices with stringent memory constraints.\n\nMaintained Accuracy: Demonstrated comparable performance on benchmark datasets such as ImageNet, with negligible loss in Top-1 accuracy.\n\nGeneralization to Downstream Tasks: Successfully applied our method to tasks like object detection and segmentation, achieving performance gains without additional modifications.\n\nTop-1 Accuracy\n\nJetson-Nano\n\nRaspberryPi4B\n\nRaspberryPi3B Plus\n\nRaspberryPi3B\n\nRaspberryPi2B\n\nJetson-Xavier\n\nJetson-Tx2\n\nJetson-Nx\n\nRaspberryPi5\n\nJetson AGX Orin\n\nSECTION: 4.4Ablation study\n\nIn this section, we ablate\n\nImpact of Reuse Attention on Interpretability and Inference EfficiencyWe compare the proposed UniForm model with Reuse Attention against Swin-T and UniForm without Reuse Attention (i.e., with standard attention). As shown in Fig.7, the CAM visualizations demonstrate that UniForm with Reuse Attention preserves strong interpretability, effectively highlighting relevant regions, similar to Swin-T and the UniForm variant without Reuse Attention. Despite the architectural simplicity of UniForm with Reuse Attention, it maintains comparable interpretability while significantly improving inference time, making it more efficient for real-time applications. This showcases the advantage of Reuse Attention, balancing between interpretability and computational efficiency.\n\nSECTION: 5Conclusions\n\nOur Reuse Attention Mechanism offers a simple yet effective solution to the computational and memory inefficiencies of the traditional attention mechanism in Transformers. By leveraging the redundancy in Query and Key components and minimizing memory access operations, we address the critical limitations that hinder the deployment of ViTs on edge devices. This approach not only reduces computational overhead but also aligns with the need for optimizing memory access patterns, directly impacting inference speed and efficiency. Our method demonstrates that significant performance improvements can be achieved through straightforward modifications, paving the way for more practical applications of Transformers in resource-constrained environments.\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.02344v1_content.txt"}, {"title": "Vision Transformers for Weakly-Supervised Microorganism Enumeration", "authors": ["Javier Ure\u00f1a Santiago", "Thomas Str\u00f6hle", "Antonio Rodr\u00edguez-S\u00e1nchez", "Ruth Breu"], "published_date": "2024-12-03T08:27:20Z", "summary": "Microorganism enumeration is an essential task in many applications, such as\nassessing contamination levels or ensuring health standards when evaluating\nsurface cleanliness. However, it's traditionally performed by human-supervised\nmethods that often require manual counting, making it tedious and\ntime-consuming. Previous research suggests automating this task using computer\nvision and machine learning methods, primarily through instance segmentation or\ndensity estimation techniques. This study conducts a comparative analysis of\nvision transformers (ViTs) for weakly-supervised counting in microorganism\nenumeration, contrasting them with traditional architectures such as ResNet and\ninvestigating ViT-based models such as TransCrowd. We trained different\nversions of ViTs as the architectural backbone for feature extraction using\nfour microbiology datasets to determine potential new approaches for total\nmicroorganism enumeration in images. Results indicate that while ResNets\nperform better overall, ViTs performance demonstrates competent results across\nall datasets, opening up promising lines of research in microorganism\nenumeration. This comparative study contributes to the field of microbial image\nanalysis by presenting innovative approaches to the recurring challenge of\nmicroorganism enumeration and by highlighting the capabilities of ViTs in the\ntask of regression counting.", "arxiv_id": "2412.02250v1", "html_link": "https://arxiv.org/html/2412.02250v1", "search_term": "ti:\"vision transformers\"", "html_content": "SECTION: Vision Transformers for Weakly-Supervised Microorganism Enumeration\n\nMicroorganism enumeration is an essential task in many applications, such as assessing contamination levels or ensuring health standards when evaluating surface cleanliness. However, it\u2019s traditionally performed by human-supervised methods that often require manual counting, making it tedious and time-consuming. Previous research suggests automating this task using computer vision and machine learning methods, primarily through instance segmentation or density estimation techniques. This study conducts a comparative analysis of vision transformers (ViTs) for weakly-supervised counting in microorganism enumeration, contrasting them with traditional architectures such as ResNet and investigating ViT-based models such as TransCrowd. We trained different versions of ViTs as the architectural backbone for feature extraction using four microbiology datasets to determine potential new approaches for total microorganism enumeration in images. Results indicate that while ResNets perform better overall, ViTs performance demonstrates competent results across all datasets, opening up promising lines of research in microorganism enumeration. This comparative study contributes to the field of microbial image analysis by presenting innovative approaches to the recurring challenge of microorganism enumeration and by highlighting the capabilities of ViTs in the task of regression counting.\n\nSECTION: IIntroduction\n\nEnumeration of microorganisms is crucial across diverse sectors including medicine, pharmaceutical quality control, or environmental monitoring[1,2,3]. This task is relevant in microbiology, and methods have been researched and improved for decades[4]. Traditional techniques are usually tedious, as counting manually (agar plate or hemocytometry) or the indirect estimations with turbidimetry[5,6,7]depends on specialized equipment, team, and time in order to improve efficiency. Hence, there has been a pursuit of automated and efficient enumeration techniques to improve this practice[8].\n\nAs a result, the efficiency of computer vision methods has improved microbial enumeration in later years. Machine learning, and later, deep learning, has made it possible to count the number of particles in images and extract various characteristic parameters of them, reducing workload and improving the accuracy of the analysis[9,10]. Research to improve numeration through deep learning is extensive and follows two lines of research: detection-based methods and regression-based methods, like image masks estimation, or density map regression[11,12]. These approaches have set the baseline and brought a new standard for efficiency all over the academic corpus.\n\nAlthough density estimation and instance segmentation offer benefits, they are not always feasible due to the lack of detailed spatial information. For example, microbial swab testing assesses surface cleanliness by providing a total bacterial count without precise localization[13]. Spatial-aware datasets complicate enumeration by requiring detailed annotations and increasing computational burden[13]. Instead, focusing on aggregate counts provides a simpler, faster, and equally effective solution.\n\nTo address this issue, weakly-supervised counting (WSC) is used as an approach that applies regression to images to predict the total number of instances without spatial information, as shown in Figure1. CNNs are the most common architectural choice to solve this problem[14,15], but recent advances have shown the effectiveness of vision transformers (ViTs) for the same task, outperforming CNNs[16,17]. This is the result of the inherent self-attention mechanisms of ViTs, which, in contrast to CNNs, outperform in capturing global image context and contextual dependencies, proving effective in image classification and segmentation[18].\n\nThe goal of this study is to highlight the use of ViTs in weakly-supervised counting and make use of its applicability in the task of microorganism enumeration as an effective solution. To achieve this, we conducted a comprehensive analysis of ViT-based backbone regression architectures. We compared them to the most popular benchmark architectures: CNNs, ResNet50, and ResNet101, by training them under the same strategy (no use of pre-trained weights nor fine-tuning to optimize results) to achieve the task of microorganism enumeration. We used four different microscopic-based datasets: the Fluorescent Neuronal Cells dataset[19], VGG-Cells dataset[20], U2OS/HL60 Human Cancer Cells dataset[21]and a self-made artificial fluorescent bacteria dataset, that is composed of high-resolution images. We created this dataset for the task of WSC in order to cover the gaps the former three datasets have in regards of dataset size, density sparsity through the images, and image resolution.\n\nOur experimental evaluation indicates that although traditional architectures such as ResNet achieve better performance, ViTs, especially CrossViT, can achieve comparable results to ResNet. CrossViT also performed exceptionally well on homogeneously distributed datasets, outperforming other ViT variants and CNNs in terms of computational efficiency. These results underscore the need to explore the use of ViTs, as further research holds the potential to achieve effective architectures for the task of weakly-supervised microorganism enumeration.\n\nThis study contributes to microorganism enumeration[8]by direct enumeration without the need for spatial information. We also analyze the use of vision transformers (ViTs) in regression counting. We evaluate popular and novel weakly-supervised counting methods, adapt them to microbial imaging, and evaluate ViTs in this context. Our analysis covers the capability of ViTs in a regression task, comparing different ViT approaches to identify the best one depending on the use case. By evaluating the performance of ViTs under the same training strategy as the other methods when configured for weakly-supervised counting, we provide insight into the limitations and potential of this architecture, thus contributing to the field of feature extraction using self-attention mechanisms[22]. We also contribute to the study by developing an artificial fluorescent bacteria dataset designed for the task of weakly-supervised microorganism enumeration. An implementation of the method and the artificial dataset generation tool are available athttps://github.com/JavierUrenaPhDProjects/vits_for_WSC.\n\nSECTION: IIRelated work\n\nSECTION: II-AMachine Learning in microorganism enumeration\n\nAutomating enumeration of microorganisms has been researched for decades, with traditional techniques like PCA, LDA or SVM and subsequently advancing to feature extraction with deep learning[8,23]. Two research strategies have been developed: detection-based and regression-based enumeration. Detection-based methods refer to the enumeration of instances once they are located in the image, being image segmentation as the most accurate method of instance detection.[24,11,25]extend the U-Net architecture[26]in its ability to count cells and bacteria by discretizing them from background and subsequently enumerating them. Regression-based methods solve the task either through density map estimation or direct regression without spatial details. Convolutional neural networks (CNNs) for density estimation treats image pixels as real-valued feature vectors and constructs density functions over pixel grids, enabling object count estimation by integrating over specific image regions[20,27,28,12]. The task is also accomplished with weak annotations, such as centroid position information of the instances, to estimate dense proximity maps[29,30].\n\nBut segmentation and density map regression in image analysis both rely on spatial information to identify instances, with segmentation using binary masks for pixel discretization[26]and density regression assessing instance agglomeration[20]. Even weakly-supervised models sometimes require centroid annotations[30]. However, in high-density scenarios common in microorganism analysis, detecting individuals becomes challenging. Moreover, when the goal is to simply count microorganisms globally, spatial details may be unnecessary. This necessitates datasets that bypass spatial data, focusing instead on correlating image features with total microorganism counts for efficiency[31,14,15]. This is avoided through direct regression, or \"weakly-supervised counting\" which is a form of supervised learning where the annotation of the images is kept to a minimum, such as providing only the overall global count[15], or the patch-labeled count[14]. This approach emerges as an end-to-end application-oriented solution for microorganism enumeration.\n\nSECTION: II-BWeakly-supervised enumeration\n\nWeakly-supervised counting refer to the enumeration approach based on direct regression, and is tackled with machine learning when problems such as high instance density or occlusion arise, and also bypasses the hard detection problem and reduces labeling cost by requiring only the ground truth number of instances in the training images. This has been extensively studied in the use case of crowd counting, by the use of CNNs[32], and microorganisms too[15,33].[33]for example implements an end-to-end WSC architecture by concatenating a ResNet with a CNN-based regressor that captures the global features of the entire microscopic image.\n\nBut CNNs convolution kernels fail to model global context information due to the limited receptive field, which is crucial when it comes to dense instance counting[34], and do not establish interactions between image patches, which makes them unable to learn contrast features between the background and the elements to count. Vision transformers[18]circumvent this issue with the self-attention mechanism that captures global dependencies and thus learn global context information.TransCrowd[17], first uses ViT for weakly-supervised crowd counting as a backbone to extract information and concatenating a regression head, creating an end-to-end architecture. Others follow example. creating end-to-end architectures that achieve crowd counting with refined iterations[35,36].\n\nSECTION: II-CObject counting with transformer architectures\n\nAs ViTs capture better global context information, most approaches use them as feature extraction backbone modules[37]. Images are segmented into fit-size patches, which are then processed through a linear embedding layer and sometimes summed with task-oriented tokens, which are then fed into the standard transformer encoder. The first use case where semantic segmentation was achieved using ViT instead of CNN as the backbone isSETR[38], which achieved state-of-the-art results on theADE20kdataset.\n\nFor regression methods, transformers are considered as an effective solution to estimate density maps for crowd counting[39,40,16]by using specialized patch tokens as a form is dense supervision[40]or achieving multi-scale 2D feature maps from a pyramid ViT backbone, namedCCTrans[16].CounTR[16]leverages the instance counting task by creating a class-agnostic counting architecture by exploiting the attention mechanisms to explicitly capture the similarity between image patches or \"exemplars\".\n\nWeakly-supervised counting has also been achieved using transformers[17,35,36,41].CCTwins[42], uses a U-shaped architecture, featuring an adaptive Twins-SVT-L backbone to extract multi-level features, uses a multi-level count estimator to regress these features to a crowd number in a coarse-to-fine manner.Learn to Count Anything[43]accomplishes class-agnostic instance counting likeCounTR[16], but without using exemplars or reference patches. Instead, it employs WSC with self-supervised knowledge distillation, where a teacher network processes global image slices and a student network processes smaller local slices.\n\nPromising results have been achieved in the paradigm of instance counting using transformer-based architectures, but to date little to no research has been done in the field of microbiology. Research in this area can contribute to the development of more effective models for counting microorganisms.\n\nSECTION: IIIMethodology\n\nWe subject a comparison of different architecture approaches covering three different categories: state-of-the-art approaches in the task of weakly-supervised counting, ViT-based backbones for regression architectures, and finally baseline traditional deep learning computer vision architectures commonly used for the task. The explanation can be followed in sectionIII-B.\n\nThe models were trained from scratch on four microorganism-based datasets consisting of neuronal cells, cancer cells, or bacteria. These datasets represent different types of use cases because they present different challenges, such as dataset size, density per image, or variability between images. The metrics used to compare the architectures are Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE). Datasets and metrics are further explained in sectionIV-B.\n\nSECTION: III-AArchitecture pipeline\n\nA common approach for WSC architectures is to concatenate two main parts: the backbone and the regression head (also called counter). The backbone is responsible for extracting the image features and placing the visual data into the latent space as feature embeddings. These are then sent to a regression head, which is used exclusively to predict the number of instances in the image. This general architectural approach, illustrated in the example of a ViT backbone in Figure2, is widely used in most studies investigating WSC[44,17,33,35,41]. Feature extraction is the most important part of the whole architecture, so this study focuses on exploring different backbones. The simplest regression head is a linear regressor, but the use of nonlinear regressors is preferable. In this study, it is implemented as a single layer fully connected network.\n\nSECTION: III-BArchitecture backbones\n\nTransCrowd[17]is a pioneering ViT-based WSC model, featuring two implementations: TransCrowd-GAP and TransCrowd-Token. TransCrowd-GAP employs global average pooling on the transformer\u2019s output tokens, while TransCrowd-Token adds a learnable token for enumeration. This model shows significant improvements in crowd counting on datasets like ShanghaiTech, outperforming both weakly-supervised (MAE andMSE improvement over MATT[45]) and fully-supervised methods (MAE andMSE improvements compared to CRSNet and BL[46,47]).\n\nViT research offers various backbones for feature extraction, chosen for their performance and proclaimed computational efficiency from novel architectural approaches. The first one being the vanilla ViT[18], which introduced the multi-head self-attention mechanism (MHSA) of the transformer as an encoder for image recognition, processing images as patch sequences. This method outperforms traditional CNNs in image classification benchmarks like ImageNet and CIFAR-100.\n\nA different ViT approach is the DeepViT[48]which addresses \"attention collapse\", a problem with ViTs that make them plateau in performance when made deeper, by introducing \"re-attention\", a technique that regenerate attention maps with minimal computational cost, improving top-1 classification accuracy by 1.6% on ImageNet with 32 transformer blocks.\n\nAn interesting approach to achieve great computational efficiency is CrossViT[49]which features a dual-branch transformer that processes different-sized patches with an efficient cross-attention mechanism that fuses these patches, reducing computational costs significantly. This model outperforms DeiT on ImageNet1K by 2%, with minimal additional computational complexity and model size.\n\nTo achieve higher model complexity without compromising parameter and compute neutrality, Parallel ViT[50]proposes parallelizing the MHSA and feed-forward blocks by reorganizing the same blocks by pairs, resulting in the same number of parameters but wider and shallower, increasing the dimension of the embedding for better spatial feature separability.\n\nFinally, to address the quadratic complexity of ViTs (), XCiT[51]introduces cross-covariance attention (XCA), which applies self-attention across feature channels. This reduces the computational cost for high-resolution images while maintaining performance for WSC tasks common in bioinformatics.\n\nTwo different common computer vision architectures are used as feature extractors. This will provide an unbiased baseline approach to achieve WSC more traditionally. The ResNet[52]was chosen because it is commonly used as a feature extractor in both academia and industry services because its residual connections allow it to be deep while being computationally affordable.[33]achieved WSC of cancer cells by implementing their version of ResNet calledxResNet. In this study, we implement ResNet50 and ResNet101 as competing backbones. Likewise, normal convolutional neural network backbones were also implemented, called CNN base, CNN medium, and CNN deep, each with different depths. These architectures are used to contrast the ResNet as computationally cheap architectures to achieve WSC.\n\nSECTION: IVExperiments\n\nSECTION: IV-AImplementation details\n\nThe experimental framework was developed in Python 3.10, using the PyTorch library for model implementation and training, which supports CUDA GPU computation. The models are implemented from scratch in the case of the ResNets, and from thevit-pytorch111Lucidrainsvit-pytorch Github page in the case of the ViT backbones:https://github.com/lucidrains/vit-pytorchlibrary was used, which faithfully implements the selected vision transformers and adapts them for WSC. The datasets go through a preprocessing stage of transformations for input normalization by the torchvision library: They are transformed into tensors, resized to a size ofpixels, normalized according to their corresponding mean and standard deviation characteristics, and finally processed as 32-bit floating point values for computational ease. Then, depending on the type of architecture, the images are tokenized (for transformers) at different patch sizes, depending on the configuration described in each architecture\u2019s respective paper:for implementations of TransCrowd, XCiT, Parallel ViT, or DeepViT, andfor ViT. CrossViT uses both patch sizes since it works at multi-granularity. In ResNets, the images are entered as a whole. The architectural implementation of each type of model is defined by the hyperparameter configuration in its own paper. The tableIsummarizes the architectural properties of each chosen model variant.", "text_file": "data\\paper_texts\\2412.02250v1_content.txt"}, {"title": "EFTViT: Efficient Federated Training of Vision Transformers with Masked\n  Images on Resource-Constrained Edge Devices", "authors": ["Meihan Wu", "Tao Chang", "Cui Miao", "Jie Zhou", "Chun Li", "Xiangyu Xu", "Ming Li", "Xiaodong Wang"], "published_date": "2024-11-30T03:20:14Z", "summary": "Federated learning research has recently shifted from Convolutional Neural\nNetworks (CNNs) to Vision Transformers (ViTs) due to their superior capacity.\nViTs training demands higher computational resources due to the lack of 2D\ninductive biases inherent in CNNs. However, efficient federated training of\nViTs on resource-constrained edge devices remains unexplored in the community.\nIn this paper, we propose EFTViT, a hierarchical federated framework that\nleverages masked images to enable efficient, full-parameter training on\nresource-constrained edge devices, offering substantial benefits for learning\non heterogeneous data. In general, we patchify images and randomly mask a\nportion of the patches, observing that excluding them from training has minimal\nimpact on performance while substantially reducing computation costs and\nenhancing data content privacy protection. Specifically, EFTViT comprises a\nseries of lightweight local modules and a larger global module, updated\nindependently on clients and the central server, respectively. The local\nmodules are trained on masked image patches, while the global module is trained\non intermediate patch features uploaded from the local client, balanced through\na proposed median sampling strategy to erase client data distribution privacy.\nWe analyze the computational complexity and privacy protection of EFTViT.\nExtensive experiments on popular benchmarks show that EFTViT achieves up to\n28.17% accuracy improvement, reduces local training computational cost by up to\n2.8$\\times$, and cuts local training time by up to 4.4$\\times$ compared to\nexisting methods.", "arxiv_id": "2412.00334v1", "html_link": "https://arxiv.org/html/2412.00334v1", "search_term": "ti:\"vision transformers\"", "html_content": "SECTION: EFTViT: Efficient Federated Training of Vision Transformers with Masked Images on Resource-Constrained Edge Devices\n\nFederated learning research has recently shifted from Convolutional Neural Networks (CNNs) to Vision Transformers (ViTs) due to their superior capacity. ViTs training demands higher computational resources due to the lack of 2D inductive biases inherent in CNNs. However, efficient federated training of ViTs on resource-constrained edge devices remains unexplored in the community.\nIn this paper, we propose EFTViT, a hierarchical federated framework that leverages masked images to enable efficient, full-parameter training on resource-constrained edge devices, offering substantial benefits for learning on heterogeneous data.\nIn general, we patchify images and randomly mask a portion of the patches, observing that excluding them from training has minimal impact on performance while substantially reducing computation costs and enhancing data content privacy protection.\nSpecifically, EFTViT comprises a series of lightweight local modules and a larger global module, updated independently on clients and the central server, respectively. The local modules are trained on masked image patches, while the global module is trained on intermediate patch features uploaded from the local client, balanced through a proposed median sampling strategy to erase client data distribution privacy.\nWe analyze the computational complexity and privacy protection of EFTViT. Extensive experiments on popular benchmarks show that EFTViT achieves up to 28.17% accuracy improvement, reduces local training computational cost by up to 2.8, and cuts local training time by up to 4.4compared to existing methods.\n\nSECTION: 1Introduction\n\nFederated Learning (FL) targets to enable collaborative training across multiple data distributed among different clients while prioritizing data privacy protection[24,25,21].\nEarly research on FL primarily concentrates on Convolutional Neural Networks (CNNs)[22,20,1]. Recently, the focus has increasingly shifted toward Vision Transformers (ViTs)[8], whose self-attention mechanisms excel at capturing long-range correspondences within images, achieving state-of-the-art performance across visual problems,e.g.,object recognition[8], detection[13,6], and semantic segmentation[40].\nDespite their impressive capabilities, training ViTs generally incurs significantly higher computational costs and longer training times due to the lack of spatial inductive biases within images[30,3], making it prohibitively challenging for resource-constrained edge devices.\n\nIn the CNN era, the resource-constrained FL problem has been explored by some researchers.\nThe workflow of these methods is summarized in Figure1(a).\nTypically, model-heterogeneous methods[23,1,4,37]train models of varying sizes on clients based on their available resources.\nHowever, these approaches are not well-suited to ViTs, as they fail to fundamentally reduce the computational demands of client-side training.\n\nIn this work, we explorewhether the training computational costs of ViTs can be fundamentally reduced without significantly compromising FL performance.\nRecent work in self-supervised learning has demonstrated that masked image modeling can effectively learn generalizable visual representations by reconstructing randomly masked pixels in input images[13,32], highlighting the substantial redundancy in images that may be unnecessary for recognition.\nTo test this hypothesis, we conduct FL experiments withno resource constraints, using masked images to examine their impact on model performance and training computational costs.\nIn the experiments, images are uniformly partitioned into non-overlapping patches, with a specified ratioof patches randomly masked. Only the unmasked patches are utilized for model training.\n\nAs illustrated in Figure2, we conduct experiments under a challenging data heterogeneity setting with, whereis a concentration parameter from the Dirichlet distributionin FL.\nResults indicate that varying the masking ratio has minimal impact on model accuracy but significantly reduces training computation costs. For instance, increasingfrom 0.00 to 0.75 reduces the computational load by up to 5.2, with only a marginal decrease in accuracy.\nThese findings suggest that using masked images in FL is a promising approach for enabling efficient ViT training on resource-constrained edge devices.\n\nInspired by these observations, we propose EFTViT, a hierarchical federated learning framework (as illustrated in Figure1(b)) that employs masked images to efficiently train ViT models across multiple heterogeneous data on resource-constrained clients, while also enhancing privacy protection by concealing client data content.\nEFTViT comprises lightweight local modules on edge clients and a larger global module on the central server, designed to accommodate limited client resources. The local modules are trained on masked images. Rather than aggregating parameters from clients, the global module receives intermediate patch features from the local modules, enabling it to learn universal representations suitable for heterogeneous data.\nTo maintain client data distribution, we propose a median sampling strategy that adjusts the patch feature count for each class to the median across all classes prior to uploading, enhancing both performance and training efficiency.\n\nOur main contributions in this work are summarized as follows:\n\nTo the best of our knowledge, we present EFTViT, the first federated learning framework to leverage masked images for efficiently training ViT models across multiple resource-constrained clients, while also enhancing client data content protection.\n\nEFTViT enables hierarchical training of all model parameters across clients and the central server, demonstrating substantial benefits for heterogeneous data. Additionally, we introduce a median sampling strategy to obscure the distribution information of intermediate features before they are uploaded to the server.\n\nExperiments on popular benchmarks demonstrate that EFTViT improves accuracy by up to 28.17%, reduces local training computational costs by up to 2.8, and lower local training time by as much as 4.4, setting new state-of-the-art results.\n\nSECTION: 2Related Works\n\nSECTION: 2.1General Federated Learning\n\nFederated learning is a decentralized machine learning approach that enhances privacy by training models directly on client devices, only transmitting model parameters to a central server.\nMost studies focus on addressing data heterogeneity[22,17,11,20]and privacy protection[2,27,5]in FL.\nFor instance, FedProx[22]adds a proximal term to optimize the local updates for addressing data heterogeneity.\nRegarding privacy protection, Asadet al.[2]apply homomorphic encryption to FL, enabling clients to encrypt their local models using private keys. Shiet al.[27]propose a FL method with differential privacy (DP).\nHowever, these works rely on the ideal assumption that clients have sufficient resources to handle model training process.\n\nSECTION: 2.2Federated Learning on Edge Devices\n\nFederated learning approaches on resource-constrained clients can be categorized into federated distillation (FD)[12,15,19,31]and partial training (PT)[7,1]. FD methods focus on aggregating knowledge from heterogeneous client models to a server model.\nFor instance, FedGKT[12]trains small models on clients and periodically transfers their knowledge to a large server model via knowledge distillation.\nPT methods divide a global model into smaller sub-models that can be locally trained on resource-constrained clients.\nFor instance, HeteroFL[7]randomly selects sub-models from the global model to distribute to clients.\nHowever, these methods adapt model size to clients\u2019 capacities, rather than fundamentally addressing the computational burden of client-side training.\n\nSECTION: 2.3Parameter-Efficient Fine-Tuning\n\nWhen dealing with transformer-based complex models, Parameter-Efficient Fine-Tuning (PEFT)[36,16,14]provides a practical solution for efficiently adapting pre-trained models across the various downstream tasks, which can reduce storage and computation costs by fixing most pre-trained parameters and fine-tuning only a small subset[10].\nSeveral studies[29,38]have explored using different PEFT techniques to assess performance improvements and resource savings in federated systems.\nHowever, the limited fine-tuning of parameters in PEFT inevitably constrains the adaptability of pre-trained models to new tasks, potentially resulting in suboptimal performance in federated systems with data heterogeneity.\n\nSECTION: 3Efficient Federated Learning with Masked Images\n\nSECTION: 3.1Problem Definition\n\nWe employ supervised classification tasks distributed acrossclients to formulate our problem. Each clientpossesses a dataset, wheredenotes the data samples andrepresents their corresponding labels. Here,represents the number of data points,denotes the input dimension, andindicates the number of classes.\n\nSECTION: 3.2Overview\n\nAs illustrated in Figure3, EFTViT employs hierarchical training across clients and the central server to enable privacy-preserving and efficient collaborative learning. Each client includes a local module withTransformer layers, a shared global module withTransformer layers, and a classification head.\nThe local module and classification head are trained on each client with unmasked image patches, enabling efficient local training and generating patch features that represent local knowledge.\nTo safeguard data distribution privacy, a median sampling strategy is applied on each client to create a balanced patch features (BPF) dataset before uploading to the server.\nThe global module is then trained on the server using the BPF dataset from clients to effectively learn global representations for all tasks. Finally, the server transmits the updated global module parameters back to clients for next training round.\n\nSECTION: 3.3Training with Masked Images\n\nTo enable efficient local training on resource-constrained clients, we present a patch-wise optimization strategy.\nFirstly, each input image is divided into a sequence of regular, non-overlapping patches, which are randomly masked at a ratio. The remaining unmasked patches, denoted as, are then used to train our framework. We define the patch features obtained by the local module on the clientas, whereandis the operation of randomly masking image patches fromand discarding the selected patches.\nTo preserve patch ordering for ViTs, the positional embeddings[28]of the remaining patches are retained.\nThis is inspired by the internal redundancy of images and reduces the amount of data that the model needs to process, thereby lowering computational complexity.\nAdditionally, these patch featuresmake it pretty challenging to reconstruct the original images since they are encoded from a very small portion of each image, inherently providing EFTViT with a content privacy advantage.\nNotably, the entire images are adopted for the inference on each client.\n\nSECTION: 3.4Data Distribution Protection with Median Sampling\n\nTo enhance privacy in EFTViT, we propose a median sampling strategy to generate a balanced patch features dataseton each client.\nIt aims to ensure that the generated patch features on each client contain an equal number of samples for each class, thereby preventing the leakage of statistical information or user preferences when uploaded to the central server.\nImbalanced data distribution on clients is a common issue in federated learning, and the median, being less sensitive to extreme values, is well-suited for addressing this challenge. Our median sampling strategy uses the median of class sample counts on each client to differentiate between minority and majority classes. It then applies oversampling to increase samples of minority classes and downsampling to reduce samples of majority classes. Specifically, for minority class samples, all patch features generated across multiple local training epochs are retained, whereas for majority class samples, only patch features from the final epoch are preserved.\nNext, downsampling is applied to reduce the number of samples in each class to the median.Empirically, we find that increasing the sampling threshold adds to computation costs but does not significantly improve final performance.\n\nSECTION: 3.5Hierarchical Training Paradigm\n\nTo effectively reduce the computational burden on clients without compromising performance, we propose a new hierarchical training strategy for ViTs that minimizes the number of trainable parameters on the clients. As aforementioned, our ViT models comprise a collection of lightweight local modules, a shared large global module and a classification head.\n\nTraining on Clients.On the client, the local moduleis responsible for mapping image patchesinto patch features, while the global moduleencodesinto representation vectors. The final classification headtransforms the representation vectorsto match the number of classes.\nOnly the parameters of the local module and classification head are trainable, while the parameters of the global module remain frozen and are iteratively updated via downloads from the server.\nFor the client, the loss function used in local training is defined as\n\nwhereis the number of classes in client, andis the probability distribution of label. The parameters,,are from the local module, global module, and classification head, respectively.\nTherefore, the optimization objective is to minimize\n\nwhereandare trainable.\n\nTraining on Server.The server aggregates heterogeneous knowledge from clients to learn universal representations across diverse tasks. The global moduleand classification headare trained using the balanced patch features dataset uploaded from participating clients in the latest training round.\nThe loss function can be formulated as\n\nwhereis the total number of classes, andis the probability distribution of labelon the data.\nThe optimization objective on the server is to minimize\n\nwhereandrepresent respective patch features and labels uploaded from clients.\n\nSECTION: 3.6Collaborative Algorithms\n\nThe overall workflow of our EFTViT is shown inAlgorithm1andAlgorithm2.\nAt the start of each round, the server will randomly choose a proportionfromclients to participate in training.\nEach client updates the parameters of its global module and classification head with those received from the server, and then initiates local training. The median sampling is applied to patch featuresto obscure local data distribution and produce a balanced dataset. The detailed process is presented inAlgorithm1.\n\nThe server receives the balanced patch features datasetfromclients to update the global dataset, storing new client data and updating existing client data. This dataset is used to train the global moduleand classification head, with the updated parametersandsent back to clients upon completion of training. The process is elaborated inAlgorithm2.\n\nInput:is the dataset inclient.represents the number of training epochs on each client.,,are the parameters of the local module, global module, and classification head, respectively.is the operation of randomly dropout image patches.\n\nInput:is the number of training rounds.represents the number of training epochs on server.,represent the parameter of global module and classification head of the server model, respectively.\n\nSECTION: 3.7Privacy & Complexity Analysis\n\nData Content Privacy.Contrary to previous beliefs, recent studies show that exchanging intermediate features during federated learning training is safer than sharing gradients. This is because attackers only have access to evolving feature maps rather than the final, fully trained maps, making data reconstruction attacksmore challenging[12,35,39,41]. Furthermore, EFTViT uploads patch features corresponding to 25% of the image area, controlled by the masking rate, which makes recovering the original image highly challenging, even if theoretically feasible. The masking rate can be further increased to enhance data content privacy, if necessary.\n\nData Distribution Privacy.To protect user statistical information and preferences, our patch features are balanced via the proposed median sampling strategy on clients, ensuring an equal number of samples for each class.\nAdditionally, our strategy is orthogonal to other privacy protection methods, such as Differential Privacy[9], which can be seamlessly integrated into EFTViT to offer enhanced protection against attacks.\n\nComplexity.Given a ViT model, letrepresent the resolution of original image,represent the resolution of each image patch,be the resulting number,be the latent vector size, andrepresent the number of Transformer layers. To simplify the calculation, we assume that size of,andis.\nEach client model hasTransformer layers, divided intolayers for local module andlayers for global module.\nThe model trains onof the image patches, whereis the masking ratio.\nThe time cost for forward propagation on the client is.\nAs the parameters of theTransformer layers in the global module are frozen, the backward propagation time cost is. Therefore, the overall time complexity in the client training stage is.\nAsapproachesandapproaches 1, the computational complexity of the model on the client gradually declines. Our default configurations are,, and, substantially reducing the computational load on the client.\n\nSECTION: 4Experiments\n\nSECTION: 4.1Datasets\n\nTo comprehensively evaluate EFTViT, we conduct experiments on two widely used federated learning datasets, CIFAR-10[18]and CIFAR-100[18], as well as a more challenging datasets, UC Merced Land-Use[34], for remote sensing.\nCIFAR-10 and CIFAR-100 datasets each contain 60,000 color images. CIFAR-10 is organized into 10 classes, with 6,000 images per class (5,000 for training and 1,000 for testing), while CIFAR-100 has 100 classes, with 600 images per class (500 for training and 100 for testing).\nUC Merced Land-Use dataset contains 21 land-use classes,e.g.,agricultural, forest, freeway, beach, and other classes, each with 100 images (80 for training and 20 for testing).\nWe partition samples to all clients following a Dirichlet distributionwith a concentration parameter, settingto simulate high or low levels of heterogeneity.\n\nSECTION: 4.2Implementations\n\nWe use ViT-B[8]pre-trained on ImageNet-21K[26]as the backbone of our framework. The input images are resized towith a patch size of. During training, data augmentation techniques such as random cropping, flipping, and brightness adjustment are applied. Following federated learning practices, we set the number of clients to 100, with a client selection ratio. The AdamW optimizer is used with an initial learning rate of, weight decay of 0.05, and a cosine annealing learning rate schedule with warm-up. We use a batch size of 32 for both training and testing on each client. All experiments are conducted on a single NVIDIA GeForce RTX 3090 GPU. In each round, clients train for 5 epochs locally, while the server performs an additional 2 epochs. The framework is trained for a total of 200 rounds, requiring approximately 24 hours.\n\nSECTION: 4.3Comparison with State-of-the-Art Methods\n\nGiven the lack of studies on training ViTs on resource-constrained clients, we adapt the FEDBFPT approach[33], originally designed for natural language processing tasks, as a strong baseline, which progressively optimizes the shallower layers while selectively sampling deeper layers to reduce resource consumption.\nTo establish additional baselines, we adapt several well-known PEFT methods to our federated learning setup: (a)Fed-Head: trains only the head layer parameters; (b)Fed-Bias: applies bias-tuning[36], focusing on training only the bias terms; (c)Fed-Prompt: incorporates prompt-tuning[16], adding trainable prompt embeddings to the input; and (d)Fed-LoRA: integrates LoRA-tuning[14]by adding the LoRA module to the query and value layers. These methods use FedAVG[24]for parameter aggregation.\nOtherwise, our method and the baseline methods share the same settings in the federated learning scenario.\n\nTesting Accuracy.The testing results of all methods across various datasets and data heterogeneity levels are presented in Table1. Note that Fed-Full means training all ViT parameters in clients without resource constraints, serving as a reference for the comparison. Compared with the baselines, EFTViT demonstrates apparent performance gains across all scenarios. For instance, we outperform the second-best method by over 7.61% on UC Merced Land-Use with. Notably, our method shows consistent results in high and low data heterogeneity settings, with even better performance under higher heterogeneity. In contrast, the baseline methods degrade heavily in performance as data heterogeneity increases. These findings underscore the importance of our hierarchical training strategy in handling data heterogeneity effectively.\n\nConvergence.We report the testing accuracy changes of EFTViT, FEDBFPT, and other baselines over 100 training rounds on CIFAR-10, CIFAR-100, and UC Merced Land-Use under high heterogeneity settings, as shown in Figure4. Our method consistently achieves the highest testing accuracy on three datasets throughout the training phase, converging faster and more stably.\nTo quantitatively compare convergence speed, we set a target accuracy of 85% and record the number of training rounds (# Rounds) required to reach this threshold. As shown in Table2, EFTViT significantly accelerates the convergence process, achieving 27.1faster convergence than Fed-Prompt on the UC Merced Land-Use dataset.\n\nComputational Efficiency.We evaluate the client-side computational efficiency of EFTViT from two perspectives: the computational cost of forward propagation during training and the maximum local training time across clients. Computational cost is measured in Giga Floating-Point Operations (GFLOPs). At a target accuracy of 85%, we report the maximum local training time (Time) for EFTViT and other baselines across three datasets.\nThe results in Table3show that our method significantly improves computational efficiency across both metrics. Specifically, EFTViT achieves at least 2the efficiency of other methods in terms of GFLOPs. For training time, EFTViT reduces local training time by 2.8compared to FEDBFPT on the UC Merced Land-Use dataset. This demonstrates that our masked image and hierarchical training strategy effectively reduces client computation, making EFTViT well-suited for federated learning in resource-constrained environments.\n\nSECTION: 4.4Ablation Study\n\nWe conduct extensive ablation experiments to investigate the key components of our approach.\n\nEffect of Masking Ratio.The masking ratiodetermines the number of masked image patches. A smallerreduces the amount of input data, thus lowering computational requirements during model training. Table4provides the GFLOPs for various masking rates, demonstrating that increasing the masking ratio significantly reduces GFLOPs. However, increasing the masking ratio also affects overall performance. We evaluate the effect of different masking rates for EFTViT.\nFigure5shows the results of EFTViT with varying masking ratios on CIFAR-100 at. Results indicate that EFTViT can support a wide masking ratio range. When the masking ratio increases from 0% to 75%, the accuracy remains larger than 90%. However, the performance decreases heavily when the masking ratio exceeds 75%. Therefore, we select a masking ratio of 75% to strike a balance between accuracy and computational efficiency.\n\nEffect of Layer Numberin Local Module.The layer numberdetermines the trainable parameter division between clients and the server, affecting the computational load of clients and final performance. Table5presents the number of trainable parameters (# Params) in each client and the corresponding accuracy achieved by the model for different values of.\nThe results show thathas minimal impact on the testing accuracy, showcasing the superior robustness of our EFTViTw.r.t.client resources. Given the higher computational cost of a largeon clients and the accuracy decrease, we selectas the default setting.\n\nEffect of Sampling Threshold.As elaborated in Section3.4, the sampling threshold determines the number of balanced patch features to upload for server training. Therefore, a higher threshold increases the training cost on the server.\nWe investigate the impact of utilizing median or higher sampling thresholds in EFTViT, as shown in Figure6. Results indicate that increasing the threshold provides minimal performance improvements.\nTo enhance the computational efficiency on the server, we select the median as the threshold in our method.\n\nSECTION: 5Conclusion\n\nIn this work, we propose a hierarchical federated framework, EFTViT, designed for efficient training on resource-constrained edge devices and handling heterogeneous data effectively. EFTViT reduces client computation by leveraging masked images with an appropriate masking ratio, which minimizes performance degradation while significantly lowering computational overhead by exploiting redundancy in image information. The masked images can also prevent the data content leakage from uploaded local features. Additionally, the hierarchical training strategy, which splits parameter training between the client and server, achieves full parameter optimization and improves performance on heterogeneous data across multiple clients. Finally, EFTViT incorporates a median sampling strategy to protect user data distribution, ensuring privacy while maintaining robust performance.\nThe extensive experiments on three benchmarks demonstrate that EFTViT significantly improves classification accuracy, reduces client training computational costs and time by large margins.\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.00334v1_content.txt"}, {"title": "Enhancing Parameter-Efficient Fine-Tuning of Vision Transformers through\n  Frequency-Based Adaptation", "authors": ["Son Thai Ly", "Hien V. Nguyen"], "published_date": "2024-11-28T18:09:49Z", "summary": "Adapting vision transformer foundation models through parameter-efficient\nfine-tuning (PEFT) methods has become increasingly popular. These methods\noptimize a limited subset of parameters, enabling efficient adaptation without\nthe need to fine-tune the entire model while still achieving competitive\nperformance. However, traditional PEFT methods may limit the model's capacity\nto capture complex patterns, especially those associated with high-frequency\nspectra. This limitation becomes particularly problematic as existing research\nindicates that high-frequency features are crucial for distinguishing subtle\nimage structures. To address this issue, we introduce FreqFit, a novel\nFrequency Fine-tuning module between ViT blocks to enhance model adaptability.\nFreqFit is simple yet surprisingly effective, and can be integrated with all\nexisting PEFT methods to boost their performance. By manipulating features in\nthe frequency domain, our approach allows models to capture subtle patterns\nmore effectively. Extensive experiments on 24 datasets, using both supervised\nand self-supervised foundational models with various state-of-the-art PEFT\nmethods, reveal that FreqFit consistently improves performance over the\noriginal PEFT methods with performance gains ranging from 1% to 16%. For\ninstance, FreqFit-LoRA surpasses the performances of state-of-the-art baselines\non CIFAR100 by more than 10% even without applying regularization or strong\naugmentation. For reproducibility purposes, the source code is available at\nhttps://github.com/tsly123/FreqFiT.", "arxiv_id": "2411.19297v1", "html_link": "https://arxiv.org/html/2411.19297v1", "search_term": "ti:\"vision transformers\"", "html_content": "SECTION: Enhancing Parameter-Efficient Fine-Tuning of Vision Transformers through Frequency-Based Adaptation\n\nAdapting vision transformer foundation models through parameter-efficient fine-tuning (PEFT) methods has become increasingly popular. These methods optimize a limited subset of parameters, enabling efficient adaptation without the need to fine-tune the entire model while still achieving competitive performance.\nHowever, traditional PEFT methods may limit the model\u2019s capacity to capture complex patterns, especially those associated with high-frequency spectra. This limitation becomes particularly problematic as existing research indicates that high-frequency features are crucial for distinguishing subtle image structures.\nTo address this issue, we introduce FreqFit, a novelFrequencyFine-tuning module between ViT blocks to enhance model adaptability. FreqFit is simple yet surprisingly effective, and can be integrated with all existing PEFT methods to boost their performance. By manipulating features in the frequency domain, our approach allows models to capture subtle patterns more effectively. Extensive experiments on 24 datasets, using both supervised and self-supervised foundational models with various state-of-the-art PEFT methods, reveal that FreqFit consistently improves performance over the original PEFT methods with performance gains ranging from 1% to 16%. For instance, FreqFit-LoRA surpasses the performances of state-of-the-art baselines on CIFAR 100 by more than 10% even without applying regularization or strong augmentation. For reproducibility purposes, the source code is available athttps://github.com/tsly123/FreqFiT.\n\nSECTION: 1Introduction\n\nThe availability of transformer-basedfoundation modelshas revolutionized the domain adaptation research. Many fine-tuning methods have been proposed to effectively leverage the good representation of foundation models[23,5,53,27,49,8,22]. These parameter-efficient fine-tuning (PEFT) methods work by focusing on a small subset of parameters. This approach retains the original pre-trained parameters for the most part, tuning only targeted operations, which dramatically reduces computational overhead while maintaining competitive performance on downstream tasks. For example, BitFit[5,53]updates only the bias term of the pre-trained backbone, Adapter[21]and AdaptFormer[8]insert bottleneck-like MLP modules with residual connection inside ViT\u2019s blocks, or Lora[22]injects trainable rank decomposition matrices into each layer of the Transformer architecture.\n\nHowever, many PEFT methods may limit the model\u2019s capacity to capture complex patterns, especially those associated with high-frequency spectra. The limitation becomes particularly problematic as existing research indicates that high-frequency features are vital for improving performance[19,36,39,46,3,32]. These high-frequency components play a key role in tasks that require a deep understanding of intricate image details, such as fine-grained classification, object detection, and medical imaging. Without effective modeling of these high-frequency patterns, PEFT methods risk underperforming, particularly when applied to complex, real-world datasets where such subtle distinctions are essential for accurate predictions.\n\nWhile it might be argued that PEFT methods have indirectly addressed this frequency limitation by tuning the necessary operations, including self-attentions and other linear layers, our empirical and theoretical findings suggest otherwise.\nAs illustrated in Fig.1, incorporating the feature transformation modules between ViT blocks improves the performance over the original PEFT method.\n\nBuilding on these insights, we propose FreqFit, a straightforward frequency fine-tuning method designed to modify the features\u2019 spectra before it passes through the subsequent ViT blocks. As shown in Fig.1, FreqFit begins by performing a Fast Fourier Transform (FFT) along the spatial dimensions to convert features into the frequency domain. The spectra are then modulated using a learnable filter. After modulation, the spectral features are converted back to the spatial domain with an inverse FFT (iFFT), followed by a learnable scaling and shifting module, and finally added as the residual connection to the original input.\n\nOur main contributions are as follows:\n\nWe propose FreqFit, a simple and effective frequency-based fine-tuning module that seamlessly integrates with existing PEFT methods to enhance model adaptation.\n\nWe provide theoretical support for why FreqFit can capture image features that existing PEFT methods cannot.\n\nWe provide a detailed analysis using 24 diverse datasets to demonstrate that FreqFit\u2019s frequency modulation improves ViT token representations and model adaptability.\n\nSECTION: 2Related Works\n\nParameter-efficient fine-tuning MethodsFine-tuning knowledge from pre-trained or foundation models has emerged as a quick and efficient approach to learning new tasks. Recent studies on fine-tuning methods could be categorized into two main approaches: (i) Adopting the subset of tunable input to frozen pre-trained models with Visual Prompt-tuning[23]as the representative method. Visual Prompt-tuning (VPT) emerges as a promising solution to address the challenges of domain adaptation. By tuning a small set of additional input tokens, VPT alternates the input domain to optimally align with the frozen pre-trained model.\n(ii) Minimally tuning a small subset of parameters of the pre-trained model while keeping the rest unaltered. The noticeable methods can be mentioned as Bias[5,53]simply fine-tune solely the Bias terms of pre-trained models[5,53,27], Adapter[21,33,34]inserts bottleneck-like modules with residual connections into ViTs backbone, and low-rank method as LoRA[22], BOFT[29], VeRA[24], FourierFT[17].\nBoth VPT and minimal weight-tuning have demonstrated their efficacy and been explored in many directions, such as long-tailed image classification[14], adversarial attacks[2,6], generative visual[50,4], point cloud analysis[47,56], and continual learning[49,48,44].\n\nHowever, these methods greatly rely on hyper-parameters such as the number of prompts and the reduction factor in the case of VPT and Adapter, respectively. Their performances show inconsistency across various hyper-parameters settings[23,38,49,8,10,57,7]. For example, the effectiveness of VPT significantly depends on the prompt tokens hyper-parameters as they directly determine how prompt tokens interact with image tokens in the spatial domain[23,49,48,52]. Another problem is that in the fine-tuning context, the pre-trained backbone parameters are frozen, including self-attention which data-dependably captures long-term dependencies. The existing fine-tuning methods which mainly perform on spatial domain have not been able to learn the equivalent knowledge as self-attention does. Our method takes another approach by modifying the spectrum of the input features to adapt to the frozen pre-trained model in the frequency domain. The learnable filter in our FreqFiT can cover all frequency signals; therefore, it can capture both long-term and short-term interaction between tokens. A frequency-based PEFT method is FourierFT[17]which aims to further compress trainable parameters compared to LoRa[22]by enjoying the powerful expressiveness of the Fourier transform. Our paper differentiates itself from FourierFT by focusing on the ability to modulate the frequency signal.\n\nFourier Transform in VisionFourier transform has been an important tool in digital image processing. There are a variety of works that incorporate the Fourier transform into the deep learning method which suggested the connection between frequency information and superior performances[19,36,40,41,51,25,11,13]. Some of these works utilize the convolution theorem to accelerate the CNNs via FFT computational advantages[25,11,13]. With the advancements of ViTs, there are lines of works that employ the Fourier transform to develop self-attention alternatives[41,36,40,26,37]. However, these prior works mainly apply in the pre-training state where the backbone\u2019s parameters are fully updated, including self-attention.\n\nSECTION: 3Methodology\n\nBackground on Fourier Transform.This section provides the background knowledge of Fourier Transform (FT) to set the foundation for understanding the proposed methods. FT has been widely used to decompose signals, such as images and audios, into their consituents frequency components and amplitudes. Concretely, FT for continuous time-domain signal is defined as follows:\n\nwhereis the time-domain signal,is the frequency-domain representation of the signal,is the angular frequency,is a complex exponential function. The output of this transformationis a complex function encoding both amplitude and phase of each frequency in the original signal. One can also convert the frequency-domain representation back to the orignal time-domain signal using inverse FT.\n\nThe original FT is typically defined for time-domain signals, but it can also be applied to image features by replacing the time index with the spatial index.\n\nFast Fourier Transform (FFT).\nIn many practical applications, signals are represented as discrete data points rather than continuous functions. Discrete Fourier Transform (DFT) is the version of the FT that is applied to discrete signals, such as time-series data or digital images. The DFT is defined for a sequence ofdiscrete values,, whereas follows:\n\nwhere. DFT can be computationally expensive, particularly for large signals, since its direct computation involvesoperations. FFT exploits the symmetrical property within DFT to improve the computational efficiency to. Otherwise, FFT computes the same features as DFT. Note that Eq.3can be written in matrix form, which will be used in our theoretical proof of Theorems 1 & 2 in subsequent sections.\n\nFreqFit - Frequency Fine-tuning.Here, we formally introduce the frequency tuning method, called FreqFit, as illustrated in Fig.1. FreqFit integrates a frequency operator, consisting of a filter basis followed by a residual connection, between ViT blocks. Given an input token, we perform FFT along the spatial dimensions to transform the input into, as shown in Eq.4.is a complex tensor representing the spectrum ofin the frequency domain. We modulateby multiplying it with the learnable filter, which has the same dimensions as, as shown in Eq.5. Finally, we convert the modulated spectral featuresback to the spatial domain using the inverse FFT, as shown in Eq.6, and add a residual connection from the original input. This process can be mathematically summarized as follows:\n\nwhere,andare the fast Fourier transform (FFT) and its inverse.denotes a learnable filter,andare the scale and shift factors, andis the Hadamard product.\nTo facilitate the straightforward incorporation of FreqFiT layer into other fine-tuning methods. The 2D DFT can be viewed as performing 1D DFT on the two dimensions alternatively which also satisfied the conjugate symmetry property.\n\nHow Does FreqFit Improve Performance?The basic idea behind frequency-based tuning FreqFit is to learn the interactions among spatial locations in the frequency domain. Many studies have demonstrated that incorporating high-frequency features leads to better performance[3,32,46,39]. We hypothesize that FreqFit amplifies high-frequency components. The spectral modulation capabilities of FreqFit are visualized and discussed in Fig.2and Sec.6, showing that integrating FreqFit increases the high-frequency features. This observation supports our hypothesis.\n\nAs shown in Eq.5, the filterKmodulates the spectrum ofXthrough element-wise multiplication. This modulation controls the strength of different frequency components in the output, allowing for the enhancement or suppression of specific frequency ranges within the token signal. In other words, the result of the element-wise multiplication between the filter and tokens is determined by the filter, which is learned through the back-propagation process. In what follows, we provide the theoretical foundation that underscores the importance of incorporating FreqFit into the existing fine-tuning paradigm.\n\nTheorem 1.FreqFit withparameters can create a feature transformation that spatial-domain parameter-efficient fine-tuning methods cannot replicate.\n\nSketch of Proof:Due to space constraints, the full proof is provided in the appendix. Here, we outline a proof sketch for better understanding. FreqFit performs a 2D FFT for each token dimension by aggregating statistics across all tokens. In other words, FreqFit operates on thedimensions of. This means that changes induced by the frequency filter depend on these aggregated statistics, affecting all tokens simultaneously. In contrast, spatial-domain PEFT methods do not aggregate statistics across all tokens. Instead, they compute aggregated statistics along thedimension ofwithin each individual token. For example, LoRa applies changes using a low-rank matrix or other spatial-domain PEFT methods modify a subset of parameters, which affects all tokens, but without relying on aggregated statistics across tokens. Therefore, spatial-domain PEFT methods cannot replicate the feature changes introduced by FreqFit.\n\nTheorem 1 demonstrates that FreqFit is a missing piece within the current parameter-efficient fine-tuning paradigm. By using onlyparameters, FreqFit can transform features in ways that existing PEFT methods cannot, thereby enhancing the model\u2019s ability to capture more complex patterns.\n\nTheorem 2.Combining FreqFit with spatial-domain PEFT methods can create a feature transformation that cannot be achieved by FreqFit or any spatial-domain PEFT method alone.\n\nSketch of Proof:Since FreqFit computes aggregated statistics within thesame token dimensions, it cannot replicate the effect of spatial-domain PEFT methods, which aggregate statistics acrossall token dimensions. Together with the result of Theorem 1, this suggests that combining these two methods can yield transformations that neither method can achieve on its own.\n\nTheorem 2 provides a compelling rationale for combining two complementary approaches: FreqFit and traditional PEFT methods like LoRa and Adapters. By leveraging their distinct strengths, this combination enables a significantly more effective fine-tuning strategy. Our experimental results across 24 diverse datasets strongly validate this theory, demonstrating substantial improvements over using either method alone.\n\nSECTION: 4Experimental Settings\n\nPre-Trained Foundation Models.In this study, we experiment with different foundation models that were pre-trained on different datasets and learning approaches, including MAE[20], MoCo[9], and ImageNet-21k[12]. All the foundation models have the same ViT-Base backbones[16]. We also follow the original configurations, such as image size, patch size, etc.\nSee Supplementary Material for more details on the experimental settings.\n\nCifar100\n\nCaltech101\n\nDTD\n\nFlower102\n\nPets\n\nSVHN\n\nSun397\n\nCamelyon\n\nEuroSAT\n\nResisc45\n\nRetinopathy\n\nClevr-Count\n\nClevr-Dist\n\nDMLab\n\nKITTI-Dist\n\ndSpr-Loc\n\ndSpr-Ori\n\nsNORB-Azim\n\nsNORB-Elev\n\nMean\n\nFine-Tuning PEFT Methods.We apply the feature transformation techniques in conjunction with the following PEFT methods. We selected these methods as they are representative examples within their respective approach families.\n\nLINEAR: only update the last linear layer as classification head.\n\nBIAS[5,53]: update only the bias term of the pre-trained backbone.\n\nADAPTER[21,33,34]: insert bottleneck-like MLP modules with residual connection inside ViT\u2019s blocks.\n\nVPT[23,52]: adding learnable tokens along with image tokens as input of ViT\u2019s blocks. We follow the number of prompt tokens reported in VPT[23].\n\nLoRA[22]: decomposes a large matrix into two smaller low-rank matrices in the desired layers. We apply LoRa for all linear layers.\n\nBOFT[29]: leverages butterfly-structured orthogonal parameterization to reduce trainable parameters. We apply BOFT for all linear layers.\n\nVeRA[24]: using a single pair of low-rank matrices shared across all layers and learning small scaling vectors instead, reducing the number of trainable parameters. We apply VeRA for all linear layers.\n\nDownstream Tasks.Following[23,8,52,28,27], we evaluate FreqFiT on the VTAB-1k Natural[54]tasks that contain natural images and the Fine-Grained Visual Classification (FGVC) tasks, including CUB-200-2011[45], NABirds[42], Oxford Flowers[31], Stanford Dogs[1]and Stanford Cars[18]. We follow the dataset split in[23].\n\nHypeparameter Configurations and Regulations/Augmentation.Our primary goal is to illustrate the effectiveness of the feature transformation approach rather than to compete directly with state-of-the-art methods. Therefore, we avoid extensive hyperparameter searches, including Adapter reduction rate, VPT prompt tokens, LoRa rank, instead applying a single default configuration across all experiments for consistency. In addition, to clearly demonstrate the effectiveness of the feature transformation approach, following[5,23], we do not utilize Mixup[55]or strong image augmentation techniques in this study. Further details are provided in Supplementary Material.\n\nSECTION: 5Results\n\nEffects of incorporating FreqFit.Tables1and2summarize the fine-tuning performance on VTAB-1k tasks with natural images, demonstrating that methods enhanced with feature transformation techniques consistently outperform recent fine-tuning approaches. Notably, these improvements hold across a range of advanced PEFT techniques\u2014including VPT[23], Bias Tuning[5,53], Adapter[21,33,34], LoRA[22], BOFT[29], and VeRA[24]\u2014and pre-trained foundation models such as MAE[20], MoCo[9], and ImageNet-21k[12]. For instance, when applied to ImageNet-21k, FreqFit achieves a mean accuracy gain of 1.5% to 16% across 19 VTAB-1k tasks, outperforming state-of-the-art PEFT methods. Note that, to clearly demonstrate the effectiveness of the feature transformation approach, we do not utilize Mixup[55]and strong augmentations. Even without those strong performance booster techniques, feature-transformed methods yield competitive results and often surpass baseline performances with these enhancements across multiple tasks. For instance, FreqFit-LoRA surpasses other state-of-the-art baselines on CIFAR100 more than 10%. Furthermore, the Linear transformation method, when used with the feature transformation module, exceeds the performance of other state-of-the-art techniques and even outperforms full fine-tuning (FULL) by 1-2% on several tasks. These results highlight the crucial of modifying features in the frequency domain in optimizing ViTs across diverse settings.\n\nSupervised versus self-supervised pre-trained models.Tab.1and2show that FreqFit methods yield better performance gains when applied to supervised pre-trained models, such as ImageNet-21K, compared to self-supervised pre-trained models like MAE and MoCo. Specifically, FreqFit techniques consistently produce higher accuracy improvements with MoCo than MAE pre-trained models. The results highlight the importance of addressing the limitation in capturing frequency patterns regardless the pre-training strategy.\n\nNext, we will discuss how incorporating FreqFit improves the performance for the representative PEFT methods.\n\n- FreqFit with VPT.Our simple yet effective method outperforms the original VPT by 23% and 7% on average across all Natural tasks for pre-trained MAE and MoCo, respectively, and 2.4% on all tasks with Imagenet-21K. We hypothesize that this superiority is due to the frequency properties of FreqFiT. Typically, VPT learns the interaction among tokens in the spatial domain by appending prompt tokens to image tokens, with long-term dependencies learned by the self-attention module[43,15,36]. This important knowledge may not be fully captured when all the backbone\u2019s parameters, including self-attention, are not updated for new tasks under VPT framework. In contrast, our FFT-based tuning approach captures both long-term and short-term interactions. It is worth noting our FreqFiT-VPT does not search for theoptimal prompt lengthor the ViTblocks to insertprompt tokens as in VPT[23]. Instead, we reuse the prompt lengths reported in VPT[23]and Gated-VPT[52], inserting the FreqFiT layer before every ViT block.\n\n- FreqFit with Adapter.FreqFiT significantly enhances the performance of the original Adapter method, achieving average improvements of 16.4% across all VTab-1k tasks with Imagenet-1K pre-trained weights. This improvement can be attributed to the original Adapter\u2019s limitations in learning long-term information. In the Adapter approach, only the bottleneck-like MLP modules are updated, while all other parameters, including the self-attention modules, remain frozen. Furthermore, we observe performance fluctuations when varying the reduction factor. For a detailed analysis of experiments on prompt lengths and reduction factors, please refer to Sec.6.\n\n- FreqFit with Bias Tuning.Bias tuning[5,53]is a competitive, parameter-efficient tuning baseline. Interestingly, despite yielding superior results with 6.1% gain in performance with Imagenet-21k[12]pre-trained model, the results with MAE[20]and MoCo[9]do not enhance performance. This suggests the distinction between supervised and self-supervised pre-trained models could have different effects on the performances. Unlike VPT and Adapter, bias tuning is the only baseline that modifies self-attention (bias terms), which is essential for learning long-term dependencies.\nSince only the bias term is updated, this strategy can be viewed as a linear transformation, which may not be sufficient to handle complex data distribution shifts or effectively capture the frequency patterns. Consequently, we hypothesize that the linearly shifted outputs from bias tuning could negatively impact the long-term dependency-capturing ability of the FreqFiT layer. To better understand this phenomenon, we will present a visualization of the frequency filter and discuss it in the subsequent section.\n\nImagenet-21K - CIFAR100\n\n(Left) LoRA. (Right) FreqFiT-LoRA\n\n- FreqFiT with low-rank methods.LoRA[22]and its variants, BOFT[29]and VeRA[24], reparameterize a large matrix into smaller low-rank matrices. In this study, we use the default hyper-parameter configurations from HuggingFace[30]and apply these PEFT techniques for all linear layers for all experiments. FreqFiT consistently enhances the performance of these methods, achieving average improvements of 1.6%, 1.5%, and 0.8% across all VTab-1k tasks with Imagenet-1K pre-trained weights with LoRA, BOFT, and VeRA, respectively. Since we apply these methods for all linear layers, including those of self-attention operations, they may be able to capture the spatial dependencies for new tasks and address the inter-block relationship problems. However, when incorporated with FreqFit, these PEFT methods achieve better performances, validating our approach of altering the feature in the frequency domain.\n\nMAE/FreqFiT-Adapter/Caltech101\n\nMAE/FreqFiT-Bias/Caltech101\n\nCLIP/FreqFiT-VPT/Flower102\n\nCLIP/FreqFiT-Bias/Flower102\n\nImagenet/FreqFiT-Lora/Cifar100\n\nImagenet/FreqFiT-Bias/Cifar100\n\nSECTION: 6Ablation Study\n\nFrequency analysis.Given that FreqFit operates primarily in the frequency domain, it is essential to analyze the learned transformations. Following[32,40,39], we compare the relative log amplitudes of the Fourier transform of the output feature maps. Fig.2visualizes the differences in the relative log amplitudes of Fourier-transformed feature maps between (left) original LoRA and (right) FreqFit-LoRA, the FreqFiT-enhanced versions. Thelog amplitude represents the relative logarithmic amplitude between frequency(center) and(boundary). Brighter colors indicate deeper layers.\n\nAcross various tasks and fine-tuning methods, a common pattern emerges: FreqFiT tends to increase the amplitude, as hypothesized in Sec.3. Since high-frequency features are important in capturing interactions among spatial locations in the frequency domain, leading to better performance as discussed in[19,36,39,46,3,32], it is logical that our method increases the amplitude. FreqFiT filters can address both low and high frequencies, enabling the FreqFiT-enhanced methods to capture high-frequency features more effectively than the original methods, as evidenced by the higher amplitudes shown in Fig.2.\n\nTo better understand this behavior, we visualize the filters of different fine-tuning settings, as shown in Fig.3. Our visualization shows that the incorporated FreqFiT-LoRA, FreqFiT-VPT, and FreqFiT-Adapter can capture high-frequency components by adopting our FreqFiT. In addition, we can also see high-pass, low-pass, and band-pass filters in the visualization. This is reasonable as the filters in our FreqFiT cover all frequency ranges. Regarding the FreqFiT-Bias visualizations, they do not show a clear pattern of capturing high-frequency components. This visualization justifies our hypothesis and results shown in Sec.5.\n\nCompared to VPT and Adapter, Bias tuning is the only baseline that modifies the self-attention mechanism. However, its effectiveness is lower than the other methods. This suggests that Bias tuning and our FreqFiT are not necessarily complementary. Importantly, our FreqFiT captures both low and high frequencies. Previous studies[19,40,32,3]have indicated a link between high-frequency components in tokens and improved performance. These findings validate our frequency-tuning approach and highlight a potential research direction: adaptive frequency-tuning, which could result in filters functioning as high-pass filters. On the other hand, we can recognize a pattern in the learned filter of Bias with the pre-trained Imagenet-1K. However, it is also less obvious compared to that of LoRA. This could be because the difference in pre-trained foundation model, suggesting the drawback of linear transformation in capturing useful frequency signals.\n\nFreqFit versus Scaling-Shifting.Fig.4presents a detailed comparison of two feature transformation techniques: Scaling-Shifting and frequency tuning (FreqFit). The results indicate that FreqFit consistently outperforms Scaling-Shifting, with a mean performance gain difference of 1.2% across all evaluated methods, underscoring the effectiveness of FreqFit in aligning input features more closely with the frozen model parameters. This superiority may be attributed to FreqFit\u2019s ability to operate across the all frequencies spectrum, enabling it to capture a wider range of spatial dependencies within the token representations. By modulating frequencies, FreqFit can more effectively capture both long-term structural patterns and short-term fine-grained details, providing a more holistic transformation that allows the frozen parameters to better adapt to variations in data distribution. Furthermore, prior research has underscored a strong connection between high-frequency signal capture and improved model performance[36,32,19,40,41,51,25,11,13]. FreqFit\u2019s frequency modulation allows it to harness these high-frequency signals effectively, thus reinforcing its potential to capture subtle spatial relationships and complex data patterns. This positions FreqFit as a promising enhancement for parameter-efficient fine-tuning, offering frozen models a level of flexibility and robustness in adapting to new tasks that Scaling-Shifting alone cannot provide.\n\nTest Acc. (%)\n\nPrompt length (log scale)\n\nTest Acc. (%)\n\nReduction factor\n\nIn contrast, the Scaling-Shifting technique offers a straightforward linear transformation by applying learnable scaling and shifting factors to input data, helping to align it with frozen model parameters. This approach adjusts the amplitude and mean of the input features, effectively addressing moderate distribution shifts without modifying the model\u2019s structure. However, Scaling-Shifting\u2019s simplicity limits its ability to capture complex spatial dependencies and high-frequency details, as it focuses only on global feature characteristics. While efficient for tasks with minimal domain shifts, Scaling-Shifting lacks the nuanced adaptability that FreqFit provides. The comparison underscores that while both techniques enhance PEFT performance, FreqFit\u2019s frequency-based modulation better captures intricate data patterns, making it particularly suitable for complex tasks.\n\nRobustness Against Randomization.Tab.3provides per-task fine-tuning results on FGVC tasks with self-supervised learning pre-trained models MAE[20]and MoCo-v3[9]. Results ingraydenote the reported performances in original Gated-VPT[52]. Since[52]does not provide seeding configurations or how many runs the results were averaged from, we use the provided configurations, including prompt length and learning rate to reproduce their results with the same 5 seeds as used for FreqFiT-VPT, denoted asGated-VPT w/ seeds. Our FreqFiT-VPT demonstrates the stability and superiority over VPT[23]and Gated-VPT[52].\nIn addition, the results of theGated-VPT w/ seedsetting are inconsistent and lower than those in the original paper. We report the standard deviation from 5 runs in brackets. The comparison points out that theseeding is importantfor Gate-VPT and also highlights the stability and improvement of our FreqFiT-VPT.\n\nSensitivity to Hyper-Parameters.We investigate the degree of performance fluctuation when varying the prompt lengths in VPT and the reduction factor in VPT and Adapter tuning. Fig.5illustrates the ablation studies on (a) prompt length for FreqFiT-VPT and (b-c) reduction factor for FreqFiT-Adapter across different tasks. We vary the number of prompts in {0, 1, 5, 10, 50, 100, 200} and the reduction factor in {8, 64, 256}. When adjusting the prompt length, accuracy fluctuates from less than 2% to roughly 50% on average for Caltech101 and SVHN tasks, respectively, while other tasks show fluctuations around 5%. Even with FreqFiT\u2019s enhanced ability to capture useful frequency signals, FreqFiT-VPT still heavily depends on the prompt length. In contrast, FreqFiT-Adapter exhibits significantly smaller performance variations, with the largest fluctuation being around 8% on average.\n\nAttention Tuning.In this experiment, we explore an alternative route for learning the long-term dependencies without updating the MSAs. Instead of plugging our frequency-filter module between ViT\u2019s blocks, we simply insert it into the self-attention module right after the QKV projection and before the softmax operation. Mathematically:\n\nwhere,andare the weights matrices and bias terms of Queue, Key, and Value in the self-attention mechanism. By doing this, we can directly modify the output of the self-attention and learn the long-term dependencies. Tab.4presents the enhanced outcomes of this strategy. Especially, it can boost the results up to 6% on the KITTI dataset[54]. The experiment substantiates the advantage of our frequency-tuning approach.\n\nSECTION: 7Conclusion\n\nIn this study, we explore the route of frequency adaptation as PEFT method. We introduce FreqFiT, a novel frequency-tuning module designed to modify ViT frequencies for better adaptation to new tasks. This module can be easily integrated into existing fine-tuning methods. We conducted extensive experiments to evaluate FreqFiT on both supervised and self-supervised foundational models, including MAE, MoCo, CLIP, and ImageNet-21k, across VTAB-1k Natural and FGVC tasks. Our comprehensive analysis demonstrates how FreqFiT effectively captures high-frequency components in tokens, leading to significant performance improvements across various tasks. We believe FreqFit establishes a new paradigm for effective adaptation of ViT-based foundation models to downstream tasks.\n\nSECTION: References\n\nSupplementary Material\n\nThe Supplementary Material is organized as follows:\n\nProofs for Theorem 1 and Theorem 2, Sec.8.\n\nAugmentation and Hyper-parameters for all experiments, Sec.9.\n\nFreqFit pseudo-code, Alg.1.\n\nPer task result for Scaling Shifting, FreqFit-FourierFT, Tab.6.\n\nMore visualization of the relative log amplitudes of Fourier-transformed feature maps of different PEFT methods, Fig.6.\n\nMore visualization of FreqFit filters on different settings.\n\nSECTION: 8Proofs\n\nHere, we employ LoRA to ease the proof as this can be generalized to other PEFT methods. We re-introduce FreqFit and LoRA equations to facilitate the proofs below.\n\nGiven the input the feature map, where each token is a D-dimensional vector spread across anspatial grid. The LoRA transformation and FreqFit are as:\n\nwhere,, andandare the low-rank matrices. Note that, for simplicity, we do not presentandparameters for FreqFit as in the main manuscript.\n\nTheorem 1.FreqFit withparameters can create a feature transformation that spatial-domain parameter-efficient fine-tuning methods cannot replicate.\n\nProof.For FreqFit to replicate LoRA transformation, and vice versa, there must exist a filterFsuch that:\n\nHowever, this is not generally possible because of the following reasons:\n\nFilterKandBAoperate in different domains, i.e., frequency domains and spatial domains, respectively.\n\nKis a 3D filter that modulates information in both the tokens 2-dimensionaland the channel dimensionD. For each position (in frequency domains) in thegrid,Kcontains a unique filter for each of theDchannels.\n\nAB, in Eq.12is a token-specific modification, where the D-dimensional representation of each token is updated. This modification captures relationships within and across channels, such as correlations or dependencies among the features in D.\n\nAs a result, FreqFit introduces implicit cross-token interaction via aggregated statistics across all the tokens. Whereas, LoRA operates locally, emphasizing token-specific updates in the channel dimension.\n\nLoRA transformation is not full rank.The product, in12, has a rank of at mostr, which limits the expressiveness of this transformation to a subspace of dimensionr. In other words,can only capture transformations in an r-dimensional subspace of.\n\nFreqFit transformation is full rank.The Fourier transform ofis given by:\n\nwhereandare the unitary Fourier transform matrix of row and column grid. Theandare the conjugate transpose ofand, respectively, i.e.,and. Thus, if the inputis full-rank andis a full-rank diagonal matrix (no zero entries for all frequency components), sinceis unitary, the rank ofis preserved in the frequency domain, then the resultmust be full-rank. This means FreqFit can learn from all information from the input feature with.\n\nFreqFit hasparameter complexity, as the frequency modulation filtercan be parameterized efficiently, focusing only on essential frequency components, regardless the input dimensions ofX.\n\nTherefore, FreqFit withparameters can create a feature transformation that spatial-domain parameter-efficient fine-tuning methods cannot replicate.\n\nTheorem 2.Combining FreqFit with spatial-domain PEFT methods can create a feature transformation that cannot be achieved by FreqFit or any spatial-domain PEFT method alone.\n\nProof.The FreqFit filterapplies a frequency-domain filter independently to each channelon the grid. This impliesdepends only onas in Eq.18. Whereas, the product, Eq.12is the same across all tokens, meaning it only introduces channel-wise dependencies. This meansdepends only onas in Eq.19. Mathematically:\n\nAssume FreqFit can replicate LoRA transformation. Then, FreqFit must create cross-channel dependencies of the form\n\nHowever, by construction, FreqFit only operates independently within each channel. This directly contradicts LoRA as it introduces dependencies across. Thus,FreqFit can not replicate LoRA transformation.\n\nTogether with the result of Theorem 1 which shows LoRA can not replicate FreqFit transformation, Theorem 2 provides a compelling rationale for combining two complementary approaches: FreqFit and PEFT methods like LoRa. By leveraging their distinct strengths, combining these two methods can yield transformations that neither method can achieve on its own.\n\nSECTION: 9Augmentation and Hyper-parameters\n\nWe use PyTorch to implement all experiments on NVIDIA V100-32GB GPUs. Following[23], we conduct a grid search to find the tuning-specific hyper-parameters, learning rate, and weight decay values using val set of each task, as shown in Tab.5.\n\nCifar100\n\nCaltech101\n\nDTD\n\nFlower102\n\nPets\n\nSVHN\n\nSun397\n\nCamelyon\n\nEuroSAT\n\nResisc45\n\nRetinopathy\n\nClevr-Count\n\nClevr-Dist\n\nDMLab\n\nKITTI-Dist\n\ndSpr-Loc\n\ndSpr-Ori\n\nsNORB-Azim\n\nsNORB-Elev\n\nMean\n\nImagenet-21K - CIFAR100\n\n(Left) Bias. (Right) FreqFiT-Bias\n\n(Left) Adapter. (Right) FreqFiT-Adapter\n\n(Left) LoRA. (Right) FreqFiT-LoRA\n\n(Left) BOFT. (Right) FreqFiT-BOFT\n\n(Left) VeRA. (Right) FreqFiT-VeRA\n\n(Left) FourierFT. (Right) FreqFiT-FourierFT\n\nImageNet / FreqFiT-LoRA / Cifar100(a) Mean of filters from 12 layers.\n\nLeft to right, Upper row: layer 1-6. Lower row: layer 7-12\n\n(b) First 252 filters from the 1st layer\n\nImageNet / FreqFiT-FourierFT / Cifar100(a) Mean of filters from 12 layers.\n\nLeft to right, Upper row: layer 1-6. Lower row: layer 7-12\n\n(b) First 252 filters from the 1st layer\n\nMAE / FreqFiT-Adapter / Caltech101(a) Mean of filters from 12 layers.\n\nLeft to right, Upper row: layer 1-6. Lower row: layer 7-12\n\n(b) First 252 filters from the last layer\n\nMAE / FreqFiT-Bias / Caltech101(a) Mean of filters from 12 layers.\n\nLeft to right, Upper row: layer 1-6. Lower row: layer 7-12\n\n(b) First 252 filters from the last layer.\n\nMoCo / FreqFiT-Adapter / CIFAR100(a) Mean of filters from 12 layers.\n\nLeft to right, Upper row: layer 1-6. Lower row: layer 7-12\n\n(b) First 252 filters from the last layer\n\nMoCo / FreqFiT-Bias / CIFAR100(a) Mean of filters from 12 layers.\n\nLeft to right, Upper row: layer 1-6. Lower row: layer 7-12\n\n(b) First 252 filters from the last layer.\n\nCLIP / FreqFiT-VPT / Flower102(a) Mean of filters from 12 layers.\n\nLeft to right, Upper row: layer 1-6. Lower row: layer 7-12\n\n(b) First 252 filters from the last layer.\n\nCLIP / FreqFiT-Bias / Flower102(a) Mean of filters from 12 layers.\n\nLeft to right, Upper row: layer 1-6. Lower row: layer 7-12\n\n(b) First 252 filters from the last layer.", "text_file": "data\\paper_texts\\2411.19297v1_content.txt"}, {"title": "QuantAttack: Exploiting Dynamic Quantization to Attack Vision\n  Transformers", "authors": ["Amit Baras", "Alon Zolfi", "Yuval Elovici", "Asaf Shabtai"], "published_date": "2023-12-03T18:31:19Z", "summary": "In recent years, there has been a significant trend in deep neural networks\n(DNNs), particularly transformer-based models, of developing ever-larger and\nmore capable models. While they demonstrate state-of-the-art performance, their\ngrowing scale requires increased computational resources (e.g., GPUs with\ngreater memory capacity). To address this problem, quantization techniques\n(i.e., low-bit-precision representation and matrix multiplication) have been\nproposed. Most quantization techniques employ a static strategy in which the\nmodel parameters are quantized, either during training or inference, without\nconsidering the test-time sample. In contrast, dynamic quantization techniques,\nwhich have become increasingly popular, adapt during inference based on the\ninput provided, while maintaining full-precision performance. However, their\ndynamic behavior and average-case performance assumption makes them vulnerable\nto a novel threat vector -- adversarial attacks that target the model's\nefficiency and availability. In this paper, we present QuantAttack, a novel\nattack that targets the availability of quantized models, slowing down the\ninference, and increasing memory usage and energy consumption. We show that\ncarefully crafted adversarial examples, which are designed to exhaust the\nresources of the operating system, can trigger worst-case performance. In our\nexperiments, we demonstrate the effectiveness of our attack on vision\ntransformers on a wide range of tasks, both uni-modal and multi-modal. We also\nexamine the effect of different attack variants (e.g., a universal\nperturbation) and the transferability between different models.", "arxiv_id": "2312.02220v2", "html_link": "https://arxiv.org/html/2312.02220v2", "search_term": "ti:\"vision transformers\"", "html_content": "SECTION: QuantAttack: Exploiting Quantization Techniquesto Attack Vision Transformers\n\nIn recent years, there has been a significant trend in deep neural networks (DNNs), particularly transformer-based models, of developing ever-larger and more capable models.\nWhile they demonstrate state-of-the-art performance, their growing scale requires increased computational resources (e.g., GPUs with greater memory capacity).\nTo address this problem, quantization techniques (i.e., low-bit-precision representation and matrix multiplication) have been proposed.\nMost quantization techniques employ a static strategy in which the model parameters are quantized, either during training or inference, without considering the test-time sample.\nIn contrast, dynamic quantization techniques, which have become increasingly popular, adapt during inference based on the input provided, while maintaining full-precision performance.\nHowever, their dynamic behavior and average-case performance assumption makes them vulnerable to a novel threat vector \u2013 adversarial attacks that target the model\u2019s efficiency and availability.\nIn this paper, we presentQuantAttack, a novel attack that targets the availability of quantized vision transformers, slowing down the inference, and increasing memory usage and energy consumption.\nThe source code is available online111https://github.com/barasamit/QuantAttack.\n\nSECTION: 1Introduction\n\nIn recent years, deep neural networks (DNNs), particularly transformers[21], have made tremendous progress in various domains, such as NLP[18]and computer vision[19].\nTheir success stems mainly from their continually expanding network size, which is based on the number of parameters they contain and the precision of the parameters (the number of bits with which each parameter is represented).\nHowever, the high computational cost of their transformer blocks makes them unsuitable for edge devices.\n\nOne way of reducing the size of the parameters is to quantize them to fewer bits and use low-bit-precision matrix multiplication.\nThere are two types of quantization techniques:\n(a) quantization-aware training[50,10,2]- training a model with quantized parameters; and\n(b) post-training quantization[29,11,40]- quantizing parameters of a pretrained model, which can be done in a static or dynamic (sample-dependent) manner.\nRecently, LLM.int8()[8], a dynamic post-training quantization technique, was proposed and integrated into Hugging Face[42], one of the largest open-source machine learning (ML) platforms.\nThis technique decomposes the features and weights into sub-matrices of large magnitude features (outliers) and other values.\nThe outlier feature matrices are multiplied in higher precision, while all other values are multiplied in lower precision, reducing inference time, memory usage, and energy consumption, without any degradation in performance.\nWhile LLM.int8() was originally proposed for the NLP domain, the application of quantization techniques extends to vision models, aiming to balance efficiency and performance while preserving crucial visual information for tasks like image classification and object detection.\nDespite the fact that quantization is effective in making DNNs more resource-efficient, it also opens up a new attack surface for adversaries aiming to compromise model availability.\n\nGiven the potential impact of availability-oriented attacks, the ML research community has begun to direct its attention to adversarial attacks that target model availability.\nShumailovet al.[35]were the first to presentsponge examples, a technique that exploits data sparsity, causing the number of GPU operations to increase; this leads to increased inference time and energy consumption.\nEmploying this attack vector, Cinaet al.[5]poisoned models during the training phase to cause delays in the testing phase.\nDynamic neural networks[14], which adapt their structures or parameters to the input during inference, have also been shown to be susceptible to adversarial attacks.\nFor example, adversarial attacks against models that employ early-exit strategies (a type of a dynamic network) have been studied extensively[16,31].\nResearch focusing on the post-processing phase of DNNs, particularly in object detection[34]and LiDAR detection[22], has shown that they are also susceptible to availability-oriented attacks.\n\nIn this paper, we introduceQuantAttack, a novel adversarial attack that specifically targets the quantization process and exploits its test-time dynamism in vision transformers.\nWe argue that this attack vector poses a significant threat to the availability of transformer models, as it operates at the core level and is broadly generalizable to any type of network.\nSuch attacks could have far-reaching implications, particularly in resource-constrained environments, where maintaining low latency and computational efficiency is paramount.\nFor instance, in cloud-based IoT systems like surveillance cameras, this attack can cause delays in anomaly detection, compromising security.\nSimilarly, in real-time edge applications such as autonomous vehicles, the attack can lead to navigation errors or delayed decision-making, posing significant safety risks.\nAbove all, this paper aims to highlight the potential risk that dynamically quantized transformer-based models comprise.\nTo perform our attack, we propose overloading the matrix multiplication operation with high-bit precision (i.e., outlier values) to trigger worst-case performance.\nTo increase the stealthiness of our adversarial examples in cases where anomaly detection mechanisms are applied (e.g., monitoring shifts in the predicted distribution), we design our attack to preserve the model\u2019s original classification.\nTo assess the proposed attack\u2019s effectiveness, in our evaluation, we perform experiments addressing:(i)modality - investigating uni-modal and multi-modal;(ii)model task - considering various computer vision applications;(iii)attack variations - single-image, class-universal, and universal;(iv)transferability - examining whether the perturbations are transferable between different models.\nOur experiments on the ViT model[9]show that the proposed attack can increase the model\u2019s use of GPU memory by 17.2%, extend the GPU processing time by 9%, and expand energy use by 7%.\nFinally, we demonstrate that quantized models, both static and dynamic, are susceptible to integrity-based attacks.\n\nOur contributions can be summarized as follows:\n\nTo the best of our knowledge, we are the first to identify dynamic quantization as a novel threat vector and propose an attack exploiting the availability of quantized models.\n\nWe design a stealthy attack that preserves the model\u2019s original classification.\n\nWe conduct a comprehensive evaluation on various configurations, examining different modalities and tasks, reusable perturbations, transferability, and ensembles.\n\nWe shed light on the vulnerabilities and provide key insights into the security implications associated with the transformer architecture.\n\nWe present various countermeasures that can be employed to mitigate the threat posed by our attack.\n\nSECTION: 2Related Work\n\nSECTION: 2.1Quantization\n\nDNNs, particularly transformers, have achieved great success in various ML tasks[19,43].\nHowever, their computational complexity and large model size pose challenges for real-time applications and resource-limited devices.\nThe size of a model is determined by the number of parameters and their precision.\nThe precision relates to the number of bits each weight value is stored with, typically 16 bits (also referred to as float16 orf16) or 8 bits (also referred to as int8 ori8).\nTo mitigate these challenges, techniques like quantization are employed.\n\nQuantization is used to reduce the computational time and memory consumption of neural networks.\nBy quantizing the weights and activations into low-bit integers (e.g., a 16-bit float to an 8-bit integer), GPU memory usage can be reduced and inference can be accelerated, due to low-bit-precision matrix multiplication.\nTwo main quantization approaches have been proposed:\n\nQuantization-Aware Training (QAT)[50,10,2]: This method involves training the model with quantized weights and activations.\nQAT maintains good performance, even when using low-precision formats.\n\nPost-Training Quantization (PTQ): This approach takes a pretrained model and quantizes it directly, eliminating the need for extensive retraining, making it generally less computationally intensive compared to QAT.\nPTQ can be categorized into two main categories:\n\nStatic quantization[29,11,40,44,3,46,23,47]: The weights are quantized to lower precision only once using calibration sets, after the model is trained.\n\nDynamic quantization[8]: The weights and activations are quantized during runtime based on specific rules.\n\nIn this paper, we focus on PTQ techniques, highlighting that dynamic techniques possess an availability-based vulnerability due to their inherent dynamism, which uniquely impacts their security.\nAdditionally, we demonstrate that both static and dynamic techniques are susceptible to integrity-based attacks.\n\nSECTION: 2.2Availability Attacks\n\nConfidentiality, integrity, and availability, also known as the CIA triad, are a model that that typically serves as the basis for the development of security systems[33].\nIn the context of DNNs, adversarial attacks targeting integrity[37,13,28,27,36,45,52,51]and confidentiality[1,17]have received a great deal of research attention over the last few years.\nHowever, adversarial attacks that target the availability of these models have only recently gained the attention of the ML research community.\nShumailovet al.[35]were the pioneers in this area, introducing the sponge examples attack, a technique that primarily targets the efficiency of vision and NLP models.\nThe authors propose to exploit:(i)computation dimensions - expanding the internal representation size of inputs/outputs; and(ii)data sparsity - forcing non-zero activations against the zero-skipping multiplications acceleration technique.\nBoth attacks lead to increased energy consumption and inference time.\nEmploying the data sparsity attack vector, Cinaet al.[6]proposed sponge poisoning, a method aimed at degrading models\u2019 throughput by subjecting them to a sponge attack during the training phase.\nAnother noteworthy extension of the sponge examples (computation\ndimension vulnerability) was presented by Boucher[4], who introduced an adversarial attack on NLP models using invisible characters and homoglyphs, which, while undetectable to humans, can significantly affect the model\u2019s throughput.\n\nDynamic neural networks[14], which optimize computational efficiency by adapting to input data during runtime, have also been shown to be vulnerable to adversarial attacks.\nHaqueet al.[15]attacked DNNs that employ an layer-skipping mechanism by generating adversarial examples that go through all the layers.\nLater, in a similar way, Honget al.[16]proposed an attack against early-exit mechanisms, generating malicious that bypass all early exits.\nPanet al.[31]proposed a unified formulation to construct adversarial samples to attack both the dynamic depth and width networks.\n\nAnother avenue of research, in which the post-processing phase of DNNs is targeted, has also been investigated.\nShapiraet al.[34]showed that overloading object detection models by increasing the total number of candidates input into the non-maximum suppression (NMS) component can lead to increased execution times.\nLiu[22]extended this to LiDAR detection models.\n\nIn this paper, we propose a novel attack vector that has not been studied before \u2013 an attack that targets the availability of dynamically quantized models.\n\nSECTION: 3Background\n\nSECTION: 3.1Dynamic PTQ\n\nWe focus on the one of the most popular PTQ techniques, LLM.int8()[8].\nWe consider a quantized modelthat receives an inputand outputsreal-valued numbers that represent the model\u2019s confidence for each class, and containsquantized layers.\nDuring inference, for every quantized layer, given the layer inputand weightswith sequence dimension, feature dimension, and output dimension, the steps for efficient matrix multiplication are:\n\nOutlier Extraction: From the input, extract all column indices that contain at least one outlier (i.e., absolute values that are larger than a certain threshold) into the set.\n\nMixed-Precision Multiplication: The matrix multiplication process is divided into two segments.\nOutliers are multiplied using the standard matrix multiplication in float16, while non-outliers are first quantized to their 8-bit representation and then multiplied in int8.\nThis involves row-wise quantization for the input and column-wise quantization for the weight matrix.\n\nDequantization and Aggregation: The non-outlier results are dequantized back to float16 and combined with the outlier results to form the final output.\n\nMore formally, the matrix multiplication can be described as:\n\nwhererepresents the output tensor in float16,represent the float16 input and weight for outliers,is the denormalization term for int8 inputs and weights, andrepresent the int8 input and weight for non-outliers.\nAdditional details on the quantization process can be found in the supplementary material.\n\nSECTION: 3.2Vision Transformers\n\nVision transformers are usually comprised oftransformer blocks, each of which containing a multi-head self-attention (MSA) and a feedforward network (FFN).\nGiven an input sequence representation, The-th block outputis calculated as follows:\n\nwheredenotes layer normalization such that:\n\nwith learnable parametersand.\nSpecifically, two different normalization layers are used to in the transformer block differing in theandvalues.\n\nA single-head attention is formulated as follows:\n\nwhere Q, K, and V are the query, key and value matrices, respectively, andis scaling factor.\nConsequently, the MSA is computed as:\n\nwhereare the parameter matrices of the-th attention head in the-th transformer block.\nFinally, as shown in Equation\u00a0(2), the output of the MSA is then processed by the FFN, a two-layer MLP.\n\nIn the context of LLM.int8(), which quantizes linear layers, we also discuss their location in the model.\nIn the standard transformer block (presented above), the MSA and the FFN consist of four and two linear layers, respectively, for a total of six linear layers.\nIn the MSA, three linear layers (with parameters) are used to derive the parameters matrices for the query, key and value; the last linear layer (with parameter) is used to combine the different attention heads.\nIn the FFN, a two-layer MLP is used to produce the final block\u2019s output.\nNotably, the inputs fed into the first three layers in the MSA () and the first layer in the FFN () are directly affected by the normalization layersand, an important aspect we will further discuss in Section5.2.\n\nSECTION: 4Method\n\nSECTION: 4.1Threat Model\n\nAdversary\u2019s Goals.We consider an adversary whose primary goal is to generate an adversarial perturbationthat triggers the worst-case performance of dynamic quantization techniques,i.e., increases the number of high-bit operations.\nAlong with our primary goal, to increase the stealthiness of the attack the adversary aims to maintain the original classification.\n\nAdversary\u2019s Knowledge.To assess the security vulnerability of dynamic quantization to adversarial attacks, we consider three scenarios:(i)a white-box scenario: the attacker has full knowledge about the victim model;(ii)a grey-box scenario: the attacker has partial knowledge about the set of potential models; and(iii)a black-box scenario: the attacker crafts a perturbation on a surrogate model and applies it to a different victim model.\n\nAttack Variants.Given a datasetthat contains multiple pairswhereis a sample andis the label, we consider three variants:(i)single - a different perturbationis crafted for each;(ii)class-universal - a single perturbationis crafted for a target class; and(iii)universal - a single perturbationis crafted for all.\n\nSECTION: 4.2The Quant Attack\n\nTo achieve the goals presented above, we modify the PGD attack[24]with a novel loss function[34,16].\nThe update of the perturbationin iterationis formulated as follows:\n\nwhereis the step size,is the projection operator that enforcesfor some norm, andis the loss function.\nThe selection ofdepends on the attack variant:(i)for the single-image variant,;(ii)for the class-universal variant with a target class,; and(iii)for the universal variant,.\nNext, we describe the proposed custom loss function, which consists of two components:\n\nwhereis empirically determined using the grid search approach.\nThe two components are described below.\n\nQuantization Loss.This component aims to achieve our main goal, increasing the number of multiplications in 16-bit precision.\nThe number of multiplications in a higher precision level depends on the existence of an outlier value in each column in the input hidden state matrix.\nTherefore, practically, we aim to produce at least one \u201csynthetic\" outlier value in each column in this matrix.\n\nFormally, letdenote the input of thequantized layer (we omit the bit-precision notation for simplicity).\nFor each input matrix, we extract the top-values of each column, denoted as, with the aim of pushing these values towards a target value, such that.\nThe loss for a single layer is defined as follows:\n\nNote that we only use values below the threshold to ensure that existing outlier values are not penalized by the loss function.\n\nFinally, the loss for alllayers is defined as:\n\nClassification Loss.To increase the stealthiness of our attack, we aim to preserve the original classification of the input image.\nTherefore, we include the classification loss component, which is defined as follows:\n\nwheredenotes the score for class.\n\nSECTION: 5Evaluation\n\nSECTION: 5.1Evaluation Setup\n\nModels.We evaluated our attack on two state-of-the-art vision transformers:\n\nVision Transformer (ViT)[9]:We use thebasesize version, pretrained on ImageNet-21K, at a resolution of 224x224.\nThe model is then finetuned on ImageNet-1K .\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16).\n\nData-efficient image Transformer (DeiT)[38]:We use thebasesize version, pretrained and finetuned on ImageNet-1K, at a resolution of 224x224.\nImages are presented to the model as a sequence of fixed-size patches (resolution 16x16).\n\nIn the supplementary material, we show that the accuracy results for the quantized models are on par with those of the non-quantized ones.\n\nDatasets.In our evaluation, we use the ImageNet dataset[7], and specifically, the images from its validation set, which were not used to train the models described above.\nFor the single-image attack variant, we trained and tested our attack on 500 random images from various class categories.\nFor the class-universal variant, we selected 10 random classes, and for each class we trained the perturbation on 250 images (i.e.,) and tested them on a distinct set of 500 images from the same class.\nSimilarly, for the universal variant, we followed the same training and testing procedure, however from different class categories.\n\nMetrics.To evaluate the effectiveness of our attack, we examine the number of outlier multiplications, different hardware metrics, and the effect of the attack on the model\u2019s original task performance:\n\nGPU Memory Consumption: how much GPU memory the process uses.\n\nGPU Throughput: how long the GPU takes to perform calculations.\n\nEnergy Consumption: the total energy usage of the GPU, with measurements obtained using the NVIDIA Management Library (NVML).\n\nNumber of Outliers: represents the number of matrix multiplications done in 16-bit precision (see Section3).\n\nAccuracy: the performance of the model on its original task.\nWe consider the model\u2019s prediction on the original images as the ground-truth label.\n\nImplementation Details.In our attack, we focus onnorm bounded perturbations, and set, a value commonly used in prior studies[25,30,41,49].\nThe results for othervalues can be found in the supplementary material.\nWe use a cosine annealing strategy with warm restarts for the attack\u2019s step size, where the maximum and minimum values areand, respectively.\nFor the dynamic quantization threshold, we set, as suggested in the original paper[8].\nWe set the target value of our attackto 70, the number of extracted values from each columnto 4, and the weighting factorto 50 as they empirically yielded the best results.\nThe results of the ablation studies we performed on the,andvalues can be found in the supplementary material.\nThe experiments are conducted on a GeForce RTX 3090 GPU.\nolgu\n\nWe compare our attack with four baselines: (a) clean - the original image with no perturbation, random - a randomly sampled perturbation from the uniform distribution, Sponge Examples[35]- an attack aimed at increasing the amount of non-zero activations, and standard PGD[24]- the original PGD attack with the model\u2019s loss function (integrity-based attack).\nTable1presents the performance of the different perturbations on the ViT and DeiT models.\nThe analysis reveals that single-image perturbations substantially increase the GPU memory usage and processing time for both models compared to the baselines.\nSpecifically, for the ViT model, the single-image perturbations cause 1681% more outliers than a clean image.\nIn terms of hardware metrics, they result in a 17.2% increase in GPU memory usage, a 7% increase in energy consumption, and an 9% increase in GPU processing time, compared to clean images.\nThe sponge examples and the standard PGD however, do not incur any substantial effect both in terms of outliers and hardware metrics.\nInterestingly, in most cases, the random perturbations lead to a degradation in performance compared to the clean images.\nWe hypothesize that random perturbations simply eliminate \u201cnatural\" outlier features (i.e., those that exist in clean images) due to the random noise added to the images.\n\nWe also investigate the effect of reusable perturbations, in which class-universal and universal perturbations (Section4.1) are trained on one set of images and tested on a distinct holdout set.\nThe results presented in Table1enable comparison of the perturbations\u2019 impact based on various metrics.\nOn the DeiT model, when compared to clean images, a universal perturbation results in a 12.4% increase in GPU memory usage, 2.6% increase in energy consumption, and 3.3% increase in GPU processing time.\nClass-specific perturbations cause an 12.8% increase in GPU memory, 4.4% increase in energy consumption, and 3.5% increase in GPU time, performing slightly better than the universal perturbation.\nNote that in this case, we usedas it is a more complex setting compared to the single variant.\n\nAlthough universal and class-specific perturbations are less resource-exhaustive than single-image perturbations, they offer a distinct advantage.\nA universal perturbation vector is capable of affecting multiple images or an entire class of images, thereby providing an efficient mechanism for broad-spectrum adversarial attacks.\nThis efficiency is especially advantageous in scenarios where the attacker aims to disrupt the model across multiple data points with minimal computational effort.\n\nWhen examining the effect of these perturbations, we observed an interesting phenomenon: relaxing the requirement for imperceptibility (i.e., the noise boundis set at a high value) causes the perturbation to completely distort the visual appearance of the input image, visually resembling sheer noise.\nThis, in turn, creates a resource-intensive scenario which could be interpreted as a denial-of-service (DoS) attack, increasing GPU memory usage by 70%, energy consumption by 12%, and GPU time by 25%.\n\nIn the LLM.int8()[8]technique, when the quantized model processes a batch ofimages, every quantized layer transforms the given 3D input matrixto a stacked 2D version, resulting inmore rows.\nThe transformation is followed by quantized matrix multiplication (Section4).\nIn this case, when an outlier value exists in a column, the entire column is processed in f16 precision, including values in rows that belong to a completely different image.\n\nIn a realistic scenario where a quantized model is deployed and serves as ML-as-a-service, input images from different sources can be stacked together and given to the model as a batch of images.\nIn this case, an adversary could potentially implant a perturbed image that will affect the resource consumption of the whole batch.\n\nTherefore, we evaluate the impact of such a scenario in which we feed the quantized model different sized batches that include asingleperturbed image.\nFigure1presents the results for the different batch sizes; the values represent the percentage difference between a benign batch and its attacked counterpart.\nThe results show that a single perturbed image implanted in a batch of images could potentially affect the performance of the entire batch, as an increase in resource consumption is seen for all of the batch sizes examined.\nNotably, smaller size batches (e.g., two images) are more sensitive than larger size batches (e.g., 16 images).\nFor example, for a two-image batch, the memory usage increases by 11.7% compared to the benign batch, while for a 16-image batch, the memory only increases by 3.5%.\nA plausible explanation for this can be found in the initial state of outliers in the batches - with a large batch size, natural outliers from different images are likely to spread across multiple columns.\nConsequently, the attacked image could contain synthetic outliers in the same columns, which are already processed in f16, and this leads to a smaller percentage difference.\n\nIn adversarial attacks, transferability refers to the ability of an adversarial example, crafted on a surrogate model, to affect other models.\nIn our experiments, we examine the effect of perturbations trained on ViT and tested on DeiT and vice versa.\nAs shown in Table2, adversarial examples trained on one model show limited impact on the other.\nInterestingly, we observed an unexpected anomaly in the GPU time when perturbations were trained on DeiT and tested on ViT, in which a negative value was recorded (i.e., the GPU time decreased).\nWe hypothesize that this occurred due to the marginal effect of just 10% more outliers.\nSuch anomalies emphasize the nuanced relationship between outliers and resource metrics, hinting at the complex performance environment within which these models operate.\nDespite this minor deviation, the general trend remains consistent: a higher number of outliers usually requires more resources.\n\nTo improve the transferability between different models, we employ an ensemble training strategy, in which the adversarial example is trained on both models simultaneously, such that in each training iteration one of the models is randomly selected.\nThis approach aims to examine the collaborative benefit of utilizing the strengths of both models to create generalizable adversarial perturbations.\nBased on the results presented in Table2, we can see that the ensemble strategy is able to affect both models, and results in an increase in GPU memory usage and GPU time, although with sightly less efficiency compared to perturbations trained and tested on the same model.\nFor example, when the ensemble-based perturbations are tested on the DeiT model, GPU memory usage and GPU time increased by 21.3% and 7.5%, respectively.\n\nTo emphasize the potential impact and generalizability of our attack, we also experiment with other models that have different characteristics: different computer vision tasks, and different modalities.\nIn addition, we also broaden our analysis from the computer vision domain to include the audio domain.\nParticularly, we experiment with the following models:\n(a) Open-World Localization (OWLv2)[26]- a zero-shot text-conditioned object detection model;\n(b) You Only Look at One Sequence (YOLOS)[12]- a transformer-based object detection model;\n(c) Generative Image-to-text Transformer (GIT)[39]- a decoder-only transformer for vision-language tasks; and\n(d) Whisper[32]- a sequence-to-sequence model for automatic speech recognition and speech translation.\nIt should be noted that in our attack against those models, we only use the single-image attack variant with the quantization loss component (i.e.,) for simplicity.\nWe also note that the same attack configuration used for the image classification models (Section5.1) is used for all the evaluated models below, which might lead to suboptimal results.\nFurther experimental details on these models can be found in supplementary material.\n\nFrom Table4we can see that transformer-based models in general are vulnerable to our attack, not limited to the image classification domain.\nFor example, on the multi-modal OWLv2, the energy consumption increased by 4.9% and on the YOLOS, the memory usage increased by 9.3%.\nBeyond the computer vision domain, our attack also successfully affects the Whisper model.\n\nBeyond the scope of availability-based attacks, we also explore the effect of generating outlier values on the models\u2019 prediction capabilities (i.e., use Equation7with).\nWe evaluate the effectiveness of our attack on both static and dynamic PTQ techniques.\nWe argue that dynamic techniques heavily rely on outlier values for successful classification, while static techniques depend on low-scale calibration sets that do not account for extreme cases (e.g., artificially crafted outlier values).\n\nWe evaluate the models\u2019 performance under various quantization techniques.\nSpecifically, in addition to LLM.int8(), we also evaluate the static quantization methods PTQ4ViT[48]and Repq-vit[20].\nAs shown in Table3, our attack successfully degrades model\u2019s accuracy, regardless of the quantization technique.\nInterestingly, the static techniques are more vulnerable, as they are not calibrated to handle extreme values.\nOn the other hand, the dynamic technique is relatively more robust to outlier values due to its increased bit-precision mechanism.\nThis highlights the trade-off between robustness to availability attacks and maintaining model performance.\nWe include transferability and ensemble results in the supplementary material.\n\nSECTION: 5.2Discussion and Insights\n\nIn Section3we discussed the presence of the normalization layers in the transformer block, and which linear layers\u2019 inputs are directly affected by them.\nIn the context of our attack, which aims to increase the values in these inputs, the normalization applied to the inputs just before the mixed-precision matrix multiplication has both negative and positive effects on our attack\u2019s performance.\nGiven an input matrix, the output of the normalization layer (Equation\u00a0(3)) is affected by the input mean, variance, and the learnable parametersand.\n\nIn Figure2a we show the percentage of outliers in the transformer blocks\u2019 six linear layers (explained in Section3).\nAs can be seen, our attack only affects the number of outliers in blocks 10, 11, and 12.\nParticularly, the outliers occur in the 4-th layer of the MSA (i.e., the linear layer that combines the different attention heads) and in both linear layers of the FFN.\nWe hypothesize that the occurrence of outliers in the MSA 4-th linear layer and in the FFN last linear layer can be attributed to the fact that their inputs are not directly affected by the normalization layers,i.e., additional computations are done between the normalization layers and these layers, allowing our attack to craft synthetic outliers.\nIn contrast, the inputs to the first three linear layers in the MSA and the first linear layer in the FFN are first processed by the normalization layers.\nInterestingly, our attack is not able to affect these MSA layers while successfully generating outliers in the FFN layer.\nThis phenomena can be explained by examining thelearnable parameter value in each transformer block (theoretically thevalues should also affect the output\u2019s magnitude; however, in practice the values are very low and only marginally affect the magnitude).\nAs presented in Figure2b, the FFN normalization layer\u2019svalue in the first nine blocks is lower than one, scaling down the input values\u2019 magnitudes.\nHowever, in the last three blocks thevalue is substantially higher, either maintaining or up scaling the input values magnitudes, which contributes to our attack\u2019s success.\nAs opposed to this gradually increasing value pattern, the MSA normalization layer\u2019svalue remains low across all blocks, preventing our attack from generating any synthetic outliers that surpass the threshold.\nTherefore, we can see a direct correlation between our attack\u2019s success and the normalization layer values.\n\nFurthermore, since the normalization layer\u2019s output is also affected by the input\u2019s meanand variance, we also tried to attack these values,i.e., decreasing the entire input\u2019s mean and variance.\nHowever, it did not improve the performance of our attack.\nThis arises from the fact that the quantization loss component (Equation\u00a0(9)) implicitly induces the same effect (we include an analysis of this phenomena in the supplementary material).\nThus, we conclude that the most influential parameter to our attack\u2019s performance is the normalization layer\u2019svalue.\n\nSECTION: 6Countermeasures\n\nIn response to the challenges posed by QuantAttack, in this section, we propose two practical steps that can be performed to enhance the security of the quantization process:Limiting the use of high-precision multiplications\u2013 implement rules that limit when and how often the model uses high-precision calculations.\nAn implementation of this approach (further discussed in the supplementary material) has shown that the 1681% increase in the outliers can be reduced to a 251% increase, without compromising model accuracy on the clean images;\nThis approach, however, requires a pre-defined threshold which depends on the specific model and system requirements.Increasing the batch size\u2013 based on our observations (Figure1), increasing the batch size reduces QuantAttack\u2019s impact due to a higher occurrence of natural outliers.\nIn this case, the synthetic outliers that our attack produces might blend with the natural outliers (i.e., the number of high-precision multiplications will not increase linearly).\nNonetheless, the quantization\u2019s efficiency decreases as the batch size grows.\nAbove all, both approaches have trade-offs between performance and security, demonstrating that there is no perfect solution that will completely eliminate the threat posed by our attack.\n\nSECTION: 7Conclusion\n\nIn this paper, we presented QuantAttack, a novel adversarial attack that both exploits the vulnerabilities and highlights the risk of using vision transformers with dynamic quantization.\nWe showed that quantized models, while benefiting from the quantization\u2019s efficiency, are susceptible to availability-oriented adversarial techniques that can degrade their performance.\nOur comprehensive evaluation demonstrated the impact of our attack on a wide range of vision transformers in various attack configurations.\nThe implications of these findings are significant, indicating the pressing need to develop more robust quantization methods that can withstand such adversarial challenges.\nIn future work, we plan to extend our attack to the NLP domain and evaluate its impact on popular large language models (LLMs).\n\nSECTION: References", "text_file": "data\\paper_texts\\2312.02220v2_content.txt"}, {"title": "MVFormer: Diversifying Feature Normalization and Token Mixing for\n  Efficient Vision Transformers", "authors": ["Jongseong Bae", "Susang Kim", "Minsu Cho", "Ha Young Kim"], "published_date": "2024-11-28T08:49:11Z", "summary": "Active research is currently underway to enhance the efficiency of vision\ntransformers (ViTs). Most studies have focused solely on effective token\nmixers, overlooking the potential relationship with normalization. To boost\ndiverse feature learning, we propose two components: a normalization module\ncalled multi-view normalization (MVN) and a token mixer called multi-view token\nmixer (MVTM). The MVN integrates three differently normalized features via\nbatch, layer, and instance normalization using a learnable weighted sum. Each\nnormalization method outputs a different distribution, generating distinct\nfeatures. Thus, the MVN is expected to offer diverse pattern information to the\ntoken mixer, resulting in beneficial synergy. The MVTM is a convolution-based\nmultiscale token mixer with local, intermediate, and global filters, and it\nincorporates stage specificity by configuring various receptive fields for the\ntoken mixer at each stage, efficiently capturing ranges of visual patterns. We\npropose a novel ViT model, multi-vision transformer (MVFormer), adopting the\nMVN and MVTM in the MetaFormer block, the generalized ViT scheme. Our MVFormer\noutperforms state-of-the-art convolution-based ViTs on image classification,\nobject detection, and instance and semantic segmentation with the same or lower\nparameters and MACs. Particularly, MVFormer variants, MVFormer-T, S, and B\nachieve 83.4%, 84.3%, and 84.6% top-1 accuracy, respectively, on ImageNet-1K\nbenchmark.", "arxiv_id": "2411.18995v1", "html_link": "https://arxiv.org/html/2411.18995v1", "search_term": "ti:\"vision transformers\"", "html_content": "SECTION: MVFormer: Diversifying Feature Normalization and Token Mixingfor Efficient Vision Transformers\n\nActive research is currently underway to enhance the efficiency of vision transformers (ViTs).\nMost studies have focused solely on effective token mixers, overlooking the potential relationship with normalization.\nTo boost diverse feature learning, we propose two components: a normalization module called multi-view normalization (MVN) and a token mixer called multi-view token mixer (MVTM).\nThe MVN integrates three differently normalized features via batch, layer, and instance normalization using a learnable weighted sum.\nEach normalization method outputs a different distribution, generating distinct features.\nThus, the MVN is expected to offer diverse pattern information to the token mixer, resulting in beneficial synergy.\nThe MVTM is a convolution-based multiscale token mixer with local, intermediate, and global filters, and it incorporates stage specificity by configuring various receptive fields for the token mixer at each stage, efficiently capturing ranges of visual patterns.\nWe propose a novel ViT model, multi-vision transformer (MVFormer), adopting the MVN and MVTM in the MetaFormer block, the generalized ViT scheme.\nOur MVFormer outperforms state-of-the-art convolution-based ViTs on image classification,\nobject detection, and instance and semantic segmentation with the same or lower parameters and MACs.\nParticularly, MVFormer variants, MVFormer-T, S, and B achieve 83.4%, 84.3%, and 84.6% top-1 accuracy, respectively, on ImageNet-1K benchmark.\n\nSECTION: 1Introduction\n\nVision transformers (ViTs) have achieved great success in the computer vision field[9].\n\nSince self-attention in traditional transformers[46]has been in the spotlight, numerous studies have proposed various effective and efficient spatial mixing methods, referred to as token mixers, to improve or substitute self-attention.\nSome studies[4,10,54,22,50,19]have proposed attention-variant methods, such as Swin[26], to enhance the efficiency of traditional self-attention,\nwhereas others[41,55,47,3]have proposed competitive non-attention token mixers.\nAmong the currently available options, the convolutional operator has recently been applied in the transformer block.\nFor example, the ConvNeXt[27]model is a milestone connecting convolution with ViT that modernizes the convolutional neural network (CNN)\nby introducing a transformer variant scheme.\nRecent studies have demonstrated the benefit of appropriate inductive bias in ViT[29,34], which has emerged as an ongoing research topic[55,7,44,11,60].\n\nMetaFormer[58,59]is an abstracted architecture scheme derived from the transformer, in which the token mixer is not specified.\nWhile token mixers have been the primary focus to ensure feature diversity, the other components in recent ViTs have generally been based on MetaFormer[48].\nAmong these components, we concentrate on normalization.\nBatch normalization (BN)[18], layer normalization (LN)[1], and instance normalization (IN)[45]produce distinct distributions and different features due to their varying normalizing dimensions.\nInspired by this, we conduct a simple visualization to observe the changes that occur when the differently normalized images are integrated, as illustrated in Fig.1.\nEach method emphasizes specific patterns in the input image. All these patterns are also visible in the composite image, which is an average of the three normalized images.\nThrough this observation, we confirmed that integrating various normalizations can convey a diverse set of features with various distributions to the token mixer.\n\nIn this work, we introduce a normalization module, multi-view normalization (MVN), to diversify feature learning.\nThe MVN combines three differently normalized features via BN, LN, and IN, using a learnable weighted sum.\nIn this manner, MVN can flexibly reflect the diverse specificities, such as batch-level, channel-level, and sample-level dependencies, providing various feature distributions to the token mixer and enabling it to use them adaptively.\nThe experiments confirm that this simple mechanism significantly improves the performance with a negligible increase in parameters and computational costs.\nMoreover, MVN can be easily applied to existing ViTs and CNNs, such as Swin[26]and ConvNeXt[27], consistently improving their original performances.\nOn top of that, experimental results strongly support the insight that the unique attributes of each normalization play meaningful roles in performance, and their appropriate combination creates beneficial synergy.\n\nIn addition, to diversify the mixing range of token mixers further, we propose a convolutional token mixer, called the multi-view token mixer (MVTM).\nSimilar to the latest convolution-based ViTs[11,60], the MVTM is a multiscale depthwise convolutional operator that employs multiple mixing filters channelwise with different receptive fields.\nBeyond the existing paradigm of bisecting local and global mixing filters, the MVTM consists of local, intermediate, and global mixing filters to enhance its mixing capacity.\nFurthermore, the MVTM introduces stage specificity, which varies the volume of each level of mixing filter and global mixing filter size differently depending on the preferred range of the receptive field at each stage to make stage-level multiscale token mixing efficient[29,58,59].\n\nWe propose a novel convolution-based ViT, the multi-vision transformer (MVFormer), by adopting the MVN and MVTM in the MetaFormer block. The MVFormer addresses the existing demands of token mixers to capture diverse patterns from multiple perspectives, further extending it to normalization.\nBoosted by the enhanced capacity of the viewpoint, the MVFormer displays notable efficiency and effectiveness through extensive experiments.\nThe MVFormer outperforms other existing convolution-based ViTs\non image classification and downstream tasks, object detection, and instance and semantic segmentation,\nwith equal or even fewer parameters and multiply-accumulates (MACs).\nParticularly, MVFormer-tiny (-T), small (S), and base (B), variants of the MVFormer, achieve state-of-the-art (SOTA) performance of 83.4%, 84.3%, and 84.6%, respectively, on the ImageNet-1K benchmark[8].\nThe main contributions of this work are summarized as follows:\n\nWe propose MVN, which integrates various normalized features to diversify feature learning, providing a range of feature distributions to the token mixer. It is the first study of a normalization integration paradigm in ViTs.\nThe MVN significantly enhances performance with negligible increases in parameters and computational costs.\n\nWe introduce MVTM, a multiscale convolutional token mixer, to better capture diverse spatial patterns.\nThe MVTM also reflects stage specificity, setting the receptive field of the token mixer differently based on the preferred mixing scale at each stage, effectively exploiting the feature pyramid structure.\n\nBy adopting MVN and MVTM in the MetaFormer block, we present MVFormer, surpassing the existing convolution-based ViTs on image classification, object detection, and instance and semantic segmentation with the same or even fewer parameters and MACs.\n\nSECTION: 2Related Work\n\nSECTION: 2.1Normalization for Computer Vision Tasks\n\nNormalization methods have been investigated as a key component of deep neural networks to enhance the training speed and stability. Typically, BN[18]plays a pivotal role in CNNs for vision-related tasks.\nHowever, its minibatch dependency has been demonstrated to cause performance degradation with a small batch size on several vision tasks, such as semantic segmentation[1].\nTo improve this, several BN variants, such as batch renormalization[17], EvalNorm[39], MABN[53], and PowerNorm[37]have been proposed.\nThe LN[1]first emerged in natural language processing (NLP) to address the summed input in recurrent neural networks.\nCompared to BN, LN calculates the channel statistics equally on all data points.\nAs LN was adopted in the initial transformer, it has been employed in recent ViTs[9,43,26].\nGroup normalization (GN) is a generalized LN that calculates grouped channel statistics.\nA previous study proposed modified LN (MLN), which equates GN with a single group, to improve the performance of PoolFormer[58].\nAdditionally, a study has investigated the use of BN for ViTs[56],\nby inserting its parameters into the linear layer.\nIN[45]is widely used in style transfer, such as in AdaIN[16], which is a representative IN-variant to transplant the style information of input features.\nMoreover, spatially modulated normalization techniques, such as SPADE[30]and MoTo[33], were proposed to prevent information loss, and global response normalization (GRN)[52]aims to enhance the interchannel feature diversity.\nIn contrast to these studies, we propose an initial paradigm of combining the existing normalization approaches in ViTs.\n\nSECTION: 2.2Vision Transformer with Token Mixer\n\nThe success of the transformer in NLP led to its use in computer vision.\nPrevious studies have reported the impressive performance of\nViT[9]and DeiT[43]on image classification, and extended the application of the Swin transformer[26]to object detection and semantic segmentation. However, owing to the high computational cost of self-attention, studies have attempted to replace it with alternative token mixers.\nAccordingly, multilayer perceptron (MLP)-like token mixers[41,42,23,57,51]have emerged as a dominant approach that employs an MLP operator to mix spatial tokens.\nAs another mainstream approach, depthwise convolution has been studied as a token mixer. The ConvNeXt[27]model applies the modernized paradigm of the CNN, completely substituting traditional self-attention in the transformer with depthwise convolution.\nFurther, other studied models, such as FocalNet[55]and VAN[12], employ convolution-based attention mechanisms, enabling the model to capture input-dependent token interactions.\nThe ConvFormer[59]model is the current SOTA convolution-based ViT, introducing the inverted separable convolution in MobileNetV2[35]as the token mixer.\nRecently, multiscale convolutional token mixers[11,60]have been introduced that employ multiple mixing paths parallelly to reflect local and global information effectively.\nThis study applies an advanced multiscale depthwise convolution, employing an intermediate mixing filter and the concept of stage specificity.\n\nSECTION: 3Method\n\nSECTION: 3.1Preliminaries\n\nMetaFormer[58,59]is an abstracted general architecture of modern ViTs as presented in Fig.2(b).\nA batch of input images,, can be patchified into the patch embedding,, whereanddenote the batch size, height, and width of, respectively,andrepresent the embedding dimension ofand, respectively.\nIn addition,corresponds to the patch size.\nThe embedded featurepasses repeated MetaFormer blocks, and the output of each block,, is calculated as follows:\n\nThe MetaFormer block consists of a token mixer subblock () and an MLP subblock (). Each subblock includes normalization (and) and a residual connection.\nTheis not specified, corresponding to certain spatial mixing modules, such as self-attention or convolution, anddenotes the two-layer feed-forward network with an activation function.\nThis work follows the overall paradigm of MetaFormer, with ConvFormer[59]as the baseline.\n\nThe BN[18], LN[1], and IN[45]are commonly used in vision architectures.\nBoth BN and LN were proposed to accelerate model training, and IN was introduced for the image stylization method.\nAlthough these methods similarly normalize the feature distribution, their normalizing dimensions differ.\nGiven an input feature, the output of each method is calculated as follows:\n\nwhereandare the mean and standard deviation of, which is calculated based on the normalization dimensions indicated by the subscripts.\nThe BN normalizes the feature distribution channelwise, whereas LN operates at the pixel level.\nMoreover, IN is similar to BN, as it normalizes sample-level spatial distributions. A channelwise affine transform commonly follows each output.\n\nSECTION: 3.2Multi-Vision Transformer\n\nThis section, explains the details of the MVFormer.\nThe overview of MVFormer is presented in Fig.2(a).\n\nCommon normalization techniques, such as BN, LN, and IN, similarly normalize the input features; thus, they are considered substitutable options in a network. However, these techniques are distinguished by their normalizing dimensions, a crucial factor changing the output distribution.\nWe expect this distribution variation to influence the overall feature learning to extract visual patterns. From the perspective of feature diversity, the model can explore the extended manifold, where all varied distributions are provided.\nTherefore, we propose a novel normalization integration paradigm to enhance performance by training the ViT on a diverse range of feature distributions.\nWe designed a normalization module, MVN, which uses a learnable weighted summation of three normalized features obtained through BN, LN, and IN.\nThrough this mechanism, MVN allows the model to capture unique specificities of each normalized feature simultaneously, enabling it to pass on more diverse features to the token mixer.\n\nThe input featureis first transformed into, and, respectively, then summed with learnable weights.\nThe resulting output feature, which is calculated as follows:\n\nwhere,, andare learnable parameters whose dimension sizes are equal to the embedding dimension of.\nTo enable the model to search for the precise ratio of,, and,\nthe affine transform is applied toat once instead of to each normalization method.\nIn this manner, the MVN can flexibly explore the preferred combination of various normalized features with just a slight number of additional parameters and MACs.\n\nRecent studies on convonlution-based ViTs have presented notable performance. These studies used multiscale depthwise convolution[11,60], which diversifies its kernel size channelwise to add various spatial inductive biases.\nIn practice, these studies have primarily been based on a dichotomous perspective of distinguishing only local and global mixing.\nIn contrast to the attention mechanism, which dynamically adjusts weights based on input values, convolution operates as a static method, where filters slide in a data-independent manner.\nConsequently, the receptive field must be diversified to extract a wide range of visual patterns.\nFrom this viewpoint, we propose a three-scale convolutional token mixer, MVTM, consisting of local, global, and newly added intermediate mixing filters to capture the intermediate range of visual patterns between the local and global receptive fields.\nWe expect this approach to mitigate the heterogeneity between local and global mixed features, and we elaborate on the robustness of the convolutional token mixer on the visual object scale.\n\nThe MVTM is based on the inverted separable convolutional module of MobileNetV2[35], which is used as the token mixer in ConvFormer[59].\nFor a normalized feature, the output featureis calculated as follows:\n\nwhereanddenote the depthwise and pointwise convolutional layers, respectively, andindicates the activation function. Unlike the baseline that uses a 77 depthwise convolutional layer for, MVTM diversifies the receptive field ofas follows:\n\nFirst,is split into three channel-wise groups of,, and. Each of these groups denotes the channel-split features fromfor local, intermediate, and global mixing, where, anddenote their corresponding channels, respectively ().\nThose are individually input into,, and, which are token mixing depthwise convolutional layers with local, intermediate, and global kernel sizes, respectively; the kernel sizes inandare fixed as 33 and 77, respectively. As for, its kernel size is adjusted at each stage, as explained in the next paragraph.\nFinally, each mixed feature is concatenated channelwise.\nThis mechanism endows the MVTM with the capability to capture multiple ranges of visual representations.\n\nFurthermore, MVTM introduces the concept of stage specificity.\nRecent ViTs primarily follow the feature pyramid structure that systemically downsizes the feature shape at the beginning of each stage[26,27,59,10].\nAccording to the previous studies on ViT architecture[29,58], it has been analytically and experimentally observed that\nemploying local constraints on a token mixer is effective in the initial stages. In contrast, wide mixing for global token interaction is required in the late stages.\nThis property has not been adopted in convolution-based ViTs, as a fixed kernel design is applied across all token mixing layers.\n\nFor the first time, we adopted this paradigm to enable convolution-based ViT to capture various ranges of visual patterns efficiently.\nTo implement this, we regulated two configurations of MVTM: 1) a channelwise ratio of three mixing filters and 2) the kernel size of the global mixing filter ().\nThe former is to determine the predominant mixing scale of MVTM, and the latter is for rearranging the scope of global mixing with varying input shapes.\nIn this manner, MVTM weights the preferred receptive field at each stage depending on the input.\nTable1details the configurations.\n\nIn MVTM, as the stage number increases, we increase the channelwise ratio of local to global filters and decrease the size of the global mixing filter step by step. We expect the MVTM to captures a productive range of visual information efficiently at each stage.\n\nAs introducing MVN and MVTM into the MetaFormer block, we propose the MVFormer block, as presented in Fig.2(c).\nIn the MVFormer block, MVN first extracts various feature distributions. Based on this, MVTM explores diversified feature spaces for token mixing.\nIn addition, by equally inserting MVN in the MLP subblock, we expect a particular beneficial interaction between them, similar to that in the token mixer subblock.\nWe reformulated Eq.1and Eq.2as follows:\n\nwhere themodule is the same as in Eq.2.\nFor the activation functions inand, StarReLU[59]is used.\n\nConsidering the unique specificities of the three normalized features and multiple scale-mixed features with stage specificity, we propose an effective convolution-based ViT, MVFormer.\nThe overall paradigm of MVFormer is the same with MetaFormer when introducing MVN and MVTM into the MetaFormer blocks.\nDepending on the parameters and computational complexity, MVFormer is categorized into MVFormer-xT, MVFormer-T, MVFormer-S, and MVFormer-B, where MVFormer-xT is the primary model for feasibility.\nThe specific configurations of each MVFormer model are described in the AppendixA.\n\nSECTION: 4Experiments\n\nSECTION: 4.1Image Classification\n\nWe conduct image classification experiments on the ImageNet-1K benchmark[8], including 1.28M training images and 50K validation images from 1K classes.\nTo augment and regularize the input images for training, we employ weight decay, RandAugment[6], Random Erasing[63], Mixup[62], CutMix[61], Label Smoothing[40], Stochastic Depth[15]and training strategy of DeiT[43]. We train all models from scratch for 300 epochs with an input size of 224224.\nWe use the AdamW[20,28]optimizer with a cosine learning rate schedule, including 20 warm-up epochs. ResScale[38]is used for the last two stages. The batch size, learning rate, and weight decay are set to 4096, 4e-3, and 0.05, respectively.\nWe also use the stochastic depth with a probability of 0.2 for MVFormer-xT and MVFormer-T and 0.3 and 0.4 for MVFormer-S and MVFormer-B, respectively.\nWe fine-tune the models trained at 224224 resolution for 30 epochs using exponential moving average[32]for 384384 resolution.\nThe proposed implementation is based on PyTorch library[31], and the experiments are run on 8 NVIDIA A100 GPUs.\n\nTable2presents performance comparisons of MVFormer with the current SOTA models on ImageNet-1K classification.\nWe compare MVFormer to existing attention-based[26,43,54,49]and convolution-based[27,60,55,12,59]SOTA models, grouped into the model size represented by the number of parameters and MACs.\nThroughout both approaches, the MVFormer variants consistently outperform other candidates. Particularly, MVFormer-T, S, and B beat the current convolution-based SOTA models, ConvFormer-S18, S36, and M36, by 0.4%p, 0.2%p and 0.1%p, respectively, regarding performance enhancements with equal or fewer parameters and MACs.\nOn higher-resolution images, the performance increases occur in all three model variants.\n\nSECTION: 4.2Object Detection and Instance Segmentation\n\nWe evaluate MVFormer regarding object detection and instance segmentation tasks on the COCO 2017 benchmark[24], with 118K training images and 5K validation images.\nWe use the ImageNet-1K pre-trained MVFormer as the backbone, which is equipped with the Mask R-CNN[14]and RetinaNet[25].\nWe train the model with single-scale inputs with a learning rate of 1e-4 for RetinaNet and 2e-4 for Mask R-CNN, decayed at 8 and 11 for 1schedule, and at 27 and 33 for 3schedule.\nThe image is resized to the shorter side, at 800 pixels, whereas the longer side remains within the limit of 1333 pixels.\nTo prevent overfitting, MVFormer-T and S have stochastic depths set to 0.3 and 0.4, respectively.\nThe implementation is based on the mmdetection[2].\n\nTable3presents the performance comparison of MVFormer with SOTA ViT models.\nIn all cases, our MVFormer-T and MVFormer-S consistently achieve SOTA performances with the highest mean average precision (mAP) on both tasks, with significantly fewer parameters and MACs.\nFor 1schedule, MVFormer-variants even present bestandwith both Mask R-CNN and RetinaNet.\nThis result underscores the exceptional generalization capability of MVFormer.\nIn the case of 3schedule, compared to Focal-T[54], MVFormer-T shows slightly loweron both tasks.\nNevertheless, considering higher mAP and, it becomes evident that MVFormer excels in providing more precise dense predictions.\n\nSECTION: 4.3Semantic Segmentation\n\nWe also assess MVFormer on semantic segmentation using ADE20K benchmark[64],\ncomprising 20K training and 2K validation images.\nWe employ the ImageNet-1K pre-trained MVFormer as the backbone, equipped with the Semantic FPN[21].\nTo train 40K iterations with a batch size of 32, we use AdamW with an initial learning rate ofand cosine learning rate schedule. Images are resized and cropped topixels for training.\nThe implementation is based on mmsegmentation[5].\n\nIn Table4, we compare MVFormer with SOTA models for the semantic segmentation task.\nBoth MVFormer-T and MVFormer-S highly outperform other models given a competitive number of parameters and MACs.\nCompared to the VAN-B2 and B3[12], which are up-to-date convolution-based ViTs, MVFormer-T and S display 0.4%p and 0.7%p performance gains, respectively, with better efficiency.\n\nSECTION: 4.4Ablation Studies\n\nWe perform ablation studies to validate the effectiveness of MVN and MVTM.\nAll experiments are conducted on the ImageNet-1K classification, with the MVFormer-xT model.\n\nWe conduct ablation experiments in Table5to evaluate the effect of each proposed module with a convolution-based ViT baseline on the ImageNet-1K classification.\nFor a fair comparison, we design a MetaFormer-based baseline with a token mixer equal to the 55 depthwise separable convolution\nbecause it requires a similar number of parameters and MACs compared to the MVTM.\nRegarding normalization, LN is applied by default.\nWhen each of the MVN and MVTM is solely used, there occur significant performance enhancements of 0.53%p and 0.17%p with negligible amount of additional parameters and MACs, respectively.\nBetween them, the MVN improves more performance of 0.38%p than MVTM.\nAdditionally, MVFormer-xT, which incorporates MVN and MVTM, achieves the highest performance of 81.30%.\nThese findings support the combined use of these proposed modules and the individual benefits each module has in improving model performance.\n\nTable6presents the ablation study of all combinations of the three normalization methods in MVN.\nCombining just two normalized features consistently enhances the performance compared to that of a single method.\nIn particular, IN significantly degrades the performance when used alone.\nNevertheless, IN exhibits beneficial synergy when combined with other methods.\nWe infer that IN contributes to the performance gains by mitigating the batch-dependence in BN and spatial distribution variation in LN.\nThe MVN, combining BN, LN and IN, significantly outperforms all other combinations, strongly supporting the conjecture that comprehensively encompassing diverse characteristics of normalization methods leads to improved performance and contributes to an enhanced perspective on extending feature diversity.\n\nTo evaluate the generalization of MVN, we apply MVN to existing variants of ViT and CNN.\nFor ViT candidates, we select Swin[26], ConvFormer[59], ConvNeXt[27], and PoolFormer[58], which are attention-, convolution- and pooling-based models, respectively, and we experiment on ResNet[13], which represents the CNN.\nFor ViTs, we substitute the LN with MVN within each block, and in ResNet, all BN layers are replaced with MVNs.\nAs listed in Table7, MVN displays impressive generalization, significantly improving the original performance of all five models.\nFor ViTs, the model achieves 0.2%p of consistent Top-1 accuracy gains on PoolFormer-S36, Swin-T, ConvFormer-S18 models and ConvNeXt-T.\nIn the case of CNN, MVN even works on ResNet50 with 0.2%p of accuracy improvement.\nThese results suggest that MVN is not restricted to just CNN-ViT hybrid architecture, indicating promising feasibility for applications in various standard vision models.\n\nTable8presents the performance variation, excluding each proposed component of the MVTM individually.\nIn terms of stage specificity, given either a single global filter size or fixed channelwise split ratio, there occurs similar degree of performance degradations, which is 0.10%p and 0.08%p, respectively.\nWhen both are applied, it gets larger that 0.15%p of performance drop is observed.\nThis result shows that adopting stage specificity enhances the efficiency of MVFormer, as improving the performance given similar MACs and parameters.\nIn addition, the ablation result on the mixing filter presents the importance of different mixing filter levels.\nWhen one of the three mixing filters is excluded, performance degradation occurs consistently.\nIt is much higher when the smaller size of filter is eliminated.\nWe infer that this is because the repeated small filters are able to cover a wide range of visual patterns, whereas the large filter struggles to focus on the local area.\n\nFig.3presents the weight distributions in MVN to identify certain preferences depending on the stage number. Interestingly, it is observed that there exists an overall tendency of the ratio between three normalization methods.\nAcross all stages, excluding the last part of the second stage, the weight of the LN consistently has the highest ratio.\nThis suggests that the model predominantly reflects the input channel distribution of each pixel, rather than the spatial distribution of each channel. The BN and IN temporarily exhibit a higher ratio than the LN in the final block of Stage 2, possibly due to the model prioritizing spatial over channel distribution during a rapid channel dimension change. Moreover, IN generally has a lower ratio than BN, except in the last stage, suggesting a preference for batch-independent sample-level spatial information.\nThis observation is consistent in the MVFormer-T and -B models.\n\nSECTION: 5Conclusion\n\nThis work proposes MVFormer, an efficient yet effective convolution-based ViT,\nby introducing a normalization module (MVN) and a token mixer (MVTM) to extract diverse features from multiple viewpoints.\nThe MVFormer outperforms the existing SOTA convolution-based ViTs in image classification and three downstream tasks, given competitive efficiency.\nSignificantly, MVN consistently boosts the performance of MVFormer and existing ViTs and CNNs, affirming its scalability.\nWe also confirm that triscale filters and stage specificity of the MVTM are crucial factors for performance.\nIn the future, we will explore the interrelationship between the normalization method and various types of token mixers, and we expect that these approaches offer valuable insights into the vision community and can be extended to other domains.\n\nSECTION: References\n\nSupplementary Material\n\nSECTION: AModel Configurations\n\nSECTION: BTraining Configurations\n\nSECTION: CLearning Curve Analysis\n\nWe compare the learning curves of MVFormer-xT trained with BN[18], LN[1], IN[45], and MVN to identify training stability and convergence rate using each method.\nIn Fig.4, we observe that the overall learning trends of all four cases exhibit similar patterns.\nFrom the perspective of training stability, there are no significant gaps between the four curves, showing similar degrees of oscillation.\nIn terms of the convergence rate, the training losses of all methods steadily decrease for 300 epochs, where the MVN almost shows the lowest values compared to BN, LN, and IN.\nThis result suggests the efficacy of MVN that enables the model training to reach to more optimal point while maintaining competitive training stability.\n\nSECTION: DGrad-CAM Visualization\n\nTo qualitatively assess the effectiveness of the proposed MVFormer, we conduct a visual comparison with the baseline ConvFormer[59]using Grad-CAM[36]visualization.\nFig.5presents the activation maps of ConvFormer-S18 and MVFormer-T models, both trained on the ImageNet-1K.\nCompared to the baseline, our MVFormer effectively captures various scales of main objects in the input images.\n\nSECTION: EVisual Comparison of Normalized Images from BN, LN, IN, and MVN\n\nFig.6shows the visualizations of normalized images from BN, LN, IN, and our MVN.\nFor the weights of MVN, we apply the ratio of the first block in Stage 1, equal to 0.36, 0.62, and 0.02 for BN, LN, and IN, respectively.\nAs reflecting the distinct characteristics of three normalized images, through the MVN, we can observe the local pattern-preserved smoothed images.\nBy this property, MVN is expected to diversify the feature learning.\n\nSECTION: FPseudo-code in PyTorch\n\nAlgorithm1and Algorithm2are the PyTorch-like pseudo-codes[31]for the MVN and MVTM modules, respectively.\nFor simplification, we do not consider the channel ordering.", "text_file": "data\\paper_texts\\2411.18995v1_content.txt"}, {"title": "Convolutional Vision Transformer for Cosmology Parameter Inference", "authors": ["Yash Gondhalekar", "Kana Moriwaki"], "published_date": "2024-11-21T18:25:18Z", "summary": "Parameter inference is a crucial task in modern cosmology that requires\naccurate and fast computational methods to handle the high precision and volume\nof observational datasets. In this study, we explore a hybrid vision\ntransformer, the Convolution vision Transformer (CvT), which combines the\nbenefits of vision transformers (ViTs) and convolutional neural networks\n(CNNs). We use this approach to infer the $\\Omega_m$ and $\\sigma_8$\ncosmological parameters from simulated dark matter and halo fields. Our\nexperiments indicate that the constraints on $\\Omega_m$ and $\\sigma_8$ obtained\nusing CvT are better than ViT and CNN, using either dark matter or halo fields.\nFor CvT, pretraining on dark matter fields proves advantageous for improving\nconstraints using halo fields compared to training a model from the beginning.\nHowever, ViT and CNN do not show these benefits. The CvT is more efficient than\nViT since, despite having more parameters, it requires a training time similar\nto that of ViT and has similar inference times. The code is available at\n\\url{https://github.com/Yash-10/cvt-cosmo-inference/}.", "arxiv_id": "2411.14392v2", "html_link": "https://arxiv.org/html/2411.14392v2", "search_term": "ti:\"vision transformers\"", "html_content": "SECTION: Convolutional Vision Transformer for Cosmology Parameter Inference\n\nParameter inference is a crucial task in modern cosmology that requires accurate and fast computational methods to handle the high precision and volume of observational datasets. In this study, we explore a hybrid vision transformer, the Convolution vision Transformer (CvT), which combines the benefits of vision transformers (ViTs) and convolutional neural networks (CNNs). We use this approach to infer theandcosmological parameters from simulated dark matter and halo fields. Our experiments indicate that the constraints onandobtained using CvT are better than ViT and CNN, using either dark matter or halo fields. For CvT, pretraining on dark matter fields proves advantageous for improving constraints using halo fields compared to training a model from the beginning. However, ViT and CNN do not show these benefits. The CvT is more efficient than ViT since, despite having more parameters, it requires a training time similar to that of ViT and has similar inference times. The code is available athttps://github.com/Yash-10/cvt-cosmo-inference/.\n\nSECTION: 1Introduction\n\nAn enthralling task in cosmology is accurately estimating the cosmological parameters describing the Universe from observational data, i.e., cosmological parameter inference. The widely accepted cosmological model, theCDM (Cold Dark Matter), describes the Universe using a few parameters:(the matter density, including normal and dark matter),(the dark energy density;, the cosmological constant, represents dark energy),(the Hubble parameter),(the spectral index of density perturbations),(the variance in the matter distribution smoothed over spheres of radius 8Mpc). The overwhelming amount of cosmological information from current and upcoming observational surveys[e.g.,13,11]will require sound statistical methodologies to achieve this goal.\n\nParameter inference aims to determine the posterior distributions of cosmological model parameters given a set of observations. Traditionally, this has been achieved by comparing the two-point correlation functions or power spectra of the tracers of large-scale structure (LSS) with theoretical predictions, or by using higher-order summary statistics that extract non-Gaussian information[e.g.,5,27]. Such methods have analytically tractable likelihoods. However, predefined summary statistics inevitably fail to fully capture the rich non-Gaussian information at non-linear scales, which makes them suboptimal. Recently, \u2018field-level inference\u2019[6]has gained a lot of attention as a potential alternative to these traditional techniques due to its ability to produce tighter constraints[see, e.g.,15,1,7,16]. In this case, the likelihood is untractable since cosmological parameters are directly derived from the full, non-linear distribution of matter fields. Field-level inference allows access to higher-order information (e.g., from the phases of the fields), which is otherwise inaccessible through conventional summary statistics. Neural networks are a promising solution for field-level inference due to their demonstrated capabilities to extract features from complex data efficiently.\n\nSince neural networks use the entire non-linear (and thus noisy) distribution of matter, they must effectively extract informative multi-scale features that help link those features to the underlying cosmological model parameters. Convolutional Neural Networks (CNNs) have consistently excelled in various tasks, primarily due to their localization through convolutional kernels, translation invariance property, and learning features hierarchically (i.e., local features in earlier layers and increasingly global features in later layers as its receptive field enlarges). Consequently, CNNs have become the dominant choice for cosmological parameter inference[e.g.,20,21,14,25].\n\nCNNs also have limitations because their receptive fields are constrained to grow larger as depth progresses, but that may not be necessary. The relatively newer Vision Transformers (ViTs)[8]do not take advantage of the strong inductive biases induced by convolutions in CNNs, allowing them to learn global spatial relationships through their self-attention layers even in the earlier layers of the model. ViTs break down an image into several patches, which are then flattened and considered tokens, similar to the terminology used in natural language processing. ViTs lack some biases, so they require large datasets for training, but given this constraint is satisfied, they have shown comparable or better performance than state-of-the-art CNNs such as ResNets. The applications of ViT for cosmological parameter inference are thus compelling, but only a few studies have explored their value[9,10]and found them competitive with CNN. We hypothesize that combining the benefits of CNNs and ViTs may alleviate their individual deficiencies and improve parameter inference.\n\nIn this study, we perform likelihood-free inference to predict the marginal posterior mean and variance of the two cosmological parameters,and, from simulated dark matter and halo distribution using the publicly availableQUIJOTEsimulation suite as our dataset. We use a convolutional vision transformer (CvT) that combines the advantages of both CNNs and ViTs. CvT has been previously applied in[4], who found it better than CNN and ViT for classifying galaxy morphologies.\n\nSECTION: 2Method\n\nSECTION: Data\n\nWe use the-body simulations of the publicly availableQUIJOTEsimulation suite[23]to obtain the DM density and halo catalogs; we use the friends-of-friends (FoF) halo catalogs. These simulations are performed with a box size ofGpc. We use standard Latin-hypercube simulations with massless neutrinos that containcold dark matter particles and use outputs at.\nThis simulation set contains 2000 simulation data, and we divide it into training, validation, and testing sets using an 80-10-10% split. Since the splitting is performed at the simulation level, all data from a simulation are either in the training, validation, or testing set; such a non-random splitting is crucial to prevent obvious bias in the results[22]. All 1600 DM data are used for pretraining, but only 200 Halo data are used for finetuning.\nAll simulations have different random seeds withvaried in [0.1, 0.5],varied in [0.6, 1.0], and the other astrophysical parameters (,,) varied within their appropriate ranges.\n\nWe project the particle and halo positions from the simulation snapshots onto agrid using the cloud-in-cell (CIC) scheme. All the halos detected inQUIJOTEare contained in halo maps, i.e., we do not apply any mass cuts. Ten random two-dimensional maps (each of thickness3.9Mpc) along each projection direction (X, Y, and Z) are selected, producing 30 maps from each simulation (these maps are individually used as inputs to the neural network). The overdensities are first calculated (), followed by a logarithm-like transformation given byto reduce the dynamic range of the pixel values, followed by standardization using the mean and standard deviation of the transformed fields from the training set. Each cosmological parameter is individually normalized tousing the corresponding minimum and maximum values calculated from the training set.\n\nFig.1shows an example comparison of the two-dimensional DM density and halo maps. It shows that the halos are biased tracers of the DM density as the former captures high local overdensities in the DM distribution, and thus leads to a visually sparser distribution.\n\nSECTION: Approach\n\nWe use a Convolutional vision Transformer (CvT)[26], which uses a multi-stage hierarchical structure (see AppendixAfor visualization of the architecture), where each stage contains a convolutional token embedding layer followed by convolutional transformer blocks. The convolutional token embedding layers learn a convolution operation transforming input tokens into a new set of tokens. Its placement across different stages allows progressive spatial downsampling (i.e., reducing the number of tokens) together with increasing feature dimensions (i.e., increasing the width of tokens), and thus allows capturing local information as in CNNs. The convolutional transformer block uses a convolutional projection, implemented as a depth-wise separable convolution layer, instead of a linear projection used in traditional ViT. Ideologically, this transformer block generalizes the transformer in traditional ViT. Since local spatial relationships are modeled through the convolutional token embedding and projection, no positional encoding is required, which allows CvT to adapt to variable spatial resolution images. The use of efficient convolutions within the transformer in CvT also makes it computationally and memory-wise more efficient than traditional ViTs. The implementation is adapted from thevit-pytorchcode111https://github.com/lucidrains/vit-pytorch.\nSpecifically, we use the lightweight CvT-13 model, i.e., with a total of 13 transformer blocks containing 17.6M parameters and the default model hyperparameters as used in the original paper.\n\nSECTION: Training details\n\nWe modify the CvT architecture to perform a regression task to predict the two cosmological parameters,and. Our model predicts the marginal posterior mean and variance forand. The loss function is designed under the framework of moment networks[12]and used previously in works such as[24]and[25], and is given by:\n\nwhereis the true value of parameterfrom simulationandandare the network prediction of the mean and standard deviation of the marginal posterior of parameter, respectively.\nDuring training, the 2D maps are rotated randomly by 90, 180, or 270 degrees. A batch size of 16, Adam with decoupled weight decay optimizer (AdamW;Loshchilov and Hutter17) with weight decay ofand a learning rate ofis used. The learning rate is reduced by a factor of 0.3 if the validation loss does not improve for five epochs. Dropout is not used during training. Training is performed for 30 epochs in all experiments, and the model weights corresponding to the lowest validation loss are used for inference. Weights & Biases[3]was used to track training and evaluation. No specific hyperparameter tuning has been performed.\n\nSECTION: 3Results\n\nSECTION: Experimental details and evaluation\n\nWe pretrain CvT on DM fields and report the test results. We also show test results when the pretrained model is finetuned on halo fields and compare its performance against the model trained from scratch on halo fields. Before finetuning, we only reinitialized the fully connected MLP head. We chose not to freeze any weights, as doing so only provided marginally better constraints on: the RMSE was almost the same, with error bars increasing by about 0.014, reflecting the RMSE better, but constraints onwere significantly worse, with the RMSE increasing by about 0.014 and error bars underpredicted by about 0.03, than not freezing. The optimal pretrained model on DM data was found after 28k iterations, the optimal finetuned model on halo data after 2.4k iterations, and the optimal model trained from scratch on halo data after 1.4k iterations, although it had a higher validation loss than using transfer learning.\n\nWe used two metrics to evaluate the prediction for each cosmological parameter: the root mean squared error (RMSE), defined as, whereis the number of test examples, anddenotes the averaged errors.determines the accuracy of the predictions, anddenotes the 1error in the prediction of the parameter value.\n\nSECTION: Prediction performance\n\nFig.2shows the predictions versus the true parameter values for DM pretraining (a), halo transfer learning (b), and halo training from scratch (c), from left to right.predictions for (a) show excellent agreement with a near 1:1 relationship (RMSE = 0.005 and appropriately small error bars), whilepredictions are moderately good (RMSE = 0.059) and appropriate error bars. To put these values in context, we report the RMSE in the case of a constant prediction equal to the mean of the true value and obtain RMSE = 0.118 for both parameters.\n\nUsing the pretrained model from dark matter and transfer learning using halo fields (case b) gives slightly worse constraints forthan (a) (RMSE = 0.064 and underestimated error bars), but theconstraints are more prominently deteriorated (RMSE = 0.079 and slightly underestimated error bars). This deterioration is not unexpected, as halos are biased tracers of the underlying dark matter field, so they contain less pertinent information. It is currently elusive why the error bars forare severely underestimated, but the fact that this also happens for (c) suggests that this is not necessarily due to transfer learning but a characteristic feature when using halo fields. The error bars forare also underestimated, but this is less severe than.\n\nFor (b), it can be seen that largevalues tend to be underestimated, smallvalues are overestimated, whereas largevalues are underestimated. So, predictions near the edges are affected and there is a tendency to regress towards the mean of the cosmological parameter set222Although we intend to talk about the mean of the \u2018test\u2019 parameter set here, we have checked the mean of the training parameter set is also similar which is because we randomly split the simulations.. The RMSE for constant prediction is 0.121 and 0.109 forand, respectively. Thus, this accuracy is still better than simply predicting the mean value. We do not have a clear explanation for these biases, but they may be due to insufficient expressiveness of the MLP head (since we only use a single-layered MLP) or due to overfitting (see[19]and[18], respectively).\n\nThe constraints in (c) are worse than in (b) as shown by the lower values ofRMSEand, and therefore the transfer learning approach (DM pretraining followed by halo finetuning) seems more beneficial than training on halo data from scratch. This can be expected because the large-scale features in the DM and halo fields are similar, so the pretrained weights of the model that is trained on DM data serve as a better starting point to learn features from the halo fields.\n\nSECTION: Comparison with traditional ViT\n\nWe compare the CvT (used in this work) with the simpler version of the traditional ViT architecture discussed inBeyer et\u00a0al. [2], which we dub the \u2018ViT\u2019333Note that this is a slightly modified version of the original ViT architecture proposed in[8]. We used a patch size of 8 for the ViT, but other common hyperparameters are the same as CvT. The results for (a), (b), and (c) forare as follows: RMSE = 0.066 and= 0.254, RMSE = 0.068 and= 0.299, RMSE = 0.074 and= 0.281. Thus, for (a) and (b), ViT is less accurate than CvT. For (c), the RMSEs are similar, but the error bar for CvT is more representative of the accuracy. For, the results for (a), (b), and (c) are: RMSE = 0.1 and= 0.24, RMSE = 0.106 and= 0.314, RMSE = 0.112 and= 0.247. However, the predictions are \u2018near-flat\u2019444The predicted parameters are visually similar irrespective of the true parameter value when visualized like Fig.2.in all cases. Thus, CvT can constrain the cosmological parameters more tightly than ViT, especially.\n\nSECTION: Comparison with CNN\n\nThe CNN architecture consists of five convolutional layers and batch normalization, followed by a fully connected layer that predicts the mean and standard deviation, just like the ViT, and other common hyperparameters are the same as CvT. The results for (a), (b), and (c) forare as follows: RMSE = 0.073 and= 0.086, RMSE = 0.21 and= 0, RMSE = 0.106 and= 0.139. CNN is less accurate than ViT and CvT, and also yields an overconfident prediction for (b) (= 0). For, the results for (a), (b), and (c) are: RMSE = 0.035 and= 0.075, RMSE = 0.151 and= 0, RMSE = 0.116 and= 0.137. CNN is better than ViT to predictfor (a), but is worse than both ViT and CvT for all other cases.\n\nSECTION: Execution time\n\nThe ViT used in this work contains far fewer parameters (1.6M vs. 17.6M) but requires only marginally shorter training time than the CvT (2.6 vs. 2.7 hours) for the DM pretraining. Thus, the operations in CvT are more efficient than those in ViT, probably due to the introduction of convolutions and the convolutional projection operation[26]. At test time, CvT requires24 seconds, whereas ViT requires19 seconds for inference on 6000 maps (this experiment was performed when the model was trained using halo data from scratch). Although CvT is cumulatively slightly slower, the per-map inference times are almost the same.\n\nSECTION: 4Conclusion\n\nWe have applied the convolution-based vision transformer (CvT) to infer theandcosmological parameters using data obtained from theQUIJOTEsimulation. We find that CvT constrains both parameters better than CNN and ViT when using dark matter and halo fields. CNN is found to be more beneficial than ViT only for inferringfrom dark matter fields, whereas ViT outperforms in all other cases. Pretraining CvT on dark matter fields has proven beneficial in better constraining the parameters when finetuned on halo fields rather than training a model from scratch on halo data, but these benefits are not apparent for CNN and ViT. One possible interpretation is that CvT is able to effectively leverage the large-scale structure similarities between dark matter and halo fields; however, more detailed tests are necessary to validate this finding. The demonstrated constraining power of CvT is noteworthy given that it was finetuned using 8lesser data than pretraining. As a result, it may be advantageous to apply CvT on data such as galaxy distribution, which require hydrodynamic simulations that are often computationally prohibitive. We also briefly demonstrate that CvT is more efficient than ViT due to the use of convolutions and has a similar inference time to it.\n\nSome future aims of this work are to interpret CvT, apply it to real data and develop guidelines for observational cosmologists instructing the regions to look at in the data, and integrate it with data simulation approaches based on deep learning (i.e., emulators).\n\nSECTION: Acknowledgments and Disclosure of Funding\n\nThis work was supported by JSPS KAKENHI Grant Number 23K03446.\n\nSECTION: References\n\nSECTION: Appendix AArchitecture of the convolutional vision transformer\n\nFig.3shows the architecture of the CvT network. The primary modifications in CvT compared to the traditional ViT are the presence of a convolutional token embedding layer, whose presence across multiple stages resembles the design of CNNs, and the presence of a convolutional projection instead of a linear projection.", "text_file": "data\\paper_texts\\2411.14392v2_content.txt"}, {"title": "ViTOC: Vision Transformer and Object-aware Captioner", "authors": ["Feiyang Huang"], "published_date": "2024-11-09T13:13:49Z", "summary": "This paper presents ViTOC (Vision Transformer and Object-aware Captioner), a\nnovel vision-language model for image captioning that addresses the challenges\nof accuracy and diversity in generated descriptions. Unlike conventional\napproaches, ViTOC employs a dual-path architecture based on Vision Transformer\nand object detector, effectively fusing global visual features and local object\ninformation through learnable vectors. The model introduces an innovative\nobject-aware prompting strategy that significantly enhances its capability in\nhandling long-tail data. Experiments on the standard COCO dataset demonstrate\nthat ViTOC outperforms baseline models across all evaluation metrics.\nAdditionally, we propose a reference-free evaluation method based on CLIP to\nfurther validate the model's effectiveness. By utilizing pretrained visual\nmodel parameters, ViTOC achieves efficient end-to-end training.", "arxiv_id": "2411.07265v3", "html_link": "https://arxiv.org/html/2411.07265v3", "search_term": "ti:\"vision transformers\"", "html_error": "Error processing HTML: HTTP Error 404: Not Found"}, {"title": "Residual Attention Single-Head Vision Transformer Network for Rolling\n  Bearing Fault Diagnosis in Noisy Environments", "authors": ["Songjiang Lai", "Tsun-Hin Cheung", "Jiayi Zhao", "Kaiwen Xue", "Ka-Chun Fung", "Kin-Man Lam"], "published_date": "2024-11-27T02:46:54Z", "summary": "Rolling bearings play a crucial role in industrial machinery, directly\ninfluencing equipment performance, durability, and safety. However, harsh\noperating conditions, such as high speeds and temperatures, often lead to\nbearing malfunctions, resulting in downtime, economic losses, and safety\nhazards. This paper proposes the Residual Attention Single-Head Vision\nTransformer Network (RA-SHViT-Net) for fault diagnosis in rolling bearings.\nVibration signals are transformed from the time to frequency domain using the\nFast Fourier Transform (FFT) before being processed by RA-SHViT-Net. The model\nemploys the Single-Head Vision Transformer (SHViT) to capture local and global\nfeatures, balancing computational efficiency and predictive accuracy. To\nenhance feature extraction, the Adaptive Hybrid Attention Block (AHAB)\nintegrates channel and spatial attention mechanisms. The network architecture\nincludes Depthwise Convolution, Single-Head Self-Attention, Residual\nFeed-Forward Networks (Res-FFN), and AHAB modules, ensuring robust feature\nrepresentation and mitigating gradient vanishing issues. Evaluation on the Case\nWestern Reserve University and Paderborn University datasets demonstrates the\nRA-SHViT-Net's superior accuracy and robustness in complex, noisy environments.\nAblation studies further validate the contributions of individual components,\nestablishing RA-SHViT-Net as an effective tool for early fault detection and\nclassification, promoting efficient maintenance strategies in industrial\nsettings.\n  Keywords: rolling bearings, fault diagnosis, Vision Transformer, attention\nmechanism, noisy environments, Fast Fourier Transform (FFT)", "arxiv_id": "2412.00085v1", "html_link": "https://arxiv.org/html/2412.00085v1", "search_term": "ti:\"vision transformers\"", "html_error": "Error processing HTML: HTTP Error 404: Not Found"}, {"title": "Beyond Grids: Exploring Elastic Input Sampling for Vision Transformers", "authors": ["Adam Pardyl", "Grzegorz Kurzejamski", "Jan Olszewski", "Tomasz Trzci\u0144ski", "Bartosz Zieli\u0144ski"], "published_date": "2023-09-23T12:03:30Z", "summary": "Vision transformers have excelled in various computer vision tasks but mostly\nrely on rigid input sampling using a fixed-size grid of patches. It limits\ntheir applicability in real-world problems, such as active visual exploration,\nwhere patches have various scales and positions. Our paper addresses this\nlimitation by formalizing the concept of input elasticity for vision\ntransformers and introducing an evaluation protocol for measuring this\nelasticity. Moreover, we propose modifications to the transformer architecture\nand training regime, which increase its elasticity. Through extensive\nexperimentation, we spotlight opportunities and challenges associated with such\narchitecture.", "arxiv_id": "2309.13353v2", "html_link": "https://arxiv.org/html/2309.13353v2", "search_term": "ti:\"vision transformers\"", "html_content": "SECTION: Beyond Grids: Exploring Elastic Input Sampling for Vision Transformers\n\nVision transformers have excelled in various computer vision tasks but mostly rely on rigid input sampling using a fixed-size grid of patches. It limits their applicability in real-world problems, such as active visual exploration, where patches have various scales and positions.\nOur paper addresses this limitation by formalizing the concept of input elasticity for vision transformers and introducing an evaluation protocol for measuring this elasticity.\nMoreover, we propose modifications to the transformer architecture and training regime, which increase its elasticity.\nThrough extensive experimentation, we spotlight opportunities and challenges associated with such architecture.\n\nSECTION: 1Introduction\n\nVision Transformers (ViT)[8]achieve state-of-the-art results in many computer vision applications[4,33,13]. They cut the image into a regular grid of non-overlapping patches and pass them as tokens to attention modules that exchange information between them. Many variations of the algorithm were created[5,21,25,32], including cross modal[1]and long-term memory[31]solutions.\n\nNevertheless, almost all transformer-based methods assume that input tokens form a regular grid ofpixel patches, limiting their applicability in real-life problems. For example, in Active Visual Exploration (AVE)[20], in which an agent has to navigate an environment by actively choosing successive observations while having a restricted field of view. AVE is commonly encountered in robotic applications, where an agent must localize dangers as quickly as possible based on incomplete images and observations of various scales and positions.\n\nIn this paper, we will consider two research questions. First, we ask if the standard ViT architectures are resilient to input perturbations that commonly occur in the vision tasks of embodied AI, such as AVE. For this purpose, we introduce an evaluation protocol that measures three necessary types of input elasticity a good model should showcase:scale elasticity,position elasticity, andmissing data elasticity.\nWe use this protocol to measure the input elasticity of common ViT architectures. Then, we move on to the second question: How can ViT input elasticity be increased by modifying model architecture and a training policy? We propose architectural and training modifications, namedElasticViT111Our code is available at:https://github.com/apardyl/beyondgrids, to increase the elasticity.\n\nWe hope this work will draw the attention of the machine learning community to the important topic of input sampling strategies. It is crucial because, according to recent studies[2], accommodating alternative patch resolutions can significantly impact the algorithm\u2019s real-life performance. We can summarize our contributions as follows:\n\nWe formalize the notion of input elasticity for vision transformers and provide a comprehensive evaluation protocol to assess it.\n\nWe propose modifications to the vision transformer architecture and training policy, including a novelPatchMixaugmentation, that increase elasticity.\n\nWe show that elastic sampling strategies can boost transformers\u2019 performance, especially for significantly limited input data.\n\nSECTION: 2Related Work\n\nVision Transformers (ViTs).Vision transformer introduced in[8]is a versatile method due to its state-of-the-art performance obtained for many visual tasks[26]. Swin[19]modifies the attention mechanism to increase the locality of information exchange between tokens. AViT[5]demonstrates how transformer neural networks can be dynamically scaled. Pyramid ViT[28]implements a multi-scale feature extraction model using ViT, inspired by previous work on convolutional neural networks. Vision Longformer[36]uses a token-based input format that enables seamless addition of global context tokens and control of the computation complexity. Other positional encoding methods for vision transformers were studied in[6,22,12].\n\nViT sampling strategies.Many works explore the possibility of using different grid resolutions as ViT inputs. Some perform different grid scale sampling during training[14,30], and others introduce position and patch encoding rescaling tricks[2]. They usually improve ViT\u2019s accuracy across grids with varying resolutions and constant patches or when using native resolution grid sampling with a variable number of patches in a batch[7].\n\nViTs applications.Initially, vision transformers were used in classical computer vision tasks, like object detection[4]and semantic segmentation[13]. However, as the field matures, the models are being deployed in real-world scenarios[15], where it is often necessary to understand input to the model as a collection of images captured from multiple views, each contributing partial information about a scene rather than a single image. Active visual exploration[11,20]is one possible setup of such a real-world scenario. While performing the tasks, robots or drones often capture images that cover only part of the scene and rarely come in grid-aligned format with consistent scale. Therefore, it is crucial to provide higher input elasticity. Some of them were already considered, like ViT resiliency to missing data[10]has already been conducted[25,18,32], but their cumulative effect on performance is unknown.\n\nSECTION: 3Evaluation protocol\n\nIn this section, we first define three types of model elasticity corresponding to configurations that occur in embodied robotics data, and then we propose the evaluation protocol, which we implement by applying perturbations to the input sampling procedure. We use the protocol to rank the models by their overall elasticity.\n\nSECTION: 3.1Elasticities\n\nWe define modelelasticityas resilience to particular forms of input data configurations that can occur in real-life applications.\n\nScale elasticityis defined as resilience to the relative scale change across image tokens processed by the model.\nStandard ViT algorithm uses fixed patch size and normalizes the size of each input image, while recent works allow for the use of native image resolution[7]or different sampling grid sizes[2].\nWe generalize the concept of scale elasticity to include varying scales of patches sampled from a single image. We measure it by sampling each patch individually and resampling it to the input token\u2019s standardsize. Note that this implementation might introducemissing datato the input as shown in Fig.2.\n\nPositional elasticityconstitutes the resilience to positional change in input patches. Vanilla ViT is always trained with a rigid grid sampling even though, except for standard ViT positional encoding, the architecture does not require this.\nWe measure the positional elasticity by sampling image patches at the dense pixel-level resolution in contrast to the usual coarse grid-level resolution. Note that this sampling procedure might introducemissing dataas in Fig.2.\n\nMissing data elasticityrepresents the resilience to missing image parts at the input.\nRegular ViT assumes complete knowledge of the image, while many derivative works mask or remove a portion of input tokens[10,21].\nIn this work, we extend the concept of missing data to take into account situations where the tokens create an overlap of their receptive fields and do not cover the whole image area at the same time. Such overlapping might occur after patch scales or position perturbations, as shown in Fig.2. However, to measure the individual impact of missing data on model performance, independent of position and scale change, we drop a subset of grid-aligned patches from the input.\n\nSECTION: 3.2Protocol\n\nThe evaluation pipeline aims to assess the model elasticity by analyzing its resistance to changes in input sampling strategies. For this purpose, we create a set of patches based on the input image and then process this set through three perturbation functions corresponding to three considered elasticities. We use the perturbed set as a transformer input and report its performance. We provide conclusions on model elasticity by analyzing the performance obtained for different types of perturbations.\n\nTo strictly define the evaluation pipeline, let us consider imagefor which we generate setof patches, whereanddenote the top-left corner\u2019s coordinates, andrepresents the relative scale (i.e. for a native patch of resolution, we sample a patchand rescale it bilinearly to size). Initially, the coordinatesandare from the regular grid and. However, in the next step, we perturb them with three functions (presented in Fig.2) corresponding to the considered elasticities:\n\n- introduces the scale perturbations, sampling theparameter of every patchindependently and uniformly from range.\n\n- applies positional perturbation, modifyingandparameters of each patch, independently moving them by offsets sampled uniformly from range, whereis the size of the patch.\n\n- adds missing data perturbations, dropping outpatches fromrandomly with equal probability.\n\nMathematically, this process can be described as follows:\n\nA disturbed set of patchesis used as an input of the transformer to test its elasticity. Elasticity is high if the predictions forare as good as those obtained for.\n\nSECTION: 4Elastic ViT\n\nIn this section, we introduce a modification of the vision transformer architecture[8]for elastic input.\n\nSECTION: 4.1Position and scale encoding\n\nStandard ViT[8]implementations utilize learnable positional embeddings. This is feasible because the grid sampling limits the number of possible patch positions. Unfortunately, such embeddings are not attainable with variable position and scale sampling. Each patch can be sampled at an arbitrary pixel position and with an arbitrary scale in elastic sampling. Therefore, the number of possible positions to be encoded is significantly larger. Consequently, we used a four-dimensional modification of the sine-cosine positional encoding of the original transformer model[27]. Let us recall the one-dimensional version of the sine-cosine encoding (is the length of the embedding andthe-th element of it):\n\nTo encode the position of a patch, we separately encode the pixel coordinates of the patch upper-left and lower-right corner () and concatenate resultant embedding vectors.\n\nFinally, we modify the input of a ViT to accept a set of cropped patches and a list of patch coordinates instead of a full image. The coordinates are used to generate positional encoding embeddings for each patch, allowing for continuous positional space.\n\nSECTION: 4.2Augmented training\n\nWe propose a training regime modification for greater elasticity. As our baseline, we use the augmentation regime introduced in[26], as it allows training the model on the ImageNet-1k[23]dataset without using any additional pre-training. We denote a ViT model trained with our modified regime asElasticViTin the following sections. A standard model trained without introduced elasticity is denoted asViTfor comparison.\n\nFirst, we introduce the elasticity functions into the augmentation pipeline. During training we use(random patch sampling size in rangeto),(no patch dropout),(unrestricted random patch positions). The native patch resolution (i.e., the resolutions all patches are rescaled to after sampling) is set to, and the native (scale = 1) image size is. We useimage resolution in the augmentation pipeline to accommodate for variable scale when required.\n\nSecond, we observe that the CutMix[34]augmentation used in[26]is not optimal with the input elasticity we introduce. As our sampling strategy may change the proportions of images mixed by CutMix due to patch overlap, we must recalculate the mixed labels after applying elasticity functions to match the actual proportions of mixed images. Therefore, we replace it with PatchMix, a custom but comparable augmentation dedicated to ElasticViT. Similarly to TokenMix[17], it mixes patches after sampling. However, contrary to TokenMix it is not dependant on standard grid sampling, and can take full advantage of the models ability to process arbitrary sampled patches (see Fig.3and supplementary materials). Moreover, we modify the MixUp[35]augmentation, performing it after applying perturbations to patch sampling and ensuring that both sequences of patches to be mixed have the same patch scales element-wise.\n\nSECTION: 4.3Experimental setup\n\nWe conduct experiments on a variety of state-of-the-art models with comparable parameter counts, differing in architecture or training policy. Namely, we evaluate our ElasticViT, ViT from DeiT\u00a0III[26], and MAE[10], all based on ViT-B architecture, and further Swin-B[19], and PVT-v2-B5[29]. All models were trained on ImageNet-1k[23].\n\nAs the tested models significantly differ in architecture, we provide the implementation details of the missing data and position perturbations. The scale perturbation is implemented by patch resampling, independent of the model.\n\nBecause ElasticViT, MAE, and DeiT\u00a0III model build on original[8]ViT architecture, we implement the action ofby removing the tokens from input entirely. Contrary to Swin and PVT, where attention mechanisms and positional encodings are more involved. For those models, we implement the action ofas zero-value masking of input tokens.\n\nIn all experiments, the magnitude of patch displacement is less than half of the patch size. Therefore, because Swin and PVT use relative positional encoding andis implemented via patch masking, we implement the action ofby stitching sampled and masked patches into an image of the original size. Each patch is aligned to the nearest corresponding position in the grid.\nViT from DeiT\u00a0III and MAE use learned discrete-valued absolute positional encoding. Therefore, we encode the sampled patch position with an embedding corresponding to the position nearest to the patch.\nElasticViT accepts continuous-valued positions, therefore we provide the model with true patch coordinates.\n\nThe experiments were performed on ImageNet-1k classification task. We report our results with bothclassification accuracyand thenumber of tokensused to achieve given results.\nAll training experiments were run usingNVIDIA A100 GPUs. ElasticViT was trained for 800 epochs using the same training parameters as in[26]. For other models we used checkpoints provided by their authors.\n\nTo analyses transfer learning capabilities, we fine-tuned the above models for the MS COCO 2014[16], Pascal VOC 2007[9]and ColonCancer[24]datasets forepochs. Only the final fully-connected classification head was trained using standard grid sampling as in standard ViT. The training regime from[26]was used, excluding the utilization of MixUp and CutMix augmentations. We report the mean class average precision scores.\n\nSECTION: 5Research questions\n\nOur goal was to investigate the resilience of transformers to input perturbations. For this purpose, we conducted extensive experiments, divided into six groups. The first three correspond to individual perturbations. The fourth group relates to their combinations. The fifth focuses on fundamental sampling strategies. In the sixth group, we examine a training trade-off between elasticity and base accuracy. Finally, in the last group we look into transferability of resilience to other datasets. All of them are described in the following subsections.\n\nSECTION: 5.1Scale elasticity\n\nWe maintain a consistent grid layout while introducing scale changes to each patch. The number of patches remains unchanged, but the perturbation inherently introduces overlapping or missing data to the input. This process is visually represented in Fig.2asScale Elasticity. The outcomes are illustrated in Fig.4(a).\n\nIs ViT robust with respect to scale?All baseline models significantly decrease in accuracy when patch scale changes. Notably, the best-performing baseline model is the original supervised ViT while the worst are PVT and Swin.\n\nDoes applying randomized patch sampling in training improve scale elasticity at inference?Ours ElasticViT despite having lower accuracy than other models at the base point (vs.of ViTs), due to high resilience to input scale variations, maintains its performance even at the ends of the measured spectrum and eventually outperforms all the other models.\n\nSECTION: 5.2Missing data elasticity\n\nMissing data can be simulated by adding perturbations to the input token set so part of the image is not represented by any of the cut patches. The patch scale experiment (Fig.4(a)) introduced missing data aspects due to changing the scale of the patches, but it was a byproduct of other types of perturbations. We use a patch dropout scheme to isolate only the missing data aspect and call it themissing dataexperiment.\nThe perturbation is depicted in Fig2. The results can be seen in Fig.4(b).\n\nHow does input dropout affect ViT performance?Introducing dropout results in a consistent decline in the accuracy of all models, being increasingly noticeable as the percentage of dropped tokens increases.\nThe best-performing baseline model is MAE, which is expected as it was trained on an image reconstruction task. The PVT significantly stands out from the other models, even from Swin, for which an identical dropout scheme was implemented and which has sparse attention too.\n\nDo elastic perturbations during training enhance performance when dealing with missing data?ElasticViT exhibits a comparable behavior to the baselines when it comes to missing data, displaying a decline in performance as the token count decreases. Nevertheless, the rate of accuracy loss is gentler. Notably, ElasticViT surpasses MAE performance after the removal of 75% of tokens.\nThis is significant because in the training the ElasticViT always accepted the full token count, contrary to MAE, which was trained to reconstruct the image from the 25% of input tokens.\n\nSECTION: 5.3Positional elasticity\n\nElastic vision transformers should not be limited to accepting only rigid sets of patch positions. Inpatch positionexperiment, we introduce a concept called \"patch shake\" \u2013 a randomized shift in the position of the original patches from a grid by a specified percentage of the patch size. ElasticViT\u2019s positional embeddings readily adapt to these positional changes. However, for the other models, we had to modify their positional embeddings to make this experiment viable. We encode each patch with embedding corresponding to the grid layout position being nearest to the patch. The patch shake operation is illustrated in Fig.2. The outcomes can be observed in Fig.4(c).\n\nIs ViT elastic to positional perturbations?Patch shake results in a minor accuracy reduction for all the models except PVT. It\u2019s important to note that patch shake can inherently introduce missing data and overlap perturbations. Therefore, this experiment doesn\u2019t isolate which specific perturbation has the most substantial impact on performance. Nonetheless, it\u2019s worth considering that the classification task might be inherently position agnostic, as ImageNet classification could be efficiently accomplished by treating patches as \"bags of words\" without explicit positional context.\n\nDoes training with randomized sampling improve positional elasticity?Like the missing data experiment (Sec.5.2), ElasticViT exhibits greater resilience to patch shake than the baselines. ElasticViT outperforms all models but MAE when the patch positions are shaken by approximately 40% of the patch size.\nNotably, the accuracy of ElasticViT remained almost constant with respect to the increase of patch shake, while the second-best model in this regard (MAE) lost 2 p.p. of accuracy.\n\nSECTION: 5.4Combining perturbations\n\nWe conduct a series of tests by combining the previously introduced input perturbations to assess their collective impact on performance. The scaling ranges for perturbation parameters are the same as in the earlier experiments. The objective is to determine whether ElasticViT\u2019s resilience will continue to outperform that of the original ViT in more complex scenarios.\n\nHow does positional elasticity combine with missing data elasticity?Inpatch position & missing dataexperiment, we introduce a combination of dropout and shake perturbations, and the results are depicted in Fig.5(a).\nNotably, the detrimental effect on ViT\u2019s performance closely aligns with the summed impact of shake and dropout when performed independently.\nThis lets ElasticViT surpass MAE at the 60% dropout threshold, instead of 75% as in themissing data4(b)experiment.\n\nWhat is the interaction between positional elasticity and scale data elasticity?Inpatch position & patch scaleexperiment, we combine the zoom perturbation with the shake perturbation. The results are shown in Fig.5(b). The ratio of performance decline between ViT and ElasticViT remained consistent with that reported in the previous experiment for patches twice as large. However, as patches became smaller, the performance gap between both algorithms widened. In the most altered scenario, ElasticViT outperforms ViT with a nearly 20 percentage point lead.\n\nHow does scale elasticity combine with missing data elasticity?Inmissing data & patch scaleexperiment, we combine dropout with patch scale change, and the results are illustrated in Fig.5(c). The outcomes resemble those of the grid zoom experiment (Sec.5.1) but exhibit a more pronounced negative impact of the perturbations. ElasticViT begins to outperform ViT at approximately the midway point of the perturbation intensity scale.\n\nSECTION: 5.5Patch sampling strategies\n\nThe elastic features demonstrated by ElasticViT create an opportunity to apply a multi-scale approach to partitioning images into patches of different scales in a non-uniform manner. Given the results obtained in the grid experiments, there\u2019s a possibility that this approach can lead to performance improvement without sacrificing accuracy compared to standard grid sampling (referred to asGRID).\n\nFirst, we test a hypothesis that, for the ImageNet, the central portion of an image might carry more significance than its periphery. To explore this, we iteratively divide the patches closest to the center of the image into four smaller patches. We refer to this algorithm asCENTRAL, see Fig.6. The rationale is that smaller patches offer a higher sampling resolution and introduce more data into the input, potentially leading to improved accuracy. The CENTRAL algorithm is designed so that the neutral point corresponds to a token count of 196, which aligns with the original ViT grid ofand a single scale. Lower values in this context result in larger patches near the image periphery, while higher values position smaller patches toward the center.\n\nNext, we extend on the previous experiment, analyzing if an adaptive patch sampling strategy can improve the results, especially in low patch count scenarios. We create a toy adaptive sampling algorithm, which we refer to asEDGE, based on the Canny edge detector[3]. The method starts by computing a bitmap of edges with a largevalue for the detector. The map is then divided intopatches, which are then sorted by their sums over the bitmap. Then, the algorithm iteratively divides a patch with the highest sum larger than the minimal size of, into 4 new patches, which are added to the list, preserving the order. The algorithm is repeated until the number of patches reaches the target. Visualization of EDGE is presented in Fig.6.\n\nDoes using higher resolution of the grid in the center improve accuracy?Our findings for the CENTRAL algorithm are presented in Fig.7. In the case of the vanilla ViT model, we observed issues with resilience to scale changes. Depending on the non-standard scale content of the input dataset, ViT results can be either negatively or positively influenced by the CENTRAL algorithm. Conversely, when considering ElasticViT, the differences are significantly reduced, and there is a higher likelihood that CENTRAL sampling leads to improved model performance.\n\nCan adaptive patch size sampling perform better?The results of this experiment are presented in Fig.7for the ElasticViT model. We observe, that EDGE performs significantly better than both GRID and CENTRAL algorithms in low patch count scenarios (less than 64 patches). When limited to 25 patches, it achieves overimprovement compared to ElasticViT andover standard ViT, increasing toand(accordingly) when only 16 patches can be sampled. Consequently, we claim that even very simple adaptive patch sampling methods can reduce the number of tokens needed to achieve targeted performance.\n\nSECTION: 5.6Trade-offs\n\nThis section explores a training trade-off for elastic sampling. Further experiments on trade-off in inference are provided in supplementary materials.\n\nWhat is the tradeoff between accuracy and elastic training?A consistent pattern emerges in all our experiments comparing ViT and ElasticViT: ViT generally exhibits higher accuracy than ElasticViT when evaluated under the original, unperturbed grid sampling scenario. However, as we introduce perturbations into the evaluation, ElasticViT can surpass ViT. Earlier we performed experiments that juxtapose classical single-scale grid training against a fully randomized sampling method of ElasticViT, representing two extremes in the spectrum.\n\nFig.8illustrates the results of training with mixed simple grid sampling (standard ViT training) and a fully randomized setup (ElasticViT training regime, see4). The evaluation was conducted by applying all three perturbation variants at the same time.\n\nThe findings reveal that for simple grid evaluation, elastic training has a relatively minor negative impact on accuracy. However, in the case of randomized evaluation that requires elasticity, even with only 15% of the training data containing randomized patches, there is a substantial gain of more than ten percentage points in accuracy. This implies that in practical applications, fine-tuning the network can be accomplished using just a fraction of perturbed input data while having minimal repercussions on baseline (simple grid) accuracy. Yet, this approach offers increased resilience to perturbations, demonstrating the practical utility of elastic inputs in the training process.\n\nSECTION: 5.7Transfer Learning\n\nWe evaluate elasticity of models pre-trained on the ImageNet-1k dataset and fine-tuned to the target tasks.\n\nDoes elasticity capabilities transfer to other datasets?An important question that remains to be answered is whether the elasticity capabilities observed on the ImageNet-1k dataset transfer to other datasets and tasks. We analyze this problem on the matter of multi-label classification of the MS COCO 2014 dataset as described in Sec.4.3. We perform evaluation applying all three perturbations (see Sec.3) at the same time. Results of this experiment are presented in Fig.9, PVT, Swin and MAE perform the best when no perturbation are introduced. However, ElasticViT exhibits the best consistency in results across all perturbation range. The standard ViT model performs the worst, even without any perturbations, indicating worse generalization capabilities. Results for VOC and ColonCancer are provided in supplementary materials.\n\nSECTION: 6Conclusions\n\nThis study examines Vision Transformers (ViT) and their adaptability to varying input sampling strategies required in real-world applications. We introduce an evaluation protocol to assess ViT\u2019s resistance to input perturbations, including scale, missing data, and positional changes.\n\nStandard ViT models prove to be vulnerable to these perturbations, displaying a significant performance drop due to potential overfitting during training. In contrast, the modified ViT we proposed exhibits better resilience thanks to randomized training, maintaining or improving performance in various scenarios, see overall comparison in Fig.\u00a08 of the supplementary material.\n\nOur experiments also explore adaptive patch sampling using CENTRAL and EDGE algorithms, which ElasticViT benefits from, particularly in scenarios with fewer patches. Adaptive patch sampling efficiently reduces token requirements while preserving target performance.\n\nFuture work in this area should focus on refining the adaptive patch sampling strategies and further investigating the trade-offs between downscaling input tokens and patch dropout under computational constraints. Additionally, research should aim to apply these findings to real-world applications, enhancing transformer models\u2019 adaptability in practical contexts.\n\nSECTION: Acknowledgments\n\nThis paper has been supported by the Horizon Europe Programme (HORIZON-CL4-2022-HUMAN-02) under the project \"ELIAS: European Lighthouse of AI for Sustainability\", GA no. 101120237, and by National Science Centre, Poland (grant no. 2023/49/N/ST6/02465, 2022/47/B/ST6/03397, 2022/45/B/ST6/02817, and 2023/50/E/ST6/00469). Some experiments were performed on servers purchased with funds from a grant from the Priority Research Area (Artificial Intelligence Computing Center Core Facility) under the Strategic Programme Excellence Initiative at Jagiellonian University. We gratefully acknowledge Polish high-performance computing infrastructure PLGrid (HPC Center: ACK Cyfronet AGH) for providing computer facilities and support within computational grant no. PLG/2024/017483.\n\nSECTION: References", "text_file": "data\\paper_texts\\2309.13353v2_content.txt"}, {"title": "CMAViT: Integrating Climate, Managment, and Remote Sensing Data for Crop\n  Yield Estimation with Multimodel Vision Transformers", "authors": ["Hamid Kamangir", "Brent. S. Sams", "Nick Dokoozlian", "Luis Sanchez", "J. Mason. Earles"], "published_date": "2024-11-25T23:34:53Z", "summary": "Crop yield prediction is essential for agricultural planning but remains\nchallenging due to the complex interactions between weather, climate, and\nmanagement practices. To address these challenges, we introduce a deep\nlearning-based multi-model called Climate-Management Aware Vision Transformer\n(CMAViT), designed for pixel-level vineyard yield predictions. CMAViT\nintegrates both spatial and temporal data by leveraging remote sensing imagery\nand short-term meteorological data, capturing the effects of growing season\nvariations. Additionally, it incorporates management practices, which are\nrepresented in text form, using a cross-attention encoder to model their\ninteraction with time-series data. This innovative multi-modal transformer\ntested on a large dataset from 2016-2019 covering 2,200 hectares and eight\ngrape cultivars including more than 5 million vines, outperforms traditional\nmodels like UNet-ConvLSTM, excelling in spatial variability capture and yield\nprediction, particularly for extreme values in vineyards. CMAViT achieved an R2\nof 0.84 and a MAPE of 8.22% on an unseen test dataset. Masking specific\nmodalities lowered performance: excluding management practices, climate data,\nand both reduced R2 to 0.73, 0.70, and 0.72, respectively, and raised MAPE to\n11.92%, 12.66%, and 12.39%, highlighting each modality's importance for\naccurate yield prediction. Code is available at\nhttps://github.com/plant-ai-biophysics-lab/CMAViT.", "arxiv_id": "2411.16989v1", "html_link": "https://arxiv.org/html/2411.16989v1", "search_term": "ti:\"vision transformers\"", "html_content": "SECTION: CMAViT: Integrating Climate, Management, and Remote Sensing Data for Crop Yield Estimation with Multimodel Vision Transformers\n\nCrop yield prediction is essential for agricultural planning but remains challenging due to the complex interactions between weather, climate, and management practices. To address these challenges, we introduce a deep learning-based multi-model called Climate-Management Aware Vision Transformer (CMAViT), designed for pixel-level vineyard yield predictions. CMAViT integrates both spatial and temporal data by leveraging remote sensing imagery and short-term meteorological data, capturing the effects of growing season variations. Additionally, it incorporates management practices, which are represented in text form, using a cross-attention encoder to model their interaction with time-series data. This innovative multi-modal transformer tested on a large dataset from 2016-2019 covering 2,200 hectares and eight grape cultivars including more than 5 million vines, outperforms traditional models like UNet-ConvLSTM, excelling in spatial variability capture and yield prediction, particularly for extreme values in vineyards. CMAViT achieved an R\u00b2 of 0.84 and a MAPE of 8.22% on an unseen test dataset. Masking specific modalities lowered performance: excluding management practices, climate data, and both reduced R\u00b2 to 0.73, 0.70, and 0.72, respectively, and raised MAPE to 11.92%, 12.66%, and 12.39%, highlighting each modality\u2019s importance for accurate yield prediction. Code is available athttps://github.com/plant-ai-biophysics-lab/CMAViT.\n\nKeywordsMultimodel Vision Transformer, Crop Yield Estimation, Remote Sensing Imagery, Data Fusion\n\nSECTION: 1Introduction\n\nCrop yield is a fundamental determinant of agricultural profitability, with early-season yield forecasting playing a critical role in guiding agronomic planning decisions, including procurement, sales, labor allocation, and essential management practices such as fertilization, irrigation, pest control, and soil and canopy management. Accurate yield forecasts enable effective planning and resource allocation, while inaccurate estimates can result in significant economic losses, environmental strain, and disruptions to the agricultural supply chain.\n\nYield is influenced by a complex interplay of factors characterized by multivariate, non-linear, temporal, and spatial relationships. These factors include site-specific conditions such as climate, soil properties, topography, microclimate, pest pressures, and management practices, all of which contribute uniquely to yield outcomes(Jiang and Thelen,2004; Ceglar et\u00a0al.,2016; Epule et\u00a0al.,2018; Ortiz-Bobea et\u00a0al.,2019; Chaloner et\u00a0al.,2021). Additionally, plant traits like canopy vigor, stress responses, flowering, and fruit development, observed early in the growing season, serve as indicators of potential yield by reflecting the cumulative effects of these factors(Johnson,2014; Liu et\u00a0al.,2017; Rudolph et\u00a0al.,2019; Bai et\u00a0al.,2019). The inherent non-linearity of these interactions, combined with spatial variability, makes accurate yield prediction highly challenging, particularly given the annual fluctuations driven by genetic, environmental, and management interactions.\n\nHistorically, yield estimation for specialty crops has relied on collecting historical data and adjusting it based on current growing season conditions, such as weather, pest pressure, and specific management actions. Growers commonly sample a small number of plants to measure characteristics like cluster weight and number of clusters, using these observations to extrapolate the overall yield(Dami,2006). While widely used, this approach is labor-intensive, costly, and prone to inaccuracies due to the reliance on limited sample sizes that may not adequately represent field conditions. In contrast, modern research has increasingly focused on employing remote sensing and ground-based imagery to enhance yield forecasting accuracy through automation and high-resolution monitoring(Malik et\u00a0al.,2016; Gandhi et\u00a0al.,2016; Sun et\u00a0al.,2017; Di\u00a0Gennaro et\u00a0al.,2019; Liu and Whitty,2019; Santos et\u00a0al.,2020; Liu et\u00a0al.,2020; Wei and Molin,2020; Panek and Gozdowski,2021; Falco et\u00a0al.,2021; Cheng et\u00a0al.,2022; Kamangir et\u00a0al.,2024). Satellite and UAV (unmanned aerial vehicle) imagery have significantly advanced yield prediction by enabling frequent, detailed data collection across large areas, providing critical insights into crop growth and health(Johnson,2014; Liu et\u00a0al.,2017; Rudolph et\u00a0al.,2019; Bai et\u00a0al.,2019). However, while remote sensing provides valuable top-of-canopy information, it cannot fully capture the intricacies of yield determinants, such as soil properties, pest pressures, and management practices, which necessitates integrating complementary data sources for a more comprehensive understanding(Jiang and Thelen,2004; Ceglar et\u00a0al.,2016; Epule et\u00a0al.,2018; Ortiz-Bobea et\u00a0al.,2019; Chaloner et\u00a0al.,2021). Combining remote sensing data with additional information\u2014such as climate, weather patterns, and site-specific management\u2014allows for a more holistic approach to yield forecasting, ultimately enhancing prediction accuracy and supporting sustainable agricultural practices.\n\nYield prediction models generally fall into two main categories: mechanistic and data-driven approaches, each with distinct advantages and challenges(Kamangir et\u00a0al.,2024). Mechanistic models explicitly incorporate plant structural traits, environmental variables, and management practices, providing a means to model linear and non-linear relationships over space and time(Kamangir et\u00a0al.,2024). These models can effectively represent complex interactions; however, they require extensive empirical calibration and often rely on predefined assumptions about variable relationships, which may not apply universally across different crops or growing conditions(Keating et\u00a0al.,2003). This limitation is particularly evident for specialty crops, where calibration and validation data are often scarce. In contrast, data-driven models, particularly those leveraging deep learning, offer a more scalable approach by learning directly from large datasets without relying on predefined assumptions. Data-driven models, such as those based on deep neural networks (DNNs), random forests, and support vector machines (SVMs), can capture complex spatial, temporal, and non-linear relationships within agricultural datasets(Khaki et\u00a0al.,2020,2021; Kamangir et\u00a0al.,2024). Hybrid approaches that integrate data-driven models with mechanistic models offer an even more powerful solution, combining the interpretability of mechanistic models with the adaptability of deep learning techniques, thereby improving the overall accuracy of yield prediction(Deines et\u00a0al.,2021).\n\nThe application of machine learning, particularly deep learning, in agricultural yield prediction has shown substantial promise for capturing complex temporal, spatial, and spectral dependencies in the data. Conventional models, such as linear regression, have been used extensively to relate vegetation indices like NDVI to yield outcomes but have struggled to fully account for the non-linear and spatio-temporal relationships present in agricultural systems(Sun et\u00a0al.,2017). In response, more sophisticated machine learning methods\u2014including random forests, SVMs, and deep learning approaches such as convolutional neural networks (CNNs), graph neural networks (GNNs), and Transformers have been developed and deployed to improve yield forecasting(LeCun et\u00a0al.,2015; Aneja et\u00a0al.,2018; Dosovitskiy et\u00a0al.,2020; Khaki et\u00a0al.,2021; Tseng et\u00a0al.,2021; Fan et\u00a0al.,2022; Cheng et\u00a0al.,2022; Lin et\u00a0al.,2023; Kamangir et\u00a0al.,2024). While these models have demonstrated significant improvements, conventional CNN architectures still face challenges in integrating heterogeneous data sources\u2014such as remote sensing imagery, climate data, and management practices\u2014due to their rigid structures. These limitations have driven the development of hybrid models that incorporate subsets of available data sources, but they often lack a holistic representation of the diverse yield-influencing factors(M\u00e5l\u00f8y et\u00a0al.,2021; Zhou et\u00a0al.,2021; Kamangir et\u00a0al.,2024). This gap underscores the need for a comprehensive multimodal approach capable of integrating multiple structured and unstructured data types to further improve yield prediction accuracy.\n\nIn this study, we propose a novel multimodal deep learning model for vineyard crop yield prediction at the field level, integrating various data types to enhance prediction accuracy. Our model incorporates Sentinel-1 and Sentinel-2 satellite data, capturing detailed imagery to provide insights into vegetation health, growth dynamics, and phenological development. Additionally, we integrate climate data from the DayMet dataset, encompassing variables such as minimum and maximum temperatures, precipitation, and vapor pressure, to accurately account for the effects of climatic variability on crop growth. We also include management and soil information from text-based reports, such as soil sampling results and descriptions of key management practices, allowing for a more comprehensive understanding of site-specific conditions. By combining these diverse data sources, our model aims to capture the complex interactions among environmental factors, management practices, and plant responses, thereby providing field-specific, accurate yield forecasts. This holistic approach is designed to support informed decision-making in vineyard management, enhance productivity, and promote sustainable agricultural practices.\n\nSECTION: 2RELATED WORKS\n\nSECTION: 2.1Single-Modality Agricultural Crop Yield Prediction\n\nMany machine learning and deep learning models have been developed for crop yield prediction, typically based on a single modality, such as satellite imagery, ground-based imagery, or meteorological observations. The most common machine learning models used for crop yield forecasting include Random Forest(Everingham et\u00a0al.,2016; Prasad et\u00a0al.,2021; Joshi et\u00a0al.,2023; Kuradusenge et\u00a0al.,2023; Bharadiya et\u00a0al.,2023; Amankulova et\u00a0al.,2023; Panigrahi et\u00a0al.,2023), SVM(Liu and Whitty,2019; Fegade and Pawar,2020). However, these models often suffer from several limitations, including the need for extensive feature engineering, insufficient ability to capture non-linear relationships, and a lack of spatial correlation representation across fields and crop areas compared to more sophisticated deep learning architectures.\n\nTo overcome these limitations, deep learning models with layers of non-linear activation functions and specialized modules have been developed to better learn the spatial and temporal correlations and the complex interactions between predictors and yield outcomes. Models like Convolutional Neural Networks (CNNs)(Khaki et\u00a0al.,2021), Graph Neural Networks (GNNs)(Fan et\u00a0al.,2022), Long Short-Term Memory networks (LSTMs)(Kamangir et\u00a0al.,2024), and Vision Transformers (ViTs)(Dosovitskiy et\u00a0al.,2021)have emerged as effective tools for capturing both spatial and temporal relationships in agricultural data. These models have been successfully applied to predict crop yield at various scales, including field-level, county-level, and global-level. Their ability to learn non-linear features and model interactions between input variables has significantly improved yield prediction accuracy compared to traditional machine learning models.\n\nIn recent years, researchers have also explored the use of hybrid models that combine different deep learning architectures to further enhance yield prediction. These hybrid approaches leverage the strengths of multiple model types to better capture the diverse aspects of agricultural data. For example, combining CNNs with LSTMs allows for both spatial feature extraction and temporal sequence learning, making these models particularly well-suited for handling spatio-temporal data like satellite imagery over time(Kamangir et\u00a0al.,2024). Similarly, the use of GNNs can help capture complex relationships between spatially connected regions, while Transformers provide flexibility in handling various input types and improving feature integration across different data sources(Khaki et\u00a0al.,2021).\n\nThese advancements underscore the importance of moving beyond single-modality models to multimodal and hybrid approaches, enabling more accurate and reliable crop yield forecasting that can support informed decision-making in agriculture. Kamangir et al., 2024(Kamangir et\u00a0al.,2024)developed the UNet-ConvLSTM model specifically for predicting spatio-temporal vineyard yields using solely Sentinel-2 imagery. This innovative model incorporates variables related to management practices, such as cultivar type, trellis type, spacing between rows, and canopy management, allowing it to learn and adapt to specific management strategies within each cultivar and field block. Despite its advancements, this model does not integrate weather data, which is a significant limitation given the impact of climatic conditions on crop yields. Additionally, the management practices are tailored uniquely to each cultivar type, and the model does not incorporate further details about variations within individual blocks, which could provide deeper insights into localized yield determinants.\n\nSECTION: 2.2Multimodel Agricultural Crop Yield Prediction\n\nThe world we live in is inherently multimodal, influencing both our observations and behaviors. Multimodal learning (MML) has been a significant research area in recent decades, with early applications like audio-visual speech recognition studied in the 1980s(Yuhas et\u00a0al.,1989). Furthermore, in the era of deep learning neural networks, transformers(Vaswani et\u00a0al.,2017), as a powerful and versatile architecture, present new challenges and opportunities to MML. The recent achievements of large language models and their multimodal variants highlight the potential of transformers, including vision transformers, in learning from diverse data sources.\n\nRecent research has focused extensively on multimodal learning approaches for agricultural crop yield forecasting, leveraging diverse data sources to improve prediction accuracy(Maimaitijiang et\u00a0al.,2020; M\u00e5l\u00f8y et\u00a0al.,2021; Kaur et\u00a0al.,2022; Mia et\u00a0al.,2023; Ramzan et\u00a0al.,2023; Lin et\u00a0al.,2023). These studies explore various combinations of data modalities, such as satellite imagery, weather information, soil data, and even genetic data, utilizing a variety of machine learning and deep learning techniques. Most of the recent efforts have centered on combining multiple sources of remote sensing data with different spatial resolutions(Ramzan et\u00a0al.,2023), integrating satellite and weather data(Mia et\u00a0al.,2023; Lin et\u00a0al.,2023), and combining satellite, weather, and in-depth soil information using multimodal models like CNN-based architectures and Vision Transformers(Kaur et\u00a0al.,2022). These advancements in multimodal fusion aim to enhance the predictive capabilities of crop yield models by capturing the complex interplay of environmental, biological, and management factors.\n\nThe study by(Maimaitijiang et\u00a0al.,2020)demonstrates the effectiveness of integrating UAV-based multispectral, thermal, and RGB imagery for predicting soybean yield using deep learning models. The research highlights that by combining spectral, structural, thermal, and texture features, prediction accuracy can be significantly improved. The study explored several regression techniques, including Partial Least Squares Regression (PLSR), Random Forest Regression (RFR), Support Vector Regression (SVR), and two deep neural network-based fusion models (DNN-F1 and DNN-F2). The deep learning model DNN-F2 outperformed others, achieving an R\u00b2 of 0.720 and a relative RMSE of 15.9%, indicating that multimodal fusion reduces the sensitivity to saturation effects commonly faced by remote sensing data in dense crop canopies. This work demonstrates that multimodal data fusion using UAVs is a viable approach for high-resolution, field-specific yield prediction, supporting precision agriculture and high-throughput plant phenotyping.\n\nMaaloy et al, 2021(M\u00e5l\u00f8y et\u00a0al.,2021)introduces an innovative approach using transformer-based models known as Performers for predicting barley yield. This model integrates single nucleotide polymorphism (SNP) genotype data with weather data to capture the intricate relationships between genotype and environmental factors. Traditional genomic selection (GS) methods, such as Bayesian models or classical neural networks, often face challenges when modeling these complex interactions. However, the Performer model utilizes a self-attention mechanism, which requires fewer assumptions and is well-suited to learning from multimodal inputs. Despite these advances, the model\u2019s capabilities are limited to two types of data, restricting its ability to integrate other essential data sources like soil and management practices.\n\nIn a further advancement,(Kaur et\u00a0al.,2022)developed CropYieldNet, a multimodal deep learning model designed to incorporate spatiotemporal meteorological data, surface reflectance bands from satellite imagery, and depth-dependent soil information for predicting crop yield. CropYieldNet utilizes Convolutional Neural Networks (CNNs) to extract spatial features from reflectance data, combined with Long Short-Term Memory (LSTM) networks for temporal sequence learning. The introduction of a depth-level selection module significantly improved the model\u2019s performance, allowing it to effectively account for soil attributes, which are crucial for understanding crop growth dynamics. The model achieved up to a 7.8% improvement in predictive accuracy compared to existing models, highlighting the value of integrating multiple modalities to capture the complexities inherent in crop yield prediction.\n\nThe research by(Mia et\u00a0al.,2023)takes a similar approach, focusing on integrating UAV-based multispectral imagery with weather data to predict rice yield, particularly in regions dominated by smallholder farming. This multimodal deep learning model effectively combined spatial data from UAV imagery at the heading stage with temporal weather data to enhance prediction accuracy. The study evaluated different CNN architectures, including AlexNet and a simpler two-layer CNN, and found that while prediction accuracy was similar for both architectures, the simpler model was computationally more efficient. A key finding was that adding an additional layer after concatenating spatial and temporal features improved prediction accuracy, although the robustness of the model still requires further evaluation.\n\nIn another example of multimodal learning,(Ramzan et\u00a0al.,2023)investigated tea yield estimation in Pakistan by combining agrometeorological data with remote sensing imagery from Landsat-8. The proposed methodology used features such as NDVI from multispectral imagery, along with agrometeorological parameters, to train various machine learning and deep learning models. The best-performing model, a deep neural network (DNN) constructed using Bayesian optimization, achieved an impressive R\u00b2 of 0.99 and demonstrated lower prediction errors compared to traditional machine learning algorithms like Decision Trees, Support Vector Regression (SVR), and Random Forest. This study underscores the potential of integrating agrometeorological and remote sensing data to accurately capture non-linear relationships and complex interactions in crop yield prediction.\nMost recently,(Lin et\u00a0al.,2023)proposed a multimodal approach that combines remote sensing imagery with meteorological data for crop yield estimation using a Vision Transformer model pre-trained on long-term weather data. This climate-aware vision transformer achieved higher performance compared to benchmark models, effectively leveraging both spatial and temporal features.\n\nOverall, these studies collectively demonstrate the progress in multimodal learning for crop yield prediction, with significant advancements in integrating diverse data sources to capture the complexity of agricultural environments. Earlier models often focused on single modalities\u2014whether satellite imagery, UAV-based data, or weather information\u2014limiting their ability to fully understand yield determinants. By combining different types of spatial and temporal data, as well as incorporating additional data such as soil properties, management practices, and genetic information, recent models have shown significant improvements in accuracy and generalizability. Deep learning techniques like CNNs, LSTMs, and Vision Transformers have played a pivotal role in this evolution, enabling models to effectively learn the interactions among multiple predictors.\n\nHowever, there remain several challenges in yield prediction research. The need for more comprehensive multimodal models that integrate structured and unstructured data\u2014such as weather patterns, soil data, management practices, and satellite imagery\u2014is becoming increasingly apparent. To address this gap, the authors propose developing a spatio-temporal multimodal vision transformer capable of incorporating various data types, such as images, weather data, and text-based management reports. Such a model could process input data temporally, offering more comprehensive predictions throughout the growing season.\n\nSECTION: 3PROPOSED METHOD\n\nThis work aims to develop a model based on vision transformers for predicting vineyard yields at the field level through time-series analysis. The architecture shown in Figure2is threefold: First, the Spatio-Temporal Multimodel (STMM) Vision Transformers capture the spatio-temporal signals from satellite imagery and integrate these with time-series climate conditions for enhanced lead-time predictions. Second, a self-attention encoder processes text-based input data, such as soil health and management practices, using a self-attention mechanism to encode pertinent information. Third, a temporal cross-attention mechanism employs cross-attention to assess the influence of the encoded text data on the spatial-temporal features for each prediction. This integrated approach leverages the strengths of vision transformers to provide precise, contextually aware yield forecasts.\n\nCapturing Crop Growth Responses to Weather Variation During the Growing SeasonThe STMM module (Figure3) is designed to capture the impact of weather variation on crop growth by utilizing both visual remote sensing and numerical meteorological data. This module employs a sophisticated multi-head attention technique that systematically combines these two types of data. Specifically, the model integrates meteorological signals with satellite imagery in an accumulative manner, enhancing its ability to predict crop responses for each lead time.\n\nBy doing so, it ensures that the temporal dynamics of weather conditions are effectively reflected in the spatial analysis provided by satellite observations. This integrated approach would improves the accuracy of predictions and provides deeper insights into how different weather patterns influence crop growth over time.\n\nHere,is the visual representation encoded by a linear projection, withindicating the temporal observation andthe number of image patches.represents the meteorological parameters after the linear projection, withindicating the number of meteorological values. Notably,,, andare learnable projection matrices. Specifically, for satellite imagery, we denote the data as, whererepresents the number of temporal observations, andrepresents the number of channels. In this experiment, the satellite images are composites of Sentinel-1 and Sentinel-2, incorporating RGB and NIR channels from Sentinel-2 and VV and HV channels from Sentinel-1, totaling six channels. The embedding patches for satellite imagery is:\n\nWhereis the temporal embedding.\nFor the numerical weather data, we denote the data asthe temporal observations align with those of the satellite imagery, andis four, representing Tmin, Tmax, Prcp, and VP.\n\nCapturing Soil Health and Management Practices Information\n\nThis study employs a vanilla self-attention encoder to effectively learn from text-based information regarding soil health and management practices using pre-trained GPT-3.5 tokenizer(OpenAI,2023). By harnessing this encoder, we aim to transform complex textual data\u2014ranging from soil composition reports to detailed management logs\u2014into a structured format that can be seamlessly integrated into larger agricultural models. This approach not only simplifies the processing of extensive textual data but also ensures that critical information influencing agricultural outcomes is accurately captured and utilized, thus enhancing the overall understanding and strategic planning of soil health management.\n\nCapturing Soil Health and Management Practices Impacts on Crop Growth During the Growing SeasonIn this research, we utilize a cross-attention module to analyze time-series outputs for predicting crop growth impacts during the growing season, considering variations in soil health and management practices. This module allows for a dynamic assessment of how different soil and management variables interact over time, focusing particularly on their temporal influence on crop development. By integrating cross-attention mechanisms, the model adeptly correlates fluctuating soil conditions and management actions with plant growth patterns, providing nuanced insights into the temporal dynamics that drive agricultural productivity throughout the season.\n\nHere,is the visual representation encoded by a linear projection, withindicating the temporal observation andthe number of image patches.represents the meteorological parameters after the linear projection, withindicating the number of meteorological values. Notably,,, andare learnable projection matrices.\n\nSECTION: 4EXPERIMENTS AND DISCUSSION\n\nSECTION: 4.1Datasets\n\nIn this study, we leverage E and J Gallo Winery\u2019s ongoing efforts to collect high-resolution (1-3 m) yield data via monitors on harvesters (Advanced Technology Viticulture, Joslin, South Australia, Australia) in California\u2019s Central Valley (Figure4). Spanning 4 years (2016-2019), the dataset encompasses 7x9 km, covering 2,200 hectares, over 5 million grapevine plants, and 15 cultivars. This extensive ground-truth data allows for robust validation of yield estimation models, utilizing deep learning to decipher complex interactions between inputs and yield.\n\nWe correlated the yield observation dataset with time-series imagery from the Sentinel-2A, Sentinel-2B, and Sentinel-1 satellites(Sentinel,2), accessed through Google Earth Engine (GEE)(Gorelick et\u00a0al.,2017). The selected images were captured on dates that were closely aligned in terms of the day of the year. Following the launch of Sentinel-2B, the temporal resolution of Sentinel-2 imagery improved to a 5-day revisit cycle. We chose weekly images (ranging from 5 to 8 days apart) with less than 10% cloud cover, spanning from April to mid-July for the studied years. Early Sentinel-2 images were in TOA (Top-of-Atmosphere) reflectance, whereas later images were in BOA (Bottom-of-Atmosphere) reflectance, requiring atmospheric correction using Sen2Cor, based on the LIBRADTRAN model(Mayer and Kylling,2005). For Sentinel-1 imagery, VV and VH channel images were denoised using non-local means filters(Buades et\u00a0al.,2005), which identify similar patches within the dataset to optimally suppress speckle noise without sacrificing image resolution.\n\nManagement practices throughout the growing season, including details such as cultivar type, trellis configuration, canopy and row spacing, soil texture, and various chemical properties, have been documented in text format. These chemical properties encompass pH, sodium, magnesium, and other key soil nutrients and elements essential for vine growth. Such comprehensive data provides valuable context regarding vineyard management, facilitating a deeper understanding of how different factors, such as soil composition and cultivation techniques, influence crop yield and health. This text-based dataset ensures that critical agronomic information is preserved for further analysis and interpretation.\n\nAs demonstrated, climate data significantly impacts the understanding of interactions between environmental changes and vineyard crop growth. In this experiment, weekly climate data was collected on the exact same days as the satellite imagery, focusing on four critical observations: minimum temperature (Tmin), maximum temperature (Tmax), precipitation (prcp), and vapor pressure (VP, in kPa). These data were sourced from the DayMet dataset(Thornton et\u00a0al.,2022), which provides a spatial resolution of 1 km. Since each vineyard field is smaller than 1 km by 1 km, a single value was assigned to represent each field for every observation period. This consistent and synchronized collection of climate variables enables a more precise analysis of how weather conditions affect vineyard growth dynamics in tandem with satellite-based observations.\n\nData preprocessing for the deep learning model involved several crucial steps to ensure data quality and suitability for model training. First, images with more than 10% cloud cover were filtered out using the s2cloudless dataset. From an initial set of 15 cultivars, we excluded those that had fewer than two blocks or less than three years of data. After this refinement process, the final dataset included 41 blocks representing 8 cultivars across 4 years. The selected cultivars were Cabernet Sauvignon (CS), Chardonnay (Ch), Malvasia Bianca (MB), Merlot (Me), Muscat of Alexandria (MoA), Riesling (Ries), Symphony (Sym), and Syrah (Syr). This curation process ensured that the dataset was representative and adequately structured for effective deep learning model training, focusing on consistent spatial and temporal characteristics across the selected cultivars.\n\nSECTION: 4.2Validation Scenario and Evaluation Metrics\n\nTo evaluate models generalization across new data blocks with varying cultivar types and management practices, we implemented validation scenario called block-hold-out (BHO), depicted in Figure5. The scenario evaluates the model\u2019s ability to predict yields for blocks that were not part of the training set.\n\nThis approach assesses how well the model generalizes to new blocks without historical yield data. For instance, in the MB cultivar type, one block was used for training, another for validation, and a third for testing. Similarly, for the Ch cultivar type, three out of nine blocks were assigned to each phase. The lower portion of the figure presents the boxplot distribution of historical yield data, showing yield variability within the same cultivar across different blocks, as well as between cultivars. This emphasizes the challenge of yield prediction due to the inherent variability in agriculture.\n\nTo evaluate model performance, we employed four metrics, each highlighting different aspects of efficacy. The coefficient of determination () measures how well the model explains variance in the data, with higher values indicating better fit, although it can be susceptible to overfitting. Root Mean Square Error (RMSE) captures prediction accuracy by penalizing larger errors, making it useful for detecting major deviations but sensitive to outliers. Mean Absolute Error (MAE) offers a straightforward average of errors, treating all equally and being less influenced by outliers. Mean Absolute Percentage Error (MAPE) provides a relative measure of accuracy in percentage terms, which is helpful for comparing performance across different scales but can be unreliable when values are near zero. These metrics collectively ensure a thorough assessment, covering model fit, error magnitude, and consistency across varying conditions.\n\nSECTION: 4.3Implementation Details\n\nIn this experiment, the images were cropped into patches of 16 by 16 pixels to account for spatial autocorrelation within the vineyard field. This resulted in data with dimensions of (T, C, H, W), where T represents time (15 time points), and C represents the number of channels\u20144 for Sentinel-2, 2 for Sentinel-1 imagery and 1 for time as an encoded day of the year of captured images using the sine transformation, given by\n\nto capture the cyclical nature of the growing season. Climate data was structured as (T, C, 1), with T also equal to 15 and C representing four climate variables.\n\nThe experiments were all implemented with PyTorch on a single NVIDIA TITAN RTX GPU with 24 GB RAM. The models are trained over 300 epochs, with early stopping implemented at 50 epochs, using the AdamW optimizer(Loshchilov,2017). We set the optimizer\u2019s parameters as,, with a weight decay of 0.01 and an initial learning rate of 1e-4. The architecture includes a self-attention module composed of 8 attention heads and 6 layers, featuring an embedding size of 768. The MLP (multi-layer perceptron) module has a hidden layer size of 2048, and a dropout rate of 30% is applied to prevent overfitting. The context embedding also have a size of 768, and text tokenization is performed using pretrained GPT-3.5(OpenAI,2023), which enhances the model\u2019s ability to process textual input effectively. These configurations are designed to balance model capacity with regularization to achieve robust performance over time.\n\nSECTION: 4.4Performance Comparison\n\nThis section presents the performance of the CMAViT model compared to the UNet-ConvLSTM model developed by(Kamangir et\u00a0al.,2024)across the entire dataset, time-series prediction, learning of extreme values, and spatio-temporal accuracy.\n\nTableLABEL:table_ch3_allpresents the performance of three categories of models across various input configurations for UNet-ConvLSTM and CMAViT. The first category is the UNet-ConvLSTM model developed by(Kamangir et\u00a0al.,2024), the second pertains to the CMAViT model, and the third category compares these two models using the YieldZone strategy developed by(Kamangir et\u00a0al.,). The input configurations include the following categories: [A] Sentinel-2 imagery with management context; [B] Sentinel-2 imagery, and management context with CSR; [C] Sentinel-1 and Sentinel-2 imagery, with management context; [D] Sentinel-1, Sentinel-2, climate data, and management context; and [E] Sentinel-1, Sentinel-2, climate data, management context, and CSR.\n\nThe results presented in TableLABEL:table_ch3_allindicate a modest improvement in the performance of the CMAViT model compared to the CNN-based UNet-ConvLSTM model across all tested configurations. For example, the R\u00b2 value improved from 0.65 to 0.69, and the MAPE decreased from 12.72% to 12.61% for the UNet-ConvLSTM and CMAViT models, respectively, based on the data configuration A and unseen test dataset. Additionally, incorporating the CSR strategy (category B) led to a further reduction in MAPE, from 12.37% to 11.00% for test dataset.\n\nThe inclusion of Sentinel-1 data (category C) further enhanced the model performance for both architectures, with CMAViT showing superior results: an of 0.74 compared to 0.71 for UNet-ConvLSTM, and a MAPE of 11.48% versus 12.57% for unseen test dataset. The CMAViT model, when provided with the full input data including meteorological observations (category D), outperformed all other configurations, achieving an R\u00b2 of 0.80 and a MAPE of 10.19%. This highlights the significant impact of incorporating meteorological and climate data for improved crop yield prediction. The category E for full input observation alongside with CSR strategy improved the results for R\u00b2 as 0.84 and MAPE 8.22% for the unseen test dataset.\n\nThe CMAViT model outperforms the UNet-ConvLSTM model in this context, and it also offers a more straightforward strategy for integrating diverse data structures, enhancing its ability to accurately predict yield values. Specifically, CMAViT effectively combines vector-based meteorological data, raster satellite imagery, and text-based management practice reports in this case study, demonstrating its versatility in handling complex datasets. The model\u2019s design is inherently open to incorporating larger and more intricate data inputs, making it highly adaptable for different agricultural applications.\n\nOn the other hand, while the UNet-ConvLSTM model also integrates satellite imagery with management information, it is limited by the number of categorical data points it can encode. In the UNet-ConvLSTM implementation, only four management features\u2014cultivar type, trellis type, canopy space, and row space\u2014are utilized, which restricts the richness of information that can be modeled. Conversely, CMAViT does not impose such limitations, as it allows the integration of management information in text format without predefined constraints, enabling the model to benefit from a broader and more detailed representation of farming practices. This flexibility in combining a variety of data types positions CMAViT as a more effective tool for yield prediction, especially in environments requiring comprehensive data integration.\n\nThe third category of results involves models based on the YieldZone strategy(Kamangir et\u00a0al.,), which demonstrates a significant improvement in learning extreme values and addressing the limitations of the MSE loss function, which tends to overemphasize learning from the majority of pixels. The YieldZone strategy initially segments the yield map into distinct categories based on three fundamental yield ranges\u2014low-extreme, common, and high-extreme. However, the number of classes can vary depending on the specific task, data, and crop types. The output from this initial step is referred to as the \"Yield Zone Map.\" The second step involves performing an element-wise multiplication of the Yield Zone Map with the input image data, which in this case is satellite images. However, this process is only deployed through training process not inference as it has been explained in Algorithm1.\n\nAlgorithm1illustrates the entire process of training based on the YieldZone strategy. Phase 1, described as Algorithm1, involves conditional training of the crop yield prediction model utilizing the YieldZone strategy. This phase processes batches of Sentinel-2 images (denoted as X) and classifies crop yields (denoted as Y) into a distinct number of classes (denoted as YZ). The model then performs element-wise multiplication of the X and\nYZ to produce Z, on which the model M is trained to minimize the loss. This conditional training process integrates spatial and class-specific information, optimizing the model specifically for varied yield zones. Phase 2 is the inference process, where the model input is solely the input image.\n\nBy applying the YieldZone strategy, the MAPE for the CMAViT model decreased significantly to 4.45%, indicating a substantial enhancement in the model\u2019s ability to accurately predict yield across different yield zones, particularly in cases involving extreme variations.\n\nThe CMAViT model is designed to predict the time series of yield from early April, which corresponds to the flowering period, until mid-July, known as the veraison stage\u2014a critical phase approximately one month before harvest. Based on the results presented in Figure6, all models exhibit a gradual improvement in prediction accuracy from week 1 through week 15, which aligns with the approach of the harvesting period. This trend suggests that as the growing season progresses, the models gain access to increasingly informative data, enabling them to better capture the complexities associated with yield prediction.\n\nThe improvement observed over the weeks underscores the importance of continuous data collection throughout the growing season. As plants progress through their phenological stages, the accumulation of data\u2014such as changes in satellite imagery, updated meteorological inputs, and ongoing management practices\u2014provides the model with richer and more representative information. This incremental data availability allows the model to refine its understanding of yield determinants, capturing dynamic growth factors and environmental influences more accurately. Consequently, this leads to enhanced predictive performance as the crop approaches maturity, where the variability in yield becomes more evident and can be modeled with greater precision.\n\nThe results presented in Figure6confirm the superior performance of CMAViT and CMAViT-CSR models throughout the prediction timeline, particularly in the final weeks leading up to the harvest. This indicates that the CMAViT models are more effective in capturing the underlying temporal dynamics as the harvest approaches. The changes observed over time are notably more pronounced for the CMAViT models compared to those based on the UNet-ConvLSTM architecture, suggesting a stronger capability of the CMAViT models in adapting to time-varying features and complexities inherent in agricultural processes.\n\nMoreover, the results emphasize the improvements achieved by applying the yield zone strategy for both CMAViT and UNet-ConvLSTM models, which contributes to enhanced predictive accuracy. By incorporating the yield zone approach, the models effectively leverage spatial heterogeneity, leading to better representation of yield variations across different regions. This strategy appears to have a particularly positive impact on the CMAViT-based models, highlighting their suitability for addressing spatially complex tasks in agricultural forecasting.\n\nTableLABEL:table:ch3_exresultspresents the results for the UNet-ConvLSTM and CMAViT-based models, both with and without the cost-sensitive resampling (CSR) and YieldZone (YZ) strategies over the entire range of yield values, only Low Extreme Range (LER, values less than 22 t/ha), Common Range (CR, between 22 and 54 t/ha) and High Extreme Range (HER, value higher than 54 t/ha). Overall, the CMAViT models demonstrate superior performance over the UNet-ConvLSTM models for predicting both low and high extreme values, based on MAE and MAPE evaluation metrics. Specifically, for models that utilize Sentinel-2 and management practices data (category A), the MAE across all samples improves from 4.50 to 3.99 t/ha, while MAPE decreases from 13.87% to 12.61%. In the low extreme range (LER), MAE is reduced from 5.68 t/ha to 5.01 t/ha, and MAPE improves from 29.98% to 26.59%. Similarly, for the high extreme range (HER), MAE significantly decreases from 17.53 t/ha to 9.02 t/ha, and MAPE is reduced from 28.98% to 14.76%.\n\nFurthermore, comparing the full CMAViT model (category E) with CSR to the UNet-ConvLSTM model with CSR, there is a notable improvement in MAPE for the LER, reducing from 12.37% to 8.22%, and for the HER, reducing from 15.13% to 7.42%. These results clearly indicate that the importance of additional input resources including sentinel-1, climate data and unlimited management practice information. However, the models incorporating the YZ strategy outperformed all other models. The MAPE for both UNet-ConvLSTM and CMAViT models with the YZ strategy was below 5% for all samples, indicating high overall accuracy. Specifically, for the CMAViT model, the MAPE for the LER was 8.78%, and for the HER it was 9.90%. These results highlight the effectiveness of the YZ strategy in enhancing model performance, particularly in reducing errors across the entire yield distribution, including the challenging low and high extreme values. This improvement demonstrates the capability of the YZ strategy to account for spatial heterogeneity, leading to more precise and reliable yield predictions.\n\nOverall, the findings demonstrate that additional information such as climate data, and management practices information, alongside with the CSR and YZ strategies help mitigate the bias towards the majority of samples, thereby improving the accuracy of predictions for extreme yield values. The CMAViT-based models, in particular, exhibit greater robustness in handling these challenging predictions compared to the UNet-ConvLSTM-based models, further underlining the effectiveness of incorporating these advanced techniques in agricultural yield prediction tasks.\n\nThis section illustrates the spatio-temporal analysis of the CMAViT model (category E), applied to the MoA-04 block (shown in Figure5) over a 15-week period, beginning April 1st and continuing through mid-July. The analysis was conducted on a test set that was withheld from the training data distribution to ensure an unbiased evaluation of the model\u2019s performance. The results indicate a clear trend of performance improvement as more temporal data is accumulated, with the Mean Absolute Error (MAE) reducing from 1.11 t/ha in week 1 to 1.02 t/ha by week 15, and the Mean Absolute Percentage Error (MAPE) decreasing from 3.23% to 3.09%.\n\nThese findings underscore the benefit of accumulating information throughout the growing season, which enables the model to make more informed predictions by incorporating knowledge of crop growth dynamics over time. As the temporal series progresses, the model gains a deeper understanding of the seasonal changes and crop development stages, allowing for more accurate predictions. This highlights the value of a spatio-temporal approach in precision agriculture, where early-season estimates can be progressively refined as additional data is integrated, ultimately leading to improved prediction accuracy and better-informed management decisions. The trend observed suggests that models benefiting from richer temporal data can more effectively reduce uncertainties, thereby enhancing the reliability of yield forecasts.\n\nSECTION: 4.5Spatial Variability Analysis\n\nFigure8presents a comparative analysis of the efficacy of various CMAViT-based models in accurately predicting extreme yield values and understanding spatial variability within vineyard blocks. The evaluation was conducted on a block named Ries-06 (as yield range depicted in Figure5) over four years of observations. The yield in this vineyard block ranged from 10 to 65 t/ha, covering low-extreme, common, and high-extreme yield values. In 2016, the observed yields were primarily common and low-extreme, whereas 2019 showed a notable increase in high-extreme values.\n\nThe analysis reveals that the CMAViT [A] model struggled to estimate yields at the lower and higher ends of the yield spectrum, especially evident in 2016 when the yields were predominantly low-extreme. The model showed a Mean Absolute Percentage Error (MAPE) of 50.49%, highlighting its tendency to converge toward the mean and thus demonstrating insufficient sensitivity to spatial variability within the blocks. In contrast, the CMAViT [B] model, presented in the third column, displayed a modest improvement in capturing extreme values compared to CMAViT [A], yet it continued to fall short in accurately estimating both extreme values and spatial variability.\n\nFurther improvement was observed when Sentinel-1 data (category C) was incorporated, enhancing the performance of the CMAViT model for all four years of evaluation. The results for CMAViT [D, E] of the figure present results from the CMAViT model with full input data, both with and without the CSR strategy. These models demonstrated significantly better performance in capturing both extreme yield values and spatial variability within the blocks compared to previous models, underscoring the importance of incorporating all available modalities. For instance, in 2016, the CMAViT [E] model was able to accurately capture the low-extreme and common yield values, effectively reflecting the spatial variability pattern observed in the ground truth data. This led to a substantial improvement in prediction accuracy, reducing the Mean Absolute Error (MAE) from 12.85 to 3.37 t/ha and the MAPE from 50.49% to 13.19%. Similarly, in 2019, a year characterized by high-extreme yield values, the model improved its performance significantly, with MAE decreasing from 4.48 to 3.43 t/ha and MAPE dropping from 9.25% to 7.14% compared to the CMAViT model. However, the results for the last row, which include the YieldZone (YZ) strategy, show a significant improvement in capturing spatial variability within the field. The integration of the YZ strategy led to a reduction in MAPE across all years to below 10%, with particularly notable performance in 2017, 2018, and 2019, where the MAPE dropped to below 4%. This demonstrates the model\u2019s enhanced capability to accurately learn and represent spatial variability, effectively capturing yield differences across different sections of the field. The incorporation of the YZ strategy not only improved the overall prediction accuracy but also allowed the model to reflect the spatial variability patterns observed within the field, resulting in a more precise depiction of the yield distribution and providing better insights for targeted management practices.\n\nAcross the years 2017 and 2018, all CMAViT-based models showed consistently superior performance compared to other years. This improvement can be attributed to the implementation of the YieldZone strategy, which enhances the model\u2019s ability to recognize similarities within input features across vineyard blocks, leading to better predictions. Moreover, the incorporation of the \u2019ExtremeWeight\u2019 strategy was crucial in honing the model\u2019s focus on both low and high-extreme values, thereby strengthening its precision in capturing the full spectrum of yield variability. The combination of these strategies and the inclusion of multimodal data highlights the model\u2019s capability to effectively adapt to both spatial and temporal dynamics, ultimately leading to more accurate and reliable yield predictions that can better inform vineyard management decisions.\n\nSECTION: 4.6Modality Maskout Analysis\n\nTableLABEL:table:maskoutpresents the results of a mask-out analysis conducted by selectively removing one of the data modalities from the CMAViT model to assess the impact of each component on the model\u2019s performance. In the first scenario, the management practices context was masked out, resulting in a noticeable degradation in the model\u2019s performance (CMAViT [D]), with the R\u00b2 value dropping from 0.80 to 0.73, and the MAPE increasing from 10.19% to 11.92%. This highlights the role that management practices play in predicting yield, as removing this modality reduced the model\u2019s accuracy. In the second scenario, the climate data modality was excluded. This also led to a decrease in performance, with the R\u00b2 value decreasing from 0.80 to 0.70 and the MAPE increasing from 10.19% to 12.66%. These results emphasize the importance of incorporating climate information, which plays a crucial role in accurately modeling the yield given its influence on crop growth and environmental variability.\n\nFinally, in the scenario where both the management practices and climate data were removed, leaving only the Sentinel-1 and Sentinel-2 satellite data, the model\u2019s performance further declined, with the R\u00b2 dropping to 0.72 and the MAPE rising to 12.39%. This outcome shows a clear reduction in the model\u2019s ability to predict yields effectively without these critical inputs. The results from this mask-out analysis underscore the importance of each data modality. The declines in performance metrics indicate that each modality\u2014whether management practices, climate data, or satellite imagery\u2014contributes unique and essential information that enhances the model\u2019s capability to learn the complexities of yield prediction. The integration of all these modalities is therefore crucial for maximizing prediction accuracy, highlighting the value of a multimodal approach in agricultural modeling.\n\nSECTION: 5CONCLUSION\n\nThis experiment aims to develop a deep learning model to better understand the complex interactions between crop growth, climate change, and management practices using the Climate-Management Aware Vision Transformer (CMAViT) for spatio-temporal vineyard yield estimation. The CMAViT model incorporates a spatio-temporal multimodal approach to capture the dynamics between satellite imagery and climate data throughout the growing season. Additionally, it learns the interactions between time-series signals and management practices. To achieve this, we utilized a large, heterogeneous yield observation dataset spanning four years, 41 blocks, and eight grape cultivars. Based on this dataset, a spatio-temporal deep learning model was developed, which simultaneously learns both spatial and temporal correlations in yield development and provides yield forecasts for each block from April 1st to July 15th.\n\nThe results show that the CMAViT model demonstrates greater flexibility in integrating data from various sources with different structures, allowing it to better capture the complexities of vineyard yield estimation, especially when considering short-term climate variations and management practices. Compared to CNN-based models like UNet-ConvLSTM, CMAViT outperforms significantly, improving the R\u00b2 score from 0.78 to 0.84 and reducing MAPE from 12.57% to 8.22% on unseen test data. Additionally, the YieldZone strategy further enhanced performance for extreme yield ranges, reducing MAPE by 6.97%, 4.10%, and 6.80% for low, common, and high yield extremes, respectively. Overall, CMAViT\u2019s proficiency in capturing spatio-temporal relationships and integrating diverse data sources makes it a powerful tool for precise and timely vineyard yield forecasting, offering significant advantages for agricultural decision-making.\n\nSECTION: 6Acknowledgments\n\nThis project was partly supported by USDA AI Institute for Next\nGeneration Food Systems (AIFS), USDA award number 2020-67021-\n32855.\n\nSECTION: References\n\nSECTION: References", "text_file": "data\\paper_texts\\2411.16989v1_content.txt"}, {"title": "Brain Tumor Classification using Vision Transformer with Selective\n  Cross-Attention Mechanism and Feature Calibration", "authors": ["Mohammad Ali Labbaf Khaniki", "Marzieh Mirzaeibonehkhater", "Mohammad Manthouri", "Elham Hasani"], "published_date": "2024-06-25T15:58:56Z", "summary": "Brain tumor classification is a challenging task in medical image analysis.\nIn this paper, we propose a novel approach to brain tumor classification using\na vision transformer with a novel cross-attention mechanism. Our approach\nleverages the strengths of transformers in modeling long-range dependencies and\nmulti-scale feature fusion. We introduce two new mechanisms to improve the\nperformance of the cross-attention fusion module: Feature Calibration Mechanism\n(FCM) and Selective Cross-Attention (SCA). FCM calibrates the features from\ndifferent branches to make them more compatible, while SCA selectively attends\nto the most informative features. Our experiments demonstrate that the proposed\napproach outperforms other state-of-the-art methods in brain tumor\nclassification, achieving improved accuracy and efficiency. The proposed FCM\nand SCA mechanisms can be easily integrated into other vision transformer\narchitectures, making them a promising direction for future research in medical\nimage analysis. Experimental results confirm that our approach surpasses\nexisting methods, achieving state-of-the-art performance in brain tumor\nclassification tasks.", "arxiv_id": "2406.17670v2", "html_link": "https://arxiv.org/html/2406.17670v2", "search_term": "ti:\"vision transformers\"", "html_error": "Error processing HTML: HTTP Error 404: Not Found"}, {"title": "TORE: Token Recycling in Vision Transformers for Efficient Active Visual\n  Exploration", "authors": ["Jan Olszewski", "Dawid Rymarczyk", "Piotr W\u00f3jcik", "Mateusz Pach", "Bartosz Zieli\u0144ski"], "published_date": "2023-11-26T15:39:57Z", "summary": "Active Visual Exploration (AVE) optimizes the utilization of robotic\nresources in real-world scenarios by sequentially selecting the most\ninformative observations. However, modern methods require a high computational\nbudget due to processing the same observations multiple times through the\nautoencoder transformers.\n  As a remedy, we introduce a novel approach to AVE called TOken REcycling\n(TORE). It divides the encoder into extractor and aggregator components. The\nextractor processes each observation separately, enabling the reuse of tokens\npassed to the aggregator. Moreover, to further reduce the computations, we\ndecrease the decoder to only one block.\n  Through extensive experiments, we demonstrate that TORE outperforms\nstate-of-the-art methods while reducing computational overhead by up to 90\\%.", "arxiv_id": "2311.15335v2", "html_link": "https://arxiv.org/html/2311.15335v2", "search_term": "ti:\"vision transformers\"", "html_content": "SECTION: TORE: Token Recycling in Vision Transformers for Efficient Active Visual Exploration\n\nActive Visual Exploration (AVE) optimizes the utilization of robotic resources in real-world scenarios by sequentially selecting the most informative observations. However, modern methods require a high computational budget due to processing the same observations multiple times through the autoencoder transformers.\nAs a remedy, we introduce a novel approach to AVE called TOken REcycling (TORE). It divides the encoder into extractor and aggregator components. The extractor processes each observation separately, enabling the reuse of tokens passed to the aggregator. Moreover, to further reduce the computations, we decrease the decoder to only one block.\nThrough extensive experiments, we demonstrate that TORE outperforms state-of-the-art methods while reducing computational overhead by up to 90%.\n\nSECTION: 1Introduction\n\nIn Active Visual Exploration (AVE)[22], the agent selects consecutive observations based on a combination of learned policies and environmental cues to optimize information gain and task performance. It is essential in various applications assuming changing environments, such as rescue operations by drones[24].\n\nModern methods of AVE[22,9]utilize Vision Transformers (ViT)[5], which have profoundly reshaped computer vision by delivering efficiency on par with Convolutional Neural Networks (CNN)[16]. The key advantage of ViTs is their ability to handle incomplete inputs, in contrast to CNNs, which require data imputation before further analysis[25]. However, ViTs require a high computational budget[3], which is problematic because AVE usually operates under strict resource constraints. Moreover, methods such as AME[22]and SimGlim[12]execute a full forward pass on all available observations, each time a new observation is collected.\n\nOne possible way to overcome this issue would be to replace ViTs in AVE with their efficient counterparts based on token pruning[30,41]or architecture modifications[16,3]. However, they are not directly applicable to the AVE setup. Token pruning removes uninformative tokens but requires a global glance of the whole scene, which in AVE is yet to be discovered. Meanwhile, the architecture modifications limit the quadratic growth of self-attention computations but require structured and complete observations (full image, see Section 5.3 for more details), making the resulting model unsuitable for AVE purposes.\n\nTo address the computational inefficiency of the modern AVE methods while preserving their ability to process unstructured and incomplete observations, we introduce the TOken REcycling (TORE) method. It divides ViT into two parts:extractorandaggregator(see Figure1). Theextractorprocesses glimpses separately, generatingmidway tokensthat are cached. Theaggregatorcombines themidway tokensand returns a prediction. This way, the results of computations made by theextractorare reused, substantially reducing the computational burden compared to previous AVE methods.\nAdditionally, we advocate for using a lightweight decoder since perfect image reconstruction is unnecessary to achieve satisfactory classification.\n\nBy conducting comprehensive experiments, we demonstrate that TORE significantly cuts computational overhead by up to 90% with improved accuracy compared to the state-of-the-art methods for AVE. Furthermore, employing thorough ablations and analysis, we motivate our architectural decisions and show the trade-off between efficiency and precision. The code is available athttps://github.com/jano1906/TokenRecycling.\n\nOur contributions can be summarized as follows:\n\nWe introduce TORE, an efficient method for Active Visual Exploration, which substantially reduces the computational burden.\n\nWe propose a training strategy with a random sampling of glimpses, increasing the model\u2019s generalizability.\n\nWe conduct extensive experiments on AVE benchmarks showing the accuracy-efficiency trade-off.\n\nSECTION: 2Related Works\n\nAVE involves an agent guiding its view to successively improve internal representations of the observed world[1,10], what can be seen in the context of simultaneous localization and mapping (SLAM)[27]. In this context, the selection strategies for successive observations (exploration) play a crucial role. Multiple approaches are proposed to address this challenge, such as image reconstruction techniques[10,14,34], dedicated exploration models[28,29,12], the utilization of self-attention mechanisms[33], and the analysis of entropy[22]. AVE has also been explored in the context of segmentation[35]and 3D vision[11].\n\nOur work contributes to this landscape by introducing a novel approach dedicated to AVE that recycles computations within a transformer-based architecture.\n\nRecent advancements in computer vision show that the increase in model accuracy is often related to the increase in its computational needs. That is why a lot of research is done to reduce the amount of computations and at the same time preserve the high fidelity of the model. One research branch is focused on efficient training techniques, including continual learning[20,31], domain adaptation[37,38], and fine-tuning[7,17]. Another one on reducing the inference computations of models, such as early exits[23,39], dedicated to a given problem models, e.g., in medical imaging[26], and efficient architectures[32,36].\n\nIn the scope of this work, we are particularly interested in methods assuring efficient ViT inference. Approaches aiming at the reduction of ViT\u2019s computational needs propose token pruning[30,42,41], low-rank factorization[43], limiting self-attention to non-overlapping local windows[18], scaling attention mechanism with sequence length[2], grouping attention layers into local and global information exchange layes[3,16], and replacing fully-connected layers with a star-shaped topology[6].\n\nOur method (TORE) aligns with recent trends aimed at enhancing the computational efficiency of Vision Transformers. Specifically, we modify the ViTs\u2019 forward pass to optimize for Active Visual Exploration because current efficient architectures are not well-suited for processing incomplete and unstructured inputs found in the AVE task.\n\nSECTION: 3Method\n\nTORE aims to reduce the computational budget in the AVE solution based on Vision Transformers (ViT) by reusing tokens. Therefore, to make this paper self-contained, we start this section by recalling ViT and AVE. After that, we describe our method and its training policy.\n\nTransformer modelof depthconsists of:\n\ntokenizer,\n\ntransformer blocks, for,\n\nprediction head.\n\nwhereis an input domain (i.e. set of images),is an output domain (i.e. logits), andis a representation space. For a given, tokenizergenerates a token set that contains[CLS], a special global information token denoted as, and, a set of remaining patch-corresponding tokens.\n\nSuppose that inputconsists ofglimpses which we receive sequentially, ie.and at time, the glimpsesforare available. In such case, we are interested in predictionsrecomputed each time when a new glimpseis selected, for. The consecutive glimpses are selected with aselection policy.\n\nSECTION: 3.1TOken REcycling (TORE)\n\nAs presented in Figure2, we divide the transformerinto two pathsextractorandaggregator:\n\nwhere a parameteris the number of transformer blocks assigned toextractor, whileaggregatoris built from remainingblocks.\nGiven a sample, theextractorproducesmidway tokens, which we denote as. We define the TORE Forward Pass (TORE FP) with parameterat timeas\n\nwhere\n\nTherefore, we apply theextractorto each input glimpse. Then, we combine themidway tokensby averagingtokens and taking the union over glimpse-corresponding tokens. The final output is produced with theaggregator.\n\nTo efficiently compute the Eq.2, we cache outcomes of already calculated forward passes at times. We update the cache as follows:\n\nand then calculate the prediction as:\n\nThis way, the consecutive values ofare computed by processing eachwith theextractoronly once. It is in contrast to ViT\u2019s original forward pass where at each timestepa full collection of inputsis processed by the network.\n\nWe adopt the AME[22]policy to guide the glimpse selection in sequential prediction during inference. To pick glimpseat the time, the policy utilizes an additional transformer-based reconstruction decoder. It is fed with embeddings of previously chosen glimpses, and mask embeddings corresponding to the missing glimpses as in original MAE[8]work. The embeddings are obtained from the last block of theaggregator, before applying the prediction head.\n\nBesides the reconstructed image, the decoder also provides attention mapfrom each headin its last transformer block, used to calculate the entropy mapwith the following formula:\n\nwhereis a row-wise Shannon entropy calculated for attention matrix.\n\nAfter zeroing entries ofcorresponding to the already chosen glimpses, AME selects the new glimpse that has the highest value in. Intuitively, the policy picks the glimpse with the highest reconstruction uncertainty, assuming it is the most informative for the prediction.\n\nContrary to the AME[22], our reconstruction decoder used to calculate the entropy map is tiny. It has only 1 transformer block with 4 heads and 128 hidden dimensions, instead of 8 blocks with 16 heads and 512 hidden dimensions used originally[8,22].\n\nOur training samplecomprisesrandomly picked image glimpses, a whole image, and a label. It is in contrast to AME[22], which uses the entropy map to select the glimpsesat the training time as well as at the inference time. The number of glimpsesis fixed for all samples and set to occupyof the whole image.\n\nFor each training batch ofsuch triplets, we sample the size of theextractorfrom the uniform distribution, as illustrated in Figure3. Next, we perform a forward pass ofto get class predictionand reconstructed image, for. Finally, we compute loss functioncombining cross-entropy classification lossand root-mean-squared error reconstruction loss:\n\nwhereis a weighting factor,denotes-th pixel of imageandis the total number of pixels.\n\nIn consequence, we obtain a single model that reliably works for multiple values ofduring inference, making the method flexible. This property allows us to balance accuracy and efficiency on demand, i.e., by increasing theextractorsize, we can reduce computations but decrease the model\u2019s accuracy.\n\nSECTION: 4Experimental Setup\n\nSECTION: 4.1Implementation Details\n\nWe implement theextractor-aggregatorframework using the transformerbased on the ViT\u2013B architecture consisting ofblocks. The model is initialized with MAE[8]weights pretrained on ImageNet-1k[4]. The originally learned positional encodings are replaced with sinusoidal ones, following implementation introduced in AME[22]. Our lightweight decoder is a downsized modification of the MAE decoder with randomly initialized weights. Both networks usepixel-sized tokens.\n\nAll models are trained for a maximum ofepochs, stopping if validation loss does not improve for consecutiveepochs. We select weights achieving the lowest validation loss. The batch sizeequals. The optimizer of our choice is AdamW[19]with beta values ofand, epsilon of, and weight decay of. The learning rate, dynamically adjusted via the cosine scheduler with a minimum value of, is initialized at. We use the same data augmentations as in AME[22], namely theandwith scale in range.\n\nSECTION: 4.2Active Visual Exploration\n\nWe test our approach on SUN360[40], CIFAR100[15], Food101[13]and Flowers102[21]datasets. SUN360 is a scene recognition dataset with 26 classes, containing both indoor and outdoor images captured with 360\u2218camera, making a suitable evaluation dataset for vision algorithms dedicated to robotics applications. The other considered datasets are object-centric image classification datasets with 100, 101, and 102 classes respectively. We keep nativeresolution for the SUN360 dataset for a fair comparison to the baselines and resize images from the other datasets toresolution. As the SUN360 dataset does not provide a predefined train-test split, we divide the dataset into train-test with a ratio of 9:1 based on image index, following the methodology used in[22,33]. For the other datasets, we use the original train-test split.\n\nDuring training and evaluation, we sample non-overlappingtoken-sized glimpses, which in total cover aboutof an input image (8 glimpses for SUN360 and 12 glimpses for the other datasets). When training, we sample the glimpses uniformly at random. During evaluation, we sample the glimpses according to the chosen policy.\n\nWe follow the evaluation protocol introduced in[22]for all experiments in the AVE setup.\n\nSECTION: 5Results\n\nIn this section, we present the results for two tasks within Active Visual Exploration: image classification and reconstruction. Subsequently, we conduct thorough ablations and analysis of our TORE method.\n\nBefore delving into the metrics for these tasks, we illustrate how AVE is performed at selected timesteps, as depicted in Figure4. It can be observed that the entropy map of the reconstruction decoder guides the model in a way tailored to each sample. As the model observes a larger portion of relevant input, it is more certain of a given prediction.\n\nSECTION: 5.1Classification\n\nTORE exhibits superior performance in the classification task compared to other AVE methods, as demonstrated in Table1. Our approach achieves notable reductions in computations of up to 90% for SUN360128\u00d7256and 80% for the other datasets while maintaining state-of-the-art accuracy. Particularly noteworthy is that our method with a ViT-B backbone outperforms AME utilizing a ViT-L backbone. To ensure a fair comparison, we present the accuracy of AME employing a ViT-B encoder and its original decoder size.\n\nThe incorporation of a lightweight decoder and a random glimpse selection policy during training significantly enhances the classification task\u2019s performance while concurrently reducing the exploration\u2019s computational costs. Additionally, theextractor-aggregatorframework for the ViT backbone in TORE provides flexibility through variousvalues, allowing prioritization of either higher accuracy or improved resource utilization.\n\nSECTION: 5.2Reconstruction\n\nAs our method focuses on the classification task, the reconstruction can be seen solely as an auxiliary task. In Table2, we present the reconstruction RMSE of our method compared to other state-of-the-art approaches. Interestingly, the performance of a model on the reconstruction task within the AVE setup does not necessarily correlate with its classification performance. Despite the significant reduction in reconstruction capabilities due to the use of a lightweight decoder in TORE, our model outperforms the other methods as evidenced in Table1.\nWe include examples of reconstructed images in the Supplement.\n\nSECTION: 5.3Ablations and analysis\n\nFirstly, we examine the analysis of architectural choices and their impact on the model\u2019s performance concerning classification accuracy and computational costs. Subsequently, we explore how randomized choice ofduring training enableseffective and flexible accuracy-efficiency tradeoff. Following this, we assess the model\u2019s effectiveness throughout the exploration process. Then, we investigate how the TORE forward pass influences exploration in AVE. Finally, we compare the efficiency and accuracy ofand EfficientFormer[16], and we provide an analysis of why EfficientFormer cannot be easily incorporated into the AVE setup.\n\nTable3presents the impact of various components of the TORE method on its performance. The results underscore the necessity of all components for achieving the best-performing model. Training with the random glimpse selection policy enhances accuracy by 10% while the utilization of a lightweight decoder reduces the FLOPs requirement for AVE by threefold and additionally improves accuracy by 4%.\n\nNote that, we examine the model\u2019s prediction accuracy forin the TORE forward pass, where the model operates similarly to the original ViT, and for, where the model operates within theextractor-aggregatorframework. In the latter case, we setfor SUN360 andfor CIFAR100, as they achieve comparable classification performance to AME as presented in Table1.\n\nThe results in Table4reveal that the training schema with randomizedvalues enables the model to robustly conserve classification performance for greater values ofwith only marginal performance decrease in the base case (). Notably, the single model trained with randomsampling outperforms dedicated models trained for specific values of.\n\nWe record model predictions at each timestep of exploration and report its accuracy. We conduct this experiment for each value ofand plot the gathered results in Figure5. The accuracy consistently improves with the increasing number of exploration steps and decreasing value of, and ranges from 60% to 80% at the 12th step on the CIFAR100 dataset.\n\nIn Active Visual Exploration, the model performance relies on two components: the predictive power of the model and the selected glimpses during exploration. We analyze the impact of those two components by analyzing the model accuracy on sets of patches gathered by different exploration processes.\n\nIn the first case, we measure model accuracy with the original ViT forward pass () on glimpses gathered by a model operating inextractor-aggregatorframework with differentvalues, see upper row of Figure6. We observe that glimpses selected using entropy maps are of similar quality for alland better than the randomly chosen ones.\n\nIn the second case, we measure the accuracy of the model operating inextractor-aggregatorframework with different values ofon glimpses gathered by a model using the original ViT forward pass (), see the bottom row in Figure6.\nWe observe that for all values of, the model achieves higher accuracy on glimpses selected based on entropy maps than the ones chosen at random. For comparison, we plot the results for TORE\u03ba, which predicts and gathers glimpses inextractor-aggregatorframework using the same values of. Note that the accuracy of TORE is almost the same as the accuracy of a model predicting on glimpses gathered with.\n\nResults in both cases show that the model using TORE forward pass can utilize the entropy maps of the reconstruction decoder effectively. Additionally, the value ofdoes not significantly impact the quality of chosen patches. We conclude that the TORE forward pass is well suited for Active Visual Exploration with policies based on the properties of the attention map.\n\nReducing computational costs for Vision Transformers (ViTs) can be achieved by designing efficient ViT architectures[3,16]. However, AVE is a specific setup that processes incomplete and unstructured input, requiring data imputation for efficient ViT variants due to their altered attention mechanisms. Traditional ViT architectures, on the other hand, are advantageous as they can process only the available image patches at any given time without requiring data imputation.\n\nTo demonstrate thatis more efficient than computationally effective ViT versions, we trained the EfficientFormer-V2-L (EF) in the same manner as. This means that we use a small reconstruction decoder and a random glimpse selection policy during training, with exploration based on the entropy of attention maps (AME). The only difference is that EF processes the same number of tokens in each iteration. Those tokens that are not discovered through the exploration are imputed as black. This ensures that the input is of a fixed size for computation and allows a fair comparison.\n\nThe results in Table5indicate that TORE outperforms EF on 3 out of 4 evaluated datasets and requires fewer FLOPS. Even with, TORE is more efficient because it uses only visible tokens instead of a fully-sized imputed image at each exploration step.\n\nSECTION: 6Conclusions and future works\n\nIn this work, we introduce the TOken REcycling (TORE) approach for enhancing efficiency in Active Visual Exploration tasks. It involves an efficient sequential inference that divides the ViT backbone into two components:extractorandaggregator. Through the concept of splitting the inference into two paths, we can fully utilize the potential of pretrained models. Additionally, we propose the use of a lightweight decoder in AVE, demonstrating that reduced reconstruction performance does not necessarily compromise the model\u2019s accuracy. Finally, we propose a training schema aimed at improving the model\u2019s downstream performance. As a result, TORE significantly reduces the computational workload while maintaining or even enhancing accuracy in Active Visual Exploration tasks.\n\nIn future research, we aim to explore further reductions in computations, such as modifying theaggregatorby integrating an attention-pooling mechanism. Additionally, we plan to refine the proposed framework by incorporating early exits to further alleviate the computational burden.\n\nThe primary limitation of the study lies in the fixed nature of the image divisions and masks used in the experiments, constrained by the size of the image patches in the ViT model. However, in future work, we will explore the impact of more random patch sizes on the model to better understand the model\u2019s behavior.\n\nThis work impacts the fields of embodied AI, robot vision, and efficient machine learning. It shows the potential of a straightforward yet powerful modification in the forward pass and training of ViTs, significantly reducing the computational load of large models and enabling efficient computations on devices such as drones.\n\nSECTION: Acknowledgements\n\nThe work of Jan Olszewski and Mateusz Pach was funded by the National Science Centre (Poland) grant no. 2022/47/B/ST6/03397. Dawid Rymarczyk was supported by the National Science Centre (Poland), grant no. 2022/45/N/ST6/04147. The work of Bartosz Zieli\u0144ski was supported by the National Science Centre (Poland), grant no. 2023/50/E/ST6/00469.\n\nWe gratefully acknowledge Polish high-performance computing infrastructure PLGrid (HPC Centers: ACK Cyfronet AGH) for providing computer facilities and support within\ncomputational grant no. PLG/2023/016555.\n\nSome experiments were performed on servers purchased with funds from a grant from the Priority Research Area (Artificial Intelligence Computing Center Core Facility) under the Strategic Programme Excellence Initiative at Jagiellonian University.\n\nSECTION: References\n\nSECTION: TORE: Token Recycling in Vision Transformers for Efficient Active Visual Exploration \u2013 Supplement\n\nWe provide examples of full exploration trajectories performed by our method as illustrated in7. Although image reconstruction is of poor quality, it is sufficient to guide the exploration process and results in the high accuracy of our method on the AVE task.\n\nAdditionally, we provide the source code to reproduce the results presented in the paper.", "text_file": "data\\paper_texts\\2311.15335v2_content.txt"}]], ["ti:\"machine learning\"", [{"title": "Machine-Learning Electron Dynamics with Moment Propagation Theory:\n  Application to Optical Absorption Spectrum Computation using Real-Time TDDFT", "authors": ["Nicholas J. Boyer", "Christopher Shepard", "Ruiyi Zhou", "Jianhang Xu", "Yosuke Kanai"], "published_date": "2024-12-06T18:49:53Z", "summary": "We present an application of our new theoretical formulation of quantum\ndynamics, moment propagation theory (MPT) (Boyer et al., J. Chem. Phys. 160,\n064113 (2024)), for employing machine-learning techniques to simulate the\nquantum dynamics of electrons. In particular, we use real-time time-dependent\ndensity functional theory (RT-TDDFT) simulation in the gauge of the maximally\nlocalized Wannier functions (MLWFs) for training the MPT equation of motion.\nSpatially-localized time-dependent MLWFs provide a concise representation that\nis particularly convenient for the MPT expressed in terms of increasing orders\nof moments. The equation of motion for these moments can be integrated in time\nwhile the analytical expressions are quite involved. In this work,\nmachine-learning techniques were used to train the the second-order time\nderivatives of the moments using first-principles data from the RT-TDDFT\nsimulation, and this MPT enabled us to perform electron dynamics efficiently.\nThe application to computing optical absorption spectrum for various systems\nwas demonstrated as a proof-of-principles example of this approach. In addition\nto isolated molecules (water, benzene, and ethene), condensed matter systems\n(liquid water and crystalline silicon) were studied, and we also explored how\nthe principle of the nearsightedness of electrons can be employed in this\ncontext.", "arxiv_id": "2412.05260v1", "html_link": "https://arxiv.org/html/2412.05260v1", "search_term": "ti:\"machine learning\"", "html_content": "SECTION: Machine-Learning Electron Dynamics with Moment Propagation Theory: Application to Optical Absorption Spectrum Computation using Real-Time TDDFT\n\nWe present an application of our new theoretical formulation of quantum dynamics, moment propagation theory (MPT) (Boyer et al., J. Chem. Phys. 160, 064113 (2024)), for employing machine-learning techniques to simulate the quantum dynamics of electrons. In particular, we use real-time time-dependent density functional theory (RT-TDDFT) simulation in the gauge of the maximally localized Wannier functions (MLWFs) for training the MPT equation of motion. Spatially-localized time-dependent MLWFs provide a concise representation that is particularly convenient for the MPT expressed in terms of increasing orders of moments. The equation of motion for these moments can be integrated in time while the analytical expressions are quite involved. In this work, machine-learning techniques were used to train the the second-order time derivatives of the moments using first-principles data from the RT-TDDFT simulation, and this MPT enabled us to perform electron dynamics efficiently. The application to computing optical absorption spectrum for various systems was demonstrated as a proof-of-principles example of this approach. In addition to isolated molecules (water, benzene, and ethene), condensed matter systems (liquid water and crystalline silicon) were studied, and we also explored how the principle of the nearsightedness of electrons can be employed in this context.\n\nDepartment of Physics and Astronomy, University of North Carolina at Chapel Hill, Chapel Hill, North Carolina 27599, USA\n\nSECTION: 1Introduction\n\nReal-time simulations of electron dynamics have attracted great interest for studies of non-equilibrium behavior in molecular systems.1,2,3,4In particular, real-time time-dependent density functional theory (RT-TDDFT) has become a widely used tool to investigate various phenomena such as optical absorbance5,6,7, energy transfer8,9, plasmons10,11, charge transfer12,13, electronic circular dichroism14, thermalization15, high harmonic generation16,17, electronic stopping18, electrical conductivity19, photocatalysis20, transient absorption spectroscopy21, spin transfer22, magnons23, core electron excitation24, exciton dynamics25, laser-induced water splitting26, and many other electronic excitation phenomena. The approach of RT-TDDFT is to propagate single-particle time-dependent Kohn Sham (TD-KS) orbitals to study quantum dynamics.1These orbitals posses a gauge freedom where a unitary transformation has no effect on the quantum dynamics27,28. One example of this gauge is the maximally localized Wannier functions (MLWFs) where the orbitals are unitary transformed in spatially-localized orbitals28. These MLWFs have previously been found to be useful in studying novel phenomena in complex systems such as Floquet engineering29and the electronic stopping response in DNA18.\nThese MLWFs also have been used in RT-TDDFT for efficient implementation of hybrid exchange-correlation (XC) functionals30.\nPerforming RT-TDDFT simulations, however, requires a large computational cost especially for simulating condensed matter systems1.\n\nIn recent years, molecular dynamics (MD) simulations have employed machine learning (ML) techniques to speed up calculations31. By either predicting the force or the potential energy from atomic positions, the MD-ML models can provide highly accurate simulations with lower computational cost as compared to first-principles MD32,33. This has motivated investigations into using these ML techniques for electron dynamics34,35. Secor et al., for example, proposed using artificial neural networks (ANN) as propagators in quantum dynamics for predicting the one-dimensional wavefunction at a future time step using the current time dependent wavefunction and potential35. However, training and choosing basis sets for higher dimensional systems proved challenging35.\n\nIn our previous work, we proposed a novel theoretical formulation for quantum dynamics in the single-particle description36. Our new approach, moment propagation theory (MPT), describes a single-particle wavefunction in terms of increasing orders of moments. We analytically derived the equation of motion for these moments. The proof-of-principle simulations employed up to the fourth-order of moments and accurately modeled the quantum dynamics of both harmonic and anharmonic well systems.\nMotivated by the analytical solution for the harmonic well, the work also proposed using ML techniques to circumvent the expensive calculation of the the moment time-derivatives. An artificial neural network (ANN) model accurately simulated the harmonic potential with low computational cost36. This is analogous to the MD-ML models that calculate the force on the atoms through approaches like ANN models.\n\nIn this work, we demonstrate the use of the moment propagation theory with machine-learning techniques (MPT-ML) for real systems through the use of RT-TDDFT simulation.\nBy using the moments of the spatially localized time-dependent MLWFs, only the moments up to a low order are needed to concisely describe the system within the MPT framework. We demonstrate the accuracy of the MPT-ML model approach for single molecular systems of water, benzene, and ethene. As a test of performance, we compute the optical absorption spectra for these molecules. We then investigate its application to liquid water and crystalline silicon and also examine how the principle of the nearsightedness of electrons can be utilized.\n\nSECTION: 2Theoretical and Modeling Details\n\nSECTION: 2.1Brief Overview of Moment Propagation Theory\n\nIn our earlier work36, we showed that the single-particle quantum dynamics can be formulated in terms of the moments of increasing orders instead of propagating the wave function using via a Schrodinger-like equation, as done using TDKS equations in RT-TDDFT simulation.\nLet us briefly summarize the key points of this MPT that are relevant in this work.\nWe express the single-particle wavefunction here asas the orders of the moments are generally not the same in the three Cartesian coordinates.\nThe moments of the single-particle probability density is\n\nwhereare non-negative integers used to denote the a-th, b-th, c-th moment indirections of the Cartesian coordinate space andis the single particle (probability) density, which is the square modulus of the single-particle wave function (i.e.).\nThe explicit equation of motion for the moments can be derived from the TDSE where\nthe first-order time derivative of the moments is\n\nand the second-order time derivative of the moments is\n\nIn our earlier work36, we showed that the numerical quantum dynamics simulation scheme can be formulated by Taylor-expanding the moments in time and truncating the expansion at the second order, as done in most classical molecular dynamics simulation.\nImportantly, we showed that the second-order time derivative can be given in terms of the moments and their first-order time derivatives albeit the explicit expression is highly complicated for numerical evaluation, especially for higher-order moments.\n\nSECTION: 2.2Machine Learning the second-order time derivatives\n\nIn general, the second-order time derivative can be expressed as\n\nwhereis a function, generally very complicated, of the moments and their first-order time derivatives.can be solved analytically only for very limited cases such as the harmonic oscillator as discussed in our earlier work36.\nWe also proposed that the use of ML approaches including a simple ANN model forinstead of its explicit evaluation, as often done for potential energy in classical MD simulation36.\nWhile it is tempting to apply popular ML techniques like ANN and deep-learning models, it is also possible to use more traditional ML techniques by incorporating known physical behavior.\nFor example, Vulpe and coworkers developed a MD potential using the physics-based many-body expansion of the potential energy37.\nHauge and coworkers noted that ANNs struggle to extrapolate data, such as dipole moments in time, so they had to enforce certain restrictions to prevent over-fitting and ensure stable extrapolation38.\nIn this work, with the analytical expression (Eq.4) in mind,\nwe examine the linear model for the ML,\n\nwhere the coefficientsare to be machine-learned from RT-TDDFT simulation.\nFor brevity, let us denote the moments using. For multi-electron systems in the TD-KS scheme, interactions between electrons must be incorporated as well\n\nwhereis the i-th moment of the j-th electronic state.\n\nSECTION: 2.3Time-Dependent Kohn Sham Equations in Wannier Gauge\n\nIn extended periodic systems, the Bloch states satisfywhereis the band index andis the lattice-periodic cell vector. Correspondingly, Wannier functions are given by\n\nwhereis the volume of the real-space periodic cell28. Wannier functions are translationally invariant such that, and it can be denoted simply as. These Wannier functions possess a gauge freedom, and it has become popular to make these Wannier functions unique by minimizing the total spread given as\n\nwhere the position operatoris defined according to Resta\u2019s formula in extended systems39,\n\nwhereis the lattice vector of the periodic cell.\nThis can be extended to second order of moments as well40\n\nwhere r is one of the Cartesian coordinates,, orand\n\nwhere r is one of the Cartesian coordinates,, orandis another Cartesian coordinate.\n\nAs discussed in Ref28, TD-KS equations can be propagated using the\nMLWFs,\n\nwhereis the operator for ensuring the maximally localization of the Wannier functions, andis the KS potential. This scheme has been successfully used for various applications and also for reducing the computational cost of evaluating the exact exchange30.\n\nSECTION: 2.4Computational Method\n\nInstead of using the Taylor series expansion as often done in classical MD simulation (see Supporting Information), we propose an alternative scheme.\nBy applying the MPT to the quantum dynamics described by TD-MLWFs, solutions to the linear model (LM) (Eq.6) can be obtained analytically.\nEq.6is written in terms of matrices as\n\nwhereis the vector with the moments,,,, andare specified in Eq.6.\nIt is convenient to rewrite this equation as\n\nwhere we define\n\nThe matricesandare zero and identity matrices respectively with the same size as other matrices.\nThe general solution to this linear ordinary differential equation (ODE) can be expressed as\n\nwhereis defined by\n\nThe initial value problem (IVP) here is\n\nwhereis the diagonal matrix such that.contains time-dependent information about the moments. When the Fourier transform of the first-order moments inare used to calculate the optical absorption spectrum as discussed in the following, the eigenvalues () can be identified as the frequencies of the absorption spectrum and the eigenvectors () provide the magnitude of the transition dipoles.\n\nIn few cases, the matrixhas diagonal elements withsuch that the solution diverges in the form of.\nThese eigenvalues tend to be close to zero, but having such real-valued non-zero elements lead to an nonphysical solution for MPT in numerical simulations. We correct this numerical artifact by setting the real part of these eigenvalues to zero to eliminate the diverging solution. Generally speaking, the cause of having positive real eigenvalues stems from fitting the linear model to a data set produced by RT-TDDFT simulation with a finite simulation time. Indeed, we observe that asincreases, the need for this correction decreases.\nIn numerically performing some of these matrix operations, we may use other standard corrections.\nIn evaluating, we set any eigenvalues with their absolute value below a certain threshold(we use c=0.005) as. Likewise, we setwhen.\nAdditionally, we employ a high eigenvalue cutoffsuch thatif(we typically use h=2 or 54.4 eV), removing nonphysical high frequency noises.\n\nRidge Regression:Regularization refers to a statistical technique to minimize errors from overfitting with training data, and so-called ridge regression is one of the most commonly-employed regularization technique for linear regression models.\nWith a large number of variables as for condensed matter systems, the overfitting becomes a practical issue because of the multicollinearity within the dynamics of MLWFs. Thus, we also examined the effectiveness of the ridge regression technique, which minimizes the loss function\n\nwhereis the linear model as described above in Eq.13.\nTheis the i-th learnable parameter in matrix.\nThe variablesandare theinputs and theoutputs, respectively whereis for the time index.\nThe hyperparameter,, is an additional adjustable parameter that is used to reproduce the training data closely.\n\nNearsightedness of Electrons:With increasingly large numbers of the moments for modeling condensed matter systems, numerical noises from fitting the first-principles data could degrade the accuracy.\nThe nearsightedness principle of electrons41,42can be invoked to reduce the number of the parameters necessary in the above proposed model based on the moment propagation theory.\nAccording to the nearsightedness principle,\nlocal electronic properties like the probability density depend on the effective external potential of only nearby points. Changes in that potential beyond a certain distance have limited effects on the local properties.\nThis allows us to introduce a cutoff distance beyond which the electrons (represented by MLWFs) are not impacted the dynamics of other electrons.\nThen, the equation of the motion for the moments (Eq.6) can be written in terms of the subset of the all MLWFs as\n\nwhereis the cutoff distance beyond which the MLWFs do not impact their dynamics.\nIn addition to allowing us to develop an effective computational method, this procedure also enable us to study the extent to which the nearsightedness principle applies in real systems.\nStudying the necessary cutoff distance for fully reproducing the RT-TDDFT result informs us about the effective distance for the such nearsightedness of electrons in condensed matter systems.\n\nThe workflow of this work is summarized in Figure1. First, the RT-TDDFT simulation is performed using the Qb@ll code28. In the RT-TDDFT simulation with the MLWF gauge, all the moments are computed at each time step.\nThe machine-learning model is then developed by fitting the equation-of-motion from the moment propagation theory (MPT-ML) to this first-principles training data.\nThe resulting MPT-ML model is examined against the RT-TDDFT simulation by computing the optical absorption spectra, which contain the electronic excitation at all frequencies.\n\nSECTION: 3Results and Discussion\n\nSECTION: 3.1Calculation of Dielectric Function\n\nTo demonstrate the above described approach based on the moment propagation theory in the context of RT-TDDFT, optical absorption spectra are calculated.\nFor extended systems, the optical absorption spectrum can be obtained from the imaginary part of the dielectric function28,\n\nwhereis the complex frequency-dependent polarizability tensor. It can be obtained by Fourier transforming the time-dependent polarization\n\nwhereis the first-order moment that is propagated as vector elements of(Eq.20).\nHereis the strength of the abrupt homogeneous electric field applied to the system indirection using the length gauge28.\nThe imaginary part of the dielectric function is directly related to the optical absorbance while the real part is related to the dispersion. For isolated systems such as gas-phase molecules the macroscopic dielectric function is not well defined, and the optical absorption is typically described in terms of the dipole strength function, which is also expressed in terms of the polarizability tensor as\n\nwhereis the speed of light.\nIn practice, we add a damping term in the form ofin Eq.24whereis chosen to be100 a.u.. This damping term reduces the noise from having the finite amount of dynamics in taking the Fourier transform.\n\nSECTION: 3.2Optical absorption spectrum of gas phase molecules\n\nTo investigate the applicability of the above-described approach of using the machine learning linear model for the moment propagation theory (MPT-ML) approach in practice, we consider several isolated molecules of water, benzene, and ethene.\nFor RT-TDDFT simulation, the PBE XC functional43was used with 40 Rydberg cutoff for planewave expansion and PBE Optimized Norm-Conserving Vanderbilt (ONCV) pseudopotentials were used44.\nA single molecule is placed in a 70 a.u. cubic simulation cell. A delta kick strength of 0.01 a.u. was used in the applied electric field, and 0.2 a.u. was used for the time step in the enforced time reversal symmetry (ETRS)45integrator for a total of 200 a.u. simulation time.\nAs discussed above, RT-TDDFT simulation was performed in the Wannier gauge and the individual moments are obtained for each MLWF. A key point of this study here is whether the electron dynamics necessary for calculating physical properties like optical absorption spectra can be adequately described using only low orders of the moments. While MPT is exact in principle, its practical advantage is limited by the orders of the moments necessary for describing electron dynamics in real systems.\n\nFigure2shows the dynamics of a MLWF on a single water molecule in RT-TDDFT.\nThe MPT-ML approach seeks to capture the dynamics of this single MLWF using increasing orders of moments. As figure2shows, the MLWF is highly localized and amenable to using a concise description using low orders of moments. This remains the case as the MLWF changes over time allowing the use of ML methods to learn the dynamics of moments in the MPT framework.\n\nFigure3shows that the results from the MPT-ML approach and the reference RT-TDDFT result, which also serves as the training dataset.\nThe MPT-ML approach uses up to the second-order moments and their time derivatives in Eq.20. The optical absorption spectrum show three prominent sharp peaks of 6.2 eV, 8.3 eV, and 12.4 eV below the broad peak centered at 20 eV. With the first-order moments only, the MPT-ML model captures the first two peaks at 6.2 eV and 8.3 eV well but it fails to reproduce the third peak at 12.4 eV.\nBy including up to the second-order moments, the MPT-MP model is able to correctly capture also the third peak in addition to the rest of the spectrum features. By using a more complete description of the MLWFs of the single water system with higher orders of moments the result is expected to match the RT-TDDFT result. We also notice that since the size of matrixfrom the IVP is larger for the second order moments, there are more frequencies that could exist in. This is seen as the increasing roughness of the second order result over the first order.\n\nWe apply the MPT-ML approach here on an ethene molecule to examine its applicability for molecules with double bonds. The optical absorption spectrum show a single sharp peak at 7.5 eV below the broad peak centered at 20 eV as seen in Figure4. In this case, the MPT-ML model well reproduces the spectrum even with the first-order moments only, and\nincluding up to the second-order moments only further make the spectrum better as in the case of RT-TDDFT result.\n\nA benzene molecule was studied here particularly because of the delocalized nature of electrons as manifested in conjugation around the carbon atoms. The same computational parameters were used for RT-TDDFT simulation as in the case of water molecule, except for using a longer simulation time of 400 a.u.\nFigure5shows the optical absorption spectrum of a single benzene molecule. A notable feature is the prominent absorption peak at 6.8 eV, and this key feature is accurately reproduced by the MPT-ML model.\nWhile the MPT-ML model with only the first-order moments is able to capture this absorption peak correctly, it gives an erroneous broad peak at 40 eV.\nBy including up to the second-order moments, the MPT-ML is able to correctly eliminate this behavior, yielding an accurate absorption spectrum.\n\nSECTION: 3.3Optical absorption spectrum of condensed matter systems\n\nWe examine here the MPT-ML approach for more complex systems of condensed matter systems. In particular, we consider the case of liquid water and crystalline silicon.\n\nLiquid Water:For liquid water a cubic simulation cell (30.6683 a.u.) containing 162 water molecules (1296 electrons) with periodic boundaries was used. The structure of liquid water was generated by taking a snapshot of the equilibrated system following a 20 picosecond classical molecular dynamics simulation at 300 K using the single point charge with polarization correction (SPC/E) model46.\nAll atoms are held fixed for the RT-TDDFT simulation, and a delta kick strength of 0.01 a.u. with a 0.1 a.u. time step was used by employing the enforced time reversal symmetry (ETRS)45integrator for a total of 250 a.u. simulation time.\nThe PBE approximation was used for the XC functional, and Hamann-Schluter-Chiang-Vanderbilt (HSCV) pseudopotentials47were used with a 40 rydberg cutoff for the planewave kinetic energy cutoff for the KS orbitals.\nPrevious work has shown that this liquid water simulation cell is fully converged with respect to cell size48,49and that PBE gives an accurate description of the optical absorption spectra49.\n\nIn Figure6, we compare\nthe MPT-ML model with the RT-TDDFT simulation. As can be seen, by including only the first-order moments in the MPT-ML model already performs quite well in reproducing the RT-TDDFT spectrum. At the same time, the tail end of the spectrum above 30 eV starts to deviate from the first-principles calculation unless the second-order moments are also included.\nFor condensed matter systems with a large number of variables for the MPT model, we also examined the use of the ridge regression technique as discussed in the Computational Method section.\nFor this particular case of water, the ridge regression does not have much impact unlike the crystalline silicon case discussed in the following section.\n\nFor linear response properties like the optical absorption spectrum, it is instructive to examine the nearsightedness principle of electrons by Kohn50in condensed matter systems.\nA particular question in the context of the MPT is to what extent the quantum dynamics of individual Wannier functions can be described by accounting for the dynamics of nearby Wannier functions.\nWe examine here such an effective radius of influence for the dynamics of individual Wannier functions, studying the non-local nature of the many-body quantum dynamics for this electronic system.\nWe do so by introducing the cutoff radius for individual MLWFs in constructing the MPT-ML model as described in the Computational Method section.\nFigure7shows how the optical absorption spectrum changes with the cutoff radius,, of 2 and 7 a.u.\nThe distance of 2 a.u. corresponds to having only the intra-molecular interactions among MLWFs on individual water molecules.\nWith the cutoff radius of 7 a.u., the model includes the inter-molecular interactions among MLWFs of their neighboring water molecules. This essentially take into account the dynamical effect within the first solvation shell around individual water molecules.\nThe7 a.u. spectrum captures all the key features as seen in Figure7while the2 a.u. spectrum shows that it is too short to capture the \u201cnearsightness\u201d as perhaps expected.\nThis analysis not only provides valuable insight into the short-range nature of quantum dynamics responsible for the optical absorption in water but also offers an effective scheme to reduce the computational cost of simulating electron dynamics in large complex systems.\n\nImportantly in the context of MPT-ML approach, this approach also allows us to significantly reduce the number of parameters to machine-learn. Table1shows the number of moments and the corresponding parameters needed for different systems and settings.\nLettingbe the number of moments, the number of parameters to be learned is.\nIn condensed matter systems like water, over 68 million parameters need to be machine-learned even when we need only up to the second-order moments.\nUsing, only 4.98% of these parameters are necessary, significantly reducing the computational complexity of the machine-learning.\n\nCrystalline Silicon:For modeling the optical absorption spectrum of crystalline silicon, we use an elongated supercell that consists of 128 silicon atoms, following our previous work30.\nThe PBE approximation was used for the XC functional, and ONCV pseudopotentials were used with a 15 Ry cutoff\nfor the planewave kinetic energy cutoff for the KS orbitals. The enforced time reversal symmetry (ETRS)45integrator was used to perform RT-TDDFT simulation for a total of 600 a.u. simulation time with 0.2 a.u. time steps.\nA delta kick was applied to excite the system in the direction of the elongation with the field strength of 0.001 a.u.\nFigure8shows the spectrum obtained using the MPT-ML model along with the RT-TDDFT result.\nUnlike for the water case discussed above, including also the second-order moments does not straightforwardly improve the linear model spectrum.\nWhile the overall shape is improved especially the high energy region (above 5 eV), the inclusion of the second-order moments introduced an artificial peak around 1.5 eV. Here, the use of ridge regression technique for reducing the overfitting problem helps significantly, eliminating the unphysical peak below 2 eV.\nFigure9shows how the use of the cutoff radius affect the spectrum.\nWhile the prominent peak at 2.8 eV is largely absent with, the cutoff radius ofis already large enough to capture the essential features of the optical absorption spectrum here.\nAs summarized in Table1, using the cutoff radius significantly reduces the number of required parameters for the machine-learning by an order of magnitude.\n\nSECTION: 4Cross-validation and CPU time requirement\n\nWe comment on the cross-validation and CPU time requirement of the MPT-ML model discussed above in this section.\nIn this proof-of-principle work for the new MPT-ML model approach, our aim here was to demonstrate its efficacy by reproducing the RT-TDDFT simulation result (also the training set) using the moment propagation theory (MPT). We trained the equation-of-motion of the MPT using the machine-learning approach.\nA natural question is whether the MPT-ML model would have been able to predict the RT-TDDFT simulation result with a smaller training data set. We focus here on the single water molecule system for simplicity to answer this question, and we consider the model that includes both the first-order and second-order moments.\nFigure10shows how the optical absorption spectrum from the MPT-ML model changes when the training data set was obtained from RT-TDDFT simulations performed for the duration of 200, 150, 100, and 50 a.u. The reference RT-TDDFT simulation result is from the 200 a.u. RT-TDDFT simulation.\nAs can be seen in Figure10, the optical absorption spectrum including the prominent peaks is well reproduced already with the training data set from the shorter 100 a.u. RT-TDDFT simulation. As expected, with increasingly larger data sets, the spectrum approaches closer to that of the 200 a.u. RT-TDDFT simulation (i.e. \u201cRT-TDDFT\u201d in Figure10).\n\nTable2shows the CPU time used for each part in the workflow (see Fig.1) for selected systems (a water molecule, condensed matter system consisting of 162 waters, and crystalline silicon). As can be seen, even with the additional CPU time required for training the MPT-ML model, the computational cost saving gained by using the MPT-ML model is significant; the computational time is reduced by several orders of magnitude.\nFor instance, in the case of the simulation with 162 waters\n(),\nthe CPU time required by\nthe MPT-ML simulation istimes lower than that of the RT-TDDFT simulation.\nThe computational scaling of matrix operations (such as diagonalization) required for the MPT-ML model scales withwhereis the number of moments.\nThis scaling can be further improved if the diagonalization (and other matrix operations) can be approximated byblock diagonal matrices of equal size; this would reduce the computational scaling to.\n\nSECTION: 5Conclusions\n\nWhile TDDFT provides a particularly convenient theoretical formalism for simulating the quantum dynamics of electrons from first principles, RT-TDDFT simulation remains computationally intensive for studying many complex chemical systems1. At the same time, data-driven modeling has become increasingly popular in many fields, especially for molecular dynamics simulation of atoms in recent years51.\nOn the other hand, the electron dynamics remains as one of the challenging cases for applying data-driven approaches like ML35. In this work, we showed how the recently formulated MPT36offers a powerful framework for machine-learning the quantum dynamics of electrons when it is combined with the RT-TDDFT simulation in the Wannier gauge28. MPT derives the equations of motion for all orders of moments. Due to the highly localized nature of individual MLWFs, we can anticipate that only low-order moments might be necessary for an accurate description.\nHowever, even for the low-order moments, their second-order time derivatives are highly complicated to calculate in practice.\nAs done in the case of classical MD simulation, we applied the ML technique for approximating the second-order time derivatives by training them against the first-principles simulation52.\nWe showed how this MPT-ML approach can be used to accurately calculate the optical absorption spectra of various systems from small gas-phase molecules to condensed phased systems even with a simple machine-learning method (i.e. linear model). For condensed matter systems, we also examined the nearsightedness principle of electrons to exploit the short-range nature of their influence to significantly reduce the number of parameters to be trained.\n\nThis work thus far remains a proof-of-principle demonstration for real systems using first-principles calculation. At the same time, one can already realize how this MPT-ML approach can significantly benefit the field especially when using advanced XC functionals like hybrids, which are an order of magnitude computationally more expensive than standard XC functionals even with recent advancements30,53.\nWhile this work focused on the use of the MPT-ML\napproach for optical absorption spectrum, linear-response property, we envision it extended for studying more complicated non-equilibrium electron dynamics phenomena in future work.\n\nSECTION: 6Supporting Information\n\nSupporting Information includes a discussion about propagation using the Taylor series expansion method and close-up views of the absorbance peaks of the molecules.\n\nN.B. was supported by the Summer Undergraduate Research Fellowship (SURF) at the University of North Carolina at Chapel Hill. This work was supported by the National Science Foundation, under No. CHE-1954894.\n\nSECTION: Author Declarations\n\nSECTION: Conflict of Interest\n\nThe authors have no conflicts to disclose.\n\nSECTION: Author Contributions\n\nN.B. led the work and performed all the calculations. N.B. and Y.K. conceived of the presented idea. All authors discussed the results and contributed to the final manuscript.\n\nSECTION: Data Availability\n\nThe data that support the findings of this study are available from the corresponding author upon reasonable request.\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.05260v1_content.txt"}, {"title": "A Machine Learning-Based Approach For Detecting Malicious PyPI Packages", "authors": ["Haya Samaana", "Diego Elias Costa", "Emad Shihab", "Ahmad Abdellatif"], "published_date": "2024-12-06T18:49:06Z", "summary": "Background. In modern software development, the use of external libraries and\npackages is increasingly prevalent, streamlining the software development\nprocess and enabling developers to deploy feature-rich systems with little\ncoding. While this reliance on reusing code offers substantial benefits, it\nalso introduces serious risks for deployed software in the form of malicious\npackages - harmful and vulnerable code disguised as useful libraries. Aims.\nPopular ecosystems, such PyPI, receive thousands of new package contributions\nevery week, and distinguishing safe contributions from harmful ones presents a\nsignificant challenge. There is a dire need for reliable methods to detect and\naddress the presence of malicious packages in these environments. Method. To\naddress these challenges, we propose a data-driven approach that uses machine\nlearning and static analysis to examine the package's metadata, code, files,\nand textual characteristics to identify malicious packages. Results. In\nevaluations conducted within the PyPI ecosystem, we achieved an F1-measure of\n0.94 for identifying malicious packages using a stacking ensemble classifier.\nConclusions. This tool can be seamlessly integrated into package vetting\npipelines and has the capability to flag entire packages, not just malicious\nfunction calls. This enhancement strengthens security measures and reduces the\nmanual workload for developers and registry maintainers, thereby contributing\nto the overall integrity of the ecosystem.", "arxiv_id": "2412.05259v1", "html_link": "https://arxiv.org/html/2412.05259v1", "search_term": "ti:\"machine learning\"", "html_content": "SECTION: A Machine Learning-Based Approach For Detecting Malicious PyPI Packages\n\nBackground. In modern software development, the use of external libraries and packages is increasingly prevalent, streamlining the software development process and enabling developers to deploy feature-rich systems with little coding.\nWhile this reliance on reusing code offers substantial benefits, it also introduces serious risks for deployed software in the form of malicious packages - harmful and vulnerable code disguised as useful libraries.Aims. Popular ecosystems, such PyPI, receive thousands of new package contributions every week, and distinguishing safe contributions from harmful ones presents a significant challenge.\nThere is a dire need for reliable methods to detect and address the presence of malicious packages in these environments.Method. To address these challenges, we propose a data-driven approach that uses machine learning and static analysis to examine the package\u2019s metadata, code, files, and textual characteristics to identify malicious packages.Results. In evaluations conducted within the PyPI ecosystem, we achieved an F1-measure of 0.94 for identifying malicious packages using a stacking ensemble classifier.Conclusions. This tool can be seamlessly integrated into package vetting pipelines and has the capability to flag entire packages, not just malicious function calls. This enhancement strengthens security measures and reduces the manual workload for developers and registry maintainers, thereby contributing to the overall integrity of the ecosystem.\n\nSECTION: 1.Introduction\n\nCode reuse drives technological innovation by enhancing developer productivity and enabling the creation of feature-rich, maintainable systems(Mohagheghi et\u00a0al.,2004; Basili et\u00a0al.,1996; Grinter,1996). Package managers like npm and PyPI facilitate this innovation, with PyPI allowing developers to contribute freely to a vast repository of Python packages(Kaplan and Qian,2021; Vu et\u00a0al.,2020a). PyPI meets diverse needs, including those in artificial intelligence, with millions of packages available(Liang et\u00a0al.,2021). Its popularity has made Python the most favored programming language as of April 2024, according to the TIOBE index.111https://www.tiobe.com/tiobe-index/\n\nDevelopers are increasingly reusing code, which inadvertently heightens the risk of integrating malicious code into applications(Ladisa et\u00a0al.,2023). Malicious actions can include inserting backdoors and stealing sensitive data. The PyPI registry faces various attacks, including compromised accounts, where attackers gain full control of a maintainer\u2019s account to publish malware(Vu et\u00a0al.,2020b; Kaplan and Qian,2021). Typosquatting(ber,2024; dat,2024)exploits typographical errors in package names, while combosquatting(Vu et\u00a0al.,2020b; Tschacher,2016)leverages the order of nouns in names. A notable example is the malicious package \u201djeIlyfsh,\u201d which replaced a character in the benign \u201djellyfsh\u201d package name to steal SSH and GPG keys, remaining undetected for a year(Vu et\u00a0al.,2020a).\n\nSeveral code-scanning methods have been proposed to identify malicious packages in popular ecosystems like NPM, PyPI, and Maven. These approaches encompass both traditional methods (e.g., anomaly detection(Liang et\u00a0al.,2021), dynamic and static analysis(Duan et\u00a0al.,2020a,b), and machine learning-based methods (e.g., unsupervised k-means clustering(Garrett et\u00a0al.,2019), supervised learning(Sejfia and Sch\u00e4fer,2022)).\nHowever, managers of large package registries, such as PyPI, struggle to handle the daily influx of packages, making it challenging for manual oversight(Zhang et\u00a0al.,2023; Gu et\u00a0al.,2023). According to the libraries.io database(Lib,2024), which is an open source repository and dependency database that catalogues libraries of the most popular ecosystems, and it has been used by previous work as a source of library metadata(Decan et\u00a0al.,2018; Alfadel et\u00a0al.,2021), analysis reveals that over the course of one week, PyPI developers publish around 1,800 public package versions, including both new packages and updated versions of existing packages.\nTo this end, prior works show that many existing scanning tools face limitations such as scalability issues(Sejfia and Sch\u00e4fer,2022; maloss,2024), a narrow focus on specific ecosystem aspects like package updates(Sejfia and Sch\u00e4fer,2022; Garrett et\u00a0al.,2019), high resource costs(maloss,2024), and high false alerts(bandit,2024; maloss,2024).\nIn this context, we have detailed code scanners that\nwork well if we analyse a few packages, however, there is a necessity for an approach that can extend its scalability to encompass the entire ecosystem. Our approach is designed to fill this gap, addressing issues of false positives and negatives. Its strength lies in the integration of newly crafted features operating at the package level, distinguishing it from most existing tools that operate at the function call level.\nOur approach considers the interactions of multiple factors across code, function, file levels, and package metadata, to provide a scalable classification of the likelihood of a malicious package.\n\nTo evaluate the effectiveness of our approach, we conduct a study to answer the following three research questions.RQ1: How accurate is our approach in classifying malicious packages?We developed six machine learning classifiers\u2014Random Forest, Decision Tree, Support Vector Machine, Multilayer Perceptron, Naive Bayes (Bernoulli version), and stacking\u2014using eight features: 2 metadata-related, 2 file-related, 3 code-related, and 1 text feature. The classifiers were evaluated on a dataset of 5,193 benign and 138 malicious packages. Additionally, we tested the method\u2019s generalizability on an unseen dataset of 397 typical and 143 new malicious packages. The stacking ensemble classifier achieved an F1-score of 94.2% in predicting malicious packages, particularly when incorporating the text-related feature. Our findings indicate strong performance and generalizability of the approach.\n\nRQ2: What features are the best indicators of malicious\npackages?The study investigates features that differentiate malicious packages from benign ones, identifying metadata-related features as the best indicators. Key tokens likegetattr, connect, read, openare crucial for malicious package identification. The research also examines the effects of keyword removal, stop word removal, and stemming on classifier performance. It concludes that stemming does not influence performance, while removing keywords and stop words has minimal impact.\n\nRQ3: Is our approach useful?We assess the viability and efficiency of our approach by comparing it with two state of the art tools, namely bandit and packj. This evaluation is conducted on a random subset comprising 50 benign packages and an additional 50 samples of malicious packages obtained from an external dataset and is performed on two scenarios: the entire package and the setup.py file.\nThe results suggest that our approach effectively detects a considerable number of malicious packages in real-world scenarios, demonstrating a low rate of false alerts when compared to the tested tools. As an illustration, our tool successfully detected 4 out of 5 recently released malicious packages, outperforming the tested tools that proved unsuccessful in this regard. Furthermore, by selecting features related to the Python ecosystem, our method can detect a broader spectrum of malicious Python packages, in contrast to the state-of-the-art approach(Ohm et\u00a0al.,2022).\n\nOur study contributes to the research and practice on four fronts.\n\nUnlike existing strategies that focus on detecting suspicious function calls, our approach operates at the package level, applicable to the entire ecosystem.\n\nThis is the first study to employ a vocabulary-based method to automatically detect malicious packages within the PyPI ecosystem.\n\nWe introduce significant new features previously unexplored in predicting malicious functions and packages.\n\nWe publicly share our dataset(Anonymous,2024)for further research on enhancing the security of package managers against supply chain attacks, comprising 5,331 packages (138 malicious and 5,193 popular) with various features.\n\nSECTION: 2.Background\n\nMalicious packages are software components intentionally designed to harm systems, often containing malware or code for unauthorized activities like stealing sensitive information or installing backdoors(thehackernews,2024; bleepingcomputer,2024). In contrast, benign packages are safe and intended to perform their designated functions without compromising security.\n\nNumerous studies indicate that malware is continuously evolving, employing diverse techniques to evade detection tools(Duan et\u00a0al.,2020b; Xu et\u00a0al.,2013,2012). Attackers target all stakeholders in the software supply chain, including end-users, developers, package maintainers, and registry maintainers. One prevalent tactic is typosquatting and combosquatting(Tschacher,2016; Vu et\u00a0al.,2020b), as many registries lack security policies(Duan et\u00a0al.,2020b). In typosquatting, malicious packages are published with names similar to popular packages to deceive developers into downloading them. Combosquatting manipulates the order of words in package names, such as changing \u2019python-nmap\u2019 to \u2019nmap-python.\u2019 An illustrative incident occurred in May 2022 when attackers published a malicious package named pymafka, which mimicked the legitimate PyKafka package, leading to 325 downloads before its removal(pymafka,2024). Additionally, attackers publish new malicious packages directly, often employing code obfuscation methods to conceal harmful code from analysis. Techniques like encoding and encryption are commonly used, as seen in the colourama@0.1.6 typosquatting variant of colorama, which utilized base64 encoding to evade detection as shown in listing1(Line 1). Malicious packages like hipid and hpid(encode32,2024)have been reported using uncommon base32 encoding, while the botaa3 typosquatting package employs bitwise XOR encryption and base64 encoding to obscure its malicious payloads(botaa3,2024).\n\nMany tools are developed to detect malicious attacks against popular ecosystems (e.g., NPM and PyPI) such as Malware-check(malwarecheck,2024), OSSGadget(OSSGadget,2024), Maloss(maloss,2024), Packj(packj,2024), Bandit4mal(bandit4mal,2024), and Bandit(bandit,2024).\nTo the best of our knowledge, the proposed tools work in detecting risky factors in Python packages, always reporting alerts at the level of Python functions. To put our results into perspective, we opt to select both Packj and Bandit as baselines in our evaluation because 1) they have been selected as benchmarks in previous research(Vu et\u00a0al.,2020a,2022; Sejfia and Sch\u00e4fer,2022)and 2) they are mature tools that provide detailed reports that facilitate our manual analysis.\nTheBandittool(bandit,2024)is a widely-used static analysis tool designed for identifying security vulnerabilities in Python files. It employs predefined rules and the Abstract Syntax Tree (AST) representation of source code to enhance its analysis. Bandit rates issues based on severity and confidence levels (low, medium, high), providing insights into potential impact and reliability(Ruohonen et\u00a0al.,2021). It has also been used as a benchmark in previous research(Vu et\u00a0al.,2022,2021).\nOn the other hand, thePackjtool(packj,2024)employs a comprehensive analysis approach with three steps: static code analysis, metadata analysis, and optional dynamic analysis. It examines package code for filesystem, network, and process API usage, validates metadata attributes, and can perform dynamic analysis. Packj is built upon the MalOSS project(maloss,2024)and is recognized in research as a benchmarking framework(Vu et\u00a0al.,2020a,2022; Sejfia and Sch\u00e4fer,2022).\nGiven that attackers employ diverse tactics to obfuscate the detection of malicious packages, and existing registries lack a robust review process for package publication(Duan et\u00a0al.,2020b), and most existing tools suffer from considerable limitations, our emphasis is on integrating various features. These include meta-related, code-related, and text-related features. These features aim to effectively capture the range of tactics employed by attackers, enhancing the ability to identify malicious packages.\n\nSECTION: 3.Related work\n\nTraditional approaches.Several methods have been proposed for identifying malicious packages.Liang et\u00a0al.(2021)used anomaly detection, combining abstract syntax tree (AST) and regular expression techniques, achieving a 97.51% reduction in review workload, though it struggles with small malicious artifacts.Duan et\u00a0al.(2020a)reported 339 malicious packages through dynamic and static analysis but lacked false positive analysis. Their MALOSS framework, while comprehensive, requires significant resources.Vu et\u00a0al.(2021)identified differences between published packages and source repositories but missed packages without repositories.Ohm et\u00a0al.(2020b)relied on dynamic analysis with heavy manual intervention, whileRieck et\u00a0al.(2011)monitored behavior in a sandbox but lacked detailed analysis. Various methods targeting typosquatting often suffer from false positives and negatives due to their focus on Levenshtein distance alone(Taylor et\u00a0al.,2020; Vu et\u00a0al.,2020b; Tschacher,2016).\n\nToolInputTechniqueMalware Checks(malwarecheck,2024)setup.py filestatic (Regular Expression)OSSGadget(OSSGadget,2024)package+artifactsstatic (Regular Expression)MalOSS(Duan et\u00a0al.,2020b)packagehybrid (metadata, static, dynamic)Packj(packj,2024)packagehybrid (metadata, static, dynamic)bandit4mal(bandit4mal,2024)packagestatic (Abstract Syntax Tree)bandit(bandit,2024)packagestatic (Abstract Syntax Tree)\n\nMachine learning approaches.Related work includesGarrett et\u00a0al.(2019), who used unsupervised learning to identify suspicious NPM package updates based on features like resource access and API usage, flagging 539 updates but not investigating malicious packages in depth.Sejfia and Sch\u00e4fer (2022)proposed an automated method for detecting malicious NPM packages that examined version changes and employed classifiers, successfully identifying 95 out of 96,287 unknown malicious packages with acceptable false positives. However, their approach lacks scalability and is limited to package updates, unable to address single-release packages.Halder et\u00a0al.(2024)developed MeMPtec, which utilizes features from package metadata. However, our approach goes deeper by analyzing metadata and file-related features. We parse the contents of license and configuration files, rather than simply reporting their existence.\nRecent research byOhm et\u00a0al.(2022)focuses on detecting malicious NPM packages using a combination of classifiers, achieving true positive rates over 70% by evaluating 25,210 models. Their optimal combination involved Support Vector Machine, Multi Layer Perceptron, and Random Forest, successfully identifying 13 previously unknown malicious packages. This work is the most closely related to ours, as it also seeks to classify malicious packages within the NPM ecosystem. However, our methodology differs significantly. We adopt a more holistic approach by incorporating features related to code, licenses, file configurations, and author information. UnlikeOhm et\u00a0al.(2022), which treats suspicious APIs as boolean features, we analyze entire lines of code as text features. To our knowledge, no previous research has integrated linter outputs with metadata to classify software packages as malicious.Ohm et\u00a0al.(2022)developed a methodology tailored for the NPM ecosystem, specifically focusing on identifying malicious JavaScript packages with features relevant to that environment. In contrast, our work is explicitly designed for the Python ecosystem. We have crafted features that incorporate python-specific setup configuration files, allowing for a more accurate analysis of potential threats within Python packages. This distinction highlights the adaptability of security measures to different programming ecosystems.\n\nPython malware detection tools.In the literature, a multitude of methods such as malware-check(malwarecheck,2024), OSSGadget(OSSGadget,2024), Maloss(maloss,2024), Packj(packj,2024), Bandit4mal(bandit4mal,2024), and bandit(bandit,2024)have been suggested for detecting suspicious package. The majority of these methods examine various facets of a package through the utilization of metadata, static analysis, or dynamic analysis as shown in Table1. Analysis methods for metadata (such as package name and author information) involve scrutinizing these metadata elements to detect potentially problematic packages. One instance of this is the utilization of package names and their popularity to identify suspicious packages that might involve tactics like typosquatting or combosquatting.\n\nHowever, existing tools struggle with unproductive time use, overlooked security issues, and high resource demands for dynamic analysis. Many, like OSSGadget and malware-check, rely on limited rule-based detection, potentially missing harmful code and hindering efficient detection of malicious actions.\n\nUnlike previous research efforts that created comparative frameworks for different ecosystems or aimed at identifying suspicious packages at the function calls level, our research stands out by focusing on identifying malicious packages at the package level. Notably, it surpasses the scope of proposed works by evaluating six classifiers on a comprehensive set of newly crafted features and features inherited from existing literature, ensuring the successful detection of malicious packages throughout the entire ecosystem.\n\nSECTION: 4.Study Setup\n\nThe main goal of our study is to identify malicious packages from a set of suspicious packages.\nMany techniques are proposed for this purpose as shown in Table1, however, they do not work at the package level, and requiring maintainers to inspect multiple alerts before identifying malicious packages.\nTherefore, in order to attain our objective and address the limitations of current methods, we develop a machine learning (ML) approach. This approach is centered around combining specific static features, as demonstrated in the upcoming Section4.2. We resort to use ML technique as it is a popular technique in the area of information security(Hou et\u00a0al.,2010; Duan et\u00a0al.,2020b; Ohm et\u00a0al.,2022; Sejfia and Sch\u00e4fer,2022).\n\nFor a comprehensive view, Figure1shows the full workflow of our approach, which consists of three\nphases: collecting benign and malicious PyPI packagess (Section4.1), feature extraction including metadata, file, code, and text related features (Section4.2), and classification phase which details the machine learning classifiers and presents the evaluation process (Section4.3).\n\nDimensionsFeaturesDefinitionReused Features FromOhm et\u00a0al.(2022)Metadata-relatedHas invalid or no homepageThe package includes invalid or no homepage.\u2717Has invalid or no author emailThe package includes invalid or no author email.\u2717Text-relatedSuspicious LOCLines of code including suspicious APIs.\u2713(Consider each API as a boolean feature)Code-relatedHas (post) install commandThe package includes install script.\u2713Has suspicious URLThe package includes suspicious URL or IP address.\u2713Has long stringThe package includes very long string (obfuscation).\u2713File-relatedHas minimum setup configurationThe developer does not specify the package details in the setup.cfg file.\u2717Has mismatch licenseThe package is missing license type uniformity in the three core positions.\u2717\n\nSECTION: 4.1.Collecting Training and Testing Datasets\n\nTraining dataset.One of the main challenges in building a classifier model to identify malicious packages is the quality and abundance of data.\nAs thousands of packages are published weekly, it is well known that only a small fraction of malicious packages are immediately flagged as security threats; hence, we cannot assume that other recently published packages are benign (non-malicious).\nTo solve this problem, we use popular packages that have been used by projects for years to compose the training/validation set of non-malicious packages, similarly as done in prior work(Ohm et\u00a0al.,2022; Zahan et\u00a0al.,2022; Taylor et\u00a0al.,2020).\nAlso, widely used and trusted popular packages are often copied by malicious packages (e.g., typosquatting), aiming to camouflage their activities and reach a larger number of users and developers(Gu et\u00a0al.,2023; Ohm et\u00a0al.,2020a).\nAs such, our approach must be able to distinguish between popular packages and their malicious copies.\nIt is important, however, to note that 1) we refrain from using any popularity metric as a feature of our classifier, our classifier should use only code and metadata-related features, and 2) we also test our model\u2019s performance on a set of benign packages that are also not popular, to simulate a real-case scenario of using our approach.\n\nTo build our training and validation dataset, we collect5,193 benign packages.\nWe first collect the top 5,000 most downloaded PyPI packages, from the PyPI registry(van Kemenade and Si,2022)and select the top 5,000 most dependent upon packages from PyPI, as recorded in the libraries.io database(Lib,2024).\nWe then merge both datasets, removing all duplicates (the vast majority), leading to a total of 5,193 popular Python packages in the PyPI ecosystem.\n\nTo build our set of malicious packages for model training, we use the dataset of 252 malicious packages collected byOhm et\u00a0al.(2020a).\nAs this dataset contains multiple versions of the same malicious packages, we only kept the latest version of malicious packages, similarly as done in previous work(Liang et\u00a0al.,2021), to avoid overrepresenting a single package in out training set. We also remove (18) packages that were deemed not complete (e.g., package that included only the setup.py file or just the malicious payload). Finally, the malicious dataset contains (138) packages.\nThis dataset of malicious packages spans 2015 to 2023, and the majority of attack vectors target install time rather than runtime.\nThe dataset includes a variety of different injection techniques, such as TypoSquatting (52%) and Trojan Horse (27%), with varying infiltration objectives, e.g., data exfiltration (44%), droppers (15%), backdoors (8%).\nMore information can be found in the study ofOhm et\u00a0al.(2020a).\n\nTest dataset.While using popular packages as a proxy for benign packages is a sensible choice, it is expected that distinguishing between popular packages and malicious packages is easier than finding malicious packages in a batch of newly published Python packages(Vu et\u00a0al.,2023).\nThus, we craft a test dataset that, by definition, does not contain any package (malicious or not) seen by the model during training but also better represents the average packages in the PyPI ecosystem.\nTo achieve this, we 1) randomly select 397 packages from the PyPI registry, hereby named astypical packages.\nFor the malicious packages, we incorporated a new collection of143 malicious packages, distinct from our training dataset selected from the dataset introduced byOhm et\u00a0al.(2020a). During the implementation of the study, the dataset maintainer supplemented the collected training dataset with these 143 malicious packages, which we subsequently designated as the test dataset.\n\nSECTION: 4.2.Feature Extraction\n\nWe need to collect features that capture different characteristics of malicious packages. We observed that majority of related work targeted NPM ecosystem, thus a set of features are adapted from NPM ecosystem(Sejfia and Sch\u00e4fer,2022; Garrett et\u00a0al.,2019), called Code-related features in our case. Other features were chosen\nbased on the grey literature(licenses,2024)and expert knowledge of the differences between benign and malicious packages (in our case called File-related features). Moreover, a few of\nmetadata-related features ( e.g., README length, dependency analysis, author information, homepage, number of versions) are explored in the context of machine learning based solution(Liang et\u00a0al.,2021; Vu et\u00a0al.,2021; Sejfia and Sch\u00e4fer,2022). In general, it\u2019s important to note that not all metadata features hold significance; certain attributes might introduce noise and subsequently impact the model\u2019s performance. Hence, we expand the feature set to include metadata-related features. Finally, a text-related feature was derived from various sensitive APIs and permissions included in the source code. These APIs have the following behaviours: produce new code during runtime (\u201dgetattr\u201d), create forks or terminate operating system processes (\u201dexit\u201d, \u201dThread\u201d, \u201dsystem\u201d), access obfuscated (hidden) code, modify system or environment variables (\u201dclear\u201d), access files and directories (\u201dopen\u201d), establish connections with external networks (\u201dopen_connection\u201d), or readwrite user input (\u201dinput\u201d, \u201dmkdtemp\u201d).\n\nOur approach involves integrating eight different sets of features summarized in Table2, to capitalize on the synergies that can arise\nfrom the interactions among these distinct features.Metadata-related features.Every PyPI package has a set of metadata that contains different information such as homepage, project-url, authors and maintainer-email.\nFor the computation of these features, we analyze the PKG-INFO file associated with each package. The validity of the email address is assessed by confirming its domain, while the homepage URL\u2019s validity is verified by ensuring its security and association with a well-known domain. The compilation of popular domains is derived from the list provided in(domains,2024).\nWe consider two features of this dimension in our approach.Has invalid or no homepage (repository):Previous work showed that the differences in source code between build artifacts of a package and the respective source code repository are a strong indicator for its maliciousness(Vu et\u00a0al.,2021; Zahan et\u00a0al.,2022).\nOur hypothesis suggests that this feature could aid in differentiating between malicious and benign packages, as supported by previous studies that emphasize the importance of having a homepage or/and source code repository for a given package. Moreover, based on a quantitative analysis of our dataset, we find that only 20% of malicious packages have a valid homepage or repository, while 73% of benign packages own a valid homepage.Has invalid or no author email:A recent investigations highlight that packages lacking a valid author email address, often a result of improper packaging guidelines, may raise suspicion and serve as an indicator of potential malicious intent(Vu et\u00a0al.,2023; Zimmermann et\u00a0al.,2019). Among the inspected packages of the before mentioned dataset, we find about 87% of benign packages have a valid author\u2019s email address in comparison to only 46% valid email address in malicious packages.\n\nText-related feature:Most known attacks had malicious code injected into setup.py file(Ohm et\u00a0al.,2020a; Vu et\u00a0al.,2020a).\nIn this particular situation, our hypothesis revolves around the possibility that the malicious attacker might disperse the harmful payload among various files. To better understand and address this scenario, we extract lines of code corresponding to the suspicious APIs. To avoid reinventing the wheel, we rely on the generated static analysis report of the packj tool(packj,2024)to construct a portion of text feature. These APIs are linked to file paths and line numbers, as demonstrated in listing2.\nTo formulate a complete text feature, we enrich these suspicious lines of code by including the source code that originates from the setup.py file, in addition to any other .py source code that involve suspicious URLs.\nMoreover, based on a quantitative analysis of our dataset, we observe that setup.py file was the most targeted file by attackers (75%). Our observation is inline with prior works(Ohm et\u00a0al.,2020a; Vu et\u00a0al.,2021; bandit4mal,2024)showed that setup.py file is the most likely file to be manipulated by attackers.\n\nCode-related features.Many characteristics have been extensively discussed in prior research studies like(Sejfia and Sch\u00e4fer,2022; Ohm et\u00a0al.,2022; Liang et\u00a0al.,2021; Garrett et\u00a0al.,2019), highlighting their significance in proficiently recognizing malicious software packages. These characteristics include installation command, suspicious URL, and long string. In this context, we leverage the previously discussed generated text feature to extract the following features.Has (post) install command:Prior works(Ohm et\u00a0al.,2022; Sejfia and Sch\u00e4fer,2022)show that the install command initiates an external operation, which is a common characteristic observed in malicious packages. Thus, we use regular expressions to search for the install command in the generated text feature.Has suspicious URL:Different studies showed that attackers often inject their IP address or URL address in malicious code(Liang et\u00a0al.,2021; Ohm et\u00a0al.,2022). We use regular expression to extract URLs and IP addresses for a particular package, then we record the suspicion of each URL/IP based on different criteria such if the URL is insecure, and if it belongs to unpopular domain. We rely on a list of approved URL domains provided by Amazon to verify the domain of a URL. This list contains the top 1 million most popular domains from Alexa, and if the domain of the URL is not present in the list we mark the URL as suspicious(domains,2024).Has long string:The obfuscated code is often very long(Kim et\u00a0al.,2011; Liang et\u00a0al.,2021). Attackers commonly apply specialized encoding methods such as base64 to obscure harmful payloads. A string is categorized as lengthy when its length surpasses a specific threshold (40 characters)(Canali et\u00a0al.,2011).\nConsequently, we adopt this characteristic with string length larger than 40, following prior works(Liang et\u00a0al.,2021; Ohm et\u00a0al.,2022; Canali et\u00a0al.,2011). Our dataset indicates that this attribute seldom appears in harmless packages.File-related features.Research conducted earlier in the NPM ecosystem(Scalco et\u00a0al.,2022)demonstrated that during typosquatting and combosquatting attacks, the malicious package imitated a popular package by utilizing the README file. Therefore, we resort to examine files other than .py files, such as PKG-INFO and setup.cfg files. Subsequently, we formulate the hypothesis that these files might play a role in discerning between malicious and benign packages. From this perspective, we extract two distinct features.Has minimum setup configuration:In the last few years, package distribution guidelines are constantly evolving(Vaidya et\u00a0al.,2019; Reitz and Schlusser,2016).\nIn Python development, both setup.cfg and setup.py are common for the purpose of packaging and distribution.\nThe setup.cfg file includes the configuration of various aspects of package distribution, such as metadata details. This approach is considered cleaner, more contemporary, and modular, enabling efficient management of settings without cluttering the main script. We compute this feature by parsing the content of the setup.cfg file. We define the minimal configuration as equivalent to the default contents automatically generated by the utilized packaging tool. We hypothesize that attackers focus on the malicious payload rather than the package design. In some instances, attackers may leave the setup.cfg file untouched, adhering to a minimal configuration. Hence, we assume that the absence of a robust design is posited as a potential indicator of a malicious package, particularly when the setup.cfg contents resemble the minimum configuration.\nTo support our assumption, we examine our dataset, discovering that 79% of the malicious packages exhibit inadequate design, while only 45% of benign packages deviate from the best practices.\n\nHas mismatch license:A grey literature(licenses,2024)highlighted that around 10% of PyPI packages lack clear usage licenses, posing a potential risk for malicious attacks. Python packages employ three methods to express licenses: license classifier, field, and file(Xu et\u00a0al.,2023; licenses,2024). We calculate this feature by examining the agreement among the types of licenses employed in the aforementioned three methods. Trusted packages adhere to proper license usage, as indicated by a recent study(Chinthanet et\u00a0al.,2021). However, our dataset analysis reveals that approximately 66% of malicious packages exhibit license discrepancies, compared to just 1% in benign packages.\n\nUntil this point, we have prepared all the features for both malicious and benign packages. Next, we perform pairwise Pearson correlation(Bollen and Barb,1981)between the features to check whether two independent variables have a linear relationship. We found no correlation.\n\nSECTION: 4.3.Classifiers and Performance Evaluation\n\nTo perform our predictions, we leverage six machine learning classifiers from Scikit-learn python library(Pedregosa et\u00a0al.,2011):\nRandom Forest, Support-Vector Machine, Decision Tree,\nMultilayer Perceptron, Naive Bayes (Bernoulli version), and Stacking. These classifiers have been used in prior works(Sejfia and Sch\u00e4fer,2022; Ohm et\u00a0al.,2022), as well as other software engineering works(Golzadeh et\u00a0al.,2021; Abdalkareem et\u00a0al.,2020; Treude and Robillard,2016). To measure the performance of each classifier, we compute the precision, recall, and F1-score.\n\nSECTION: 5.Case Study Results\n\nBest configuration (bold):num-folds = [10, 15] \u00a0\u00a0\nnum-trees = [12,100, 200] \u00a0\u00a0\nnum-words =[200,500,1000, 2000]tokenizer modes = [binary, tfidf, count, freq] \u00a0\u00a0\nlower-states = [ False,True] \u00a0\u00a0 (M/B) = (Malicious/Benign)\n\nOverallPer-class (M/B)Input FeaturesPrecision (%)Recall (%)F1-score (%)Precision (%)Recall (%)F1-score (%)Metadata-Code-File947784(89/99)(55/100)(68/99)Text978389(96/99)(67/100)(79/100)Text + preprocessing988389(97/99)(67/100)(79/100)Text + Metadata-Code-File989194(96/100)(82/100)(88/100)Text + Metadata-Code-File + preprocessing989194(97/100)(81/100)(89/100)\n\nSECTION: 5.1.RQ1: How accurate is our approach in classifying malicious packages?\n\nMotivation:Most registries have little to no review process for publishing packages(Duan et\u00a0al.,2020b), which can be exploited by attackers to publish different types of malware to harm all downstream stakeholders. To maintain the ecosystem health from malicious actions, we need to identify the malicious packages among millions of published packages. Furthermore, different scanning methods have been developed to detect packages that raise suspicion rather than those that are undoubtedly malicious. In this RQ, we aim to evaluate the effectiveness of our approach in detecting malicious packages. Our approach helps registry maintainers to have a timely and accurate prediction of the\nmalicious packages even before publishing them, and keeps the registry clean.\n\nApproach:To answer this question, we evaluated the classifiers in predicting malicious packages in the two datasets discussed in Section4.2.\nDuring training and validation, we performed a stratified 10-fold cross-validation, which trained six classifiers on 90% of the dataset and measured precision, recall, and F1-score on the remaining 10%.\nTo ensure that the results are not skewed by a particular random initialization or split, this process is repeated ten times for each classifier, and then an average performance of these runs is computed to present the overall performance for that classifier.\nMoreover, to understand the sort of vocabulary used in distinguishing malicious packages from benign ones, we extracted all tokens from a text feature and converted them to lowercase. We removed stop words from the set of tokens, and we evaluated the impact of these choices on the performance of classifiers. Then we used a combination of standard NLP techniques, including keywords and stop-word removal to convert the text into tokens. Then we fed these tokens as inputs for the text classification algorithms with/without the feature set extracted from the metadata-related, code-related, and file-related features.\nWe relied onScikit-learnpython library(Pedregosa et\u00a0al.,2011)for all classifiers, with the best configurations as shown in Table4, andKeraspython library(Chollet et\u00a0al.,2018)for text prepossessing (Tokenizer).\nNext, to understand the generalizability of our model, we tested our trained model on the test set discussed in Section4.1, applying the same process shown in Figure1to extract the corresponding features. For this evaluation, we reported only results from our best model, the Stacking classifier.\n\nClassifierP (%)R (%)F1 (%)Random Forest (RF) [n_estimators=12]978389Support Vector Machine (SVM) [kernel= linear]998087Decision Tree (DT) [max_depth=15]898989Multilayer Perceptron (MLP) [activation= logistic, solver= adam]979093Naive Bayes-Bernoulli (NB)548152Stacking (RF+SVM+MLP+DT+NB)989194\n\nFitness results:Table4presents the performance of experimented classifiers in terms of precision, recall, and F1-score values. Generally, the classifiers achieve high performance (F1-score \u00bf 87%) in identifying malicious packages in all classifiers except the Naive Bayes (52%). Ensemble stacking classifier outperforms all other classifiers achieving the highest F1-score (94.2%) considering text vocabulary and metadata-related, file-related, and code-related features. Upon closer examination of the impact of using text vocabulary for the best performed classifier (i.e., stacking), the results show that the text vocabulary has a significant contribution to the prediction. When comparing the performance of stacking classifier only on metadata-related, file-related, and code-related as input features, we find that while the combination of most extracted features (i.e., metadata-related, file-related, and code-related features) achieve F1-score (84%) , text vocabulary feature achieves higher F1-score (89%) as shown in Table3. The evaluation assures our conjecture that text vocabulary could be used as a strong candidate feature to distinguish malicious\npackage from benign one.\n\nDatasetPackage typeTotalFPFNPrecisionRecallF1-score(%)(%)(%)Trainingpopular5,1933-989194malicious138-23Testtypical3970-968790malicious143-38\n\nTest results:To test the generalizability of our trained model, we report the classification performance when inferring only the test set (the model was not trained further).\nThe results shown in Table5display both the instances of false positives and false negatives generated by our approach when employed on the test set.\nWe observe that our approach performs consistently well, reaching a F1-score of 90%, suggesting that our trained model is capable of identifying malicious packages even when regular (non-popular) Python packages are in the mix.\nWe can observe from Table5that all typical packages are correctly classified as benign packages.\nOn the other hand, when looking at the malicious packages, we observe that 105 out of 143 are classified correctly as malicious packages.\nHowever, the 38 malicious packages that were wrongly classified as benign were clones of popular packages, to inject their malicious payload.\nUpon manual analysis, we note some of these package\u2019s malicious payloads seem insufficient for our model to distinguish them from their benign counterparts. Further exploration of other features may help reduce these false negative mistakes.\n\nSECTION: 5.2.RQ2: What features are the best indicators of malicious packages?\n\nMotivation:Given that stacking classifier achieves a high fitting and test performance in identifying the malicious packages, we want to better\nunderstand the most important features that contribute to the prediction. By knowing these important features, we can set the characteristics that distinguish malicious packages from benign ones.\n\nApproach:We rely on the permutation feature importance technique(Altmann et\u00a0al.,2010)to find the most useful features in the stacking classifier.\nThis technique randomly permutes the values of one feature\nwhile preserving the values of the remaining features.\nThis process is applied to\nall features discussed in Section4.2. We use permutation_importance function in the Scikit-learn library(Pedregosa et\u00a0al.,2011)to compute the feature importance values of the stacking classifier.\n\nResults:We find that the most important features are related to the following features: has suspicious url, has a licence, has a valid author email, has minimum configuration, has long string, and has a set of suspicious API (getattr, connect, open, and read), as shown in Figure2.\nUpon a deeper examination of the percentage of the most important feature in malicious and benign packages in our dataset, we find that while 28% of malicious packages contain a suspicious url, only 9% of the benign packages include it.\nAnother important feature is the \u201dHas a License.\u201d Based on our analysis, the result shows that 77% of the malicious packages have a mismatch license between the three known locations (i.e., file, classifier, and field), and only 1% of the benign packages have this property. This confirms that these features have a significant impact on the target variable, at least in the given model\u2019s context. Moreover, it is evident that features such as \u201dHas long string\u201d and \u201dHas suspicious URL\u201d possess attributes indicative of malicious packages.\nMoreover, our dataset suggests that malicious packages tend to have more invalid homepage (80% compared to 27% for benign packages). The same observation for the \u201dHas minimum configuration\u201d (79% compared to 45% for benign packages).\nNote that only the suspicious APIs are not enough to capture most malicious packages (low recall) as shown in Table4, but when combined with metadata features, the model reaches excellent performances.\n\nSECTION: 5.3.RQ3: Is our approach useful?\n\nMotivation:In RQ1, we assessed the performance of our approach and found that the stacking classifier achieves the best results in identifying malicious packages with F1 score of 94.2%. Moreover, a recent study(Vu et\u00a0al.,2023)has brought attention to the fact that popular packages are different from\na typical Python package. The study emphasizes that these popular packages demonstrate enhanced engineering and a stronger alignment with standard Python programming conventions. Consequently, using only popular packages as the benign\ndataset might lead to unrealistic benchmark results since these packages might be relatively easy for detection tools to classify as benign.\n\nApproach:To put our results into perspective, we conducted two experiments. We compared our approach with (1) two benchmarking tools: Bandit and Packj, as in prior works(Vu et\u00a0al.,2020a,2022; Sejfia and Sch\u00e4fer,2022), (2) the method ofOhm et\u00a0al.(2022).\nFor the first experiment, we utilized the above mentioned tools for the reasons specified in Section2. We selected these tools, despite their differing threat models, to explore their effectiveness in detecting malicious dependencies and to measure the manual effort needed to filter through all signals they generate. We run both tools and manually examine the returned alerts to identify malicious packages, simulating how practitioners use these tools to identify suspicious functions and conclude the presence of malicious packages in their applications.\nWe avoid the computational overhead associated with Packj tool by focusing solely on the static code analysis capability for identifying API usage, which is then processed further to isolate the relevant phantom lines of code that constitute the text-related feature.\nIn this experiment, we determine the number of alerts generated by applying the bandit and packj tools on a random subset containing 50 benign packages and another 50 samples of malicious packages sourced from the test dataset (Table5).\nThis evaluation is conducted in two situations: the whole package and exclusively the setup.py file.In the second experiment, we exploited the method proposed byOhm et\u00a0al.(2022), due to its close similarity to ours. Their dataset, drawn from the same source as ours, comprised nearly equivalent sizes, with 150 malicious npm packages compared to our 138 Python packages.\nWe leveraged their method, which relied on the intersection of outcomes from three classifiers (SVM, RF, and MLP).\nIt is important to highlight, however, that the approach of Ohm et al(Ohm et\u00a0al.,2022)was tailored to the JavaScript ecosystem, while our approach accounted for the specificities of Python packages.\nThus, in this experiment, we are also evaluating how ecosystem-tailored features contribute to classifying malicious Python packages.\n\nMalicious Package# Packj Alerts# Bandit Alertswhole pkgsetupwhole pkgsetup2022-requests-3.0.0691655521typing-unions-3.10.0.1380611rumihelling-0.0.115040dlcsord-1.0.30000\n\nResult:Figure3presents the average number of alerts generated by bandit and packj tools in two scenarios, whole package and only setup.py file. The presented findings include both true positives and false positives.\nThe result of Figure3(a) reveals that both the bandit and packj tools generated a considerable number of false alerts when assessing the entire benign package. Nonetheless, by considering only the setup.py file, Figure3(b), the average alert count peaked at a maximum of two alerts.\nClearly, the number of alerts, for both tools has risen when considering the whole package both of benign and malicious packages, Figure3(a) and (c).\nMoreover, it can be noted from the same figure that when focusing solely on the setup.py file, we see that malicious packages tend to produce a higher number of alerts, averaging around six alerts, Figure3(d) , in contrast to benign packages (two alerts as in Figure3(b)). This reconfirms the earlier conclusions presented in references such as(Ohm et\u00a0al.,2020a; Vu et\u00a0al.,2020a), which identified that the setup.py file was the most targeted file by attackers.\n\nTo provide further details, Table6shows the number of alerts produced by the packj and bandit tools for a subset of a selected sample of malicious packages . We observed a significant volume of alerts being generated by both tools, especially when examining the complete package scenario. This necessitated significant manual effort from developers to verify the status of these packages. The bandit tool, in particular, generated 555 alerts, and a manual investigation revealed that a number of these alerts were wrongly triggered. Our approach misclassified 16 samples out of 50 (32%) where the total misclassificaton was 38 out of 143 samples (about 27%) as shown in Table5. This highlights the capability of our approach to reduce the manual workload for registry maintainers.\nIn evaluating the two tools for package security, it is evident that both tools have limitations in identifying suspicious packages, as shown in instances like typing-unions-3.10.0.1, rumihelling-0.0.1, and dlcsord-1.0.3 (Table6), particularly when focusing solely on the setup.py file. Notably, our approach successfully classified all benign packages, in contrast to packj and bandit tools, which generated excessive false alerts for benign packages. Nonetheless, it\u2019s worth emphasizing that our method results in a relatively small number of false negatives.\nIn the second experiment, we compared our approach, with features tailored to the Python ecosystem, against the approach ofOhm et\u00a0al.(2022).\nWe observed that the method proposed byOhm et\u00a0al.(2022)failed to identify certain malicious packages. In contrast, our approach proved to be more effective, successfully detecting a wider range of these packages, as depicted in Figure4. When employing a stacking classifier, we managed to detect 115 out of 138 in the training dataset and 105 out of 143 in the testing dataset. However, relying on the intersection method of the three classifiers only enabled us to detect 73 out of 138 and 81 out of 143 for the training and testing datasets, respectively.\nThis indicates that our approach, which is tailored to the Python ecosystem, is more effective in detecting malicious Python packages compared to the approach proposed by Ohm et al.(Ohm et\u00a0al.,2022).\n\nSECTION: 6.Model Misclassifications\n\nThis section analyzes the misclassification of packages by our model in both training and external datasets to understand the underlying reasons. In the training dataset, 23 out of 138 malicious and 3 out of 5,193 benign packages were misclassified. For the external dataset, 38 out of 143 malicious packages were misclassified, with no benign misclassifications among 397.\n\nMisclassification in popular packages.Our investigation found that less than 0.06% of popular packages were misclassified by our model, primarily due to poorly structured code and deviation from standard Python conventions. Misclassification occurred because of a lack of essential metadata, such as homepage, author email, or license, as seen in packages like ordereddict-1.1, cookies-2.2.1, and blob-0.16. For instance, cookies-2.2.1 had an unknown license and used suspicious APIs like \u2019getattr\u2019, \u2019setattr\u2019, \u2019open\u2019, and \u2019call\u2019, which can indicate malicious behavior. We merged vocabulary from the setup.py file with lines from suspicious files, highlighting how missing metadata and the presence of suspicious APIs lead to misclassification. Ultimately, misclassifying benign packages as malicious is considered less risky than the reverse scenario.\n\nMisclassification in malicious packages.Our model falsely classified some packages due to many reasons.\n(1) The package incorporates a reference to an external harmful function. For examaple, libpesh-0.1 package has been identified as malicious because it includes a reference to a harmful function called \u201drn\u201d which can be found in the file namedentry_point.txtas \u201deggsecutable = libari.pr:rn.\u201d Due to the absence of the text code, which are critical feature that our model relies on, this incident has the potential to deceive the model.\n(2) Using a group of suspicious APIs that are commonly employed by benign packages. For example, the package bzip-0.98 has been designed to appear as the legitimate bz2file package, and its installation script,setup.pyhas been altered to contain a malicious code that is not particularly harmful.\n\nSECTION: 7.Threats to validity\n\nInternal Validity.The internal validity threats in this study include the potential for false positives and false negatives, although these are manageable. Manual review helps mitigate false positives. The dataset may also be biased due to clustering of similar malicious samples and assuming popular PyPI packages as benign. The selection criteria for popular packages could introduce variability, impacting the results. Despite these threats, we believe the dataset is sufficient for the experiments.\n\nConstruct Validity.One threat to construct validity is the reliance on the Packj tool(packj,2024)for static analysis, as we used it primarily to extract features for training our text classifier rather than drawing final conclusions. Another threat arises from the feature set construction, as we inherited some features from prior studies that effectively identify malicious packages(Sejfia and Sch\u00e4fer,2022; Garrett et\u00a0al.,2019; packj,2024; Duan et\u00a0al.,2020b). However, this reliance may limit our outcomes, highlighting the need for future research to investigate additional features and their impact.\n\nExternal Validity.External validity concerns the generalization of our findings. In our study, we assessed the performance of our approach using 5,193 bening packages and 138 malicious ones. Hence, our results may not generalize to other datasets, as malicious dataset may not accurately represent all malicious packages in the wild, and may there are malware with different characteristics than those in our dataset. However, we still believe that our dataset is comprehensive and serve the goal of this study. Moreover, to alleviate this threat, we evaluate the model on an external (unseen) dataset, and we found that our approach\nperforms very well, suggesting its generalizability.\n\nSECTION: 8.Conclusion\n\nWe presented a machine learning-based method for detecting malware in PyPI packages using features from text, file, code, and metadata. Among the six classifiers evaluated, the stacking classifier performed best, while Naive Bayesian failed to detect known malicious packages. Our approach demonstrated practical effectiveness with low false negatives on external datasets. This method holds promise for automatic malware detection in PyPI. Future work includes expanding features and applying the technique to other ecosystems like NPM and RubyGems.\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.05259v1_content.txt"}, {"title": "Dirac-Equation Signal Processing: Physics Boosts Topological Machine\n  Learning", "authors": ["Runyue Wang", "Yu Tian", "Pietro Li\u00f2", "Ginestra Bianconi"], "published_date": "2024-12-06T15:38:58Z", "summary": "Topological signals are variables or features associated with both nodes and\nedges of a network. Recently, in the context of Topological Machine Learning,\ngreat attention has been devoted to signal processing of such topological\nsignals. Most of the previous topological signal processing algorithms treat\nnode and edge signals separately and work under the hypothesis that the true\nsignal is smooth and/or well approximated by a harmonic eigenvector of the\nHodge-Laplacian, which may be violated in practice. Here we propose\nDirac-equation signal processing, a framework for efficiently reconstructing\ntrue signals on nodes and edges, also if they are not smooth or harmonic, by\nprocessing them jointly. The proposed physics-inspired algorithm is based on\nthe spectral properties of the topological Dirac operator. It leverages the\nmathematical structure of the topological Dirac equation to boost the\nperformance of the signal processing algorithm. We discuss how the relativistic\ndispersion relation obeyed by the topological Dirac equation can be used to\nassess the quality of the signal reconstruction. Finally, we demonstrate the\nimproved performance of the algorithm with respect to previous algorithms.\nSpecifically, we show that Dirac-equation signal processing can also be used\nefficiently if the true signal is a non-trivial linear combination of more than\none eigenstate of the Dirac equation, as it generally occurs for real signals.", "arxiv_id": "2412.05132v1", "html_link": "https://arxiv.org/html/2412.05132v1", "search_term": "ti:\"machine learning\"", "html_content": "SECTION: Dirac-Equation Signal Processing: Physics Boosts Topological Machine Learning\n\nTopological signals are variables or features associated with both nodes and edges of a network. Recently, in the context of Topological Machine Learning, great attention has been devoted to signal processing of such topological signals. Most of the previous topological signal processing algorithms treat node and edge signals separately and work under the hypothesis that the true signal is smooth and/or well approximated by a harmonic eigenvector of the Hodge-Laplacian, which may be violated in practice. Here we propose Dirac-equation signal processing, a framework for efficiently reconstructing true signals on nodes and edges, also if they are not smooth or harmonic, by processing them jointly. The proposed physics-inspired algorithm is based on the spectral properties of the topological Dirac operator. It leverages the mathematical structure of the topological Dirac equation to boost the performance of the signal processing algorithm. We discuss how the relativistic dispersion relation obeyed by the topological Dirac equation can be used to assess the quality of the signal reconstruction. Finally, we demonstrate the improved performance of the algorithm with respect to previous algorithms. Specifically, we show that Dirac-equation signal processing can also be used efficiently if the true signal is a non-trivial linear combination of more than one eigenstate of the Dirac equation, as it generally occurs for real signals.\n\nPhysics and Artificial Intelligence are strongly related[1]as the theory of information is at the core of natural physical systems as well as of learning. Indeed, it is not by chance that the theory of learning has its roots in physically inspired models such as the Hopfield model[2]strongly related to statistical mechanics of disordered systems[3,4]. In more recent developments of the field, however, not only classical statistical mechanics has become relevant to understanding learning but also high-energy physics[5], quantum physics[6]and network science[7,8]that are closer to a geometrical and topological interpretation of data.\n\nTopological Signal Processing[9,10,11,12]and Topological Machine Learning[13,14,15,16], are currently at the forefront of Artificial Intelligence and combine algebraic topology[17]and higher-order networks to learning. At the core of the field, lies Topological Data Analysis[18,19]that is now one of the principal approaches in computational neuroscience and has been shown to be very successful in extracting important topological information encoded in brain data[20,21,22,23,24]. More recently, growing scientific interest has been addressed in the development of machine learning algorithms for processing and learning topological signals defined on networks (graphs) as well as simplicial complexes. Topological signals are variables associated not only to nodes but also to the edges of a network or higher dimensional simplices of simplicial complexes.\nTopological signals and specifically edges signals are ubiquitous, as they can in general be used to represent fluxes defined on the edges and also vector fields[9]such as currents in ocean[25]or speed of wind at a given altitude and different locations on the Earth. Edge signals are also considered to be key for brain research: at the neuronal level, they describe synaptic signals, while at the level of brain regions, there are new proposals to extract and analyze these signals from brain data[26,27].\n\nFrom the perspective of higher-order networks[7,8,28,29,30], the study of topological signals greatly enriches the dynamical description of the networks. Indeed topological signals can undergo collective phenomena such as topological synchronization[31,32,33], and topological diffusion[34,35,25,36]that display significant differences with their corresponding node based dynamics.\n\nFrom the perspective of Artificial Intelligence, signal processing of topological signals[9,10,11,12]leads to new connections with Topology that were less significant for graph signal processing[37], and leads to the definition of a new generation of neural network architectures based on topology[38], on topological message passing[39,39]and on sheafs[40,41,42].\n\nMost of the topological signal processing algorithms are based on the properties of the Hodge-Laplacians[43,44]and treat the topological signal on nodes, edges, triangles, and so on separately, one dimension at a time.\nHowever, the Hodge-Laplacian is not the only topological operator that can be used to treat topological signals. Recently the Topological Dirac operator[7]has been proposed as the key topological operator that can treat jointly topological signals on nodes and edges exploiting all the information in the data across different dimensions. In this context it has been demonstrated that the Topological Dirac operator can be used to propose Dirac signal processing (DSP)[45]that outperforms Hodge-Laplacian signal processing when the true signal deviates significantly from a harmonic signal. Following these two works, the Dirac operator has become a new playground not only to test new emergent dynamical properties of networks and simplicial complexes[46,47,32,48,49]and to perform Topological Data Analysis tasks[50,51,52,53,54,55,56]but also to formulate Dirac-based Gaussian processes[57]and Dirac-based neural networks[58,59].\n\nIn this work, we propose the Dirac-equation signal processing (DESP) algorithm that can jointly process node and edge signals of a network. This algorithm is based on the mathematical properties of the Topological Dirac equation[60]that is the generalization to arbitrary lattices of the staggered fermions by Kogut and Susskind and the Dirac-K\u00e4lher fermions defined on lattices[61,62]and is inspiring further research in theoretical physics[63,64,65]and Artificial intelligence[59].\nThe DESP greatly improves the performance of the algorithm with respect to the DSP algorithm proposed in Ref.[45].\nIndeed, the use of the eigenstates of the Topological Dirac equation allows us to treat node and edge signals of different scales whose offset can be modulated by learning an additional parameter of the model that plays the role of the mass in the Topological Dirac Equation.\nThe DESP can be used to reconstruct signals that are not harmonic under very general conditions. In particular, if the true signal is aligned to an eigenstate of the Topological Dirac equation, DESP can be used to efficiently reconstruct the signal, outperforming both the Hodge-Laplacian signal processing and DSP. In this case, the learning of the mass parameter can be done by minimizing the loss of the algorithm or can be achieved by implementing physics insights and looking for the reconstructed signal that more closely obeys the relativistic dispersion relation which characterizes the eigenstates of the Topological Dirac equation.\nWhen processing real topological signals, however often the true signal is not aligned along a single eigenstate of the Dirac equation. In this case, we propose to use the Iterated Dirac-equation Signal processing (IDESP) algorithm that reconstructs the true signal by singling out the eigenstates of the Topological Dirac Equation forming its decomposition, one eigenstate at a time.\n\nHere the performance of the DESP and the IDESP is validated over both network models and real networks with both synthetic and real data.\nThe performance of the physics-inspired DESP and IDESP has greatly improved with respect to the simple DSP, and this research opens the way for further use of the Topological Dirac equation in machine learning.\n\nSECTION: IBackground\n\nSECTION: I.1Topological spinor\n\nA graphis formed by a setofnodes and a setofedges.\nThe dynamical state of a networkis fully determined by thetopological spinor[60]which comprised both the node and edge topological signals. Mathematically the topological spinoris given by the direct sumwhere indicated by a-cochainencoding for the node signals and a-cochainencoding for the edge signals. Thus the topological spinorcan be represented as thecolumn vectorwithof block structure\n\nwithbeing thecolumn vector representing the node signals andbeing thecolumn vector representing the edge signals.\n\nSECTION: I.2Hodge Laplacian signal processing\n\nDiscrete exterior calculus[17,7,29]allows us to perform discrete differential operator on topological signals that are fundamental to be able to process and filter them.\nThe exterior derivativemaps node signals to edge signals and encodes the discrete gradient of the node signal. In particularis a-cochain associating to each edge the difference between the-cochaincalculated at its two end nodes, i.e.\n\nOn an unweighted network, the discrete divergence of the edge signalmaps edge signal into node signal such that\n\nIt follows that both of these operators can be encoded by the boundary matrixis thematrix defined as\n\nwhereencodes for the discrete gradient andencodes for discrete divergence.\nFrom the boundary operator we can construct two Hodge Laplaciansalso called the graph Laplacian andalso called the-Hodge-Laplacian of the network.\nThe Hodge Laplaciansanddescribe respectively the diffusion from nodes to nodes through edges and the diffusion from edges to edges through nodes.\n\nIn this paragraph, we introduce Hodge-Laplacian signal processing (LSP) which is an umbrella model including both graph signal processing[37]and simplicial signal processing[9,10,11,12].\nSuppose we were given a noisy node or edge signalwithgiven by a true signalplus noise, i.e.\n\nwhereis the noise usually assumed to given by i.i.d. variables associated to each node (for) or each edge (for).\nFor, the Hodge-Laplacian signal processing (LSP) assumes that the true node signal is smooth, and thus is formed predominantly by low eigenmodes of the graph Laplacian[37]. Similarly, for, LSP[9,10,11,12]assumes that the true edge signal is almost harmonic, and thus able to capture fluxes going around the holes of the network.\nUnder these assumptions, the Hodge-Laplacian signal processing allows to generate a reconstructed signalthat minimizes the loss function\n\nHodge Laplacian signal processing is attracting significant attention for its ability to efficiently reconstruct almost harmonic true signal on networks. Moreover, its extension to higher-order topological signals allows the treatment of almost harmonic topological signals of higher dimension, i.e. defined also on higher dimensional simplices and cell complexes[9,10,11,12].\n\nHowever, the Hodge Laplacian signal processing also has important limitations. On one side, it cannot be used to reconstruct true signals that deviate strongly from harmonic signals. This is relevant because, while for diffusing signals smoothness is a natural assumption, in general, if we consider topological signals that correspond to real features associated to the nodes and edges of a network, we cannot always assume that the signal is smooth or close to harmonic.\nThe other limitation of this approach is that Hodge Laplacian signal processing treats separately node and edge signals while treating node and edge signals jointly might in principle contribute to reducing the error in the reconstructed signal.\nIn order to address these two important limitations we will need to use a regularization kernel defined in terms of the topological Dirac operator, defining first the Dirac signal processing and then further improving on this latter algorithm with the Dirac-equation signal processing inspired by theoretical physics.\n\nSECTION: I.3Dirac signal processing (DSP)\n\nDirac signal processing (DSP) has been recently introduced in Ref.[45]in order to jointly process noisy node and edge signals defined on a network. The algorithm can also be generalized to treat more general signal processing problems defined on simplicial complexes. The key idea of DSP is to reconstruct the true signal by minimizing a loss function that depends on the Dirac operator[60]rather than just on the Hodge Laplacian. This key idea is shown to be central in order to efficiently filter the noise from true signals that are not harmonic. In order to introduce DSP let us first discuss the major properties of the Dirac operator.\n\nThe Dirac operator[60]is a differential operator that maps topological spinors into topological spinors and allows topological signals of nodes and edges to cross-talk. On a network, the Dirac operatoris defined asand thus the matrix representationof the Dirac operator is amatrix with the following block structure:\n\nwhere the boundary operator is defined in Eq.4.\nInterestingly, the Dirac operator allows topological signals of different dimensions to cross-talk as it is apparent from evaluating the action of the Dirac operator on the general topological spinorgiven by Eq.\u00a0(1). Indeed we have\n\nthus the Dirac operator allows to project node signals into edge signals and edge signals into node signals.\nThe constitutive property of the Dirac operator is that its square is given by the Gauss-Bonnet Laplacian, i.e.\n\nThus the Dirac operator can be interpreted as thesquare root of the Laplacian. Therefore the Dirac operator has a zero eigenvalue with degeneracy equal to the sum of the Betti numbers, and there is always a basis in which the harmonic eigenvectors are localized only on nodes or on edges. Moreover, sinceandare isospectral, the non-zero eigenvaluesof the Dirac operator are given by\n\nwhereis the generic non-zero eigenvalue of the graph LaplacianThe eigenvectors associated to eigenvalueand eigenvalueare related by chirality (see for instance discussion in Refs.[60,45]), thus ifis associated to the positive eigenvalue,is associated to the opposite eigenvalue.\nThus, the structure of the eigenvectors of the Dirac operator associated to eigenvalues of increasing values (from negative, to zero, to positive) is given by the eigenvector matrix,\n\nwhereandare the matrices of left and right singular vectors of the boundary operator associated to its non-zero singular values, whileandare the matrices of left and right singular vectors of the boundary operator associated to its zero singular values.\nIn particular, we note that the non-harmonic eigenmodes of the Dirac operator associated to the eigenvalueenforce that the node signalis related to the edge signalbyand vice versaThus node and edge topological signals of single eigenmodes of the Dirac operator need to have a compatible normalization, and are not allowed to have arbitrarily different scales.\n\nThe key idea of DSP introduced in Ref.[45]is to process jointly node and edge signals in order to be able to exploit all the relevant information present in the topological spinor.\nWe assume that the true data is encoded by the topological spinor, but that we have only access to the noisy signalgiven by\n\nwhereindicates the noise.\nAs we have seen in the previous chapters, the underlying assumption of LSP is that the true signal is harmonic, or close to harmonic. On the contrary, in DSP the underlying assumption is that the signal has a major contribution aligned with the eigenvector associated to the eigenvalueof the Dirac operator, where the exact value ofcan be actually learned by the algorithm.\nGiven the noisy signal, DSP reconstructs the signalby minimizing the loss function,\n\nwhereandindicates the identity matrix. The regularization termfilters more the components of the measured signal associated to an eigenvalueof the Dirac operatorthat depart more significantly from, i.e., for whichis large. Note however that the parameteris not an external input of DSP algorithm and can be learned by the algorithm under very general conditions[45].\n\nIt is also instructive to consider the limit in which, i.e., the true signal is indeed almost harmonic. In this case, the lossreduces to\n\nand sinceis the Gauss-Bonnet Laplacian defined in Eq.11, it follows that DSP in this limit reduces to the LSP treating node and edge signals independently.\n\nDirac signal processing has been shown[45]to have an excellent performance when the true signal is an eigenstate of the Dirac operator, while when it is applied to true data the accuracy of the signal reconstruction decreases.\nHere we identify two reasons for this decrease in the performance on real data. One reason is that the non-harmonic eigenmodes of the Dirac operator imply a strict relation between the norm of the node signal and the norm of the edge signal, while on real data node and edge signals might have a different scale. The second reason is that the true signal might be given by the combination of more than two eigenmodes of the Dirac operator.\nIn order to address these two limitations, in this work we propose the Dirac-equation signal processing and the Iterated Dirac-equation signal processing that greatly improves the performance of the Dirac signal processing on real data.\n\nSECTION: IIDirac-equation signal processing (DESP)\n\nHere we introduce the Dirac-equation signal processing (DESP), a signal processing algorithm that can jointly process node and edge signals that reduces to LSP and to DSP in limiting cases and in the most general case can overcome the limitations of the previously discussed signal processing algorithms. The formulation of the DESP is inspired by theoretical physics and builds on the mathematical structure of the eigenstates of the Topological Dirac equation[60].\nThus, before discussing the DESP algorithm and its performance on synthetic and real data, let us first outline the main properties of the Topological Dirac equation.\n\nSECTION: II.1Topological Dirac equation\n\nThe Topological Dirac equation[60]is a differential equation for a quantum wave function defined on an arbitrary network. This equation is the natural extension to an arbitrary network of the staggered fermions by Kogut and Susskind[61]and the Dirac-K\u00e4hler fermions[62]defined on a lattice.\nThe Dirac equation is a wave equation for the topological spinor, defined as\n\nwhere the Hamiltonianis linear on the Dirac operatorand depends on the massas\n\nwith the matrixbeing given by\n\nThe eigenstatesof the Topological Dirac equation associated to energysatisfy the eigenvalue problem\n\nUsing the definition of the Dirac operator Eq.10and the definition of the gamma matrixEq.21this eigenvalue system can be written as\n\nThus, after a few algebraic steps we get\n\nThis implies that the node signalis an eigenvector of the graph Laplacianwith eigenvalueand that the edge signalis an eigenvector of the-Hodge Laplacianwith the same eigenvalue, where the energyis related tothrough therelativistic dispersion relation\n\nIn particular, it can be shown that both positive and negative energy states are realized with\n\nThus the role of the mass is to introduce a gap in the energy spectrum, as the energy values need to have an absolute value greater or equal to the mass, i.e..\n\nThe mass changes also significantly the properties of the eigenstates associated to non-harmonic eigenvectors.\nIn order to see this, let us discuss the structure of the eigenvectors, encoded in the matrix of eigenvectors\n\nHereare the matrices associated to the eigenvectors withandor, respectively, which are given by\n\nwhereandare the left and right singular vectors of the boundary operatorassociated to the singular valueandare normalization constants.\nWe note that the mass allows now to tune the relative normalization of the node and the edge signal which can now have very different scales. Only forthese eigenvectors reduce to the eigenvector of the Dirac operator.\nThe eigenvectors that are associated toand energyare instead the harmonic eigenvectors. These eigenvectors are independent of the value of the mass and are given by\n\nNote that the degeneracy of the eigenvalueis given by the-Betti number, while the degeneracy of the eigenvalueis given by the-Betti number.\nIn Figure1, we represent the eigenstates of the topological Dirac equation on two different networks: the network skeleton of the Network Geometry with Flavor (NGF) model[66,67]and a real fungi network from Ref.[69].\nFrom this figure, it is apparent that the harmonic eigenstates with energyare significantly different from the non-harmonic eigenstates. Indeed the harmonic eigenstates are non-trivially defined only on the nodes () or only on the edges () with the harmonic mode atbeing constant on the nodes and the generic harmonic mode atbeing a linear combination of modes localized on the cycles of the network. However the non-harmonic eigenstates of the topological Dirac equation atinvolve non-trivial pattern localization and non-trivial distribution of the signal on both nodes and edges.\nIt is clear that in general, an arbitrary topological network signal might not be harmonic, thus formulating a signal processing algorithm to infer these signals is an important research question.\n\nSECTION: II.2DESP: Problem set up and algorithm\n\nConsidering a noisy topological signaldefined on both nodes and edges and given by the true signalof the unitary norm, i.e., plus the noise, i.e.\n\nwhereindicates the noise with noise level(see Methods for details).\nThe DESP aims at reconstructing the true signal making minimal assumptions.\nThe assumption of the DESP is that the true signal is a general eigenvector of the Topological Dirac equation with energyand massto be determined by the algorithm where here and in the following.\nFor, this assumption coincides with the underlying assumption of LSP, i.e.\u00a0that the signal is harmonic or close to harmonic, and indeed the DESP algorithm reduces to LSP in this case. For, this assumption coincides with the underlying assumption of DSP that the topological signal can be a general eigenmode of the Dirac operator, and indeed in this limit we recover DSP.\nHowever in the general case where, DESP cannot be reduced to any of the previous algorithms and displays a much better performance for general signals than the previous two algorithms as it allows node and edge signal to have a different scale.\nInterestingly it is to be noted that the DESP admits a variation, the Iterative Dirac-equation signal processing (IDESP) that would allow us in the next section to go even beyond the assumption that the true signal is aligned to a single eigenstate of the Topological Dirac equation and to reconstruct efficiently true signals that are linear combinations of different eigenstates of the Topological Dirac equation that occur in real data.\nIn DESP the reconstructed signalis obtained by minimizing the following loss function:\n\nwhere here and in the following we use the notation.\nNote that here the regularization term leaves unchanged the component of the noisy signal aligned to the eigenstate of the Topological Dirac equation with energyand masswhile filtering out components associated with an energythat deviates fromwith a filter proportional to.\nFor, we get the loss function of DSP given by Eq.18, and when also, the algorithm reduces to the two decoupled LSP algorithms for node and edge signals.\nThe significant benefit to considering DESP with respect to DSP is the fact that by introducing the mass, DESP allows us to treat efficiently topological spinor whose node and edge signals have different scales as it occurs in general in data.\nThe loss function can be minimized with respect to the reconstructed signalobtaining\n\nMoreover, the losscan also be minimized with respect toandgetting\n\nNote that for the purpose of the DESP we will allow the massto take also negative real values as this is allowed in this topological setting (it is equivalent to changing the sign in front of thematrix).\n\nTheoretically, it is possible to optimizesimultaneously. However, we would also like to guarantee computational efficiency, with a cost of negligible difference in accuracy.\nThe DESP Algorithm (see pseudo-code in Algorithm1) considers a sweep over different values of, where in practice the values of the masswill span an interval bounded by the extrema of eigenvalues of the Dirac operator. For each value of, the DESP algorithm optimizes the reconstructed signaland learns the best value of the reconstructed energy. This is done by iteratively interpolating the value of the estimated energy with the estimated value of the energy that minimizes the loss function. This iterative optimization is performed using the Armijo rule[71]that ensures that the interpolation parameter is chosen in such a way to guarantee the decrease of the loss function at each step of the iteration.\nHaving performed the sweep over the relevant values of the mass, the best choice of the mass can be selected according to different criteria. The default possibility is to minimize the losscalculated over the reconstructed signaland energy, associated to the mass, i.e.\u00a0minimizinggiven by\n\nThus the reconstructed signalis the reconstructed signalcorresponding to the optimized value of the mass. Note that alternatively, we can optimize the value of the mass using the relativistic dispersion relation as we will discuss in the next paragraph.\n\nIf the true signalis known, the performance of the DESP algorithm for every value of the masscan be directly evaluated by calculating the errorgiven by\n\nwhereis the reconstructed signal assuming the mass of.\nFinally the error made by the DESP is given bygiven by\n\nIn Figure2, we show the performance of the DESP algorithm when the true signal is aligned to a single eigenstate of the Topological Dirac equation under very general conditions on the noise level. For each value ofconsidered by the algorithm, the iteration procedure lowers the error(panel (a)) and finds the energy that best approximates the true energy (panel (b)). In particular ifis given by the true value, the energyconverges to the true energy valueas the number of iterations increases (panel (b)).\nMoreover, if we do not know the value of the true mass, by performing the sweep over, the algorithm can efficiently recover the true value of the energyand the mass(panel (c) and (d)).\n\nSECTION: II.3The role of the relativistic dispersion relation in DESP\n\nIn order to optimize for the mass of the signal, we can formulate a physics-inspired optimization method that exploits the fact that eigenstates of the Topological Dirac equation satisfy the relativistic dispersion relation given by Eq.26. Therefore the reconstructed signal that more closely approximates an eigenstate of the Topological Dirac equation should minimize the relativistic dispersion relation error (RDRE)over. The RDREis given by\n\nwhere for any choice of,is the expectation of the reconstructed signalover the Laplacian andis the expectation of the same signal over the Hamiltonian, given by:\n\nThus, optimizingaccording to the RDRE entails finding the value of the massthat minimizes:\n\nwherein general and equal to zero if and only ifis an eigenvector of the Dirac equation.\n\nWe observe that optimizing the loss functiongiven by Eq.43in general gives different results with respect to the ones obtained by minimizing the RDRE. However, as long as the noise is not too high, the difference in the error made in reconstructing the true signal remains low (see Figure3).\n\nSECTION: II.4The improved performance of DESP\n\nThe DESP algorithm reduces forto LSP and forto DSP. Therefore, the DESP algorithm can only provide an improved performance with respect to the two previous algorithms. In order to compare DESP with DSP and LSP and assess the entity of the improved performance of DESP, we consider the error in the reconstructed signal generated by the three algorithms when the true signal is aligned to a single eigenstate of the Topological Dirac Equation (see Figure4).\nWe show that when the eigenstate is associated to energyand mass, DSP can outperform LSP, in particular when the energydeviates significantly from zero. Thus also DESP can greatly outperform LSP in this case. When the eigenstate is an arbitrary eigenstate associated to energyand an arbitrary value of the energy, DESP can also outperform DSP.\nThis is a great indication that DESP constitutes an important step forward in processing general node and edge topological signals.\nNote that, while here we work under the assumption that the true signal is aligned to a single eigenvector of the Topological Dirac equation, in the next section we will also address this limitation by formulating the Iterated Dirac-equation signal processing (IDESP) algorithm.\nWhen validating the performance of the DESP algorithm, it is also important to answer the question whether jointly filtering node and edge signals can be beneficial to extract more information from data.\nIn order to address this question, we have considered the scenario where the noise level over node and edge signal is different and parametrized respectively by the parametersand(see Methods for details).\nIn particular, we have considered the error made by DESP on the reconstruction of the node signalwhen the noise on the edge signal is decreased, showing that a less noisy edge signal can contribute to reconstruct better the edge signal (see panel (a) of Figure5).\nSimilarly we have shown that the error made by DESP on the reconstruction of the edge signalwhen the noise on the node signal is decreased, showing that a less noisy node signal can contribute to reconstruct better the node signal (see panel (b) of Figure5).\nThese results indicate clearly that jointly processing node and edge signals can allow to extract more information from data, leveraging on the information content encoded by both node and edge signals.\n\nSECTION: IIIIterated Dirac-equation signal processing (IDESP)\n\nFor treating real data, we need to go beyond our hypothesis that the true signal is a single eigenstate of the topological Dirac equation. Indeed in general, the true signal in real data will be a linear combination of different eigenstates of the Topological Dirac equation. Therefore, Algorithm1can only provide a prediction of the primary eigenstate. However, we can iterate the algorithm onto get the secondary eigenstate and we can iterate the process until the reduction of the coefficient of variation to the true or the estimated true value. This leads to the Iterated Dirac-equations signal processing (IDESP) algorithm2, in which the DESP algorithm is iteratedtimes, providing the reconstructed signal\n\nHowever, iterating the DESP algorithm is not enough as we need reliable criteria for determining when to stop iterating it. Indeed, increasing the number of iterationsmay not always lead to an an increase in accuracy, as after a certain number of iterations, we might end up reconstructing also part of the noise.\nIn the following, we assume that the true coefficient of variation (noise-to-signal ratio)of the measured signal, given by\n\nis either known or reliably estimated.\nIn this case, the Iterated Dirac-equation signal processing (IDESP) algorithm will iterate the DESP process up to the iterationthat minimizes the absolute difference of the coefficient of variation of the reconstructed signal and the true coefficient of variation.\nSpecifically, the IDESP will stop forwith\n\nwhere thecoefficient of variationof the reconstructed signal after theiterations is given by\n\nOnly in this way, we have that if the reconstructed signal is equal to the true signal,is the true noise-to-signal ratio and thus we guarantee that our optimization criterion given by Eq.51effectively stops at the right place.\nIn the scenario in which the true coefficient of variation is not known, this algorithm can always be used to provide an ensemble of signal reconstructions, i.e.\u00a0providing for any possible value ofthe reconstructed signalgiven by Eq.49with.\n\nWe test the IDESP on the real dataset of drifters in the ocean from the Global Ocean Drifter Program available at the AOML/NOAA Drifter Data Assembly Center already analyzed in Ref.[25,45](data available at the Repository[68]see Methods for details), finding fairly good results (see Figure6for a visualization of the performance of the IDESP algorithm).\nIn order to quantify the performance of the IDESP on this real dataset, in Figure7we monitor the true errorat iterationof the algorithm, i.e.\n\nWe observe that the error lowers up to, validating the performance of the adopted IDESP algorithm.\nDue to the nature of the signal, IDESP can offer a great improvement. Note that this improvement can be observed not only when in the DESP algorithm we determine the mass by minimizing the lossbut also when we determine the mass by minimizing the RDRE.\nThe iterated procedure can be also be applied to the DSP algorithm leading to the Iterated Dirac signal processing (IDSP) algorithm finding very significant improvements as well, however using the IDESP allows to achieve the same coefficient of variations with fewer iterations, indicating the better suitability of the IDESP in approximating the true signals.\n\nSECTION: IVConclusions\n\nIn this work, we propose Dirac-equation signal processing (DESP), a physics inspired algorithm that leverages on the properties of the Topological Dirac equation to filter and process jointly node and edge signals defined on the same network.\nWe have demonstrated through both theoretical insights and numerical verification on synthetic and real data that DESP reduces to the previously proposed LSP and DSP and that in general scenarios can outperform both of them. In particular, DESP allows to jointly process both node and edge signals, extracting relevant information across the topological signal of different dimensions, adaptively adjusting for their different scales thanks to the introduction of the learnable mass parameter.\nWhile the DESP processes signals assuming they are formed by a single eigenstate of the Topological Dirac equation, the IDESP allows to treat more general signals formed by a linear combination of eigenstates of the Topological Dirac equation. This latter algorithm can further boost the performance of DESP on real signals as demonstrated here by applying this algorithm to an extensive dataset of drifters around Madagascar.\n\nWe hope that these results will raise further interest into the use of the Topological Dirac operator and the Topological Dirac equation in Artificial intelligence, stimulating further research in both signal processing and neural networks. For instance, in signal processing, an open question is to filter topological signals across a multiplex network or knowledge graph formed by networks of networks, thus exploiting the relevant information in the different layers without simply aggregating the data. Although the focus of this paper is on topological machine learning, it is noteworthy that the Dirac operator by jointly processing node and edge signals could improve the long-range information from distant nodes and therefore ameliorate over-squashing and over-smoothing problems of topological deep learning found in graph neural networks.\n\nSECTION: Methods\n\nSECTION: Noise model\n\nIn DESP the noiseassociated to the noise levelis generated as follows.\nFirst we draw the vectorof i.i.d.\u00a0Gaussian variableswith average zero and standard deviation, associated to each simplexof the network (node or edge) i.e.and then we filter out their harmonic component, putting\n\nwhereindicates the pseudo-inverse of the Dirac operator andits rank.\nThis is the same noise model adopted in for DSP in Ref.[45].\nIn Figure5, we consider a variation of this noise model in which the vectoris formed by i.i.d. Gaussian variableswith different standard deviations depending on the dimension of the simplex. In particular we associate the nodes with a noise of standard deviation, i.e.and the edges with standard deviation, i.e.. The noiseis then given by Eq.54.\n\nSECTION: Drifter dataset\n\nWe test the IDESP algorithm on the real dataset of drifters in the ocean from the\nGlobal Ocean Drifter Program available at the AOML/NOAA Drifter Data Assembly Center[72]. The drifters data set already analyzed in Ref.[25,45]consists of the individual trajectories ofbuoys around the island of Madagascar in the Pacific Ocean. Projected onto a tessellation of the space, this yieldsedge-flows, each representing the motion of a buoy between pairs of cells (data available at the Repository[68]. The resulting network is formed bynodes, andlinks. The edge topological signalis given on each edge by the sum of all thetrajectories passing through that edge, representing the net physical flow along each edge. In the absence of a true node signal, we generate a non-trivial topological spinor playing the role of our true signalfrom the exclusive knowledge of the edge signal.\nSpecifically, we consider the topological signaldefined on both nodes and edges and we put\n\nwhereis the normalization constant that enforces.\n\nSECTION: Acknowledgments\n\nThe authors would like to thank the Isaac Newton Institute for Mathematical Sciences, Cambridge, for support and hospitality during the programme Hypergraphs: Theory and Applications, where work on this paper was undertaken. This work was supported by EPSRC grant EP/R014604/1 and partially supported by grants from the Simons Foundation (Y.T.\u00a0and G.B.)\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.05132v1_content.txt"}, {"title": "Quantum Machine Learning Algorithms for Anomaly Detection: a Review", "authors": ["Sebastiano Corli", "Lorenzo Moro", "Daniele Dragoni", "Massimiliano Dispenza", "Enrico Prati"], "published_date": "2024-08-20T17:55:25Z", "summary": "The advent of quantum computers has justified the development of quantum\nmachine learning algorithms , based on the adaptation of the principles of\nmachine learning to the formalism of qubits. Among such quantum algorithms,\nanomaly detection represents an important problem crossing several disciplines\nfrom cybersecurity, to fraud detection to particle physics. We summarize the\nkey concepts involved in quantum computing, introducing the formal concept of\nquantum speed up. The review provides a structured map of anomaly detection\nbased on quantum machine learning. We have grouped existing algorithms\naccording to the different learning methods, namely quantum supervised, quantum\nunsupervised and quantum reinforcement learning, respectively. We provide an\nestimate of the hardware resources to provide sufficient computational power in\nthe future. The review provides a systematic and compact understanding of the\ntechniques belonging to each category. We eventually provide a discussion on\nthe computational complexity of the learning methods in real application\ndomains.", "arxiv_id": "2408.11047v2", "html_link": "https://arxiv.org/html/2408.11047v2", "search_term": "ti:\"machine learning\"", "html_content": "SECTION: Quantum Machine Learning Algorithms for Anomaly Detection: a Review\n\nThe advent of quantum computers has justified the development of quantum machine learning algorithms, based on the adaptation of the principles of machine learning to the formalism of qubits. Among such quantum algorithms, anomaly detection represents an important problem crossing several disciplines from cybersecurity, to fraud detection to particle physics.\nWe summarize the key concepts involved in quantum computing, introducing the formal concept of quantum speed up. The review provides a structured map of anomaly detection based on quantum machine learning. We have grouped existing algorithms according to the different learning methods, namely quantum supervised, quantum unsupervised and quantum reinforcement learning, respectively. We provide an estimate of the hardware resources to provide sufficient computational power in the future. The review provides a systematic and compact understanding of the techniques belonging to each category. We eventually provide a discussion on the computational complexity of the learning methods in real application domains.\n\nSECTION: 1Introduction\n\nAnomaly detection takes advantage from a wide range of artificial intelligence algorithms, which \u2013 combined with human supervision \u2013 may raise the degree of protection and of integrity of systems and data.\nOn the other hand, the advent of quantum computing has made possible to implement quantum algorithms on real prototypical \u2013 but already commercial \u2013 quantum hardware. Such algorithms include machine learning-related algorithms which may inherit the quantum speed-up typical of quantum algorithms.\nThe differentiation among quantum computing architectures (gate based[1,2], adiabatic[3,4], measurement based[5,6,7]), encoding (digital[8]versus continuous variables[9,10,11,12]), hardware (several substrates from solid state to trapped ions[13,14]to photons[15]), the diversity of the quantum algorithms and the unclear advantage carried by some of them in the field of quantum machine learning, and also the range of potential application domains and the different methods to achieve the same goal, call for a systematic review of what has been done and what is known in the field, in order to address more efficiently the investigation towards meaningful, feasible and relevant applications.\nThe quantum machine learning algorithms proposed in literature for anomaly detection purposes are updated to Q1 of 2024, and clustered by applying the criteria of training method. Indeed, the latter represents the criterion which drives the choice among a families of algorithms. Therefore, we classify the quantum algorithms for machine learning according to the same classification of classical algorithms, namely among supervised learning, unsupervised learning and reinforcement learning, respectively.\nDespite its recent birth, the topic of quantum machine learning has been systematically reviewed in the time span 2015-2023[16,17,18,2,19,20]. We privileged the literature which includes a practical implementation on either an actual quantum computer or the simulator of some existing hardware. The literature reviewed by such sources is integrated by more recent articles not included there.\nIn the next sections we describe the summary of aims and AI algorithms used in anomaly detection, the basics of quantum computing and quantum advantage, the key quantum algorithms developed in the field which are relevant for anomaly detection purposes, and we systematically analyze the most recent advancements in the field of quantum machine learning applied to anomaly detection. In the second section, we summarize the application for quantum machine learning in the field of anomaly detection. In the third section, we introduce the concept of quantum advantage, along with a classification for the possible quantum speedups. In the fourth section, the groundings of quantum computing are introduced: from the definition of qubits, qudits and qumodes to the encoding of classical information into these units of computation. In the fifth section, different architectures of quantum computation (adiabatic and circuital models) are introduced, along with the support of classical computers. The sixth section is dedicated to focus on the role of HPC and classical computation to interface with quantum hardware and algorithms. Quantum neural networks and variational circuits, to translate on a quantum device the classical neural networks, are explained in section seven. In the remaining sections, a review on specific quantum algorithms for anomaly detection is provided, classified with respect to the categories of supervised, unsupervised and reinforcement learning. A summary of all the exposed algorithms can be found at Table3.In order to cover all of the classes of quantum algorithms and computational architectures in Figure1, we select and summarize a paramount paper for each of these classes. For instance, Killoran\u2019s work[10]in 2019 defined how to build continuous variables neural networks for quantum computers, along with their employment for anomaly detection, while Tacchino in 2019[21]proposed a model of fully quantum neural perceptron. The other works we report are from Useche[22](2022) for performing classification tasks with qudits, Herr[23](2021) for introducing the QGAN in the field of anomaly detection, Moro[24](2023) to boost the performances on the Restricted Boltzmann Machine via an annealer, the Harrow, Hassidim, Lloyd paper[25](2009), which introduced the namesake HHL algorithm employed for the support vector machines (and beyond), and the paper by Albarr\u00e0n-Arriagada[26](2018) for the quantum Reinforcement Learning.\n\nSECTION: 2Application domains for quantum anomaly detection\n\nAs this review elaborates on the intersection of quantum machine learning methods with applications in the anomaly detection, we first briefly assess which classes of algorithms are related to such topic, from image recognition and data classification to clustering analysis. Moreover, another topic of interest is provided by the domain of applications: cybersecurity is a major focus for quantum machine learning[27,10,28,29,30](and for all\u2013around machine learning), but a considerable interest arises on disparate topics such as big data for science, i.e. detecting Higgs\u2013boson decay at LHC collider[31,32], geophysical analysis[33], detection of new particles at LHC[34,35]or even audio recognition[36,37].\n\nTypically, cybersecurity is characterized as a collection of technologies and processes designed to protect computers, networks, programs, and data against malicious activities, attacks, harm, or unauthorized access. In the field of cybersecurity, anomaly detection is of paramount importance. Many datasets exist, including intrusion analysis, malware analysis, and spam analysis, which are used for different purposes[38].\n\nAll cybersecurity matter, while becoming increasingly crucial to all modern industrial and institutional activities, has also grown in the past decades in terms of complexity of the multiple bodies and structure which have been created to implement cyber defence functionalities.\nSOCs (Security Operation Centers) are for example hugely complex cybersecurity systems, exploited to monitor infrastructures, to supervise networks, to detect threats also able to guarantee early warning and security awareness.\nOnce security incidents have been detected, they have also to be managed. The so called Computer Emergency Response Teams (CERTs) come into play.\nSuch complexity of cyber monitoring and countermeasure systems is strongly related to an equivalent rearrangement of the threat side, where more conventional private cyber crime groups and cyber terrorists or hacker sources were joined by government linked teams. Such groups carry out a so called cyber warfare by systematically implementing cyber attacks to national or institutional IT services[39].\nComplexity grows together with continuous and rapid adaptations and modification of threats themselves. Ransomware groups and other malicious players, e.g., are changing their initial access vectors while the digital attack surface and vulnerabilities shift, also exploiting commercial tools to disguise their breaches and deploying new ransomware schemes.\nAnomaly detection[40]is a fundamental tool for this task and such continuously evolving cyber threat landscape ultimately calls for actions by SOCs and CERTs to be largely become automated only asking for man-in-the-loop in few very critical steps; which on its turn naturally links to the use of machine learning[41,42,43,44], as a means of automated response.\n\nThe main purpose of such algorithms is to provide early warning of attack, possibly even before the attack is launched[45].\nCyber intelligence deals with amount of data, their heterogeneity and their high production rate. AI is believed[46]capable to enhance cyberspace security more effectively than conventional methods for three reasons, namely:\n\nDiscovery of new and sophisticated changes in attack flexibility, by better adaptation to detect anomalous, faster and more accurate operations.\n\nNaturally handling high volumes of data\n\nLearning over time to respond better to threats.\n\nMoreover, between different AI methods such as neural networks, fuzzy logic, expert system, machine learning, and deep learning, the two latter bring the most achievements.\nIn the field of cybersecurity, the applications span mainly on malware detection, intrusion detection (ID), endpoint detection (ED), phishing detection and advanced persistent threat (APT). All of them take advantage of different methods based on a number of possible algorithms (or combinations of them): Naive Bayes method, Support Vector Machines (SVM), Decision Trees and, more recently deep Neural Networks[47,48,49,50]. One should keep in mind that since spurious transactions are far fewer than the normal ones, the highly imbalanced\ndata makes fraud detection very challenging and calls for ways to address it beyond the traditional\nmachine learning approach[28]. Furthermore, the development for instance of new fraud\ndetection methods is made more difficult due to the severe limitation of the exchange of ideas in fraud detection[51].\nMore recently, Reinforcement Learning proved to be a robust but flexible method to prevent cyber attacks[52,53,54], also thanks to a vast range of available algorithms, such as Deep Deterministic Policy Gradient (DDPG)[55], Trust Region Policy Optimization (TRPO)[56], Proximal Policy Optimization (PPO)[57], Generalized Advantage Estimation (GAE)[58].\n\nIn the broader field of anomaly detection, Neural Networks have also been successfully employed for medical and public health domain[59], fault detection for mechanical components and structural damage detection[60,61,62].\nAs for image and pattern recognition or data text analysis, along with detection of spurious elements in datasets from the corresponding domains, techniques such as Support Vector Machines[36,63], Neural Networks[64,65]and clustering based algorithms[66,67]have been deployed. Support Vector Machines have also been addressed to foil phishing attacks, achieving a rationale performance with 99.6% of True Positive Rate and 0.44% of False Positive Rate[68].\nFinally, alsoadvanced persistent threatcan be performed by deep learning algorithms, such as dilated convolutional auto-encoders (DCAEs) algorithm[69].\n\nAnomaly detection represents a major method in the field across these different purposes, and it turns out to be promisingly explored also in the field of quantum machine learning. In the next sections we therefore anticipate the key concepts of quantum information and quantum algorithms, so to consistently review anomaly detection from the perspective of the solution of security-related tasks, as outlined above.\n\nSECTION: 3Quantum algorithms and quantum advantage\n\nQuantum algorithms belong to computational classes defined by quantum Turing machines[70]instead of the conventional Turing machines.\nIn the complexity theory, for both classical and quantum computation, the runtime of an algorithm is measured in terms of number of elementary operationsinvolved[71]. In the circuit models for quantum computing, such operations match the application of native gates on the hardware for the gate-based architecture.\nTherefore, the same problem can be mapped as NP but not P for classical machines while it can be of class BQP if defined on the Hilbert spaces on which quantum Turing machines rely[72]. Jager and Krems demonstrated that there exists a feature map and a quantum kernel that make variational quantum classifiers and quantum kernel support vector machines efficient solvers for any BQP problem. Therefore, some problems which are classically NP but BQP from a quantum approach, can be solved exponentially faster by using the appropriate quantum algorithm instead of a classical algorithm. Such property is called quantum speed-up. Different degrees of speed-up have been defined by a panel of experts in 2014[73], which can be summarized as follows:\n\nthere is a proof that there can be no classical algorithm that performs as well or better than the quantum algorithm. Example: Grover\u2019s algorithm scales quadratically better than classical, provided there exist an oracle to mark the desired state;\n\nthe quantum algorithm performs better than\nthe best possible, not necessarily known explicitly, classical algorithm (i.e. lower bound to classical algorithm is not known)\nExample: Shor\u2019s quantum algorithm to factorize prime numbers (grows polynomially instead of exponentially with the number of digits of the prime number);\n\nthe quantum algorithm performs better than thebest availableclassical algorithm, as often the best available classical algorithm for strong quantum speed-up is not known;\n\nif there is no consensus about which is the best available classical algorithm, it refers to the comparison with an arbitrary classical algorithm;\n\nit refers to the benchmark of two corresponding algorithms. Example: classical and quantum annealing.\n\nThe landscape of quantum algorithms shows a range of possible speed-ups, which is even more difficult to systematize for the domain of quantum machine learning methods, where the (possible) speed-up is sometimes not quantified. The evaluation increases in difficulty as more architectures and more encoding methods are possible (see Section5and Figure1).\n\nFrom the point of view of the implementation of quantum machine learning proposed in literature, three approaches are common. First, by the direct speed-up of machine learning techniques, by using algebra-related algorithms like those in the Table1, of which the HHL algorithm is paramount[17]. Secondly, by implementing variational quantum circuits and finally multilayer perceptron-based quantum neural networks. Recently, the competitiveness of quantum models based on variational circuits compared to classical models has been raised by the demonstration[20]that explicit models[74,75]outperform implicit models, and data re-uploading models exponentially outperforms simple explicit models. Explicit models rely on a parametric definition of the unitary operators,collecting the family of parameters. Such models therefore can be easily encoded into any variational quantum circuit. The QAOA algorithm is an example of explicit model.\n\nSECTION: 4Encoding data with quantum systems\n\nSECTION: 4.1From bits to qubits\n\nThe qubit is the fundamental unit of information encoded by a quantum computer. The qubit is a quantum state, defined by a vectorin a Hilbert space. It is the transposition of the classical bit, but instead of assuming two discrete values, formally, such values are transposed in a vector state[8]:\n\nThevectors form a orthonormal basis in thespace, therefore any qubit can be set in a linear combination as in the rightmost expression for[2, pag.94]. It follows immediately that, i.e. the state vectoris normalized. Theandcoefficients represent the probability for the state to be found either in theorstate. All the logic operations on a single qubit are implemented by operatorswhich transform such state fromto:. To preserve the normalization of the vector, the single-qubit operatorsare given by the unitary group. The most known single-qubits operators are the NOT gate, thegate and the Hadamard gate:\n\nThegate flips thestate into theand vice versa, acting in fact as the classical NOT gate. The Hadamard gate, instead, is an isomorphism onmapping the computational basisinto the conjugated oneone, where. For a comparison, see Table2.\n\nIt is possible to encode bits into qubits in several ways. The most vanilla method consists of a 1-to-1 encoding, making one bitto be encoded by one quantum state. In literature, such encoding is referred to as multi-register encoding[76], whereis the number of available qubits, andcan assume the values ofor. When all the qubits are initialized in thestate, to flip a single qubit it suffices to apply aNOT gate. Such operation can be performed simultaneously, yielding a circuit depth ofoperations to be performed.\nHowever, an enhancement can be given by the superposition principle: in a register ofqubits, it is possible to encodebits, each permutation being given by the superposition of the different states. In fact, the analog encoding makes usage of all the possible permutations ofand. In the following line, the multi-register and the analog encoding are respectively shown[77]:\n\nTherefore, within the analog encoding (rightmost expression) viaqubits it is possible to storeunits of memory, which meansbits requireavailable qubits. At this point, one should also notice that in the latter case the encoding process requires an exponential numberof steps[76], calling for methods or approximations to circumvent the issue. In the past, quantum RAM (qRAM) have been proposed to feed directly the register of qubits asked to load the data[78], but hitherto no example does exists[77,79,80].\n\nOne last family of data encoding is given by the Hamiltonian encoding. Within this frame, the data need to be formatted into a Hamiltonian operator, for instance the Ising Hamiltonian. Such a technique turns to be useful mainly in order to perform quantum annealing, see Section5.2, and it will be explored in details in Section4.4.\n\nSECTION: 4.2Qudits\n\nThe qudit is a-dimension generalization of the qubit. Instead of a basis spanned byvectors, the space of qudits is generated byvectors. It is possible to choose a computational basis[81], therefore a vector in thisspace will given by\n\nThe state in Equation4must be normalized[82], so that. Referring to Section4.1.1, givenqudits it is possible to storeunits of binary memory. A feature comparison between qubits and qudits is provided in Table2.\nThe qubits operators from Equation (2) can be adapted in the qudits formalism as well. The phase Hadamard gate should be able to put anyelement from the computational basisinto a superposition over all the generators of such basis[83], along with a generalization for the NOT and theoperators[22,84]:\n\nwhere the angleis defined as. The qudits in their conjugated basis, after a Hadamard transformation, are reported in Table2. Theis the sum modulo(for the qubits,). In[22], it is introduced the control gatesas a two-qudits operators. Here a target and a control qudits need to be specified: the operationholds on the target qudit only if the control one is set in thestate, otherwise it does not. The generalized control gate operator is: theunitary operator is applied on the target qudit when the control one is set in thestate.\n\nSECTION: 4.3Qumodes (Continuous Variables)\n\nIn the frame of Continuous Variables (CV) for quantum computing, quantum information is encoded in continuous degrees of freedom such as the amplitudes of the electromagnetic field[10]. Specifically, the unit of information we described before as a qubit is substituted by the so-called qumode. In the phase space representation, the state of a single qumode is described by two real and conjugated variables such as, whetherqumodes are depicted by. Qumode states also have a representation as vectors or density matrices in the countably infinite Hilbert space spanned by the Fock states, which are the eigenstates of the photon number operator, whereandare the position and momentum operators. A comparison[9]between qubit, qudits and qumode architectures is provided in Table2.\n\nKilloran et al.[10]report some possible operations to implement in the CV frame. First, the position and momentum operators are introduced:\n\nBeinganddefined on the entire real line, the orthonormality relations hold:\n\nThe so-called Gaussian operators implement the linear transformations. On a set of a singlequmode, the rotation operatoracts between positions and momenta, while the displacement operatorperforms the translations over the qumodes:\n\nTogetherandare able to implement affine transformations on a single qumode. Another Gaussian transformation is given by the beamsplitter, which is a 2-qumodes operator:\n\nThe last of the Gaussian operator is given by the squeezing one,:\n\nDefining, it is straightforward to state the uncertainty relation in the CV frame:\n\nSECTION: 4.4Embedding into QUBO problems\n\nAdiabatic quantum computers can find the optimal solution to a specific class of optimization problems: Quadratic Unconstrained Binary Optimization (QUBO) problems. A QUBO problem is mathematically described as:\n\nwhereare the Pauli matrices that acting along the-direction, andandrepresent the parameters to the problem to be solved. We callcouplings or weights andbiases. Since the eigenvalues of the Hamiltonianrepresent the possible solution to the problem, the goal is to set the couplings and the biases so that the ground state ofrepresents the optimal solution to the optimization problem.\n\nThe QUBO problems can be represented as graphs, where nodes are associated with biases and edges with couplings. Ideally, we want to map the optimization problem graph directly into the quantum annealer QPU. However, in a quantum annealing system, the hardware graph topology, which represents the pattern of physical connections for qubits and the couplers between them, is fixed. Since we cannot modify the qubit connectivity of a specific quantum annealer, we must map the model parameters into the hardware topology by a suitable embedding to solve an optimization problem. The basic idea of embedding is to identify groups of qubits (chains) so that they form the topology of the QUBO problem under investigation by behaving as individual units. The connectivity of each group can be enhanced by creating strong ferromagnetic couplings between the qubits, which forces coupled qubits to stay in the same state.\n\nSECTION: 5Processing quantum information with quantum computers\n\nThree available architectures are provided in the field of quantum computing, see Figure1. Two out of three, the MBQC and the gate models, reproduce on a quantum device the classical Von Neumann-Zuse paradigm, i.e. processing the inputs into outputs via a sequence of commands. Such architectures can be labeled as circuital model, as both of them aim to install a circuit of logical, controlled and sequential operations on the qubits. On the contrary, the adiabatic computation provides a scheme of computation which is unedited for any classical device.\n\nSECTION: 5.1Circuital model\n\nIn the gate model, the logic gates are given by certain physical operations on the qubits. Such gates, apart from being described by unitary matrices in an algebraic fashion, can just be taught as the classical logic gates (OR, AND, XOR and so on) transposed in the quantum frame. Such logic transformations can involve just one single qubit, or rather two as well as, see Figure2. Any logic gate can be built by composing a set of universal gates, generally given by two single-qubit and one two-qubit operators[1].\n\nIn the measurement-based model instead, such logic gates are built relying on quantum phenomena such as entanglement and measurement[7]. Nevertheless, the gate model can be mapped into the MBQC one[85,6], proving that the two models are able to yield the same output.\n\nSECTION: 5.2Adiabatic quantum model\n\nQuantum annealers are quantum computers capable of finding the optimal solution to a QUBO problem by measuring the ground state of the QPU, i.e., the qubit configuration corresponding to the minimum energy of the system. The basic idea of quantum annealing is to prepare the qubits in the ground state, an easy-to-build configuration described by a Hamiltonian, and then let the system evolve until it becomes equal to, as in Equation12. If the evolution is sufficiently slow, the adiabatic theorem[3,86]guarantees that the systems stay in the ground state. Therefore, it is possible to find the solution to the optimization problem by simply measuring the system.\n\nQuantum annealers realize quantum annealing by introducing a time-dependant transverse field resulting in a total Hamiltonian:\n\nwhererepresents the time, and the functionsandcontrol the annealing evolution and are referred to as the annealing schedule. At the beginning of the annealing, the system is prepared in the ground state of(). At the end of the annealing, the system should be in the ground state of().\n\nSECTION: 6Emulation of quantum computing resources by High-Performance Computing\n\nQuantum computing is a potentially disruptive computational paradigm that will enable efficient solutions to problems that are inherently difficult for classical digital devices. Although large-scale, error-corrected quantum computers are not yet available, hardware technology is evolving at a rapid pace, and demonstrations of quantum supremacy have already been achieved on current noisy intermediate-scale quantum (NISQ) devices for selected problems of academic interest[87,88,89,90]. From the practical applications standpoint, however, NISQ devices still need to operate alongside classical digital hardware. Real-world applications are in fact characterized by complex computational workflows and large problem instances in which most of the computational burden is necessarily carried by traditional resources. For these reasons, NISQ computers are currently utilized as accelerators or co-processors embedded in hybrid quantum-classical computation.\n\nThe orchestration of hybrid hardware resources is currently implemented by means of a loose-integration paradigm, in which the classical and quantum processing is typically performed via local machines with consumer capacities and via remote quantum devices respectively. From an end-user perspective, this paradigm is considered the most effective as it allows the evaluation of alternative vendor solutions while limiting the risks associated with the experimentation of highly prototypical technologies which might suffer from rapid obsolescence. The main drawbacks of such an approach are instead related to the latency associated with the continuous data transfer, along with the additional concerns regarding the exchange with third-parties servers of sensitive or restricted data.\n\nTo mitigate these issues, the scientific community is also starting to investigate on-premises scenarios with the co-location of quantum and digital classical hardware. This is commonly considered as a first step that, in the long term, should yield to a tight-integration paradigm in which quantum and classical processors are both co-located and interconnected via dedicated high-speed, high-capacity links[91]. The first experimentations in this direction are being conducted in various high-performance computing (HPC) centers, see ateuroHPCJUandHPCQS.\n\nBeyond providing the means for an effective exploration of hybrid quantum-classical integration paradigms,\nHPC resources enable the emulation of quantum computers with up to the equivalent of 40 to 50 qubits, which is more than what most NISQ devices deliver today. Given that the current quantum hardware is still difficult and expensive to access, HPC emulators provide unique opportunities for conducting impactful R&D that would not be possible otherwise. Typical activities enabled by HPC emulators are test/development of new algorithms for real-world applications, evaluation of the solution quality and time-to-solution behavior when scaling up the size of the problem, and the investigation of co-design of quantum algorithms and hardware.\n\nA variety of emulators are currently available that can be used to implement a range of quantum algorithms, including those that are presented in this review. Most of them target a qubit architecture that implements the gate-based computational setting, either with an exact quantum state representation (state-vector, density matrix) or with approximate/compressed state representation (tensor networks). Such emulating libraries implement standard linear algebra operations that emulate the behavior of physical gates operated on the qubits register. Some of the most known software development kits (SDKs) that provide emulator backends areQiskit(IBM),Cirq(Google), and Pennylane[92](Xanadu). The majority of such libraries, which have been initially written to run on local machines, are now capable to exploit distributed memory protocols and GPU acceleration, which enable the simulation of intermediate-size quantum states (30+ qubits) and the execution of deeper circuits with acceptable running times.\n\nIn addition, as most of the hybrid quantum machine learning algorithms currently under investigation rely on variational circuits (VQE[93], QAOA[198], Quantum NN), which are parameterized circuits trained by a classical computer through the optimization of a differentiable loss function, many of these SDKs have also been designed or integrated with state-of-the-art machine learning software packages such as PyTorch[95], TensorFlow[96],Paddle-Paddleto leverage automatic differentiation techniques such as backpropagation. These can take advantage of GPU acceleration to reduce the overall execution time but incur additional memory overhead due to the need to store partial derivatives of the forward pass. The utilization of premium, large-memory GPUs that are typically available in HPC centers can boost performances.\n\nWithin the qubit architecture, a few emulators have been developed to deal with the measurement-based computational setting. Examples are Parceval (Quandela)[97]andPaddle-Quantum(Baidu), the latter taking also advantage of backpropagation methods.\nAs for architectures based on qudits,Jet(Xanadu)[98]andCirq(Google) libraries are available. Emulators based on continuous variable architectures are also available.\n\nSECTION: 7Generalization of neural networks in quantum circuits\n\nThe classes of Machine Learning and Deep Learning are often juxtaposed in literature and applications, but indeed the first includes the second as a special case based on neural networks. Deep learning exploits a deep hierarchy of layers of artificial rate neurons, resulting in a non-Von Neumann-Zuse architecture that is virtualized on standard digital CMOS hardware. A software tunes a set of hyperparameters of the NNs, called synaptic weights, to re-elaborate the inputs to outputs.\n\nA ML approach based on traditional and classical computing is straightforward to be translated on a quantum hardware, as it suffices to encode the inputs and the prompts into a quantum circuit. The main bottleneck is the constraint on the current size of the hardware, but from a theoretical perspective the problem can be treated as any classical-to-quantum algorithm. In such a way, it is quite standard to benchmark the performances between any classical algorithm and its quantum counterpart. A comparison for some of the most known classical machine learning methods and routines is given in Table1. Instead, neural network may require a paradigm shift towards new architectures. In the following two subsections, we present the two main approaches to neural networks, namely the variational and the quantum perceptron approaches, respectively.\n\nSECTION: 7.1Variational approach\n\nAs said, several algorithms rely on an Artificial Neural Network (ANN, or more simply NN). NNs allow to transform input data to outputs (labels, actions) encoding the inputs through various layers of artificial synapses. According to the Hornik\u2019s theorem[99], a sufficiently complex NN can always approximate the label output given an input.\n\nIn quantum computing, many models have been proposed to replace the classical architecture of multiperceptron-based neural networks. The main feature consists of introducing a specific circuit model, able to process the input states of the system through a series of iterations. Of course, shallow circuits belong to this architecture, as well as one-layer classical neural networks.\nIn order to process the information, instead of layers of neurons, quantum circuits display a block of unitary operations to be performed. The angles, implemented by unitary operators such as rotations, substitute the synaptic weights. In this frame, a quantum neural network (QNN) can be implemented via a variational quantum circuit (VQC)[100,101].\n\nHowever, a broad range of quantum neural networks models have been proposed[102,103,104,105,106]: hybrid quantum circuit-classical neural networks approaches[107], quantum neuron models[108]as an alternative for the classical activation functions and so on. A classical activation functionis defined as\n\nwhereis called the weight function, andbthe bias vector. There are several classes ofactivation functions, among them the perceptron, the sigmoid, the ReLu and others. The perceptron, in its classical description, given a set ofinputs acts as a step activation function[109][2, pag.49], i.e.is substituted by the Heaviside step function, or rather by a sign function. In Section8.1a model for a quantum perceptron is presented.\n\nIt must be noted that variational circuits are known for suffering the barren plateau effect[110]. As random circuits are often proposed as initial guesses for exploring the space of quantum states during optimization of the parameters, one discovers that the exponential dimension of Hilbert space and the gradient estimation complexity make such choice unsuitable on more than a few qubits. In general the probability that the gradient along reasonable directions is non-zero to some fixed precision is exponentially small as a function of the number of qubits. Mitigations to this issue have been proposed thanks to smart inizialization[111], also avoiding it thanks to quantum convolutional neural networks[112].\n\nSECTION: 7.2Neurons into qubits\n\nThere are more options to encode neurons into a qubit. Nevertheless, given the definition of a qubit, few encoding options can be defined, bounded by the maximum encoding capability of a register of N qubits. The first option consists of the-to-encoding, where each and every input neuron of the network corresponds to one qubit[113,114,115,116,117]. The information is provided as a string of bits assigned to classical base states of the quantum state space.\nSimilarly, a 1-to-1 method consists of storing a\nsuperposition of binary data as a series of bit strings in a multi-qubit state. Such\nquantum neural networks refer to the concept of the quantum associative memory[118,119]. A different-to-option is\ngiven by the quron (quantum neuron)[120]. A quron is a qubit whoseandstates stand for the resting and active neural firing state,\nrespectively[120].\n\nAlternatively, a radically different encoding option consists of\nstoring the information as coefficients of a superposition of quantum states[121,21,122,123,109,124]. The\nencoding efficiency becomes exponential as a-qubit state is an element of a-dimensional vector space, but one has to remember that also operations required to store the state increase exponentially. From one hand, loading a real image classification problem of few megabits in a quantum neural network makes the-to-option currently not viable[125], while the choice-to-allows to encode a\nmegabit image in a state by usingqubits only. In the latter case one should anyway deal with the difficulty of preparing such a state, unless it is for instance generated by a circuit which approximates some aimed distribution, or alternatively it comes directly from the physical conversion of flying qubits containing quantum data acquired by some quantum sensing system.\n\nSECTION: 8Quantum supervised learning\n\nSupervised Learning is the branch of Machine Learning which has been more transposed in a quantum formulation. Here we present the most significant quantum algorithms relevant for cybersecurity tasks, along with a description of their classical counterparts: activation functions for binary decisions[21,123,126], Support Vector Machine (SVM)[127,128,129,33,130,131,132]and kernel methods in general[74].\nOne should notice that while current cybersecurity data are fundamentally classical in nature, in the future incoming quantum data from either quantum sensors or quantum communication networks may carry quantum (entangled) data, which in turn can be classified for instance by quantum tensor networks, as demonstrated by one of the authors[133].\n\nIn the broader field of classical anomaly detection, a paramount role is played by image classification, e.g. to spot medical diseases[59,134,135]or mechanical defects in industrial processes[61,136,60], therefore a specific Section of our analysis is dedicated to their quantum counterpart.\n\nClassification by quantum tensor network on reduced MNIST with 4 categories has shown to return the same performances as best supervised learning algorithms, but more interestingly, it was able to discriminate quantum ground states carrying entanglement.\n\nIn the following Section, we introduce a range of techniques which vary from the implementation of natively quantum perceptron to the employment of adiabatic computation to improve performances of the Restricted Boltzmann Machine. Moreover, we show how to encode quantum neural networks on the continuous variables and classification tasks on qudits. All of the aforementioned techniques performs well when dealing with sampling from probabilistic distributions. Eventually, we show how to achieve quantum advantage on the Support Vector Machines via the HHL algorithm.\n\nSECTION: 8.1Quantum feed-forward Neural Network for binary decisions\n\nRecently[123,126], a model of perceptron implemented by a quantum circuit has been proposed. This model features an input vectorand a weight vector, such that the activation response depends on theirscalar product. In such scenario, the components of the vectors. The vectors can be encoded into a quantum register by the following states:\n\nwhere the statebelongs to. Being,all, andbeing the dimension of the input and weight vectors,andare real equally-weighted (REW) superpositions of all the computational basis states. The stateslive in aHilbert space, where.The inner product betweenandreturnstimes. To prepare the input state, the following transformation can be implemented as\n\nwherecan be composed by anymatrix withon the first column[21]. To perform theinner product, it is possible to define a unitary operatorsuch that.\nTo do so, choose any unitarywithon the last row. The inner product betweenandcan thus be performed by\n\nwhereis simply. Therefore,yields the scalar product. The coefficientcan be obtained, in a circuital computation, by entanglingwith an ancilla, through a multi CNOT gate (i.e. a CNOT whose control qubits are given by thestate). The sole state on which such multi-CNOT operatoracts on is thestate, i.e. the last one ():\n\nwhereis the multi-qubits control state andthe target one. Measuring the ancilla qubit on thebasis, it is possible to activate the perceptron with probability. Such achievement reproduces the perceptron in a quantum circuit. One should notice that such activation function ends the circuit with the measurement process, so the quantum information cannot travel further to other nodes. The issue has been addressed by one of the Authors[137]by replacing the measurement process with a quantum circuit performing the Taylor series of the aimed activation function. Such method enables to program a multilayered perceptron.\n\nSECTION: 8.2Quantum restricted Boltzmann Machine\n\nRestricted Boltzmann Machines (RBMs) are neural network generative models first introduced by Hinton et al. in 1983 to improve upon the Hebbian learning method used in Hopfield networks. These models are designed to learn the underlying probability distributions of a dataset by using the Boltzmann distribution in their sampling function. A RBM consists of two layers: a layer of visible binary units (representing the input/output) and a layer of hidden binary units (which help the model mimic the dataset\u2019s structure). The units are connected by real-weighted connections, as illustrated in Figure3. RBMs do not allow connections between units within the same layer, resulting in a bipartite system.\n\nRBMs are flexible neural network models that can be used for various tasks, such as generating samples, making recommendations, or extracting features. They can also be used as classifiers by using different techniques, such as using them as feature extractors and appending a separate classifier or training them supervised with the label appended to the input data. These supervised RBMs are called discriminative restricted Boltzmann machines (DRBMs), which combine descriptive power with classification ability. The idea is to train the DRBM with a dataset where the label is appended to the input, then remove it from unseen inputs and reconstruct it using the RBM.\n\nThe RBM is an energy-based model where every specific configuration of visible and hidden units is associated with an energy, where,are biases andare the weights that represent the connection strength between units. Specifically, the joint probability of a configuration is given by the Boltzmann distribution\n\nThe objective of training an RBM is to adjust the model\u2019s weights to increase the energy of states in the training dataset and lower the energy of all other configurations, allowing the model to learn how to generate and reconstruct the critical information encoded in the dataset. However, training an RBM can be challenging due to the large number of states that increases exponentially with the number of visible and hidden units, making it impractical to compute the partition function. Although an exact computation is not possible, several classical methods can be used to train the model, such as Contrastive Divergence[138](CD), Persistent Contrastive Divergence[139](PCD), and Lean Contrastive Divergence[140](LCD).\n\nAlthough these methods are effective in practice, RBMs can be more difficult and costly to train than other models that rely on backpropagation techniques, such as neural networks. Their training is often unstable and requires significant computational resources, and the approximations made during training can affect overall performance. Quantum computers provide an alternative approach to training RBMs, allowing for faster computation and better gradient estimates by querying the quantum processing unit. D-Wave quantum annealers, which are commonly used to sample the ground state of a QUBO problem, can also be used to train RBMs, resulting in faster computation and a better gradient estimate. These RBMs trained on a quantum annealer are called Quantum Restricted Boltzmann Machines (QRBMs).\n\nThe basic idea to train a RBM on a quantum computer[141,142,143]is to extract a batch of samples from the quantum machine, which are dispersed according to the Boltzmann distribution associated to the RBM. If the computational cost of initializing the quantum computer is neglected, the quantum algorithm computational complexity to obtain a single sample scales as. The advantage of employing the D-Wave adiabatic quantum machine to exploit RBMs could emerge as an increase of performance metrics, such as the accuracy and the likelihood, or as a reduction in the computational complexity or computational times depending on the specific problem under consideration. The quantum RBM has been used to address anomaly detection of IP traffic data, performing 64x faster than classic hardware in the inference[24].\nMore general machines called Boltzmann machines, based on a complete (not bipartite) graph, have also been addressed on an adiabatic quantum computer[144].\n\nSECTION: 8.3Neural networks in Continuous Variables\n\nAn architecture to set up a Neural Network (NN) by continuous variables (CV) has been provided by Killoran et al.[10]. It is shown that through the gates of CV encoding it is possible to reproduce the classical layer for a NN:\n\nwhereis the activation function,is the weight matrix andbis the bias vector. Such layer can be embedded in the CV formalism via the following sequence of operators/logic gates:\n\nHere,, whereandare the Gaussian operators in Section4.3.1, while theoperators are given by a composition of beamsplitter. Instead,is a new non-Gaussian operation we are going to define in this Section.\nTo perform Machine Learning tasks in CV, is therefore possible to implement a variational circuit built by a set of layers such as in Equation21. In the following, it is shown how a quantum neural network has been built in ref.[10]. The first three operations can be decomposed into a direct sum of two blocks:\n\nwhere the first block on the diagonal acts over thevariables, the second one over, in a similar fashion as the beamsplitter operator in Equation9. Afterwards, it is possible to apply the shifting by the displacement operator, so that the initial state, up to this point, is morphed into\n\nwhere. The next step is to implement a non-linear transformation, which is given by the non-Gaussian operations. To build up such single-qumode gate, define a non-linear transformation, which can be written in a Taylor expansion offor a certain degree of approximation, thereafter implement the operation in the form\n\nNow the unitary operation is nothing but a translation over the second qubit:\n\nEventually, a similar operation as from Equations14and20has been implemented in the framework of Continuous Variables, as stated in Equation (20):\n\nwhere.\n\nFrom the formalism in Section8.3, it is possible to develop a hybrid algorithm, where it is possible to alternate classical Neural Network with quantum circuits for Supervised Learning tasks. As both the hyperparameters from the classical NNs and the variational circuit need to be tuned, a backpropagation can be performed by training the data on a classical device. In Ref.[10], a mean square error (MSE) was introduced as loss function:\n\nThe model, when correctly trained, should return. In[10], such model of hybrid Supervised Learning was employed to detect fraudulent transactions. The performance of the training outputted a ROC curve with area, whereas the ideal curve should return a unitary area.\n\nIn the implementation of Killoran et al..[10], three potential advantages are proposed for employing the CV quantum neural networks. In the first place, CV neural networks can be performed on any photonic device, as the operation to implement them are universal in the photonic technology platform.\n\nSecondly, Hornik\u2019s theorem, as explained in Section7.1, guarantees that any Lebesgue measurable function can be reproduced by a neural network. Quantum neural networks embed this property along with effects such as superposition and entanglement, which are intrinsic of the quantum realm. Furthermore, dealing with qumodesit is possible to rely on both the positions and momenta representations,xandpbeing the Fourier transform of each other.\n\nAs the last point, quantum neural network in the CV framework can be employed for nonlinear transformations over distributions of probability. For instance, given a single-mode state, it is possible to encode its amplitude and transform it via a unitary transformation, due to the transformations acting inside the layers in Equation\u00a0(26):\n\nSECTION: 8.4Classification with qudits\n\nGiven a finite set of vectorsin, partitioned betweenclasses, the Density Matrix Kernel Density Classification method (DMKDC) is an algorithm developed by Useche et al.[22]which aims to reproduce the probability functionsfor an elementto belong to a certain class.\n\nGiven a system of qudits in aspace, suppose to have a number of classes. Thereafter, a collection of training datais provided, along with a feature map. Such a map can be set by a softmax encoding, see Ref.[145], rather than via random Fourier features (RFF), see Ref.[145,146]. Both of these encodings would provide a normalized vector such that,being encoded in the fashion of a qudit vector, as in Equation (4), whose coefficients are computed by the chosen encoding methods. In second place, the density matrix, associated to such states, is constructed as a maximally mixed state over all the samples, see the below Equation. At the same time, it is possible to define a specific density matrixcorresponding to each-th class:\n\nbeing the cardinality of the entiredataset. The frequencyaccounts how many times a databelongs to the-th class,counting the number of samples into the-th class.\nThe posterior probability for a genericsample to belong to the-th class reads as[145]\n\nThe aim of the algorithm is to samplein order to get the probability.\nAs theare known from the data, andbeing a Hermitian operator, it is possible to diagonalize it via atransformation:\n\nTheare the-th eigenvalues for theoperators,andranking fromto. For each operator, there exists a specificunitary transformation capable to diagonalize the density matrixinto the computational basis, where. The expectationcan therefore be written as\n\nAfterwards, it is possible to introduce theoperator:\n\nBefore to show the qudit implementation of the DMKDC circuit, we sum up the pipeline for the training of the training process based on the density matrix estimation:\n\nmap the datainto a qudit vector, thanks to a RFF or a softmax encoding;\n\nsample thefrequencies, thus estimating theprobability densities;\n\nintroduce theoperator able to diagonalize theobservables.\n\nIt is worthy to notice that such training procedure does not involve iterative operations. The training samples are solely employed to prepare thematrices, the time complexity of the algorithm scaling linearly on the size of training dataset. In fact, as remarked by Gonzalez[145], the complexity of the algorithm isfor the estimation of theelements,being again the cardinality of the dataset, andfor the diagonalization of the same probability densities.\n\nTo prepare the DMKDC register, in the first place a qudit with all the frequenciesis prepared as follows:\n\nIn the second place, a quditencodes the classical data to be classified, at last aancilla. The overall state, before to compute, results in\n\nAs a first step, apply agate on thequdit, which consists of a sum moduloover thegenerators, so that the new state reads\n\nwhereis the sum modulo. Afterwards, apply aand agates, with the first qudit as control and the second and third respectively as targets:\n\nwhere the first controlled gate apply aon the second qudit, while the second c-gate morphs thequdit into. Applying back agate, the state turns to be\n\nAs a second step, from the above state apply theand thegates, which outputs\n\nTo iterate the process, apply thegate on the first qudit, thereafter theandgates, for. Eventually the circuit returns the following state:\n\nThe second and the third qudits can be rewritten as\n\nwhere thecoefficients are. It is possible to recombine the tensor product coupling the diagonal terms and the off-diagonal apart:\n\nApplying thegate using the second qudit as target and the third as control, it leads to\n\nThe probabilityin Equation30can be achieved by measuring the first qudit in the-th element and the second one in:\n\nThe second passage has made usage of Equation32. At the end of the process, in the phase of testing, it is possible to point out which class the databelongs to by maximizing the probability:\n\nSECTION: 8.5Classical and Quantum Support Vector Machines\n\nIn the field of quantum machine learning, support vector machines (SVM) have been deployed for instance to distinguish anomalies from normal activities. More specifically, such algorithms has been employed to spot fraudulent credit card transactions or spurious bank loan[28], to address malware detection[147]or rather to prevent cyber attacks, such as DDoS attacks[27].\nSupport Vector Machines are a classical supervised learning algorithm which aims to learn from the training samplesin order to classify a new data sample into positive or negative class[148]. The data samples are given in the form, with say two possible classesandand a relation to satisfy given by[149]\n\nThe space where the dataare set is. In such a formulation, it is possible to define a new set of coordinateszsuch that. Theare called feature maps, mapping theto the space of thez, i.e., with.\nThe purpose is to set a hyper-plane, given by theequation, wherezare the generic coordinates in aspace anddefine the parameters for the hyperplane. The vectorwof parameters is defined as\n\nwhereare the data for the training, andtheir corresponding weights.For the classification to succeed, at the end of the trainingwandbshould be set such thatfor a training pointin the positive class, andfor a training pointin the negative class. Via the formulation in Equation47, the hyperplanecan be defined as[130]\n\nwhereis called kernel function.\n\nIn Equation48, the kernel function is defined as the inner product between feature maps, but many other definitions may arise. Linear kernels are defined as[132,150,34]\n\nIn such case,and, the dimensions of the data and the feature space, are equal.\nNevertheless, many models of different kernels may arise. Depending on the nature of the problem to be tackled, different kernels may induce different metrics for the classification tasks.\nThe polynomial kernel is defined as[131,33,34],\nwhereis the polynomial degree and,are constants to be tuned. Another class is given by the gaussian kernel[131,150,33],,\nwhereis again a constant to be tuned. The last kernel model frequently cited in literature is the the Radial Basis Function kernel (RBF)[34],.\nAgain,is a parameter to tune for best fitting the real model.\n\nIn any of these formulations,still remains a symmetric matrix.\n\nRegardless of the choice for the kernel, the final goal of the algorithm should be to reproduce the function in Equation46and ref.[127]:\n\nand therefore, the hyperparameters which need to be trained are now. A way to express the affiliation of adata to one of the two classes, for(thus being the identity map) is the following[18]:\n\nWith such a formulation, the closest datato the hyperplane yield an equation. In AppendixB, we proveto be the distance between such points, thus callingto be minimized for the classification to succeed at best.\nHowever, some datamay fall into a so-called grey region, with distancewith respect to the corresponding hyperplane. Thecan be thought as errors, or soft-variables (because the margins of the hyperplanes are now \u201csoft\u201d). The condition in Equation51can be translated into an equality:\n\nbeing. We will refer to this condition in the next steps. Beingwthe normal vector to the hyperplane with coordinatesx, the distance between the two separation hyperplanes for the two classes is given by, i.e. by the norm ofw. Choosing the direction ofw, it is possible to minimize the distance between the two regions in the phase space which define the two classes, reducing therefore the probability to find an error. The purpose is now to minimize such distance, so that any object falling in the between of the two planes can be classified with no ambiguity. Such geometrical deduction can be pursued in Figure6a. Nevertheless, when some outliers inevitably occur, as in Figure6b, we are even interested in reducing thetotal distance. Summing these conditions with the constraint in Equation52, the following system is provided:\n\nwhereis the sensitivity to the total amount of errors. This optimization problem can be formulated via the Lagrangian multipliers, where the conditionis given by the inner product ofwand the sum over the, while the constraintby the last equation of the system[129]:\n\nThecoefficients play the role for the Lagrangian multipliers. To get the best parameters, we derive the Lagrangianwith respect tow,,and:\n\nAfterwards, substitute the first and the third equation in the fourth one, which yields\n\nas. The same expression can be rewritten in terms of the kerneland with all the parameterson the left member:\n\nTaking into account the constraintfrom Equation55, the same expression in Equation57can be reformulated into a matrix fashion as\n\nThe first row holds because of the third equation in the system from Equation55. The dimension of the kernel matrixand the identitywhich multipliesis, i.e. the number of data from the training. Therefore, it is possible to obtain the parametersby just inverting thematrix:\n\nThematrix can be expressed as, where\n\nBack to Equation58, it is possible to encode the training parametersas\n\nwhere the normalization constant is set to be. As in the classical case, there are many available definitions of kernels, the first one of which can be\n\nJust for instance, in order to reproduce a polynomial kernel, it is possible to embed a state vectorin a higher dimensional space[127],,\nand therefore the inner product. To invert the matrix in Equation58, it is possible to apply the HHL algorithm, achieving an exponential speed-up. An overall explanation for the HHL algorithm is provided in AppendixA. In the first place, the linear system in Equation58can be embedded into a quantum transformation as,\nwhere the matrix(and consequently theoperator) is defined as. Thematrix has adimension, along with a norm(being its eigenvectors) because of the trace normalization. Thanks to the Lie-Trotter formula, it is possible to decompose the exponentiation ofas\n\nwhereis the trace normalization. In order to apply the HHL algorithm, the statemust be endowed with an ancillary qubit, in order to store the eigenvalues of, and decomposed into a basis for:\n\nThus, applying the HHL algorithm, thestate transforms into\n\nSECTION: 8.6Natively quantum kernel methods\n\nThe kernel method consists of embedding a set of data into a higher-dimensional space (even infinite-dimensional) called feature space[74]. Given the space of data, the kernelis defined as a map.\nSuch a map can be considered as a metric in thespace of the data. In the previous section, the kernel functionhas been alternatively defined through the feature maps, where,\nwhere. The feature mapis a map between the space of dataand the feature space,.\nA visual representation for this encoding can be seen in Figure6c. Such method turns to be useful in quantum machine learning, where classical data need to be encoded into a Hilbert space to perform some computation. It follows that the kernel function, encoding the data into the quantum circuit, induces a norm on the Hilbert space, and therefore a distance in thespace. Such feature accomplishes the purpose for a classification algorithm: given a target, it is possible to compute the distancethanks to the embedding in the feature space. By this approach, the data can be directly analyzed into a Hilbert space of features, where it is possible to deploy linear classifiers, relying on the inner products between quantum states. Increasing the size of the Hilbert space, such kernels turn to be classically intractable.\n\nA way to encode the information into qubits (i.e. in the Hilbert space) could be formulated by introducing an operator, acting as[34]. In such formulation, theoperator acts as a creator over the vacuum state. Thus the kernel can be estimated by confronting theoperators:\n\nBy such formulation, the kernel entry can be evaluated on a quantum computer by measuring thestate in the computational basis with repeated measurement shots and recording the probability of collapsing the output\ninto thestate.\n\nSECTION: 9Quantum unsupervised learning\n\nRecently, unsupervised learning gained wide success due to generative techniques, which allow to produce genuine new data mimicking the original dataset. Such techniques rely on sampling data from an unknown distribution, according to which the original ones are distributed. Quantum devices allow to generate samples from any distribution very efficiently, due to the intrinsic probabilistic nature of quantum mechanics. For instance, in[24]it has been proved that a Boltzmann Machine on an adiabatic quantum computer performs 64 times faster than its classical counterpart, involving tasks and data concerning the field of cybersecurity. In the next Section, we detail how classical generative-adversary techniques are designed and suited for quantum computers and anomaly detection purposes.\n\nSECTION: 9.1QGAN for anomaly detection\n\nDifferently from the approach based on SVM mentioned in the previous section, another approach has been proposed by Herr et al.[23]relying on hybrid quantum GANs for the anomaly detection task.\n\nGAN (Generative Adversary Network) is an unsupervised algorithm of machine learning. This algorithm is based on two agents, the discriminative and generative one. Given a distribution of dataand a set of labels to pairwith, the former model tries to fit the best conditional probability, the latter how to generate the joint probabilityfor the data distribution. It is possible to introduce some latent variables,z, to mimic the distribution for thexdata. Given a distributionfor the latent variablesz[151], the purpose for the generative model is to get the best parametersto build a function.\nThe data generated fromwill be distributed according to a probability distribution, while the true (unknown) distribution of data in thespace will be given by(standing for true). The purpose of the discriminative model, now that the hidden variableszare embedded in thespace, is to distinguish which variables are distributed according torather than:.\nThe problem can be reformulated as aminmaxone. In the WGAN (Wasserstrein GAN) formulation[23], given a set of dataxdistributed accordingly to(written as), and a set ofzwith adistribution, the purpose is to maximize the expectation function over, neutralizing at the same time the \u201cfraudulent\u201d action of:\n\nwhereis the class of all the-Lipschitz functions. A-Lipschitz functionis defined such that\n\nwhere(for,is defined 1-Lipschitz),andare respectively the domain and codomain of the functionand are metric spaces endowed with a distance and a norm (expressed byin Equation68). Thereafter, it is possible to state the proposition by Gulrajani et al.[152]:\n\nLetandtwo distributions in acompact metric space. Then, there is a-Lipschitz functionwhich is the optimal solution of.\n\nAs a corollary to the proposition, it is stated thathas gradient normalmost everywhere underand. Without entering the mathematical details and proof for such proposition, which we recommend to Gulrajani\u2019s paper[152], it is possible to set the minmax problem in Equation67with Lagrangian multipliers:\n\nwhere,being uniformly distributed in the range, i.e., whileandstill. The multiplier term is called gradient penalty term to the critic loss function.\n\nThe training for a WGAN consists of two steps: in the first one, given a set of generatedand truexinputs, the purpose is to find the global minimum for the Lagrangian loss function defined in Equation69, w.r.t.hyperparameters for themodel via a stochastic gradient descent technique.\n\nIn the second step, the purpose is instead to enhance the pursuit of the generator. The way to achieve such goal is to maximize the first Lagrangian term, w.r.t. both theandhyperparameters:\n\nIn its classical fashion, the generative modelis built up by a series oflayers. In the first place, the latent variablesare reshaped through a series of maps, so that the overall NN model results in\n\nwhere. A set of activation functions can be applied for eachlayer: in[23]leaky ReLU were deployed. Secondly, the final form of the generator will be given by thefunction:\n\nin Ref.[23]is set to be a sigmoid function on, withcollecting the corresponding hyperparameters of the weight matrixand the bias vectorb. To update the hyperparameters, it is possible to adopt any gradient descent method. The updating proceeds by the usual chain rule in the derivation process:\n\nThe discriminative modelis a NN endoewd with several hidden layers, mapping the data in aspace to the label space in.\n\nQuantum circuits support the generative procedure of the model. In fact, quantum computers are expected to sample efficiently from distributions which are hard in a classical way[153,154,87,155,76]. On the contrary, the critic model needs lots of classical data, which requires too much time to be loaded and makes such transposition unfeasible in the NISQ era[156].\n\nWhen implementing the generative model on a quantum device, thezlatent variables are given into a uniform distribution, whereas the encoded stateis given by the preparation layer. The operator, which implements such preparation layer, is composed as,being the X-rotation over theangle. After the state has been encoded, it follows a layer of rotationsin all thebasis, alternated to CNOT gates. Therefore, two hyperparameters are given:stores the basis on which to perform rotations,the angles. Whileencodes the architecture of the circuit, theangles are the variational parameters to optimize on. The multilayer generative functionin Equation70is transposed in an expectation value over:\n\nwhere instead of composinglayers of activation functions, there is a sequence ofoperators. At the end, a classical activation functionis applied to compose the last layer for the generative model. Beside this difference, the gradient descent method is applied in the same manner as from Equation73, but instead the derivative ofis given by\n\nSECTION: 9.2Other approaches to quantum anomaly detection by unsupervised learning methods\n\nGenerally speaking, quantum anomaly detection has been intensively explored in the past few years[29]. Anomaly detection for cybersecurity can take advantage of its development in other fields.\nFor instance, a field of application of anomaly detection is particle physics. There, a number of algorithms have been proposed.\nFor instance, Alve et al.[157]have applied QAD to an analysis characterized by a low statistics dataset. They have explored anomaly detection task in the four-lepton final state at the Large Hadron Collider that is limited by a small dataset, by a semi-supervised mode, without finding any evidence of speed-up. On the contrary, other examples sharing the unsupervised approach provided quantum speed-up, as follows.\n\nQuantum auto-encoders have been assessed for unsupervised machine learning models based on artificial neural\nnetworks. The aim consists of learning background distributions by quantum auto-encoders based on\nvariational quantum circuits, as problem of anomaly detection at the LHC collider. For representative signals, it turns out that a simple quantum auto-encoder outperforms classical auto-encoders[31]. There, a quantum auto-encoder has been developed, consisting of a circuit divided into three blocks, namely the state preparation that encodes classical inputs into quantum states, the unitary evolution circuit that evolves the input states, and the measurement and postprocessing part that measures the evolved state and processing the obtained observables.\n\nAn anomaly detection algorithm based on density estimation (ADDE) has been proposed by Liang et al.[158]to potentially express exponential speed-up, but it was later found not executing. Then, another group demonstrated such an exponential speed-up based on a modified version[159]. Such a new quantum ADDE algorithm is based on amplitude estimation. It is shown that such algorithm can achieve exponential speed-up on the number M of training data points compared with the classical counterpart.\n\nIn 2022, anomaly detection for credit card fraud detection have been demonstrated by quantum\nkernels on 20 qubits by authors including HSBC Bank affiliation[160]. The benchmarks consist of kernel-based approaches, in particular unsupervised modeling\non one-class support vector machines (OC-SVM). Quantum\nkernels are applied to different type of anomaly detection, leading to observe that quantum fraud detection challenges\nthe equivalent classical protocols at increasing number of features, which are equal to the number of qubits\nfor data embedding. The better precision has been achieved by combining quantum\nkernels with re-uploading, with the advantage increasing with the size of the\nsystem. The Authors claim that with 20 qubits the quantum-classical separation of average precision\nis equal to 15%. The Authors estimate the computational cost to estimate the Gram matrix representing the kernel iswhereis the number of samples, while the continuous retraining to update on-the-fly the kernel is. Instead, the time needed for inference (to assign a label fraud or not) for detectingnew-coming samples is ofkernel evaluations. The report is of particular interest as an evaluation is made for what concerns such inference time for three different hardware platforms: 1) superconducting circuits, 2) trapped ions and 3) optical systems.\nOne can evaluate the training time for a dataset of 500 elements.\nIn superconducting qubits, operation happen at MHz speed. A reproducible kernel measurements may require at leastmeasurement shots. Withkernel evaluations, the training time is 100 s - 28 h training time at optimistic MHz\nrate same cost of 28 hours. The inference time is significantly smaller down to 0.5 s in the case of reduced dataset.\nInstead, for large datasets (100000 samples), it may raise to 16 weeks, which could only be reduced with partial inference of Gram matrix.\nIn trapped ions, for 10 kHz per shots, withkernel evaluations the training time andis 3 h - 17 weeks.\nWith photons on deterministic gates, which is currently still an open field of research, the expected time ranges between 10 ms and 10 s.\n\nSECTION: 10Quantum Approximate Optimization Algorithm\n\nData clustering is the process of identifying natural groupings or clusters within multidimensional data based on some similarity measure. Clustering is a fundamental process in many different disciplines[161], for instance, it can be employed to divide the data set into a specified number of clusters, trying to minimize certain criteria (e.g. a square error function) falling into the class of optimization problems. Moreover, clustering algorithms are employed to perform network traffic identification[162]and for graph-based network security[163].\n\nIn literature, in fact, a common approach consists of representing the servers as nodes of a graph, and the flow of data between them as the edges of the graph itself[164]. By monitoring the topology of the graph, relying on a technique called graph similarity, any anomaly can be straightforwardly detected. In 2022, Li et al. proposed an algorithm of clustering in order to monitor the traffic flow on the web[162]. Nevertheless, despite the effectiveness of such approach, the graph encoding for anomaly detections turns the problem to an NP-hard one by scaling with the number of nodes \u2013 see[164,163]. Instead, quantum computation is able to tackle graph-based problems in a polynomial time. In the next Section, we provide a paramount example about how an NP-hard graph problem could be leveraged by a quantum machine learning approach.\n\nSECTION: 10.1The MaxCut problem\n\nThe MaxCut algorithm is a NP-hard combinatorial problem[165,166]which can be set as follows. Given agraph,being the vertices of the graph andtheir connections, the weights of theconnections are given by the weight matrix. The purpose in the MaxCut problem is to find the best subsetof vertices and its complementto maximize the sum over the weights connecting the two subsets[165]:.\nThe MaxCut problem can be formulated as the following integer quadratic program[165,166]and therefore mapped into a Hamiltonian formulation, inspired by the Ising model:\n\nThe constraintholds, allowing to replace such classical variables with the third Pauli matrix fromalgebra, i.e. theoperator. Nonetheless, it holds that. The cut is defined by the condition, and it can be set by maximizing theobservable[167].\nWhen two vertices belong to the same subsetor, it follows thatand the contribution tois null. Thus, the set ofandcan be thought as a partition of the system in up and down spins.\n\nThe MaxCut problem can be employed in the field of data mining and machine learning[168], with special regards to unsupervised learning[169]: it is possible to recreate unsupervised learning clustering of data by mapping the problem to a graph optimization problem and finding the minimum energy for a MaxCut problem formulation.\n\nSECTION: 10.2QAOA formulation\n\nThe quantum approximate optimization algorithm (QAOA) has been many times applied to tackle the MaxCut problem[198,170,169,171]. Such algorithm consists of preparing a register of-qubits in the eigenstate of a Hamiltonian:\n\nMore specifically, the stateis the maximum for the Hamiltonian. The purpose is now makingevolve to the maximum eigenstate for theHamiltonian from Equation76via the adiabatic theorem, i.e. to the minimum eigenstate for. The next step is thus to encode a time-dependent Hamiltonianto make the stateevolve[4]:\n\nThe annealing schedules are set by theandterms, theHamiltonian is shaped on the form of theHamiltonian in Equation76, while the transverse field Hamiltonians (and) still keep the same form.\nVia the adiabatic theorem, it is possible to makeevolve from the higher energy state ofto the higher energy state of(and therefore to the ground state of). As the Hamiltonian in Equation (78) depends on time, the corresponding time evolution is given by\n\nwhere. It is possible to approximate the above evolution by splitting the continuous trajectory along(or) in a patchwork ofsmall, discrete steps of duration[4,172]. By applying the Trotter formula[173], the operator in the above equation can be approximated as\n\nwhere. In the limit for, it is possible to involve again the Lie-Trotter formula[198,174]to split eachHamiltonian into theandterms:\n\nThe bigger is, the better both the approximations in Equations80and81work. It is possible to treat the terms in the round brackets as a set of angles,, to map the overall evolution into\n\nIn such formulation, the time evolution via a parameterhas been substituted byunitary transformations parameterized by a set ofangles. When implementing such operator on a circuit, the number of repetitions over thelayers stands for the depthof the circuit itself. The stateis therefore evolved naturally to the solution under the action of the following unitary operator:\n\nRecall the cost functionin terms of the numberof layers which are inserted in the evolution from Equation83, e.g.:.\nMore layers are inserted (i.e. the higher is), the more the solution is supposed to be exact. Afterwards, it is possible to define the maximum value over the expectation of:.\nTherefore, by the adiabatic theorem, it is possible to state that.\nEventually, it is possible to map the adiabatic process into an optimization for theparameters,, which can be achieved by a hybrid algorithm combining gradient descent methods on CPUs/GPUs with backpropagation on the quantum circuits. A useful metric, to assess how far the state is from the solution (i.e. the ground state of), is given by the approximation ratio parameter, formulated as[167,175,176,177,178,179,180,181,182,34].\nHere, withbeing the actual state of the system andthe maximum eigenvalue of the Hamiltonian operator, whose corresponding eigenstate is the goal of the problem under consideration. Whentunes to, the exact solution is provided. The approximation ratio, by such definition, returns the cost function (in our case, the Hamiltonian spectrum) normalized in thecodomain.\n\nSaid this, some critical considerations have to be done. Once the QAOA was proposed for finding approximate solutions to combinatorial optimization problems[198], it was subsequently shown that QAOA solves the combinatorial problem Max E3LIN2 with better approximation ratio with respect to any polynomial-time classical algorithm known, at the time, but soon a better classical algorithm with better approximation ratio was found[183]. The assignment of a quantum algorithm to a class of speed-up may suggest the priority around the aspects which can be investigated.\n\nSECTION: 11Quantum reinforcement learning\n\nReinforcement learning (RL) has been poorly explored in the field of quantum information, and just in the last years some interest has been raising towards this branch of Machine Learning[184,185,186].\nFor instance, Chen et al.[187]proposed a variational quantum reinforcement learning algorithm via evolutionary optimization with no evidence of quantum speed-up. Another variational implementation is due to Acuto et al.[188]. Dalla Pozza et al.[189]developed a quantum RL framework to solve a quantum maze with speed-up, and Cherrat et al.[190]show a quadratic speed-up under certain conditions for their quantum RL based on policy iteration.\nThe field looks currently less developed with respect to quantum supervised and unsupervised paradigms and more development should be expected before prospecting an evident impact on security related tasks. Nevertheless, for sake of completeness, this Section outlines some key aspects of quantum RL, which may inspire future research around its intersection with cybersecurity.\n\nAccording to one of the first proposals, when transposing classical algorithms of reinforcement learning in the quantum domain, the actions and the states of the system can be described as elements spanning two different Hilbert spacesand, or even(wherestands for environment)[191,192]. Apart from the qubits belonging to these twoandsystems, an auxiliary system, called register, can be added[26,184,193]. In such case, when initializing the overall system, the state will be presented as\n\nAs from the classical RL algorithms, three functions are required: a policy function, a reward function (RF) and a value function (VF)[26,194]. The reward function is the criterion to evaluate the goodness of an action taken by the agent, with respect to the fixed task. The value function evaluates the general convergence of the algorithm to the goal it has to be achieved. The policy function defines which action to take with respect to the fixed purpose. However, due to the nature of quantum mechanics, even extracting information from the environment to the space of actions needs a decision problem, which task is relied to the policy function. The process of extracting information from the environment to the actions can be though as an interaction with the two systems.\n\nThe simplest case deals with one qubit for the environmentand one qubit for the action space. Depending on the RL protocol to implement, the register space can be endowed with one or two qubits. Generally, such states are initialized to, i.e..\nIn the first step, the data need to be uploaded into thespace:\n\nIn the second place, apply a set of CNOT gates withas control and theas targets:\n\nSECTION: 11.1Quantum adaptation algorithm\n\nThe quantum adaptation algorithm, proposed by F. Albarr\u00e1n-Arriagada et al.[26]and applied by Shang Yu et al. (including Albarr\u00e1n-Arriagada himself) in a semiquantum way[193]has been tested to rebuild a quantum state, in order to describe a quantum system. It consists of the following steps: start from a system where all of the,andsubsystems take into account a single qubit. In the first place, encode the overall system in a similar fashion as in Equation85:\n\nTherefore, apply aoperator (CNOT onas control,as target):\n\nAfterwards, perform a measurement overin the computational basis, so that thestate is going to collapse inorwith probabilitiesor, respectively. If the superposition collapses to, the environmentand the actionshare the same state, otherwise the latter needs to be updated. To update, introduce the following operator:\n\nHereandstand for the elements from Pauli algebra, and the suffixshows that they are acting over the action space. The indexstands for the iteration over the process. The angles of rotation are defined in the following range:,\nwhereis the parameter to update per iteration. The operatoracts on thestate depending on the outcome from the measurement:\n\nwhereis the outcome from the-th measurement,if, otherwise. To update theparameter, the following rule has been proposed:\n\nandare called the reward and punishment ratios, respectivelyand, so that every time the outcome isthe value ofis reduced, whenit is increased.is a hyperparameter to tune for every set of simulations, the better theparameter, the higher the fidelity between the simulated qubit and the initial state.\n\nAt the-th iteration, the system will be set in the state,\nwhere theoperator accounts into memory all the previous actions over:\n\nSECTION: 12Concluding remarks\n\nAnomaly detection performed on quantum computers by quantum machine learning algorithms is at its infancy, but reveals high potential. At the same time, one may expect a transition for what concerns the kind of data and the applications to be managed. Indeed, quantum machine learning suffers of the bottleneck of the data loading issue. Given that no qRAM does still exist, theparallelized encoding of data in qubits is currently not viable because of its exponential data loading cost. Therefore, three options can be considered: (i) a robust but qubit-expensiveencoding of classical data to be loaded by the register used as input of the quantum algorithm, or, alternatively \u2013 in some special cases \u2013 (ii) to generate the data by a pre-trained quantum circuit returning an approximate probability distribution (derived from another probability distribution easier to generate) which can introduce entangled states as input, or (iii) to feed the quantum algorithms by quantum data \u2013 another option which potentially inputs an entangled state.\nWhile it is still under investigation to which extent the quantum machine learning can be more precise and faster than classical machine learning methods on classical data, it is likely that the major advantage appears when quantum data are considered. Indeed, a quantum circuit naturally manages quantum states, while instead this is not straightforward in machine learning.\nIn several cases, there is no knowledge whether strong quantum advantage does hold or not. Algorithms withcommonquantum advantage should be better explored by looking at demonstrating some stronger quantum speed-up degree while empirically evaluating the trend of its performances when scaling for instance the number of qubits. One should be aware that the empirical search of asymptotic behavior may change the estimate of the trend as soon as larger number of qubits are achieved.\nCybersecurity inherits the algorithms from quantum machine learning, but carries the specificities of dealing with large datasets. Here, the mutually exclusive choice between kernel-based and variational learning shows the tradeoff: kernel-based guarantees optimal kernel can always be found, but it scales with, while for\nvariational learning it is possible in time.\nAny decision concerning application of machine learning to real anomaly detection datasets should begin with a real problem based on the dataset, on which a direct benchmarking comparison with classical methods could be evaluated.\n\nSECTION: Appendix AThe HHL algorithm\n\nThe Harrow, Hassidim and Lloyd (HHL) algorithm aims to solve a linear systemusing a quantum computer[17]. Such algorithm was proposed in[25]. The classical method known as the best scales roughlyoperations, versus asteps on a quantum computer, yielding an exponential speed-up in terms of number of operations[25]. The other parameters, in the time-scaling complexity, are the sparsityof the matrix, i.e. the most number of non-null entries from the rows of thematrix[77], the condition number, i.e. the ratio between the largest and smallest eigenvalues of[25]and eventually the error. Thus, the HHL algorithm, to be better performing than the best classical algorithms, requires some caveats, as thematrix to be sparse and to read out an expectation value overx, such as(being an observable), rather than outputting the exactstate. Nevertheless, such routines are quite common in quantum computation, and may pave the road to future applications in quantum machine learning.\n\nIn fact, the matrix inversion is a common routine for many computational processes. The HHL algorithm is a frequent subroutine for many machine learning methods. As in Fig. (7), the HHL algorithm consists of three main blocks:\n\nencoding thebvector into a quantum state(or assumeto be already prepared);\n\nperform a quantum phase estimation (QPE), apply a conditioned rotation on an auxiliary qubit by the achieved result and transform back the state by an inverse QPE;\n\nmeasure the ancilla qubit.\n\nUntil the qRAM or other techniques of encoding will be leveraged, the first step could turn to be the main overhead[17], in terms of numberof operations, as the classical information, encoded inbits, needs to be compressed intoqubits.\nOnce such step has been accounted, the QPE algorithm takes as input a state, an ancillaand a unitary operationto perform on. Thevector must be eigenvector for, under which hypothesis the QPE works in the following manner:\n\nwithacting over. From, it is possible to get the binary encoding of. Therefore, to getit is mandatory to divide the result byand multiply by. Just for instance, suppose to apply agate on thequbit:\n\nFor, the QPE algorithm, applied on, returns, which is the binary encoding for. Thus, in order to get the correct phase, multiply byand divide by, obtaining.\n\nTo apply the QPE for the HHL algorithm, the unitary operatorcan be decomposed as the complex exponentiation of a Hermitian generator:\n\nwhereare the eigenvalues forandits eigenvectors. Secondly, as anyHermitian operator can generate a basis inby its eigenstates, thevector can be decomposed into itsgenerators:\n\nbeing the coefficients forin thebasis. Afterwards, it is possible to apply the QPE transformation:\n\nUp to this point, thebvector has been encoded into astate, on which a QPE routine has been acted. The next step is to introduce another ancilla qubit in thestate on which to perform a conditioned rotation, using theas control qubits:\n\nwith. Applying the inverse for the QPE yields\n\nMeasuring inthe ancillary qubit (otherwise the algorithm needs to be run again), the global output turns to be\n\nApart from a normalization factor, the final output is the state encoding(being the eigenvalues for). In casenot being Hermitian, it is always possible to fix it by building up the following matrix:\n\nwith the linear system turning to be\n\nSECTION: Appendix BDistance between two hyperplanes\n\nIn the following, we prove the distance between thedata closest to the hyperplaneto be. For such points, it holds that. In the first place, we choose two points,such that,and their midpointto lie on the hyperplane, i.e., as pictured in Figure8for the 2D case. The distancebetween the two points is given by the sum of the distances between the points and the hyperplane, which we call:\n\nTherefore, the overall distanceis set to be\n\nfor thecase, as stated in Equation (104). In such case, the hyperplane consists of a line.\n\nSECTION: References", "text_file": "data\\paper_texts\\2408.11047v2_content.txt"}, {"title": "Enhancing Fourier pricing with machine learning", "authors": ["Gero Junike", "Hauke Stier"], "published_date": "2024-12-06T14:27:59Z", "summary": "Fourier pricing methods such as the Carr-Madan formula or the COS method are\nclassic tools for pricing European options for advanced models such as the\nHeston model. These methods require tuning parameters such as a damping factor,\na truncation range, a number of terms, etc. Estimating these tuning parameters\nis difficult or computationally expensive. Recently, machine learning\ntechniques have been proposed for fast pricing: they are able to learn the\nfunctional relationship between the parameters of the Heston model and the\noption price. However, machine learning techniques suffer from error control\nand require retraining for different error tolerances. In this research, we\npropose to learn the tuning parameters of the Fourier methods (instead of the\nprices) using machine learning techniques. As a result, we obtain very fast\nalgorithms with full error control: Our approach works with any error tolerance\nwithout retraining, as demonstrated in numerical experiments using the Heston\nmodel.", "arxiv_id": "2412.05070v1", "html_link": "https://arxiv.org/html/2412.05070v1", "search_term": "ti:\"machine learning\"", "html_content": "SECTION: Enhancing Fourier pricing with machine learning\n\nFourier pricing methods such as the Carr-Madan formula or the COS\nmethod are classic tools for pricing European options for advanced\nmodels such as the Heston model. These methods require tuning parameters\nsuch as a damping factor, a truncation range, a number of terms, etc.\nEstimating these tuning parameters is difficult or computationally\nexpensive. Recently, machine learning techniques have been proposed\nfor fast pricing: they are able to learn the functional relationship\nbetween the parameters of the Heston model and the option price. However,\nmachine learning techniques suffer from error control and require\nretraining for different error tolerances. In this research, we propose\nto learn the tuning parameters of the Fourier methods (instead of\nthe prices) using machine learning techniques. As a result, we obtain\nvery fast algorithms with full error control: Our approach works with\nany error tolerance without retraining, as demonstrated in numerical\nexperiments using the Heston model.Keywords:Machine learning, computational finance, option\npricing, Fourier pricing, error control, Heston modelMathematics Subject Classification:65T40, 91G20, 91B24,\n68T05\n\nSECTION: 1Introduction\n\nFourier methods, such as the Carr-Madan formula and the COS method,\nseeCarr and Madan (1999)andFang and Oosterlee (2009), are widely\nused to price European options. In order to speed up option pricing,Liu et\u00a0al. (2019a,b),Yang et\u00a0al. (2017)andSirignano and Spiliopoulos (2018)propose a prediction of option prices using\nneural networks.Ruf and Wang (2020)provide a comprehensive review\nof neural networks for option pricing.Liu et\u00a0al. (2019a,b)use a parametric approach and consider an advanced stock price model,\nsuch as the Heston model, seeHeston (1993). They use\na set of market parameters, including strike price and maturity, as\nwell as model parameters, to predict the corresponding option prices.De\u00a0Spiegeleer et\u00a0al. (2018)use machine learning techniques based on Gaussian\nprocess regression for prediction of option prices.\n\nWhileDe\u00a0Spiegeleer et\u00a0al. (2018)andLiu et\u00a0al. (2019a,b)were able to accelerate the existing Fourier methods to some extent,\ntheir approaches also exhibited certain limitations.Liu et\u00a0al. (2019a,b)obtain a mean absolute error (MAE) of about.De\u00a0Spiegeleer et\u00a0al. (2018)also obtains a MAE of aboutand a maximum absolute error\nof approximatelyon their sample.De\u00a0Spiegeleer et\u00a0al. (2018, Table 2)compare the numerical effort with the Carr-Madan formula and obtain\nan acceleration factor between 10 and 40 for European options.\n\nHowever, the approaches described inLiu et\u00a0al. (2019a,b)andDe\u00a0Spiegeleer et\u00a0al. (2018)suffer from a lack of error control: To\nachieve higher numerical pricing accuracy, deeper neural networks\nare necessary and the machine learning methods need to be retrained\nwith more samples, which is very time-consuming and impractical in\nmost situations.\n\nIn this paper, we propose an indirect use of machine learning methods\nto improve the accuracy and efficiency of existing pricing techniques\nwith full error control. We focus on the COS method, but our approach\nis also applicable to other methods, i.e., we also discuss the Carr-Madan\nformula.\n\nWe describe the main idea of the COS method, details can be found,\ne.g., inFang and Oosterlee (2009); Oosterlee and Grzelak (2019); Junike and Pankrashkin (2022):\nGiven only the characteristic function of the log-returns of the underlying,\nthe density of the log-returns is approximated in two steps: i) truncate\nthe density on a finite intervaland ii) approximate the\ntruncated density by a finite Fourier-cosine approximation withterms. There is a clever trick to obtain the cosine-coefficients of\nthe truncated density efficiently from the characteristic function.\nThe CPU time of the COS method depends linearly on the number of terms. Note that the choice of the truncation range has a significant\ninfluence on the number of terms required to achieve a certain accuracy.\nThere are explicit formulas for the truncation range and the number\nof terms depending on an error tolerance, seeJunike and Pankrashkin (2022)andJunike (2024). However, the truncation range formula\nrequires evaluating higher-order derivatives of the characteristic\nfunction, which can be very time-consuming, e.g., in the case of the\nHeston model. The formula for the number of terms requires integration\nof the product of the characteristic function and a polynomial, which\nis also very time consuming. Fortunately, the time-consuming part\nrequired to obtainanddoes not depend on the\nrequired error tolerance.\n\nIn this paper, we use machine learning techniques to learn the-th\nderivatives of the characteristic function evaluated at zero and learn\nthe integral of the characteristic function times a polynomial, which\nis independent of the required error tolerance. Then, we use these\npredicted values and the error tolerance to obtain the truncation\nrange and the number of terms. The COS method can then be applied\nto price European options.\n\nDifferent traders may use different error tolerances, but our machine\nlearning techniques do not require retraining. This error control\nis an advantage over direct prediction of option prices by machine\nlearning techniques. The actual calculation of the option price using\nthe COS method is then very fast.\n\nThe paper is structured as follows. Section2gives an overview of the Heston model, which will be used in the numerical\nexperiments. In Section3, we introduce the COS\nmethod and the Carr-Madan formula and machine learning techniques.\nSection4provides the numerical experiments\nto demonstrate the performance of the proposed method. Section5concludes the paper.\n\nSECTION: 2The Heston model\n\nConsider a financial market with a riskless bank-account and a stock\nwith deterministic pricetoday and random priceat some future date. In the Heston model with parameters,,,and, the stock\nprice is described by the following system of differential equations\n\nandare correlated Brownian motions such that,\nseeHeston (1993).\n\nThe CIR process, described by Equation (2), stays positive\nif, which is known as theFeller\ncondition, seeAndersen and Piterbarg (2007). The characteristic function\nof the log stock price, seeBakshi et\u00a0al. (1997), is given\nby\n\nwhere\n\nSECTION: 3Algorithms: Numerical tools and machine learning\n\nSECTION: 3.1The Carr-Madan formula\n\nCarr and Madan (1999)showed that the price of a European call option\nwith strikeand time to maturityis given by\n\nwhereis a damping factor such thatandis the characteristic function of.denotes the real part of a complex numberandis the complex unit. The integral in Eq. (3)\ncan be truncated to, for some, and then be evaluated\nusing, e.g., Simpson\u2019s rule withgrid points.\n\nSECTION: 3.2The COS method\n\nWe summarize the COS method. This section is based onFang and Oosterlee (2009),Junike and Pankrashkin (2022)andJunike (2024). Letbe the expectation ofunder the risk-neutral measure\nand assume that the characteristic functionof the\ncentralized log-returnsis given in closed-form.\nThe functionis explicitly given for many models such\nas the Heston model. The price of a European put option with maturityand strikeis given by\n\nwhereis the density of. The price of a call option can\nbe obtained by the put-call-parity. Very often,is not explicitly\ngiven and the COS method can be used to approximateand the price\nof the option.\n\nFor some, the densityis truncated and the truncated density\nis approximated by a cosine series expansion:\n\nwhere for, the coefficientsare defined by\n\nThe second Equality in (6) follows from a simple analysis,\nseeFang and Oosterlee (2009). The price of a European put option can\nbe approximated by replacingin (4) with its approximation\n(5), which gives\n\nwhere\n\nThe coefficientsare given in closed form whenis given analytically and the coefficientscan also be computed\nexplicitly in important cases, e.g., for plain vanilla European put\nor call options and digital options, seeFang and Oosterlee (2009). This\nmakes the COS method numerically very efficient and robust.\n\nWe provide formulas for the coefficientsfor a European put\noption: Let. For a\nEuropean put option, it holds thatifand otherwise\n\nwhere\n\nand\n\nseeJunike and Pankrashkin (2022, Appendix A). To price a call option\nit is numerically more stable to price a put option instead and use\nthe put-call parity, seeFang and Oosterlee (2009).\n\nTo apply the COS method, one has to specify the truncation rangeand the number of terms. For a given error tolerancesmall enough, both parameters can be chosen as follows to ensure an\napproximation error smaller than, seeJunike and Pankrashkin (2022)andJunike (2024). Ifis small enough andhas semi-heavy tails, the truncation range of a put option can\nbe chosen using Markov\u2019s inequality by\n\nwhereis even andis the-th root\nof the-th moment of, which can be obtained using a computer\nalgebra system and the relation\n\nOften,is a reasonable choice, seeJunike and Pankrashkin (2022, Cor. 9).\nIfis alsotimes differentiable with bounded\nderivatives, then the number of terms can be chosen by\n\nwhere\n\nseeJunike (2024, Eq. (3.8)). The last integral can be\nsolved numerically by standard techniques and in some cases it is\ngiven explicitly. One should choosesuch that the left-hand side\nof Inequality (9) is minimized.\nFor the Heston model,is set toinJunike (2024).\nAn implementation of the truncation range, the number of terms and\nthe COS method for the Heston model can found in AppendixA.3.\n\nSECTION: 3.3Machine learning techniques\n\nDecision trees (DT), seeBreiman et\u00a0al. (1984), operate\nby recursively partitioning the input data into subsets, thereby forming\na tree-like structure, see Table1and Figure1. At each internal\nnode of the DT, the algorithm selects a feature and a threshold value\nto split the data into two subsets.\n\nFor example, in the first row of Table1,\nall input values with maturityless than or equal toare assigned to node, all other values are assigned to node.\nThe goal of these splits is to create child nodes with greater homogeneity.\nThe recursive splitting process continues until a stopping criterion\nis met, such as a maximum tree depth or a minimum node size for splitting.\n\nTo build a DT for regression, the splitting is based on variance reduction.\nThe algorithm selects the features and thresholds that most strongly\nreduce the variance at each node for splitting.\n\nGiven new samples, predictions are made at the leaf nodes, where the\nmodel assigns the average of the data points within the node. This\nsimplicity and transparency make DT highly effective at handling complex\ndata sets while maintaining interpretability.\n\nRandom forests (RF), seeBreiman (2001)are an ensemble\nof DTs to improve the accuracy and robustness of predictions. Each\nDT in the RF is trained on a random subset of the data using bootstrap\naggregation. At each node, a random subset of the features is used\nfor the splitting. In a RF, each DT makes a prediction independently\nand the final output is determined by averaging the individual predictions\nof each single tree.\n\nA neural network (NN) consists of one or more layers, each consisting\nof a number of artificial neurons, seeGoodfellow et\u00a0al. (2016).\nA single neuron transforms its multidimensional inputinto a one-dimensional output. For some weights,\nthe weighted mean of the input is then transformed by an activation\nfunction, i.e., the output of a neuron\nis given byExamples\nof activation functions are the ReLU functionor\nthe Sigmoid function. In the first layer\nof the NN, the neurons receive the input data and the output of each\nneuron is passed to all neurons in the following layers until the\nlast layer is reached.\n\nAt the start of training, the weights of the NN are randomly initialized.\nDuring the training phase, the weights are chosen in such a way that\nthe functional relationship between input and output data is mapped\nas well as possible.\n\nIn this work, we test the following regularization techniques that\ncan improve the robustness of the NN: Dropout means randomly deactivating\nsome neurons. Gaussian noise is a regularization technique that adds\nnormally distributed numbers with zero mean and small variance to\neach weight at each update step. Batch normalization standardizes\nthe inputs of each layer. These and other regularization techniques\nare discussed in detail in, for example,Goodfellow et\u00a0al. (2016).\n\nSECTION: 4Numerical experiments\n\nIn this section, we use the machine learning techniques DT, NN and\nRF to predict the tuning parameters of the Carr-Madan formula and\nthe COS method. For training, we randomly generate parameters of the\nHeston model. The ranges of the six parameters are shown in Table2. The wide ranges of these parameters\ninclude parameters that are typically used for the Heston model, seeAndersen (2008); Cris\u00f3stomo (2015); Cui et\u00a0al. (2017); Engelmann et\u00a0al. (2021); Fang and Oosterlee (2009); Forde et\u00a0al. (2012); Levendorski\u012d (2012)andSchoutens et\u00a0al. (2003).\n\nFor each sample (consisting of the five parameters for the Heston\nmodel and the maturity), we computeandandfor the entire data set, using Eqs. (8,10). The derivatives ofare calculated using a computer algebra system. As a side note: One\nmay also approximate the moments as inChoudhury and Lucantoni (1996)to avoid the computation of the derivatives.\n\nWe exclude all the model parameters for which Eq. (8)\ngives negative results, assuming that the moments do not exist in\nthese cases and we remove all parameters for which the Feller conditionis not satisfied.\n\nIn the following numerical experiments, we price a European call option\nwith, strikeand interest rate. We also\ntested other strikes, i.e.,and obtained similar\nresults. For each sample, we calculate a reference price. To obtain\nthe reference prices we use the COS method with truncation rangeand number of terms, where we set.\nTo confirm the prices we use the Carr-Madan formula with truncation\nrange,and appropriate damping factors. We remove\na few samples where the prices were too unstable and the COS method\nand the Carr-Madan formula give completely different results. For\nall remaining options, the COS method and the Carr-Madan formula coincide\nat least up to seven decimal place.\n\nWe receive a cleaned data set ofsamples. We takesamples for training and validation and use the remainingsamples as a test set. All experiments are run on a laptop with an\nIntel i7-11850H processor and 32 GB of RAM.\n\nSECTION: 4.1On the tuning parameters of the COS method\n\nTo apply the COS method, we use the formulas for the truncation range\nand the number of terms in Eq. (7)\nand (9). For the Heston model,\nit is time-consuming to computein Eq. (8)\nand to solve the integralin Eq. (10).\nTherefore, we use the machine learning techniques DT, RF and NN for\na fast estimation ofand.\n\nTo identify an appropriate architecture for the different machine\nlearning techniques, we perform a rough hyperparameter optimization.\nFor the DT, we optimize over the maximum depth and the minimum node\nsize. In addition, the number of DTs in the RF is optimized, resulting\nin the hyperparameters shown in Table3.\nThe R packagerangeris used for both DT and RF. We consider\na big DT (bDT) of arbitrary depth and a small DT (sDT) of depth 5.\nThe sDT forand the sDT forare tabulated in\nAppendixA.3andA.3and\ncould be implemented directly without using additional software packages.\n\nThe architectural specifications of the NN are described in Table4. The NN is trained with\n100 epochs, a validation split ofand the mean squared error\n(MSE)\n\nas the loss metric. For the starting values of the weights we use\nthe He initialization, seeHe et\u00a0al. (2015). For the NN, we\nuse tensorflow via the keras package called from R.\n\nTable5shows the MSE on the test set for the different\nmachine learning techniques. It can be observed that for,\nthe NN has a smaller MSE than the RF, while the bDT has a comparatively\nlarge MSE. With regard to, the RF has the smallest MSE,\nwhile the MSE of the NN and the bDT are aboutlarger. The\nsDT has a significantly larger MSE for bothand.\n\nNext, we calculate the price of the call option for different model\nparameters. We use the COS method withorand, where.\n\nThe Table6shows the percentage of samples\nin the test set for which the required accuracy is achieved by obtaininganddirectly from Eqs. (8,10), which is very time-consuming,\nor by estimatingandvia DTs, RF or a NN, which\nis very fast. The direct way of obtainingandand the estimation by the RF result inaccurate option prices\non the test set for all. The NN also achieves a high\naccuracy of aboutfor all. This result could\nbe further improved with a different NN architecture and additional\ntraining. It can be observed that a single bDT is also able to estimateandwith sufficient accuracy to price the call\noption with different error bounds for at leastof the\nsamples. And even a simple technique like the sDT already achieves\nan accuracy of at leaston the test set.\n\nThese very good results are a consequence of the fact that the formulas\nin Eq. (7) and (9)\nare derived using many inequalities, thus overestimating the minimum\ntruncation rangeand the number of termsneeded to accurately\nprice the option. Therefore, a rough estimate ofandis sufficient for precise option pricing.\n\nThe Table7illustrates the CPU\ntime of the COS method, whereandare obtained by different\nerror tolerances. The COS method is implemented in C++ using for-loops\nwithout parallelization. It is well known, thatis usually closer to the optimal truncation range than,\nseeJunike and Pankrashkin (2022). It is therefore not surprising that\nthe average CPU time is about 10 times faster using the truncation\nrangecompared to,\nsee Table7.\n\nLet us setand let us consider two scenarios:\ni) A trader estimatesanddirectly. (Estimatingdirectly is too time consuming for the Heston model). ii)\nA trader estimatesandusing machine learning\ntechniques. From Table6, we can see that\nboth approaches will price the options very accurately for different\nerror tolerances and parameters of the Heston model. What is the impact\non the total CPU time? As shown in Table8,\nthe CPU time to obtainanddirectly takes about\n0.011sec. (Most of the time is used to estimate, we used\nR\u2019s functionintegratewith default values for numerical integration).\nThe computation ofanddominates the total CPU\ntime, since the pure application of the COS method takes aboutsec., see Table7. On the other\nhand, the CPU time to estimateandusing machine\nlearning techniques is about a factor oftotimes\nfaster than the direct computation ofand. The\ntotal CPU time of the COS method estimatingandvia a NN is aboutsec. In summary, approach ii)\nis almosttimes faster than approach i).\n\nSECTION: 4.2On the tuning parameters\nof the Carr-Madan formula\n\nIn order to apply the Carr-Madan formula, one must specify three parameters,\nnamely the damping factor, the truncation rangeand\nthe number of grid points. In the following, we use a NN and\na RF to estimate these parameters. We setand determine\noptimal parametersandfor the entire training set,\nsuch thatis minimal to achieve an error bound of.\nWe then train a NN and a RF to learn these optimal parameters. Since\nthe estimateof the NN and the RF sometimes significantly\nunderestimates the true, we double the output of the NN and the\nRF to improve the accuracy of the Carr-Madan formula. This step was\nnot necessary for the COS method, since the theoretical formulas for\nthe truncation range and number of terms are larger than the minimal\ntruncation range and number of terms.\n\nTo measure the accuracy of the Carr-Madan formula, we price a call\noption withand, using the predicted values\nforandof the NN and the RF. We obtain the required\naccuracy offorandof\nthe samples in the test set for the RF and the NN, respectively.\n\nTo compare these results, we also use standard parameters of the Carr-Madan\nformula:Carr and Madan (1999)suggest the default valuesandas a rule of thumb. The Carr-Madan formula is very\nsensitive with respect to the damping factor, we choose.\nFor these default values, the accuracy ofis reached in\nonlyof the samples in the test set (any other fixedleads to an even lower proportion). Consequently, RFs and NNs are\na useful tool for improving the accuracy of the Carr-Madan formula,\nsince there is no single damping factorand number of grid\npointsfor all cases.\n\nSECTION: 5Conclusion\n\nIn this paper, we proposed an indirect use of machine learning to\nimprove the efficiency and accuracy of the Carr-Madan formula and\nthe COS method for option pricing.Junike and Pankrashkin (2022)andJunike (2024)provide explicit bounds on the truncation\nrange and the number of terms to apply the COS method. These bounds\nensure that the COS method prices a European option within a predefined\nerror tolerance. It is generally time-consuming to obtain these bounds\nusing classical numerical tools. In this paper, we instead estimate\nthese bounds using machine learning techniques such as RF, DT and\nNN. We summarize the advantages:\n\nCompared to directly estimating the option prices using machine learning\ntechniques as inLiu et\u00a0al. (2019a,b)andDe\u00a0Spiegeleer et\u00a0al. (2018),\nour approach allows for full error control.\n\nCompared to estimating the bounds using classical numerical methods,\nour approach is much faster: about a factor.\n\nCompared to using a fast rule of thumb (as proposed inFang and Oosterlee (2009)andCarr and Madan (1999)) to estimate the tuning parameters of\nthe COS method or the Carr-Madan formula, our approach is much more\nreliable. For the COS method, seeJunike and Pankrashkin (2022)for examples\nwhere a rule of thumb based on cumulants leads to serious mispricing.\nFor the Carr-Madan formula, see Section4.2.\n\nWe tested RF, DT and NN to estimate the bounds to obtain the truncation\nrange and the number of terms to apply the COS method. Among these\ntechniques, the RF works best (accurate on 100% of the test set).\nThe NN has a similar performance. But even a small DT gives very satisfactory\nresults (accurate on 98.2% of the test set). Estimation of the tuning\nparameters of the Carr-Madan formula by a RF or a NN works in about\n90% of all samples in a test set.\n\nSECTION: Appendix AAppendix\n\nSECTION: A.1Decision tree of depthto predict\n\nSECTION: A.2Decision tree of depthto predict\n\nSECTION: A.3Simple implementation\n\nThe following algorithm implements the COS method in R for the Heston\nmodel to price European put and call options.\n\nAlgorithm 1Implementation details of the COS method in the\nHeston model\n\n#Characteristic function of log-returns in the Heston\nwith parameters params.\n\n#The characteristic function is taken from Schoutens\net. al (2004).\n\npsiLogST_Heston = function(u, mat, params, S0, r){\n\nkappa = params[1] #speed of mean reversion\n\ntheta = params[2] #level of mean reversion\n\nxi = params[3] #vol of vol\n\nrho = params[4] #correlation vol stock\n\nv0 = params[5] #initial vol\n\nd = sqrt((rho * xi * u * 1i - kappa)^2\n- xi^2 * (-1i * u - u^2))\n\nmytmp = kappa - rho * xi * u * 1i\n\ng = (mytmp - d) / (mytmp + d)\n\nexpdmat = exp(-d * mat)\n\ntmp0 = 1i * u * (log(S0) + r * mat)\n\ntmp1 = (mytmp - d) * mat - 2 * log((1 - g\n* expdmat) / (1 - g))\n\ntmp2 = theta * kappa * xi^(-2)\n* tmp1\n\ntmp3 = v0 * xi^(-2) * (mytmp\n- d) * (1 - expdmat) / (1 - g * expdmat)\n\nexp(tmp0 + tmp2 + tmp3)\n\n}\n\nlibrary(Deriv) #There are much faster alternatives\nlike SageMath.\n\npsiLogST_Heston1=Deriv(psiLogST_Heston, \"u\")\n\n#mu is equal to E[log(S_T)]\n\nmu = function(mat, params, S0, r){\n\nRe(-1i * psiLogST_Heston1(0, mat, params,\nS0, r))\n\n}\n\n#Characteristic function of centralized log-returns\nin the Heston model.\n\nphi = function(u, mat, params, S0, r){\n\npsiLogST_Heston(u, mat, params, S0, r) * exp(-1i\n* u * mu(mat, params, S0, r))\n\n}\n\n#cosine coefficients of the density.\n\nck = function(L, mat, N, params, S0, r){\n\nk = 0:N\n\nreturn(1 / L * Re(phi(k * pi / (2 * L),\nmat, params, S0, r) * exp(1i * k * pi/2)))\n\n}\n\n#cosine coefficients of a put option, see Appendix\nJunike and Pankrashkin (2022).\n\nvk = function(K, L, mat, N, params, S0, r){\n\nmymu = mu(mat, params, S0, r) #mu = E[log(S_T)]\n\nd = min(log(K) - mymu, L)\n\nif(d <= -L)\n\nreturn(rep(0, N + 1)) #Return zero vector\n\nk = 0:N\n\npsi0 = 2 * L / (k * pi) * (sin(k * pi\n* (d + L) / (2 * L)))\n\npsi0[1] = d + L\n\ntmp1 = k * pi / (2 * L) * sin( k * pi\n* (d + L) / (2 * L))\n\ntmp2 = cos(k * pi * (d + L) / (2 * L))\n\ntmp3 = 1 + (k * pi / (2 * L))^2\n\npsi1 = (exp(d) * (tmp1 + tmp2) - exp(-L)) /\ntmp3\n\nreturn(exp(-r * mat) * (K * psi0 - exp(mymu)\n* psi1))\n\n}\n\n#approximation of put option by COS method\n\nput_COS = function(K, L, mat, N, params, S0, r){\n\ntmp = ck(L, mat, N, params, S0, r) * vk(K,\nL, mat, N, params, S0, r)\n\ntmp[1] = 0.5 * tmp[1] #First term\nis weighted by 1/2\n\nreturn(sum(tmp))\n\n}\n\n#approximation of call option by COS method using put-call\nparity\n\ncall_COS = function(K, L, mat, N, params, S0, r){\n\nreturn(put_COS(K, L, mat, N, params, S0, r)\n+ S0 - K * exp(-r * mat))\n\n}\n\n#Derivatives of the characteristic function of the\ncentralized log-returns in the Heston model.\n\nphi1 = Deriv(phi, \"u\")\n\nphi2 = Deriv(phi1, \"u\")\n\nphi3 = Deriv(phi2, \"u\")\n#Takes very long but has to be done only once.\n\nphi4 = Deriv(phi3, \"u\")\n#Takes very long but has to be done only once.\n\nsave(phi4, file = \"phi4.RData\")\n#save for later use. Load with load(\"phi4.RData\").\n\n#Price a put option in the Heston model by the COS\nmethod.\n\neps = 10^-6 #error tolerance\n\nK = 90 #strike\n\nS0 = 100 #current stock price\n\nr = 0.1 #interest rates\n\nparams = c(0.6067, 0.0707, 0.2928, -0.7571, 0.0654)\n\nmat = 0.7 #maturity\n\nmu_n = abs(phi4(0, mat, params, S0, r)) #4-th moment\nof log-returns.\n\nL = (2 * K * exp(-r * mat) * mu_n / eps)^(1\n/ 4) #Junike (2024, Eq. (3.10)).\n\ns = 20 #number of derivatives to determine the number\nof terms\n\nintegrand = function(u){1 / (2 * pi) * abs(u)^(s\n+ 1) * abs(phi(u, mat, params, S0, r))}\n\nboundDeriv = integrate(integrand, -Inf, Inf)$value\n\ntmp = 2^(s + 5 / 2) * boundDeriv\n* L^(s + 2) * 12 * K * exp(-r * mat)\n\nN = ceiling((tmp / (s * pi^(s + 1)\n* eps))^(1 / s)) #Number of terms, Junike (2024,\nSec. 6.1)\n\nput_COS(K, L, mat, N, params, S0, r) #The price of\nput option is 2.773954.\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.05070v1_content.txt"}, {"title": "The Survey of Surveys: machine learning for stellar parametrization", "authors": ["A. Turchi", "E. Pancino", "F. Rossi", "A. Avdeeva", "P. Marrese", "S. Marinoni", "N. Sanna", "M. Tsantaki", "G. Fanari"], "published_date": "2024-12-06T13:56:43Z", "summary": "We present a machine learning method to assign stellar parameters\n(temperature, surface gravity, metallicity) to the photometric data of large\nphotometric surveys such as SDSS and SKYMAPPER. The method makes use of our\nprevious effort in homogenizing and recalibrating spectroscopic data from\nsurveys like APOGEE, GALAH, or LAMOST into a single catalog, which is used to\ninform a neural network. We obtain spectroscopic-quality parameters for\nmillions of stars that have only been observed photometrically. The typical\nuncertainties are of the order of 100K in temperature, 0.1 dex in surface\ngravity, and 0.1 dex in metallicity and the method performs well down to low\nmetallicity, were obtaining reliable results is known to be difficult.", "arxiv_id": "2412.05047v1", "html_link": "https://arxiv.org/html/2412.05047v1", "search_term": "ti:\"machine learning\"", "html_content": "SECTION: The Survey of Surveys: machine learning for stellar parametrization\n\nWe present a machine learning method to assign stellar parameters (temperature, surface gravity, metallicity) to the photometric data of large photometric surveys such as SDSS and SKYMAPPER. The method makes use of our previous effort in homogenizing and recalibrating spectroscopic data from surveys like APOGEE, GALAH, or LAMOST into a single catalog, which is used to inform a neural network. We obtain spectroscopic-quality parameters for millions of stars that have only been observed photometrically. The typical uncertainties are of the order of 100K in temperature, 0.1 dex in surface gravity, and 0.1 dex in metallicity and the method performs well down to low metallicity, were obtaining reliable results is known to be difficult.\n\nSECTION: 1INTRODUCTION\n\nIn the last few years large spectroscopic surveys provided a huge amount of photometric measurements for hundreds of millions (up to billions) of stars, either at low-medium resolution such as the RAdial Velocity Experiment (RAVE) [1], the Sloan Extension for Galactic Understanding and Exploration (SEGUE) [2] and the Large sky Area Multi Object fiber Spectroscopic Telescope (LAMOST) [3], or at high-resolution such as the Galactic Archaeology with HERMES (GALAH) [4], the Apache Point Observatory Galactic Evolution Experiment (APOGEE) [5] and the Gaia-ESO survey [6]. The data provided by these surveys allow to give a precise estimates of key parameters such as effective temperature (T), surface gravity (log) and iron metallicity ([Fe/H]) for few millions of stars in the Milky Way.\nThe availability of high-quality spectroscopic measurement, together with a good estimation of distance and reddening, is of paramount importance to derive high quality estimates of the above parameters from photometric surveys.In recent years, these surveys spawned a many works focused on Machine Learning (ML) methods (i.e. Neural Networks or simpler methods), such as the Cannon [7], the Payne [8] and StarNet [9]. ML saw a huge development in the last decades of XX century and rose to a widespread usage in the first decades of XXI century. The term can be used as a general hat to cover different disciplines from Artificial Intelligence to Neural Networks and Computational Statistics. In general we refer to ML techniques when based on algorithms that make use of heterogeneous data to automatically \u201clearn\u201d and build a \u201cmodel\u201d that is used to produce a desired output, using statistical methods.ML methods applied to spectroscopic star catalogues mainly focus on the analysis of the provided data to many different purposes, i.e. trying to enhance the physical model used to compute the parameters, thus this field is already mature in the case where the previous parameters are directly derived from spectroscopic data.However spectroscopic quality measurements are hard to perform on large stellar samples, requiring a lot of telescopes time and the scientific community is thus lacking high-quality estimates on large stellar samples, thus limiting the impact of studies that use the ML-derived spectroscopic data.There are however many photometric surveys that include hundreds of millions (up to billions) of stars that provide observations in many different bands, such as the Sloan Digital Sky Survey (SDSS) [10], the SkyMapper Southern (SMSS) [11] and the Two Micron All-Sky Survey (2MASS) [12]. While not as accurate as spectroscopic data, photometry can indeed be used to derive a rough estimate of T, logand [Fe/H], and is used by many astronomers to analyze huge star samples.With this in mind, in this contribution we provide a first preliminary ML application that aims to enhance low-quality photometric measurements and thus improve the accuracy from pre-determined analytic estimates of the star parameters. The strength of this method is that we do not try to produce estimates from scratch, thus we are less impacted by large deviations of the model from the real measurement. To achieve this goal and train our ML model, we used the spectroscopic estimates of T, logand [Fe/H] provided by the SoS catalogue [13], which is a critical compilation of spectroscopic parameters from survey data. Also this allow us to make use of the astrometric, photometric, and spectroscopic data from the Gaia mission, combined with large photometric datasets such as SkyMapper Southern Survey (SMSS), SDSS and others that provide us huge samples with hundreds of millions of stars, allowing us to maximize the applicability of the method itself.\n\nSECTION: 2Input catalogs and training set\n\nOur neural network (NN) model was designed to produce survey-quality values for effective temperature (Teff), surface gravity (log g), and iron metallicity ([Fe/H]) with spectroscopic-like accuracy using solely standard photometric data.\n\nInitially, we integrated Gaia data (Gaia Collaboration et al. 2016) with Gaia distances (Bailer-Jones et al. 2021). This was supplemented by data from the SkyMapper Southern Survey (SMSS, Keller et al. 2007), forming a comprehensive input dataset that encompass much of the visible spectrum.\nWe performed a set of training cycles utilizing the NN framework. The supervised training methodology was guided by the \u2019true\u2019 values for Teff, log g, and [Fe/H], which the NN sought to accurately predict.\n\nSECTION: 2.1Input dataset\n\nWe started to build our input dataset from theGaiaDR3 dataset111https://gea.esac.esa.int/archive/[14]. We used the software from Marrese et. al. [15,16] to perform a precise cross-matching between catalogs, a process greatly enhanced by the detailed proper motions available, with all coordinates conforming to theGaiaDR3 system. For sample purification, we applied metrics such as stellar blending, binarity, non-stellarity, and variability from theGaiacatalog. During the neural network\u2019s training phase, we leveraged data from the catalog including magnitudes, colors, and other stellar parameters, aiming to enrich the data input and improve the precision and accuracy of our results. Crucially, we incorporated distances derived by [17], which are critical for accurately determining logand [Fe/H] values.\n\nWe applied several selection criteria toGaia DR3catalog in order to start from very clean data in this preliminary NN test:\n\nsources lacking eitherphot_bp_mean_magorphot_rp_mean_magwere removed;\n\nsources withipd_frac_multi_peak10oripd_frac_odd_win10were removed, to avoid disturbance by neighboring objects [18], as well as sources withruwe1.4[19];\n\nsources with thenon_single_starorVARIABLEflags were removed, as well as those with thein_qso_candidatesandin_galaxy_candidatesflags;\n\nwe only included sources having a distance in [17] and in particular we decided to use the geometric distance determinations because we noted, a posteriori, that they produced slightly better results than the photo-geometric ones;\n\nwe removed stars with a spectroscopic rotational broadening (vbroad) of more than 30\u2009km\u2009s-1, for the few stars for which this parameter was available, because these stars can have altered colors and less reliable spectroscopic parameters;\n\nwe removed all stars with a photometric temperature (teff_gspphot) greater than 7500K;\n\nfinally, we also removed stars with G18\u2009mag, parallax_error0.1 or astrometric_sigma5d_max0.1, which have worse astrometric quality parameters, because they do not always allow for a reliable distance determination. After the first tests, in fact, we noticed that13% of the stars in the training sample had conflicting properties in different catalogs. In the spectroscopic surveys they were clearly giants, while they appeared to lie on the main sequence of the absolute and dereddenedGaiacolor-magnitude diagram. This conflicting information confused the algorithm, providing wrong logdeterminations for a large part of the training set. Therefore, for this experiment, we decided to use the cleanest possible sample. The adopted cut reduced the stars with conflicting information to about 1%.\n\nAs a result, we pre-selected an initial sample of almost 27 million stars from theGaiaDR3 catalogue, that we further selected as described in the following.\n\nWe then moved to integrate the SkyMapper DR2 dataset222https://skymapper.anu.edu.au/table-browser/dr2/[20], which offers photometry in sixuvgrizbands for approximately half a billion stars. We performed a cross-matching of SMSS data with a selected set of sources fromGaiaDR3, employing algorithms developed by [21,22]. We eliminated all stars in each catalog that corresponded to multiple entries in the other, keeping only the cleanest sources.\n\nTo filter out non-stellar objects from the SkyMapper catalog, we applied the criterionand excluded problematic data using the conditionsand. Stars with any magnitudes exceeding 25\u2009mag were removed. Following these filtering steps and cross-matching with the refinedGaiaDR3 sample, we retained a significant sample of nearly 11.4 million stars, spatially distributed in the southern hemisphere. In subsequent work we plan to add other surveys to cover the Northern hemisphere as well. In a future paper we already planned to add other surveys (e.g. SDSS) to cover also the northern hemisphere and expand the dataset.\n\nThe absolute magnitudes were computed using the distances fromGaia(), according to the formula:\n\nwhereindicates the magnitudes, whether absolute or relative, for each band. The extinction coefficients () were derived fromextinction maps, indicated by the parameter ebmv_sfd, and adjusted using the transformation coefficient for each of the SMSS bands333https://skymapper.anu.edu.au/filter-transformations/.\n\nIn table1we list all the parameters selected to be used as an input to the NN model.\n\nSECTION: 2.2Training data\n\nOur star sample with known spectroscopic parameters\u2014T, log, and [Fe/H]\u2014was derived from the first SoS data release [13]444http://gaiaportal.ssdc.asi.it/SoS/query/form. This release, primarily featuring radial velocities (RV), also included an early version of a stellar parameter catalog that we will call SoS-spectro, for brevity. This catalog was utilized to examine the influence of various parameters on RV trends across different surveys and incorporated data from APOGEE DR16 [24], GALAH DR2 [25], Gaia-ESO DR3 [26], RAVE DR6 [27,28], and LAMOST DR5 [29]. Despite the straightforward nature of its homogenization method [13], SoS-Spectro displays typical uncertainties around100,K for Tand about0.1,dex for logand [Fe/H], encompassing nearly 5.5 million stars. SoS-Spectro has been effectively used to characterize both Landolt and Stetson secondary standard stars [23], providing reliable results for challenging parameters such as logand [Fe/H].\n\nFinally, after cross-matching the SoS Catalogue with our previously collected input samples, we developed a new datasets with 624410 stars. These contain the spectroscopic measurements for Teff, log, and [Fe/H], which will be pivotal for the reference outputs during the machine learning algorithm\u2019s training and testing phases.\n\nSECTION: 3Neural network model\n\nPrior to the research presented in this paper, we evaluated several machine learning (ML) and deep learning (DL) architectures to identify the most effective approach. In previous work [23], we tested basic machine learning algorithms such as Random Forest (RF), K-Neighbours, and Support Vector Regression (SVR) on a relatively small dataset of approximately 6000 stars from Landolt and Stetson photometry. From this evaluation, SVR emerged as more effective than more complex deep learning techniques like Multilayer Perceptron (MLP) networks, despite its simplicity. SVR, a linear model used for both regression and classification, operates by identifying a hyperplane that maximizes the margin between the support vectors while minimizing the regression error, effectively separating the N-dimensional data points onto an N-1 dimensional hyperplane, which then acts as the regression function.\n\nConversely, MLP utilizes a nonlinear approach involving multiple layers of artificial neurons, each with specific activation functions, making it part of the broader DL category. This network includes an input layer that receives data vectors, several hidden layers, and an output layer that generates the regression results. Each neuron across the layers is linked to all inputs, with weights that influence the activation of the neuron based on a nonlinear activation function. MLP networks are trained using a dataset to fine-tune these weights to minimize output errors compared to known true values, as determined by a loss function (e.g., mean error, RMSE).\n\nDespite MLP\u2019s ability to handle larger datasets more effectively as they often perform better with increased data volume, they are less interpretable and considered \u201cblack boxes\u201d because it is challenging to discern how each layer\u2019s calculations contribute to the final outcomes. Additionally, MLPs are prone to overfitting, which means they might perform excellently on training data but generalize poorly to new datasets. To combat this, techniques such as weight regularization and dropout layers are employed to prevent over-strong connections and enhance network robustness by randomly deactivating neurons during training (see section3.1).\n\nSECTION: 3.1Implementation\n\nFor this study, we adopted an MLP network architecture using the Keras Python interface for the TensorFlow library. After establishing a clean, cross-matched training and testing catalogue (see section4), the entire training process was executed on a standard desktop PC, with each session taking only a few hours of CPU time.\n\nThe MLP was constructed with sequential layers: a fully connected (dense) layer with a Leaky Rectified Linear Unit (Leaky ReLU) activation function for improved performance and convergence, followed by a batch normalization layer to stabilize the mean output near zero with a standard deviation of one, and a dropout layer to prevent overfitting. This setup is particularly crucial given the potential noise in the dataset, which could lead to overfitting rather than learning from the actual physical properties.\n\nOur network design included 18 hidden layers of varying sizes, from 80 to 160 elements, arranged in a diamond shape to optimize processing. During the initial phase of training, we minimized the mean absolute error to address the maximum error across the dataset and mitigate the impact of outliers. For the subsequent phases, we adjusted to minimize the symmetric mean absolute percentage error (SMAPE), defined by the equation:\n\nwhereis the actual value,is the predicted value, andprevents function explosion.\n\nEach network was tailored to predict specific parameters\u2014Teff, log, and [Fe/H]\u2014with a single-element wide output layer for each. Additional strategies were implemented to handle missing data effectively, ensuring the network could still operate robustly when encountering NaN values in large catalogues.\n\nSECTION: 4Training and Testing\n\nDuring the training of the MLP model we allocated 80% of the data for training, 10% for internal validation, and the remaining 10% for final testing. These subsets were randomly selected, and the networks were run multiple times to ensure consistent and repeatable results across different test subsamples.\n\nDuring the training, the network cycles through the dataset multiple times in what are called epochs, each time adjusting its internal parameters (or coefficients) to minimize the loss function tested against the validation subset, which is not used for training. The goal is to align the network\u2019s output as closely as possible with the known spectroscopic values (Teff, log, and [Fe/H]) during training. Once the network achieves satisfactory performance or runs for a predetermined number of epochs, its final performance is evaluated on the test subset, which is entirely separate from the training and validation data, to verify the accuracy of the network\u2019s output against the desired parameters post-training.\n\nIt is important to recognize that any significant biases or deviations in the test subset\u2019s Teff, log, and [Fe/H] measurements will similarly influence the neural network\u2019s output, as the machine learning algorithm\u2019s accuracy cannot exceed that of the reference measurements used during training. If this happens this is a clear sign of overfitting.\n\nIn Fig.1, we compare the spectroscopic measurements from SoS with the ML predictions on the test subsample for each parameter. To quantify the model\u2019s performance, we present both the mean and median errors in Tab.2to identify potential biases in the ML predictions. We also provide the standard deviation and the Median Absolute Deviation (MAD), defined as:\n\nwhereis the normalization constant to compare MAD to standard deviation.\n\nOur findings indicate that the results of this initial implementation of the MLP model are quite accurate. Predicted Teffaligns with the SoS spectroscopic measurements within a 80-85 K range, depending on the statistical measure used. Logaccuracy is within 0.14-0.09 dex, and the error for [Fe/H] predictions ranges from 0.12-0.08 dex. The residual bias on the parameters, when compared with standard deviation, is completely irrelevant. We are still trying to understand if we can further improve the NN model to improve performance on the rare outliers.\n\nAlthough the average and median performances are very reliable, it\u2019s noteworthy that the disparity between the standard deviation and the MAD suggests occasional large errors by the model. In Fig.2, we illustrate the complete error distribution on a logarithmic y-scale, highlighting these extremely rare but largely discrepant results.\n\nSECTION: 5Final sample\n\nSECTION: 6Conclusions\n\nIn this contribution we presented a preliminary study that shows the feasibility of using an ML model trained on high-quality spectroscopic data to derive key stars parameters, such as T, logand [Fe/H], from lower quality photometric data. The performance of this method applied to photometric data from Gaia and SMSS merged catalogues shows extremely good performance in determining estimates for the above parameters, with errors (on the test set) characterized by a standard deviation of 85.0for T, 0.14for logand 0.12for [Fe/H]. We managed to obtain this thanks to the high-quality spectroscopic measurements provided by the SoS catalogue which were used for the training. This allow us to apply the method to a potentially huge star datasets with hundreds of millions and up to billions of stars, thus providing to the astronomic community the largest high-quality data sample up to date. Future improvements of this work will include an extension to datasets covering also the northern hemishpere (such as SDSS) and a more detailed statistical analysis on the ML predictions outside the test set, where no spectroscopic-quality reference measurements are available. A comparison with other sources of high-quality spectroscopic measurements will also allow us either to improve or to better characterize the model performance.\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.05047v1_content.txt"}, {"title": "Intelligent Spark Agents: A Modular LangGraph Framework for Scalable,\n  Visualized, and Enhanced Big Data Machine Learning Workflows", "authors": ["Jialin Wang", "Zhihua Duan"], "published_date": "2024-12-02T13:41:38Z", "summary": "This paper presents a Spark-based modular LangGraph framework, designed to\nenhance machine learning workflows through scalability, visualization, and\nintelligent process optimization. At its core, the framework introduces Agent\nAI, a pivotal innovation that leverages Spark's distributed computing\ncapabilities and integrates with LangGraph for workflow orchestration.\n  Agent AI facilitates the automation of data preprocessing, feature\nengineering, and model evaluation while dynamically interacting with data\nthrough Spark SQL and DataFrame agents. Through LangGraph's graph-structured\nworkflows, the agents execute complex tasks, adapt to new inputs, and provide\nreal-time feedback, ensuring seamless decision-making and execution in\ndistributed environments. This system simplifies machine learning processes by\nallowing users to visually design workflows, which are then converted into\nSpark-compatible code for high-performance execution.\n  The framework also incorporates large language models through the LangChain\necosystem, enhancing interaction with unstructured data and enabling advanced\ndata analysis. Experimental evaluations demonstrate significant improvements in\nprocess efficiency and scalability, as well as accurate data-driven\ndecision-making in diverse application scenarios.\n  This paper emphasizes the integration of Spark with intelligent agents and\ngraph-based workflows to redefine the development and execution of machine\nlearning tasks in big data environments, paving the way for scalable and\nuser-friendly AI solutions.", "arxiv_id": "2412.01490v4", "html_link": "https://arxiv.org/html/2412.01490v4", "search_term": "ti:\"machine learning\"", "html_content": "SECTION: Intelligent Spark Agents: A Modular LangGraph Framework for Scalable, Visualized, and Enhanced Big Data Machine Learning Workflows\n\nThis paper presents a Spark-based modular LangGraph framework, designed to enhance machine learning workflows through scalability, visualization, and intelligent process optimization. At its core, the framework introduces Agent AI, a pivotal innovation that leverages Spark\u2019s distributed computing capabilities and integrates with LangGraph for workflow orchestration.\n\nAgent AI facilitates the automation of data preprocessing, feature engineering, and model evaluation while dynamically interacting with data through Spark SQL and DataFrame agents. Through LangGraph\u2019s graph-structured workflows, the agents execute complex tasks, adapt to new inputs, and provide real-time feedback, ensuring seamless decision-making and execution in distributed environments. This system simplifies machine learning processes by allowing users to visually design workflows, which are then converted into Spark-compatible code for high-performance execution.\n\nThe framework also incorporates large language models through the LangChain ecosystem, enhancing interaction with unstructured data and enabling advanced data analysis. Experimental evaluations demonstrate significant improvements in process efficiency and scalability, as well as accurate data-driven decision-making in diverse application scenarios.\n\nThis paper emphasizes the integration of Spark with intelligent agents and graph-based workflows to redefine the development and execution of machine learning tasks in big data environments, paving the way for scalable and user-friendly AI solutions.\n\nSECTION: 1Introduction\n\nThe development of information technology brings convenience to life and fast-growing data. With the maturity of big data analysis technology represented by machine learning, big data has tremendous effect on social and economic life and provided a lot of help for business decision-making. For example, in the e-commerce industry, Taobao recommends suitable goods professionally to users after analyzing from large amounts of transaction data; in the advertising industry, online advertising predicts users\u2019 preferences by tracking users\u2019 clicks to improve users\u2019 experience.\n\nHowever, the traditional business relational data management system has been unable to deal with the characteristics of big data including large capacity, diversity, and high-dimension .[1]In order to solve the problem of big data analysis, distributed computing has been widely used, among which the Apache Hadoop[2]is one of the widely used distributed systems in recent years. Hadoop adopts MapReduce as a rigorous computing framework. The emergence of Hadoop has promoted the popularity of large-scale data processing platforms. Spark[3], a big data architecture developed by AMPLab of the University of Berkeley, is also widely used. Spark integrates batch analysis, flow analysis, SQL processing, graph analysis, and machine learning applications. Compared with Hadoop, Spark is fast, flexible, and fault-tolerant, which is the ideal choice to run machine learning analysis programs. However, Spark is a tool for developers, which requires analysts to have certain computer skills and spend a lot of time creating, deploying and maintaining systems.\n\nThe results of machine learning depend heavily on data quality and model logic, so this paper designs and implements a Park-based flow machine learning analysis tool in order to enable analysts to concentrate on the process itself and not spend energy on compiling, running, and parallelizing the analysis program. Formally, each machine learning analysis task is decomposed into different stages and is composed of components, which reduces the user\u2019s learning cost. In technology, general algorithms are encapsulated into component packages for reuse, and the training process is differentiated by setting parameters, which reduces the time cost of creating machine learning analysis programs. Users can flexibly organize their own analysis process by dragging and pulling algorithm components to improve the efficiency of application creation and execution.\n\nSpark, as a powerful distributed computing system, has significant advantages in the field of big data processing and analysis. With the rapid development of large model technology, the application of Spark combined with large model agents has gradually become a hot topic in research. Large model agents can more accurately perceive the environment, react and make judgments, and form and execute decisions. Supported by Spark, large model agents can process large-scale datasets, achieving efficient data analysis and decision-making. LangGraph is an agent workflow framework officially launched by LangChain, which defines workflows based on graph structures, supports complex loops and conditional branches, and provides fine-grained agent control. The integration of Spark large model agents with the advanced tools and workflow management of the Langchain and LangGraph frameworks is driving innovative applications in various fields, including large model data analysis.\n\nThis paper will show the characteristics of the tool by comparing with the currently existing products, and then make a detailed description on the system architecture design, business model with use cases, and function operation by in-depth system modules. At the same time, this paper will also provide a technical summary and look forward to the future research directions based on LangGraph for Spark agents.\n\nSECTION: 2Brief Introduction of Related Technologies\n\nSECTION: 2.1AML Machine Learning Service\n\nAzure Machine Learning (AML)[4]is a Web-based machine learning service launched by Microsoft on its public cloud Azure, which contains more than 20 algorithms for classification, regression, and clustering based on supervised learning and unsupervised learning, and is still increasing. However, AML is based on Hadoop and can only be used in Azure. Different from AML, the tool in this paper is designed and implemented based on Spark, therefore, it can be deployed on different virtual machines or cloud environments.\n\nSECTION: 2.2Responsive Apache Zeppline Based on Spark\n\nApache Zeppline[5]is a Spark-based responsive data analysis system, whose goal is to build interactive, visualized, and shareable Web applications that integrate multiple algorithmic libraries. It has become an open source, notebook-based analysis tool that supports a large number of algorithmic libraries and languages. However, Zeppelin does not provide a user-friendly graphical interface, and all analyzers require users to write scripts to submit and run, which improves the user\u2019s programming technical requirements. The tools mentioned in this paper provide component graphics tools and a large number of machine learning algorithms, which make users simply and quickly define the machine learning process and run to get results.\n\nSECTION: 2.3Haflow big data Analysis Service Platform\n\nIn reference[6], Haflow, a big data analysis service platform, is introduced. The system uses component design, which can make users drag and drop the process analysis program, and open an extended interface, which enables developers to create custom analysis algorithm components. At present, Haflow only supports MapReduce algorithm components of Hadoop platform but the tool mentioned in this paper is based on Haflow, so that it can support Spark\u2019s component application and provides a large number of machine learning algorithms running in Spark environment.\n\nSECTION: 2.4Large Language Models Usher in a New Era of Data Management and Analysis\n\nWith the rapid advancement of artificial intelligence technology, large model technology has become a hot topic in the field of AI today. In the realm of data management and analysis, the emergence of large language models (LLMs), such as GPT-4o, Llama 3.2, ERNIE -4, GLM-4, and other large models, has initiated a new era filled with challenges. These large models possess powerful semantic understanding and efficient dialogue management capabilities, which have a profound impact on data ingestion, data lakes, and data warehouses. The natural language understanding functionality based on large language models simplifies data management tasks and enables advanced analysis of big data, promoting innovation and efficiency in the field of big data.\n\nSECTION: 2.5LangChain and LangGraph Frameworks\n\nLangChain is a large-scale model application development framework designed for creating applications based on Large Language Models (LLMs). It is compatible with a variety of language models and integrates seamlessly with OpenAI ChatGPT. The LangChain framework offers tools and agents that \"chain\" different components together to create more advanced LLM use cases, including Prompt templates, LLMs, Agents, and Memory, which help users to improve efficiency when facing challenges and generating meaningful responses.\n\nLangGraph is a framework within the LangChain ecosystem that allows developers to define cyclic edges and nodes within a graph structure and provides state management capabilities. Each node in the graph makes decisions and performs actions based on the current state, enabling developers to design agents with complex control flows, including single agents, multi-agents, hierarchical structures, and sequential control flows, which can robustly handle complex scenarios in the real world.\n\nThe combination of Spark with LangChain and LangGraph allows data enthusiasts from various backgrounds to effectively engage in data-driven tasks. Through Spark SQL agents and Spark DataFrame agents, users are enabled to interact with, explore, and analyze data using natural language.\n\nSECTION: 3Process-oriented Machine Learning Analysis Tool Based on Spark\n\nSECTION: 3.1Machine Learning Process\n\nThis paper aims at designing a process machine learning tool for data analysts, so it is necessary to implement the functions of common machine learning processes. Machine learning can be divided into supervised learning and unsupervised learning, mainly depending on whether there are specific labels. Labels are the purpose of observation data or the prediction object. Observation data are the samples used to train and test machine learning models. Feature is the attribute of observation data. Machine learning algorithm mainly trains the prediction rules from the characteristics of observation data.\n\nIn practice, machine learning process is composed of a series of stages, including data preprocessing, feature processing, model fitting, and result verification or prediction. For example, classify a group of text documents including word segmentation, cleaning, feature extraction, training classification model, and output classification results[7].\n\nThese stages can be seen as black-box processes and packaged into components. Although there are many algorithmic libraries or software that provide programs for each stage, these programs are seldom prepared for large-scale data sets or distributed environments, and these programs are not naturally supporting process-oriented, requiring developers to connect each stage to form a complete process.\n\nTherefore, while providing a large number of machine learning algorithm components, the system also needs to complete the function of automatic process execution, taking into account the operational efficiency of the process.\n\nSECTION: 3.2Design Process of Business Module\n\nThe system provides components to users as main business functions. The analysts can freely combine the existing components into different analysis processes. In order to be able to cover the commonly used machine learning processes, this system provides the following categories of business modules: input/output module, data preprocessing module, characteristic processing module, model fitting module, and the results predicted module. Different from other systems, the business module designed by this tool is using each stage of the process as a definition.\n\n1. Input and output module. Used to realize data acquisition and writing, dealing with heterogeneous data sources, this module is the starting point and endpoint of the machine learning process. In order to be able to handle different types of data, this system provides data input or output functions of structured data (such as CSV data), unstructured data (such as TXT data), and semi-structured data (such as HTML).\n\n2. Data preprocessing module. This module includes data cleaning, filtering, the join/fork, and type change, etc. Data quality determines the upper limit of the accuracy of the machine learning model, so it is necessary to improve the data preprocessing process before feature extraction. This module can clean up null or abnormal values, change data types, and filter out unqualified data.\n\n3. Feature processing module. Feature processing is the most important link before modeling data, including feature selection and feature extraction. The system currently contains 25 commonly used feature processing algorithms.\n\nSECTION: 3.3Feature Extraction and Classification\n\nFeature selection is a multi-dimensional feature selection. The most valuable feature is selected by the algorithm. The selected feature is a subset of the original feature. According to the selected algorithm, it can be divided into information gain selector, chi-square information selector, and Gini coefficient selector.\n\nFeature extraction is to transform the features of observed data into new variables according to a certain algorithm. Compared with data preprocessing, the rules of data processing are more complex. The extracted features are the mapping of the original features, including the following categories:\n\n1. Standardized component. Standardization is an algorithm that maps numerical features of data to a unified dimension. Standardized features are unified to the same reference frame, which makes the training model more accurate and converges faster in the training process. Different standardized components use different statistics to map, such as normalizer components, Standard Scaler components, MinMax Scaler groups and so on.\n\n2. Text processing components. Text type features need to be mapped to new numeric type variables because they cannot be calculated directly. Common algorithms include TF-IDF components that index text by word segmentation, Tokenizer components for word segmentation, OneHotEncoder components for hot encoding, etc.\n\n3. Dimension-reducing components. This kind of components compress the original feature information through a certain algorithm and express it with fewer features, such as PCA components of principal component analysis.\n\n4. Custom UDF components. Users can input the function of SQL custom feature processing.\n\n5. Model fitting module. Model training uses certain algorithms to learn data, and the obtained model can be used for subsequent data prediction. At present, the system provides a large number of supervised learning model components, which can be divided into classification models and regression models according to the different nature of observation data labels.\n\n6. Results prediction module. This module includes two functions: results prediction and verification.\n\nThrough the above general business modules, users can create a variety of common machine learning analysis processes in the system environment.\n\nSECTION: 3.4System Architecture Design\n\nThe system provides a user interface through Web, and the overall architecture is mainly based on the MVC framework. At the same time, it provides business modules of machine learning and execution modules of processes. The system architecture is shown in Figure 2.\n\nUsers create formal machine learning processes through the Web interface provided by the system and submit them to the system. The system converts the received original processes into logical flow charts and verifies the validity of the flow charts. Validation of process validity is a necessary part of the analysis process before actual execution. When the process has obvious errors such as logic or data mismatch, it can immediately return the error, rather than wait for the execution of the corresponding component to report the error, which improves the efficiency of the system.\n\nThe execution engine of the system is the key module, which implements the multi-user and multi-task process execution function. It translates the validated logical flow chart into the corresponding execution model, which is the data structure identifiable by the system and used to schedule the corresponding business components. The translation of the execution model is a complex process, which will be introduced in detail in Section 4.3.\n\nSECTION: 3.5Architecture Design of Spark Agent Based on LangGraph\n\nAs shown in Figure 3, the architecture of the agent implementation that combines Apache Spark with LangChain and LangGraph is designed to enhance the level of intelligence in data processing and decision-making. This architectural diagram displays a Spark-based agent system capable of accomplishing complex tasks by integrating various technological components.\n\nLangChain and LangGraph frameworks are introduced on top of Spark. LangChain is a framework for building and deploying large language models, enabling agents to interact with users or other systems through natural language. LangGraph provides a graph-structured workflow, allowing agents to plan and execute tasks in a more flexible manner. The core of the agent is the LLM (Large Language Model), which is responsible for understanding and generating natural language. The LLM receives instructions (Instruction), observations (Observation), and feedback (FeedBack) from LangChain and integrates these with data insights from Spark to form thoughts (Thought). These thoughts are then translated into specific action plans (Plan and Action), which are executed by the action executor (Action Executor).\n\nThe action executor is tightly integrated with Spark, processing structured data through Spark DataFrame Agent and Spark SQL Agent, performing complex data analysis and data processing tasks, and feeding the results back to the LLM. The LLM adjusts its action plans based on this feedback and new observations, forming a closed-loop learning and optimization process that further refines its performance and decision-making quality.\n\nSECTION: 4Research on System Implementation and Key Technologies\n\nSECTION: 4.1Storage of Intermediate Data\n\nIn the whole process of machine learning, data is in the state of flow, and components with sequence dependence need to transfer intermediate data. In order to avoid the problem of heterogeneity of intermediate data, the system stipulates that components communicate with each other using a unified determinant storage structure based on DataFrame[8], which is a Spark-supported distributed data set and is classified as the main data set. Conceptually, it is similar to the \"table\" of relational database, but a lot of optimization has been done on its operation execution at Spark. In this way, the relationship of structured data is retained, special data attributes are defined, features and label are specified as the head of data required in the model fitting stage, so as to facilitate the validation and execution of the process.\n\nThis determinant storage structure can be quickly persisted to the intermediate data storage layer by the whole system, and quickly restored to the required data objects when the later components are used.\n\nIntermediate data needs different management in different life cycles. When components process the previous data, that is to say, during the generation phase of intermediate data, the system records the generation location of intermediate data and transfers it to the next component. After the execution of the process, all the intermediate data generated by the process will no longer be used and will be deleted by the system. At the same time, the intermediate data storage space of a single process has a specified upper limit. When too much intermediate data is generated, the resource manager of the process will use the Least Recently Used algorithm (LRU)[9]to clear the data, so as to prevent the overflow of memory caused by too much intermediate data.\n\nIn order to ensure the IO efficiency of intermediate data, Alluxio[10]is used as the intermediate storage reservoir to store all the intermediate data in memory. Alluxio is a virtual distributed storage system based on memory, which can greatly accelerate the speed of data reading and writing.\n\nSECTION: 4.2Implementation Method of Business Components of Machine Learning\n\nIn section 3.2, the design of the machine learning module is described in detail. These modules complete main data processing and modeling functions in the form of components. In order to quickly provide as many algorithmic components as possible, only a small part of the processing program components are programmed according to the characteristics of the machine learning process, such as input and output components, data cleaning components, and so on, and a large number of the component functions are automatically converted to Spark jobs using Spark MLlib [11], which is Spark\u2019s own machine learning algorithm library, containing a large number of classification, regression, clustering, dimensionality reduction, and other algorithms. For example, to classify with the help of Random Forest, the Random Forest Classifier object with corresponding parameters is instantiated by the execution engine of the system according to the node information of the process. The fit method is used to fit the input data, and the corresponding Model object is generated. Then the model is serialized and saved through the intermediate data management module for subsequent prediction or verification components to use.\n\nThere are two ways to run components in the process. One is to call as an independent Spark program and start the Spark Context once for each run. When the Spark program starts, it creates a context environment to determine the allocation of resources, such as how many threads and memory to be called, and then schedule tasks accordingly. The general machine learning process is composed of many components. It will take a lot of running time to start and switch the context. Another way is to share the same context for each process. The whole process can be regarded as a large Spark program. However, the execution engine of the system needs to create and manage the context for each process and release the context object to recover resources at the end of the process.\n\nIn order to achieve context sharing, each component inherits SparkJobLife or its subclasses and implements the methods createInstance and execute. Figure 4 is the design and inheritance diagram of components classification, among which Transformers, Models, and Predictors are respectively the parent of data cleaning and data preprocessing model, learning and training model, validation and prediction model.\n\nSECTION: 4.3Logical Analysis Flow of Machine Learning\n\nAfter the user has designed and submitted the machine learning analysis process through the graphical interface, the system will start to create the logical analysis process. First, the system will make a topological analysis of the original process and generate the logical flow chart expressed by the Directed Acyclic Graph (DAG). The logical flow chart includes the dependence and parallelism of each component, as well as the input and output information and parameters.\n\nAfter the logical structure of the current process is generated, the validity of the overall process will be verified. The specific steps are as follows:\n\n1. Check the input and output of each node in the graph and other necessary parameter information, and return errors if missing. For example, component users of feature processing must define input column and output column;\n\n2. Check the integrity of the whole process to see if there is at least one input component and output component as the beginning and end, otherwise return the error;\n\n3. Check whether there is self-circulation in the flow chart, otherwise return error;\n\n4. Check whether each component conforms to the pre- and post-dependencies of machine learning process, for example, feature processing must be prior to model fitting, and return errors if it does not conform.\n\nSECTION: 4.4Translation and Execution of Spark MLlib\n\nAfter validating the process, the flow chart will be submitted to the execution engine. Firstly, the system needs to represent the logical flow chart as a model that can be executed directly, and then convert it into a machine learning algorithm component based on SparkMLlib, which can be executed serially or in parallel. This process is called process translation and execution.\n\nMLlib[11]is a distributed machine learning algorithm library with Spark built-in support, which optimizes the parallel storage and operation of large-scale data and models. With Spark MLlib, a large number of efficient component programs can be developed quickly. This section will focus on how the system translates the process into an executable model to speed up the operation of the machine learning analysis process.\n\nJoin component is a component that merges different data sets into the same data set, with a many-to-one relationship with the former component. Fork component is a component that applies the same data set to different process branches, with a one-to-many relationship with the later component. Join/fork component has a lot of applications in practice. Collaborative filtering algorithm for commodity recommendation, taken as an example, needs to join all kinds of related data such as transaction data, brand data, birth and residence information of users at the same time in order to depict user information. The specific user profile obtained is then forked to each commodity to get the corresponding preference probability[12].\n\nWhen multiple data sets join at the same time, in order to execute the process efficiently, divide-and-conquer algorithm is used to execute different join branches separately and merge them finally. When multiple process branches are generated from the same data set fork, parallel execution of each process branch will not affect the final model results. In a word, for machine learning processes with multiple join and fork tasks, it is necessary to execute in parallel as much as possible to improve operation efficiency.\n\nThe translation method used when multiple join/fork parallel tasks occur in the process was introduced in the previous section. But the actual machine learning process is not a simple serial or parallel, but a combination of serial tasks and parallel tasks, so the actual machine learning process is more complex. The difficulty to convert complex processes into execution engines lies in executing the process as parallel as possible without disrupting the data dependencies between components. The following are the translation methods for composite processes:\n\n1. The flowchart is traversed breadth first to determine the topological relationship between business components.\n\n2. To divide the sub-processes of the same stage according to the stages of data preprocessing, feature processing, model fitting, and prediction.\n\n3. Critical path algorithm is used to judge the internal execution of each sub-process to determine the hierarchical relationship of branches in the sub-process.\n\n4. The branches of the same level obtained after the last step are optimized according to the algorithm in the previous section.\n\nSECTION: 4.5Key Technologies of Spark Agent Based on LangGraph\n\nThe implementation of Spark agents based on LangGraph involves the following key steps:\n\n1. Within the LangChain framework, detailed interaction prompts are defined for Spark SQL, clarifying the role and functionality of the agent. The agent is designed to interact with the Spark SQL database, receive questions, construct and execute syntactically correct Spark SQL queries, analyze the results, and provide accurate answers based on these results. To improve efficiency and relevance, the query results are limited to a maximum of top k entries and can be sorted by relevant columns to highlight examples in the database. The agent only queries columns directly related to the question, uses specific database interaction tools, and relies on the information returned by these tools to construct the final response. Before executing the query, the agent performs a strict check of the query statement to ensure it is error-free. If problems are encountered, the agent rewrites the query statement and tries again. Additionally, the agent is strictly prohibited from executing any DML (Data Manipulation Language) statements, such as INSERT, UPDATE, DELETE, DROP, etc., to maintain the integrity of the database. For questions unrelated to the database, the agent will clearly respond with \"I don\u2019t know.\"\n\n2. By instantiating SparkSQLToolkit and passing the llm and toolkit as parameters to the create spark sql agent method, an instance of agent executor is constructed. This instance integrates four types of Spark SQL tools: QuerySparkSQLTool, InfoSparkSQLTool, ListSparkSQLTool and\nQueryCheckerTool, which provide the agent with the capability to interact effectively with the Spark SQL database.\n\n3. The agent executor\u2019s invoke method is used to respond to user questions. Within the LangChain framework, the agent performs tasks through three core components: Thought (thinking), Action (action), and Observation (observation). Initially, the agent conducts an in-depth analysis and reasoning upon receiving input to determine the best course of action. Following this, the agent executes specific operations based on the results of its thinking. Then, the agent provides feedback and evaluates the outcomes of its actions, recording observations that serve as new inputs for the next round of thinking. Through the iterative cycle of these three steps, the agent can dynamically handle complex tasks and continuously optimize its behavior to achieve its goals.\n\nSECTION: 5Experimental analysis\n\nSECTION: 5.1Experimentation Environment and Explanation of Experimentation Data\n\nAt present, the system is still in the prototype stage. In order to test the function of the system, this paper uses a four-core processor, 8G memory, and a 64-bit Ubuntu system to deploy a pseudo-distributed environment for the experiment.\n\nThe experimental data is from the public dataset of Kaggle[13]. Through the crime record data of Los Angeles city from 2003 to 2015, the crime category is modeled. In order to facilitate the process description, three original features are selected in this paper, and the common machine learning analysis method is used to create the process. The data characteristics of the features and labels are shown in Table 1. In conclusion, the characters and labels are mainly character strings, which require data preprocessing to extract features and map them to numerical features.\n\nSECTION: 5.2Data Preprocessing\n\nIn order to convert the original features into numerical eigenvectors that can be computed by the training model, a series of data preprocessing tasks are needed to be implemented. In Table 2, each feature processing method is illustrated. All parameters are set by default, and any changes will be noted.\n\nThe feature obtained after pre-processing will be merged into feature vectors by Join components. After TF-IDF, the feature vectors have high dimensionality but are sparse. ChiSqSelector is used to select 100 features fitting models with the largest chi-square information. Logistic Regression with LBFGS is used to fit the multi-classification model, and then the test data is predicted through the trained model, and save the output results as a CSV file. The interface of the above analysis process after system creation is shown in Fig. 6.\n\nSECTION: 5.3Analysis of experimental results\n\nBy comparing the predicted value of the test data with the actual label, it is found that the accuracy is about 72.54%. If more features are added to the process, the complexity of the model will increase, and the accuracy will also increase. With this system, machine learning processes can be created easily and quickly, and users can focus on the improvement of the analysis method.\n\nThe parallel execution optimization of the process is introduced in Section 4. In order to test the effectiveness of the optimization method, the data of this experiment are randomly extracted and divided into ten groups of data including 10%, 20%, 30%\u2026 100%, which are made to execute the analysis process in this experiment separately with the optimized method and the non-optimized method. No optimization refers to the sequential execution of components in the process to obtain the running time of each process in ms, as shown in Figure 7.\n\nIt can be seen that, with the linear growth of data volume, the time of non-optimized process execution increases faster, and the growth rate of time tends to increase in the later period. While with the increase of data volume, the time growth of the optimized process execution scheme is relatively slow, which shows the effectiveness of the system by implementing the optimization scheme.\n\nSECTION: 5.4Experimental Analysis of Spark Agent Based on LangGraph\n\nThis paper uses California housing price information as a case study to explore the practical application of Spark agents based on LangGraph. The dataset provides a wealth of information for analysis, including Longitude, Latitude, Housing Median Age, Total Rooms, Total Bedrooms, Population, Households, Median Income, and Median House Value, as shown in Figure 8.\n\nIn the practical project, the California dataset was loaded via Spark, and a Spark agent was constructed using LangGraph. This agent, powered by a large-scale language model, is capable of processing and analyzing the data to provide in-depth insights into California\u2019s housing prices. The LangGraph framework utilizes a StateGraph class to define a workflow graph, which is used to construct the execution process of an agent. It allows for conditional looping between different nodes, execution of various tasks, and decision-making on whether to continue or terminate the workflow based on the response from the tools, as shown in Figure 9.\n\nAs shown in Figure 10, by invoking the agentexecutor.invoke method, complex data analysis tasks within the Spark environment can be executed to retrieve information about database tables or calculate average housing prices. During the execution of the agent, the parsing of the output from the Large Language Model (LLM) plays a crucial role. If there are issues with the parsing process, it can lead to interruptions in the analysis. To ensure the accuracy of data analysis, a more powerful language model can be introduced to enhance text parsing capabilities, thereby optimizing the performance of the agent.\n\nSECTION: 6Conclusion\n\nIn order to solve the problems that appear when data analysts use Spark to carry out machine learning analysis of large-scale data, this paper designs and implements a prototype of a distributed, flow-based Analysis System that supports multiple machine learning algorithms. In section 3 of this paper, the business model and architecture of the system was introduced as a whole. In section 4 of this paper, the key technologies from each module are described in detail, including the storage and management of intermediate data, the implementation of business components of machine learning, the creation and validation of machine learning processes, the translation and execution of machine learning processes. It also optimizes the execution of complex machine learning processes logically and translates the logical flow chart into a model that can be executed in parallel as efficiently as possible in the physical execution phase.\n\nAt present, the system converts all Spark MLlib algorithms into components automatically, which will be required to expand the algorithm library in practice. Meanwhile, in the future, relevant research can be carried out in the aspect of data dependence; for example, the system can automatically slice the data set, allocate the processing tasks of different features of the same data set to different distributed nodes for parallel processing, and improve the performance efficiency of feature processing tasks and the utilization rate of distributed resources.\n\nThe Spark agent based on LangGraph provides a powerful tool for big data analysis systems, which not only simplifies the data analysis process but also enhances the scalability of the system. With the continuous advancement of Spark agent technology, it will play an even more critical role in the fields of data analysis and machine learning in the future.\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.01490v4_content.txt"}, {"title": "Bootstrapping string models with entanglement minimization and\n  Machine-Learning", "authors": ["Faizan Bhat", "Debapriyo Chowdhury", "Arnab Priya Saha", "Aninda Sinha"], "published_date": "2024-09-26T20:09:59Z", "summary": "We present a new approach to bootstrapping string-like theories by exploiting\na local crossing symmetric dispersion relation and field redefinition\nambiguities. This approach enables us to use mass-level truncation and to go\nbeyond the dual resonance hypothesis. We consider both open and closed strings,\nfocusing mainly on open tree-level amplitudes with integer-spaced spectrum, and\ntwo leading Wilson coefficients as inputs. Using entanglement minimization in\nthe form of the minimum of the first finite moment of linear entropy or\nentangling power, we get an excellent approximation to the superstring\namplitudes, including the leading and sub-leading Regge trajectories. We find\nother interesting S-matrices which do not obey the duality hypothesis, but\nexhibit a transition from Regge behaviour to power law behaviour in the high\nenergy limit. Finally, we also examine Machine-Learning techniques to do\nbootstrap and discuss potential advantages over the present approach.", "arxiv_id": "2409.18259v2", "html_link": "https://arxiv.org/html/2409.18259v2", "search_term": "ti:\"machine learning\"", "html_content": "SECTION: Bootstrapping string modelswith entanglement minimizationand Machine-Learning\n\nWe present a new approach to bootstrapping string-like theories by exploiting a local crossing symmetric dispersion relation and field redefinition ambiguities. This approach enables us to use mass-level truncation and to go beyond the dual resonance hypothesis. We consider both open and closed strings, focusing mainly on open tree-level amplitudes with integer-spaced spectrum, and two leading Wilson coefficients as inputs. Using entanglement minimization in the form of the minimum of the first finite moment of linear entropy or entangling power, we get an excellent approximation to the superstring amplitudes, including the leading and sub-leading Regge trajectories. We find other interesting S-matrices which do not obey the duality hypothesis, but exhibit a transition from Regge behaviour to power law behaviour in the high energy limit. Finally, we also examine Machine-Learning techniques to do bootstrap and discuss potential advantages over the present approach.\n\nSECTION: 1Introduction\n\n\u201cThe garbage of the past often becomes the treasure of the present (and vice versa).\u201d\u2014Alexander Polyakov.\n\nSECTION: 1.1A historical perspective:\n\nDual resonance models, describing pion-nucleon scattering, posit that the sum over-channel poles should be reproduced by the sum over-channel poles[1]. This led to the birth of string theory, as explained in the beautiful introduction chapter by Green, Schwarz, and Witten[2]. In the same chapter, however the authors are careful in admitting that\n\nThe duality hypothesis never had more than slender experimental support, and the Veneziano model was merely an ad hoc way of satisfying this not-so-well motivated hypothesis.\n\nThey question:Is duality555In modern literature, this is referred to as world-sheet duality to distinguish from other dualities in string theory. In this paper, when we mention duality, we always mean world-sheet duality.an approximation or a principle?By assuming the latter, we get the world-sheet picture of string theory, which appears to be an important ingredient in the ultraviolet finite properties of string theory, especially when reinterpreted as a consistent theory of quantum gravity. However, let us continue to examine the older literature. This will nicely set up much of the motivation behind the way we will re-examine string-like theories using the bootstrap. Our approach, while having some similarities with several recent papers[3,4,5,6,7,8,9,10,11,12,13,14,15,16], differs substantially in spirit.\n\nDolen, Horn, and Schmid (see around eq.(20) in their seminal work[1]as well as the recollection by Nambu in[17]) mention that the experimental evidence suggested that the correct prescription for the amplitude describing pion-nucleon scattering data should be:\n\nwhere the first two terms are the sum over the-channel and the-channel respectively, as one would expect in usual quantum field theory. The last term is needed to make the amplitude not-too-big and avoid \u201cdouble-counting\u201d, while still allowing for the resonance structures\u2013without the last term, the representation was historically called the \u201cinterference model\u201d. Dolen, Horn, Schmid observe that if the resonances overlap, thenand only then the duality hypothesis emerges as\n\ni.e.the crossing symmetric amplitude in terms of a single channel emerges (the second equality follows from crossing symmetry). This indirectly implies that the high-energy behaviour is compatible with the unsubtracted fixed-dispersion relation.\nHowever, we should emphasise that one cannot take eq.(1.1) literally as it is not entirely clear how to calculate the last term. Rather, we interpret eq.(1.1) as a phenomenological possibility of adding some extra piece that would add both channels, avoid double-counting while being able to explain experimental results. In light of our recent work[18], we will make eq.(1.1) precise.\n\nAlthough most of the recent attempts have focused on having duality ala eq.(1.2) in-built, it is worthwhile to expand the framework by allowing for theories which deviate from duality\u2013in addition to the caveats in[2], see e.g.[19]for an old review motivating this. In this paper, we take a step in this direction. A related motivation is the fact that glue-ball scattering in gauge theories exhibits power-law behavior in the high-energy limit and not the Regge behavior observed in 10-dimensional string theory. The beautiful paper by Polchinski and Strassler[20]uses AdS/CFT insights to argue that this may indeed happen, specifically hinting that there should be a cross-over from the Regge behavior to the power-law behavior. They consider glueball scattering and argue that at low values of negative, one would havefor somewhile for higher negative\u2019s this would change to. The bootstrap program must be able to investigate such behavior in amplitudes non-perturbatively (we will have only modest observations, which will hopefully spur more interest in this direction). Note that the hypergeometric deformation of the bosonic string amplitude in[6,9]behaves as; therefore, it exhibits a similar kind of transition, while respecting duality.\n\nIs there a representation of the amplitude that resembles eq.(1.1)? String field theory suggests that the answer must be yes. Simply put, thinking about perturbative string theory in the framework of string field theory suggests that there must exist a representation of the amplitude that is exactly as in quantum field theory[21,22]. The phenomenological expectation of eq.(1.1) then can be reinterpreted by positing that the third termarises from the contact terms that arise in string field theory. We should point out here that a split in terms of channels and contact terms is subject to the usual field-redefinition ambiguities in quantum field theories. We will exploit this in our work.\n\nSECTION: 1.2Recent developments\n\nIn[18], using a local crossing symmetric dispersion relation[23,24,25,26,27,28,29,30,31,32,33,34,35,36], a field-theory representation of tree-level string theory amplitudes was found. The advantage of this representation is that it converges everywhere except at the poles, which is what is expected from string field theory arguments. An independent mathematical proof based on partial fractions for the representations in[18]and its generalizations was found by the mathematician Hjalmar Rosengren in666In the same paper, it was appreciated that the series corresponds to generalizations of the hypergeometric series, which have not been studied in the mathematics literature.[37]. In this paper, we will use fresh insights gained from the analysis in[18]to set up the bootstrap of tree-level string theory in a new way. First, let us use the results of[18]and correlate with eq.(1.1).\n\nUsing the local two-channel symmetric dispersion relation, we can obtain a one-parameter family of representations for tree-level open string amplitude, given by\n\nwhereis a free parameter andfor convergence of the series; whilecan be complex, we will just focus on real values in this paper. An important feature of this series representation is that not only does the sum converge for any values ofandaway from the poles, but each of the three pieces on the rhs converges on its own. This is reminiscent of the proposal by DHS in eq.(1.1). To understand how this is still compatible with eq.(1.2), we need to work a bit harder. Let us call the term containinginside the sum in (1.3) as-channel sum, similarly the term withas-channel sum and the remaining term withas the contact sum. It can be shown that each of these sums is individually convergent.\n\nTable1also indicates the ambiguity in splitting into channels. What we have identified as the-channel in table 1 also includes contact terms, which are vital for the convergence of the sum. A different decomposition is discussed in the Appendix (A.4). That the Veneziano amplitude respects both (1.1) and (1.2) serves as encouragement to use similar ideas in numerics. An immediate advantage over the fixed-dispersion relation is that we will be able to impose constraints on a wider domain, since our representation converges everywhere except at the poles. We will next summarize the salient features of the new approach we propose in this paper.\n\nSECTION: 1.3A summary of our approach\n\nLet us consider tree-level, four-point, color-ordered scattering amplitude of massless external states, belonging to the Yang-Mills supermultiplet, in open string theory[38],\n\nVarious projections of the prefactorcorrespond to different component fields contained in the SYM multiplet. For example, for the four-gluon amplitude,becomes.is the universal part of the amplitude, and we refer to it as the form factor. This form factor can be thought of as four-point amplitude of identical scalars.\n\nIn this work, we are interested in studying the analogue of the form factor\u2014by abuse of terminology we henceforth call the form factor as the amplitude\u2014in putative theories. The key assumptions in our analysis are the following:\n\nThe amplitude is crossing symmetric inand,\n\nAt tree-level,is analytic for all values ofand, except at the locations of physical singularities. Furthermore, since we are interested in tree-level amplitudes, we will considerto be a meromorphic function with simple poles inand. The last consideration follows from locality.\n\nWe consider an equidistant spectrum with poles at integer values ofand,i.e., for. At any given mass-level only a finite number of particles are exchanged. This rules out any accumulation points in the spectrum. This assumption is not compatible with the Coon amplitude[39,40], which has infinitely many states below a particular energy level. Although one could accommodate this amplitude by considering delta functions in the absorptive part having support at-deformed integer values, we will not look into it here.\n\nWe assume spins of the exchanged particles at any given mass-level are bounded from above. In the case of open string amplitude, at the-th mass level, spins are, with the condition that only even (odd) spins contribute whenis odd (even). We use this as an input to find a class of amplitudes; we study amplitudes relaxing the even-odd constraint in the appendix. This assumption can also be relaxed to include all spins fromtocontribute as is the case for the Veneziano amplitude describing tachyon scattering.\n\nThe residue of the amplitude at a pole in a particular channel is given by a polynomial in the other channel. Unitarity at tree level implies that the polynomial can be expanded in terms of Gegenbauer polynomials with positive coefficients. This is because at the pole the four-point amplitude factorizes to two three-point amplitudes, and the four-point coupling constant is the three-point coupling squared. The positivity of the coefficients follows from the fact that four-point couplings should be real. Therefore, the absorptive part of the amplitudes in the-channel can be given by777is the discontinuity of the amplitude inchannel and is given by(1.6)Overall negative sign in (1.8) appears because(1.7)\n\nwith. Hereis the Gegenbauer polynomial in thedimension andare the partial-wave coefficients (we will mainly be interested in). While writing (1.8) we have only considered poles corresponding to massive exchanges. To justify (1.8) we can first choose to work with amplitudes with the pole at,removed and then add the massless pole separately888As an example consider an amplitude,. The left side of (1.8) then corresponds to. The reason for distinguishing between massless and massive poles is that residues over massless poles are not always polynomial in nature and hence cannot be expanded in terms of a finite number of Gegenbauer polynomials..\n\nRather than the locality constraints or the equivalent null constraints, we will use the independence of the amplitude from the parameter in the representation. This independence is a consequence of the field redefinition ambiguity as discussed in[18]. The resulting constraints are given in eq.(4.14). These constraints are non-trivially related to the locality/null constraints. This is because the difference between the local crossing symmetric dispersion relation and the nonlocal one are the locality/null constraints[24,34]and the parametric dependence arises due to the freedom to add a linear combination of these constraints to the representation[24]. For reasons we explain in the appendix, setting up the constraints in this manner provides us with certain numerical advantages.\n\nWith the given ansatz in (1.8), we find a one-parameter family of local, crossing symmetric representations of amplitudes given by (2.12). The motivation to look for such a parametric representation comes from (1.3). Details of the derivation of this expression are present in section2. Our goal is to obtain the partial wave coefficientsusing numerical analysis subject to certain extremization constraints. We will carry out a similar analysis for the closed-string in section5. The immediate advantages of this new representation for the bootstrap are obvious: (a) With truncation, the representation is expected to capture all the features of the amplitude[18]\u2014this is not possible with a truncated fixed-representation, which only converges when. (b) One does not have to worry about analytic continuation since the representation is expected to converge everywhere except at the poles.\n\nSECTION: 1.4What quantity to minimize?\n\nWhat principles do we use to select interesting theories using bootstrap? In all investigations carried out so far, one has tried to bound ratios of Wilson coefficients. This is a perfectly sensible approach, since presumably the first few Wilson coefficients are precisely measurable in experiments and can be considered as inputs. Is there any other way to zoom into the space of interesting theories? There is yet no sharp answer to this question[41,42,43]. Investigations[44,45,46,47,48,49]related to entanglement minimization have suggested the possibility that this could be a physics principle to use. The rationale behind this is that generating entanglement is resource intensive (for sure entangling photons in the laboratory needs non-linear crystals and it is hard to make two photons interact!) so it is possible that natural processes will try to lower complexity by using minimum entanglement. We will discuss this further in section 3. To use this for bootstrap, we need to carefully specify the important parameters that we keep fixed. There is an implicit coupling constant that sits in front of the tree-level amplitude. We will assume that it is non-zero. We will also fix two normalized low-energy Wilson coefficients (to be specified below) to the superstring values. At this stage there is a lot of freedom left, and the theory that minimizes entanglement does not necessarily have to be close to string theory. However, crossing symmetry and locality, together with a mild assumption of high-energy behavior, appear to pick out an S-matrix, which is very close to superstring theory! For other Wilson coefficients, we get a wide variety of theories. We will examine one in particular, which exhibits the Polchinski-Strassler-type transition.\n\nSECTION: 1.5Help from Machine Learning\n\nThe numerical approach described above only involves constraints that are linear in the unknownss. The constraints are the independence of the amplitude from the field redefinition parameterand need to be linear in the unknowns. However, we can only impose the constraints up to some tolerance as we truncate in the mass-levels. The choice of this tolerance is somewhat ad hoc as we do not a priori have knowledge of how well the level-truncation works. As we increase the number of mass-levels, we should be able to reduce the tolerance. However, implementing this is a painful exercise. Instead, what is desirable is to demand that the ratio of the amplitudes for two different values ofis close to unity. However, since we do not know the sign of the amplitude at different-values (barring some general statements in specific domains), we cannot convert this into a linear programming problem.\n\nWe examine Machine Learning techniques to get a handle on this problem. Our analysis of ML techniques is not exhaustive; rather, our hope is that our preliminary attempts will spur further research in this direction. Indeed, the problem that we analyse is one of the simplest one to investigate from the ML and bootstrap points of view.\n\nWe organize our paper as follows. In section2, we review the local crossing-symmetric representations and the usefulness of the new parametric representations in a general set-up for 2-2 scattering of identical particles, having 2-channel symmetry. In section3, we turn to linear entropy or entangling power in perturbation theory, keeping in mind gluon scattering in open superstring theory. In section4, we set up the bootstrap problem and discuss our findings. Section5briefly examines the closed string bootstrap, leaving a more detailed study for future work. In section6, we set up a Machine Learning approach and explain the advantages of considering such an approach. In section7, we give a unified dispersion relation whose various limits go over to known dispersion relations. We conclude in section8.\n\nSECTION: 2Crossing-symmetric parametric representations\n\nLet us consider a tree-level amplitudethat has only simple poles in theandvariables. We start with the local two-channel symmetric dispersion relation obtained in[18],\n\nwhereis the location of pole at lowest massive exchange.\nHere we have defined\n\nWhile writing (2.1) we implicitly assume that there are no massless poles and thatis finite. If there is pole at,then we can separate the singular term and replace\n\nis finite. Note that the first and second integrals in (2.1) cancel each other out at.\nAs an example, let us examine how the dispersion relation works for the amplitude. Hereblows up so let us shift the variables by(which can be thought of as a regulator in this example). Then using the dispersion relation (assuming) we have:\n\nUsing shifted variables in the arguments of the amplitude we can obtain a one-parameter family of dispersive representation from (2.1),\n\nA derivation of the above representation is given in (7).Let us consider the partial wave expansion of the amplitude in the-channel, which is given by\n\nNote that-channel discontinuity of the amplitude is given by\n\nwhich leads us to\n\nHere we have defined\n\nNow plugging (2.8) in the parametric representation of (2.5), we obtain\n\nIn case of tree-level amplitude which contains only poles, we can write\n\ndue to unitarity.runs over spins of the exchanged states. We specify the spectrum of the states and this acts as an input in our set up.\n\nFor open superstring, at mass-levelwe havewith.\n\nIn case of the bosonic string and its deformations, the condition isat mass-level.\n\nFinally we obtain the following parametric representation for the tree-level amplitude\n\nwhere,.is the expression for massless pole.depends on the details of the specific amplitude being considered. As an example, we take. In this case, we haveand the only non-zero partial-wave coefficient,. There is no massless pole for this amplitude. As examples for amplitudes with massless pole,for the following two cases are:\n\nThe goal of our work is to solve for the partial wave coefficients,using physical constraints.can be determined by fixing Wilson coefficients that are part of the data used as inputs. Since both sides of (2.12) are independent of, we can use constraint, for any positive integer values ofas constraints in our analysis.\n\nHenceforth, we will be interested in gluon scattering, keeping open superstring theory in mind, and will use\n\nSECTION: 2.1Locality/Null constraints\n\nA two-channel symmetric dispersion relation was obtained in[27],\n\nHere. The above dispersion relation contains non-local terms, which eventually drop out after performing an infinite sum.\n\nBy following a similar analysis as before, we can obtain a one-parameter family of representations from (2.15) for two-channel symmetric amplitudes,\n\nThe difference between (2.16) and (2.12) gives null constraints at different values of. We have checked that the open string amplitude indeed obeys the resulting constraints. We could have used these locality/null constraints to set up our bootstrap analysis, but employingproves to be more efficient in our work.\n\nSECTION: 2.2Advantage of using-parametrization\n\nHere we briefly discuss why we prefer using-parametrization than the naive situation where. Let us examine the open-string solution for this. For (2.16) to converge, we will need[18]\n\nwith. Thus, one can impose the locality/null constraints only when this condition holds.\n\nAs the plot in Fig.(2.17) shows, by exploiting non-zero, a larger region in theplane opens up where one can impose the constraints. With the fixed-representation, the constraints come from equating the s-channel to the-channel\u2014this can only be done in the 3rd quadrant. Thus, one can expect the numerical bootstrap to exhibit faster convergence with non-zero. This also motivates us to use the independence ofinstead of locality constraints, since clearly the choice ofshould not affect the outcome, and it is prudent to impose this independence as constraints.\n\nSECTION: 3Entanglement minimization\n\nNow we have to specify what physical quantity we extremise to get bootstrap solutions. For this, we turn to quantifying the entanglement generated in scattering of gluons. See[47,41,43,44,42,45,46,48,49]for recent developments.\nGiven two subsystems,and, we can define linearized entropy as\n\nHere,is the density matrix constructed from the stateand. In[47]linearized entropy has been studied in the context of perturbative scattering. Let us considerscattering, whereandare the corresponding asymptotic scattering states. These states are labeled by momenta,of the particles and internal quantum numbers, if any, denoted by. We are interested in the difference in linearized entropy between final and initial states in scattering, which is given by[47]\n\nHereis the S-matrix element with initial states denoted byand final states are.\nIn case, the initial linearized entropy vanishes and this implies thatcan be written as a product state. Then the above equation reduces to\n\nHere, the normalization is, withcoming from the energy conservation andcoming from the 3-momenta conservation.\nIt is to be noted that in the above expression the scattering amplitude is in the forward limit in kinematic space and elastic in the internal quantum numbers. The entangling power is obtained by integrating over all initial state configurations999In[44,41,42]entangling power is defined as. Here,represents the phase space. In the perturbative set up we are working at leading order in coupling constant where the amplitude in the forward limit appears. The difference in linearized entropy given in (3.3) is sign semidefinite, so the phase-space integration does not make any qualitative difference in our analysis. In a slight abuse of nomenclature, we will continue using the terminology \u201centangling power\u201d when referring to. The entangling power is related to the absorptive part in the forward limit, which is proportional to the total forward scattering cross-section. In weakly coupled theories, this can be expected to be small[50]. We can examine the S-matrices in theoretical space that would minimize this. Alternatively, the heuristic considerations in the Introduction about minimizing the complexity associated with entangling gates in a circuit picture give us another motivation to examine the same question.\n\nConsider the four-point color-ordered scattering amplitude of gluons,.is the helicity ofth particle. To apply (3.3), it will be convenient to take linear combinations of helicity amplitudes and work with. We follow the convention where all outgoing momenta are positive, such that\n\nWe want to calculate the entanglement between two gluons in a scattering process. In open superstring theory color ordered four-gluon scattering amplitude is given by\n\nwhere the prefactor is given by (trace over gauge indices is implied)\n\nwithfor theth particle.\n\nIn the forward limit, we can choose a frame such that the momenta of the incoming and outgoing particles lie along theaxis and the polarization vectors are in theplane. Therefore,, for all. In this limit, the prefactor reduces to\n\nIf we had considered instead a tensor structure of the form\n\nthen the leading order difference in linear entropy would be\n\nand so for the superstring case the first term vanishes\u2014we will focus on the superstring case.\nTo implement (3.3) in bootstrap, we retain the tensor structure of four-gluon scattering and consider an arbitrary form factor,, with the assumption thathas simple poles at positive integer values ofand. Then we obtain from the massive poles,\n\nThe massless pole contributes a term proportional toasand is the same for all theories. Thus, this will not affect our results.\nSince the right-hand side of the above equation is non-negative, our goal is to minimize this expression. However, note that for tree-level amplitudes, we have a sum over delta functions, and hence to make sense of this expression, we will need to do an averaging over. We will be interested in cases whereand hence\n\nwill be finite. This is also the first finite moment of the entangling power. Alternatively, we can also think of this expression as an average w.r.t.. This is the quantity that we will minimize. We can also considerfor some realwith. We will discuss this case in the appendix.\n\nSECTION: 4Bootstrap\n\nWe will use (2.12) to set up the bootstrap analysis. We remind the reader that while the unsubtracted fixed-dispersion relation holds for theories for which the amplitude at fixed negativeand largebehaves as101010It is possible to consider higher subtracted dispersion relations such that the amplitude behaves asfor, but we will refrain from considering these theories.with, (2.12) holds more generally, even for positive.\n\nIn the low-energy expansion around, let us denote the constant part of the amplitude as. This is obtained by subtracting the massless pole and settingin (2.12).\nSpecifically, the low energy expansion takes the form:\n\nwhere.\nIn what follows, we will absorb thefactor inside the Wilson coefficients so that the massless pole is. To emphasize again, unlike most attempts in the literature, we will not bound ratios of Wilson coefficients. Rather, we will minimize the linear entropy/entangling power as in (3.11) with string coupling,andheld fixed.\n\nWe can findin (2.12) by fixing the Wilson coefficient,from the following relation,\n\nThis gives the representation:\n\n(4.3) will play a crucial role in our analysis. As we explain below,is related to entangling power.\n\nWe wish to minimizeas this is the quantity appearing in the first finite moment of the entangling power111111Higher finite moments would allow forbehaviour; we will deal with this in the appendix.. Explicitly, we have (thepart is-independent):\n\nTo make sense, this must be finite (which can be checked in the numerics by increasing the number of modes) as. Hence, we need to figure out when this can happen. In other words, what high-energy behavior is compatible with the finiteness of this moment of the entangling power.\nLet us first note the unsubtracted single channel (fixed-) dispersion relation for an amplitude given by\n\nwhereis the boundary contribution coming from the arc at infinity.\n\nThere are two interesting cases which for brevity we will refer to as Category I and II:\n\nCategory I: The boundary termvanishes if the amplitude decays at large,i.e.,, for. This condition can hold in certain domain of, mostly for.\n\nTherefore, in this case, we conclude that ifis finite, then by Cauchy\u2019s residue theorem it must be equal to the amplitude with the massless pole subtracted from it,i.e.,. Further, in this case, since the unsubtracted fixed-dispersion relation holds, we expect these theories to respect the dual resonance condition.\n\nCategory II: The amplitude goes to a constant for largeand fixed. Naively, it would appear that the integral in (4.5) would diverge, but this does not always happen as we will explain below. In this case the boundary term would contribute a constant (since this is at fixed-, the constant can depend on) and the absorptive integral could still be finite, for instance, if the absorptive part went to zero faster than a constant. One could say that in this casehas a single channel representation. Such theories would be beyond the usual duality condition. A simple example of this category of theories, where for fixed-the boundary term goes to a constant, is as follows. Consider a deformation of the string amplitude by a single scalar at a fixed mode number. Using (4.3), for a scalar, we find that the contribution is-independent and contributes. For, this gives the-dependent contribution. Further, since the absorptive part for this additional scalar has delta function support, it would contribute as a constant to the absorptive integral. The question is if the entanglement minimisation procedure picks up such theories. The answer is yes! Note that one can adjust the unfixedin (4.3) such that, to yield S-matrices which satisfy the dual resonance condition.\n\nCategory III: For completeness, we will also consider models where the (4.4) does not have a convergentrhs. In this case, we will need higher (negative) moments of the entangling power. This will enable us to locate models which can in principle behave asfor, which is the maximum behaviour captured by (4.3). We will not consider higher subtracted dispersion relations, which would allow for, with.\n\nLet us consider the first possibility. The last term in (2.1) is, which by the above argument is the amplitude evaluated at,and therefore is equal to. This together with (2.1) implies that the amplitude can be simplified to\n\nThis gives rise to the representation:\n\nIt is clear that for the representation to work, we must have.\nThe amplitudes arising from eq.(4.6) have the feature that they coincide with the fixed-single-channel representation in the forward limit. In this caseusing the fixed-form is:\n\nEquating this to thearising from (4), we can define a duality measure:\n\nThis is obviously true for.\nNote that for this case, (4.4) is just(truncated); therefore, minimizing the entangling power is maximizing.\nFor the situation where the amplitude goes to a constant at largefor some fixed, this condition and, conversely, the dual resonance condition will not hold. Here,and (4.4) are not related. In the following, we also write down the formulae forandwhich we fix for the bootstrap. These are given by:\n\nTheterms contribute fromonward. Using (4.10), we notice that\n\nNow for, the Gegenbauers are positive andif. The upper limitgives the boundand the lower limitmakes therhsnegative and gives the rigorous upper bound\n\nThis bound is the same as[27], obtained using different means.\n\nFor numerical bootstrap, we will use (4) or (4.3). Since the constraints involve derivatives w.r.t., from (4.3),will remain unfixed by the numerics, which will fix. One may have wondered how restrictive are (4.10). To perform numerics we always deal with a truncated series instead of keeping all terms up to. Whenever we truncate a series, say, it starts depending on. Now suppose that numerically we wantedandto beindependent, would we get any interesting theories? The answer is no. This is easy to see, as the contributionis always independent of, hence if we only imposed the-independence of (4.10), we would only get terms withturned on. The best we can hope to do with level truncation is to impose-independence up to a tolerance.\n\nThus, we can proceed in two ways:\n\nWe start with (4), find solutions satisfying the bootstrap conditions (to be elaborated below) including (4.4) and then verify (4.9), in which casewill be given by (4.8). If this holds up to some tolerance, we can say that the resulting amplitudes satisfy the dual resonance condition. This will locate models incategory-I.\n\nWe start with (4.3), find solutions satisfying the bootstrap conditions (to be elaborated below) including (4.4) and look for solutions which do not obey (4.9). By construction, such amplitudes which generically do not obey (4.9), if they exist, should go to a constant for fixedand large. Hereis an unfixed parameter. This will locate models incategory-II.\n\nWe start with (4.3), find solutions satisfying the bootstrap conditions and minimising a higher moment of the entangling power. We will relegate a study of suchcategory-IIImodels to the appendix.\n\nTo locate string-type models, we could have just proceeded with the line-of-attack in point 2 but we will find that convergence is faster to locate dual resonance models, if we proceed along point 1. We will be able to study the daughter Regge trajectories in this way.\n\nFor the bootstrap, we truncate the level in the-sum in (4) uptoand then we impose the tree-level unitarity constraints\n\nand the field redefinition invariance constraints:\n\nHenceforth, we shall call these constraints-constraints. We want to impose these constraints up to some tolerance on a grid of points in theplane.\nWe useFindInstancein Mathematica to make this grid, avoiding positive integer values ofto avoid poles of the open-string amplitude with the help of the following two conditions:and. Since there are poles when, we expect the convergence rates to be different in the different quadrants in theplane.\nLet us fix the number of modes to be. We will choose121212Since, the appropriate quantity to consider would have beenbut we cannot handle inequalities on ratios of this sort in what we are doing. This affects how far we can go in the 1st quadrant (where the amplitude becomes large) and the 3rd quadrant (where the amplitude becomes small).in the 1st quadrant with 10 points,in the 2nd and 4th quadrant with 50 points each andin the 3rd quadrant with 30 points. We also use a sprinkle of 40 points as additional constraints in the regionand. The points are chosen using theRandomSeeding -> Automaticcommand forRealsand the bootstrap is rerun for different random grids, so chosen. The reason for the restriction in the 3rd quadrant is that convergence slows down asapproach[18].\nWe impose\n\nwhereis a suitable tolerance forto. We use SDPB[51]to find an accurate solution (although Mathematica could have sufficed, SDPB gives better control on accuracy). A plot with a random grid of points where these constraints are imposed is shown in fig.(2). We use,,. Our seed. We checked the independence ofin the solution so obtained. We perform the bootstrap in. We could also have considered imposing the-independence on a combination of differentvalues. The results are similar and the choice of the approach relies on the available resources.\n\nSECTION: 4.1Category-I theories: Dual resonance models\n\nA nontrivial consequence of the representation (4) is that it implies thatis negative in theregion shown in the fig.(3). To obtain this, we first set. This is the region whereis negative for allandso that the Gegenbauer argument is greater than 1. This region is given by\n\nThis would imply that each term in the non-zero mode sum in (4) is negative. The domain thus obtained is shown in fig.(3(a)). Extra negativity is obtained by dialing. First, note that the argument of the Gegenbauer, for some large mode-number will become less than 1 if. Hence, to find extra negativity domains, we restrict ourselves to\u2014the lower limit is set by the convergence of the open string amplitude. By dialing, we obtain the extra wings, indicated by gray in fig.(3(b)). This illustrates an advantage of the parametric representation. We have checked that the hypergeometric deformations discussed in the appendix obey this negativity. It is not possible to see these negativity conditions starting with the fixed-dispersive representation.\n\nFirst, let us fixto the known values for the open superstring case:,. Then we run the bootstrap. Remarkably, the S-matrix we find on minimizing the entangling moment leads to an almost exact agreement with the full superstring amplitude, as shown in fig.(4). The lower bound obtained on minimization of the entangling moment in this manner is\n\nFor a better comparison, we show the values ofs for the leading and subleading Regge trajectories in fig.(5). Amazingly, we find a very close match with the values from the exact expression for both the leading and the subleading Regge trajectories. In fig.(6) we show the allowed region of S-matrices in the space of the low-energy coefficientsand, which are now fixed to different values. The color coding is set byin (4.9). As the plot shows, there are several models that respect (4.9) to the same tolerance as the string theory. We quantify this further in fig.(7), where we plot the partial sums ofin (4.9), to check for convergence. As this plot indicates, the S-matrices labeled byin fig.(6) do not respect (4.9) as well as the others, while the S-matrixhas questionable convergence. The S-matrices for various cases are shown in fig.(8). In the same plots, we show how they compare with the single-channel representation at. The S-matrixobeys (4.9) and is similar in nature as what arises in the hypergeometric deformations (see appendixE). Note that the strengths of the resonances vary substantially from plot to plot. In appendixFwe find that the open superstring amplitude can be bootstrapped with a general integer spaced spectrum with all spinsat mass level.\n\nSECTION: Leading and subleading Regge trajectories\n\nWe have already explained analytically that\u2014the upper boundary in fig.6respects this. Let us now comment on the lower boundary. It is curious to note that here\u2014the question arises, why? These theories obey (4.9) so let us examine this equation more carefully.\n(4.9) can be used to check for the presence of spin-0 dominance in category-I theories. Taking derivative ofw.r.tand then puttingwe get\n\nTheterms contribute fromonward. The above equation can be separated into two parts: one corresponding to spin-0 and the other corresponding to higher spins,\n\nThe spin-0 contribution is always negative and it can be easily checked that the contribution from higher spins is always positive which proves that spin-0 must exist and furthermore the contribution from spin-0 must be greater in magnitude than the contribution from any particular spin in the higher spin sum. Similarly, calculating thederivative ofw.r.tand checking the contributions from various terms it can be concluded thatandwill be needed to satisfy. This means that infinite\u2019s and infinite\u2019s have to be turned on. The implication of this spin-0 dominance now feeds into (4.10). By examining the signs of various spins forin (4.10), we find that at, spin-0 contributes positively and all other spins contribute negatively. By comparing thedependence in eq.(4.18), it is then suggestive that the spin-0 dominance would lead to, which is what we find in fig.(6). Let us perform a numerical check to verify this. Consider the S-matrix withandlabelledin fig.(6). This S-matrix lies exactly on the lower boundary of the allowed region and numerically we find that\n\nwhich demonstrates spin-0 dominance.\n\nSECTION: 4.2Category-II theories: away from dual resonance\n\nThe category-II S-matrices are found in the region shown below. Since we use (4.3), we have to specify. We choose this to be such that the boundary term in (4.5) is zero for. In this manner close to, we expect to have Regge behavior. The legend in fig.(9(a)) quantifiesdefined in (4.9), and is a measure of how closely the S-matrices obey the dual resonance condition. In this particular case, since we started with (4.3), we find an interesting feature in fig.(9(b)). The theories along the straight lines are related to each other by a scalar deformation. The minimization picks up only themode for this deformation. We can say that theories on the same line belong to the same universality class. In the category-I theories, this was not possible\u2013the reason is that a single scalar deformation in the ansatz used in this case is not-independent. For the rest of this section, we will focus our attention on a specific category-II S-matrix, indicated by the black star in fig.(9(a)). This S-matrix will turn out to obey duality only in the forward limit and will deviate elsewhere. It is also observed that the upper boundary of the allowed region for category-II theories has onlyturned on with. Therefore, we can obtain the S-matrices on the upper boundary using the bootstrap by simply restricting thespectrum to the leading Regge trajectoryi.e.. Note that the upper boundary in fig.(9) is exactly the upper bound onin (4.12).\n\nAs mentioned in the introduction, Polchinski and Strassler in[20], used insights from the AdS/CFT correspondence to suggest how the power-law behavior in glueball scattering amplitudes could arise, despite the string amplitude in 10 dimensions having Regge behavior. They found that there could be a transition from Regge behavior for lowto the expected power law behavior at higher. The condition ondepends onand it was found that for fixed, for sufficiently large, the amplitude exhibited power-law dependence on. We will refer to this as a Polchinski-Strassler type transition and ask if the bootstrap gives amplitudes showing similar transitions. The hypergeometric deformations considered in[6,9]are of this nature, arising from amplitudes that obey the dual resonance condition. Here, we will demonstrate that similar transitions also happen in theories that do not obey the duality hypothesis except in the low-energy regime.\n\nWe consider the black star point in fig.(9(a)) using the fit function\n\nHere,,andare the fit parameters. We use (4.3) for category-II S-matrices. We choose the boundary term in (4.5) to vanish in the forward limit, that is,. Making this choice for the S-matrices in category-II,in (4.3) is given by (4.8). We use (4.3) and (4.8) to make a data table offor the black star S-matrix obtained from the bootstrap for fixedand varyingsuch thatwith a step size of. We use theFindFitfunction in Mathematica withMaxIterations -> 10000to obtainusing this data table. The results obtained for the fit parameters are\n\nSince, for any negativeand sufficiently large,dominates, similar to the Polchinski-Strassler case reviewed above.\n\nTo obtainfor the black star S-matrix using this fit we notice from (4.3) that\n\nThis perfectly matches withfrom (4.8) which is\n\nOf course, this is equal to the bound on minimization of the entangling moment (4.4). We plot our results in fig.(10). Fig.(10(a)) clearly demonstrates the amplitude obeying the duality condition for low values of. As fig.(10(b)) illustrates, at higher negative values of, the amplitude does not obey the duality hypothesis. A final point we wish to remind the reader here is that we made an assumption about what spins contribute at each mass level (see assumption 3 in the introduction). We relax this assumption in the appendixFto see what happens. The conclusion is that the leading Regge trajectories are similar while differences appear for subleading ones. At the level of the plots, the differences are negligible.\n\nSECTION: 4.3Upper critical dimension\n\nFrom the bootstrap we can also calculate the upper critical dimensionfor the category-I and category-II S-matrices that we have investigated so far. This means that if we consider an S-matrix with thes from the bootstrap solution and then compute thes inusingGegenbauer polynomials then some specifics (e.g.) become negative indicating violation of unitarity. For example, consider. For the open superstring S-matrix in fig.(6), the, but the. Similarly, for the black star S-matrix in fig.(9) we find the, but the. We summarize our results in table2.\n\nThis follows from the property that higherGegenbauers can be written as positive linear combinations of lowerGegenbauers. So a unitary solution inmust be unitary in all, but unitarity foris not guaranteed. The upper critical dimension of all the S-matrices in category-I in fig.(6) is. In category-II all the S-matrices in fig.(9) haveexcept those living on the upper boundary for which onlyis non-zero and rest all other coeeficients are zero. This is because for the S-matrices where only spin-0 coefficients are non-zero, for example, the theories living on the upper boundary in fig.(9) where onlyis turned on, there is no need for higher spins and the sole spin-0 coefficient remains positive when calculated in all.\n\nSECTION: 5Closed-string\n\nHere, we will briefly examine whether entanglement minimization in 2-2 scattering of gravitons (assuming type II supersymmetry) can pick up the closed-string amplitude. The answer is yes, but the convergence is slower than that of an open-string.\n\nSECTION: 5.1Parametric representation\n\nIn[18], parametric representations for the closed-string tree amplitude were derived by recycling the 2-channel symmetric representation.\nA parametric series representation resembling the closed-string tree amplitude, maintaining 3-channel symmetry, for a general case where, has been obtained in[37]using the method of partial fractions; here we will use an approach motivated from dispersion relation (see[53]for details). From the equation, we find\n\nLet us consider a general version of closed-string amplitude, given by\n\nHereandare any non-zero parameters. We can work out a series representation of the above amplitude by using local crossing symmetric dispersion relation[34],\n\nHere,is the discontinuity of the amplitude inchannel andis given by (5.1).denotes the location of the lowest pole in.\nNote that (5.2) is invariant under,,and. If we perform these shifts, followed by setting,and, we then obtain a one-parameter family of representations for the closed-string amplitude,\n\nThe series converges absolutely for. The caseagrees with[18]and the general case with[37]131313One has to massage the expressions in this paper into the form quoted above..\n\nSECTION: 5.2Bootstrap analysis\n\nFollowing the same argument as in Sec.(2), we denote discontinuity of the amplitude as\n\nWe use the above equation in (5.3). It is implicitly assumed that the amplitude is invariant under,and. Since we are interested in massless external states, we setand. We then obtain\n\nIn this case, at mass levelthe spectrum contains,in steps of. Therefore, the spectrum contains even spins only, and we can choose either sign before the square root. For even spins, we only have even powers of the argument, and hence the representation maintains locality.\n\nA consequence of (5.2) is that it implies thatis negative in the-region showed in fig.11(a) below. Here, we set. This is the region whereis negative andso that the argument of the Gegenbauers is greater than 1. This would imply that each term in the non-zero mode sum in (4) is negative. Hence, the full amplitude is negative in this region, which is given by\n\nThe extended domains by dialingare shown in fig.11(b). The amplitude now is negative in the combined red-gray region141414It will be interesting to see if the positivity in the context of Celestial Holography, observed in[54]is related to this..\n\nIn fig.(12) we show our results for the closed-string amplitude. We fixedandto the closed-string values:and used the same grid as the open-string. We usedinwithas our seed. The tolerance we used was. As the plots indicate, the closed-string is obtained from minimization. Compared to the open-string case, here we expect the entangling power to be proportional to. Thefollows from the expectation that in supersymmetric closed-string theories, the leading correction is proportional to. The residue goes likefor. Thus, themoment of the entangling power will be finite. However, for this class of amplitudes, the convergence of (5.2) suggests that themoment is generally finite. We have considered both in the plots below. Both give comparable results. However, the convergence of the numerics is slower than what we found in the open-string case. In fig.(13) we show the our results for the ratio offrom bootstrap to thefrom the exact solution for closed-string for the leading and subleading Regge trajectories. Ideally this ratio should be 1 and indeed we find nice agreement with 1 especially for the leading Regge trajectory.\n\nSECTION: 6Non-Linear constraints via Machine Learning\n\nA thorny issue in the numerics that we have avoided addressing so far is how do we decide what tolerance to use for the magnitude of the-independence constraints. One could take the approach that since we know at least one solution to our bootstrap problem - the string theory solution, we can compute how well the string solution (truncated to some) satisfies the-independence constraints and use that to guide our estimate for the tolerance. Certainly, we want the string theory amplitude to be allowed, so the tolerance cannot be smaller than that dictated by the string solution. This approach however requires prior knowledge of the full string amplitude which somewhat defeats the purpose of bootstrap. In addition, since the magnitude of the amplitude may differ widely for various values of, the derivatives (truncated to some) themselves may differ in magnitude significantly, making a naive approach based on setting a small tolerance infeasible.\n\nOne way to circumvent these difficulties is to consider ratios of the derivatives with respect to the amplitude. In this case, using a small tolerance is reasonable and one can impose (\n\nAnother approach could be to directly impose\n\nHowever, both the above constraints are non-linear in the parametersand not imposable via the usual linear (or semi-definite) optimization methods. One could try to recast them in a linear fashion but that would require knowledge of positivity properties of the amplitude. For example, if the amplitude were known to be positive at some, we could imposeor. Since such positivity properties are not known in general, imposing such linear constraints would leave out a subspace of allowed amplitudes.\n\nIn this section, we explore the use of Machine Learning methods to address this issue. Neural networks are well-known to be universal function approximators[55]. In particular, Physics-Informed Neural Networks (PINNs)[56]can be used to learn functions that satisfy a wide variety of constraints and have found several physics applications such as numerical learning of Calabi-Yau metrics[57,58], the conformal bootstrap,[59,60,61,62,63,64], and scattering in quantum mechanics[65]among others. Recently, PINNs have also been implemented for the S-matrix bootstrap to constraint the phase of the scattering amplitude when the modulus is given[66,67]. PINNs are characterized by the fact that the constraints equations are directly incorporated into the loss function and as the network trains to minimize the loss function, it approximates the true solution to the constraint equations better and better. The use of PINNs to do bootstrap could be a very promising direction. Most notably, this approach removes the need to make an ansatz to set up the numerical S-matrix bootstrap, which can prove to be very helpful in cases where a suitable ansatz is hard to write down. In our case, we will employ PINNs to learn the scattering amplitudes (s) that satisfy the non-linear constraints (6.1) and (6.2) which are otherwise not possible to impose via linear optimization methods.\n\nSECTION: 6.1Machine Learning Implementation\n\nA neural network is basically an input-to-output map with several tunable parameters. In our case, we use a fully-connected, feed-forward neural network that we implement usingPyTorch[68]. Since we want to learn a two-variable function, the input layer of our neural network has size 2 (see fig.14). Let us pause here and elaborate a bit more as to why this is a useful thing to do. One may argue that instead of learning this two-variable function, why don\u2019t we directly optimize using an optimizer? There are two points to make here: a) We want to examine Machine-Learning methods for bootstrap and more importantly for the future b) Finding this two-variable function will enable us to go into the complexplane treatingas a continuous variable and examine Regge theory. A preliminary attempt towards this end is given below.\n\nEach input corresponds to a specific neuron in the input layer; so we have a neuron for each of theandinputs. The input layers are followed by hidden layers of neurons. The number of hidden layers required and their sizes depend on the complexity of the function that the neural network is trying to learn. We use 2 hidden layers, both of size 64. Each connection between neurons has associated tunable parameters called weights and biases and outputs a weighted sum of its inputs plus a bias. The output is then acted upon by an activation function before being passed on to the next layer. The activation functions add non-linearity to the neural network and are crucial for it to learn complex relationships between inputs and outputs. We use the very commonly used ReLU (Rectified Linear Unit) activation function defined as. ReLU is computationally simple, leading to a faster training process. The final layer is the output layer. In our case, it just has size 1 and gives the learnedvalues for given inputvalues. To ensure that thevalues our neural network outputs are always positive, as required by unitarity, we pass the output through the Softplus activation function defined as. We give the full details of the various hyperparameters of our neural network in Table3.\n\nPINNs are a sub-class of neural networks where the loss function includes some physical constraint equations. The true solution to the constraint equations is learned iteratively as the PINN updates its parameters (weights and biases) every iteration (called an epoch) via the Gradient descent method to approach the minimum of the loss function. In our case, the positivity constraint on thes is already imposed as the neural network output is passed via the Softplus activation function. The constraints that remain left to impose are fixingandto the open string values, and the non-linear-independence constraints. The task of PINN is to maximizegiven these constraints. This can be achieved via the following loss function:\n\nHere,are the-independence constraints imposed on a setofpoints of length.s are positive weights. As different terms compete with each other during the minimization of the loss function, we can tune the magnitude ofs according to the tolerance required to satisfy the respective constraints they multiply. By construction,; thus, as the PINN trains to minimize loss, it also learns thes that correspond toand satisfy all constraints.\n\nSECTION: 6.2Bootstrap results for Open String\n\nWe now present the results from imposing the non-linear constraints in (6.2) and (6.1) via the PINN implementation discussed above.\n\nCase 1:When.We choose. We also pick. We impose-independence on the following set of points\n\nWith these parameters, we train the neural network forepochs. In practice, we do this multiple times and then pick the solution for which the product of all the constraints is the smallest. This leads us to the following solution.\n\nwhere we have defined. For the leading Regge trajectory, the solution compares to the open string values as follows:\n\nCase 2:When.We will focus on the three derivatives case () and pick,,. We chooseforwhereasfor. We use 3 hidden layers of size 64 each. We choose the following set of points for the-independence constraints:\n\nClose to, for, the open string amplitude is very large and its first derivative is. For example, at, the amplitude and its first derivative areandrespectively, making an approach based on setting a small tolerance for the first derivative itself infeasible. Since the ratio is, we can impose the small ratio condition safely using the PINN implementation.Training forepochs gives us the following solution (again selected by looking for the solution with the smallest product of the constraint terms)\n\nHere we have defined.\n\nThe solution for the leading Regge trajectory is as follows:\n\nIn fig.(15) we show the convergence of values of the coefficients on the leading Regge trajectory, specificallyandwith the number of derivativesand the number of-independence constraints at each derivative order. In fig.(15(a)) we setand varyfromto. We find that the coefficients on the leading Regge trajectory have nicely converged to the value obtained from tree-level open superstring theory. In fig.(15(b)) we setand varyfromto. Again we find convergence of the values of the coefficients on the leading Regge trajectory to the value obtained from tree-level open superstring theory.\n\nSECTION: 6.3Regge analysis from Machine Learning\n\nAn advantage of implementing the neural network in the way described above is that we are learning solutions -s that are continuous functions ofand. This allows us to examine whether the neural network also learns the leading Regge pole. To highlight the continuous nature of the solutions, we will use the notation. Our working definition for the Regge pole is the following. We make a data table ofas a function offor fixedand interpret the global minimum as the Regge pole of. We repeat this for severaland plotversus. The results are shown in fig.(16). Remarkably, we find that this curve is a straight line. We interpret this as the linear Regge trajectory\n\nwhereis the Regge intercept andis the Regge pole. Our fit gives the values\n\nNote that Regge interceptmatches the leading Regge intercept of the open superstring amplitude for which we have\n\nWe leave a more systematic examination of Regge theory using these ideas for future work. Our findings suggest that this is an interesting direction to pursue.\n\nSECTION: 7A unified dispersion relation\n\nBefore we conclude, we would like to give a unified dispersion relation involving the parameter. We will focus on the 2-channel symmetric case; a similar analysis can be made for the 3-channel case and will be presented elsewhere. The motivational question we ask is the following. In fig.17, we exhibit pictorially the famous dual resonance hypothesis, where in the string picture, the two channels are equivalent descriptions, getting deformed into each other. Is there a way that the fixed-dispersive representation can be deformed into the fixed-one? We will show that different limits ofgive various known dispersive representations giving the cartoon in the figure below.\n\nGiven a functionwhich issymmetric, satisfiesfor some fixedand has branch cuts (or poles) alongand no other singularities, we can represent it via the following dispersion relation\n\nwhereis the-channel discontinuity ofalong the branch cut. Lets apply the above dispersion relation to the functionthinking of it as a function ofand. We can write,\n\nMaking the change of variable of integrationwe obtain\n\nThelhsof (7.3) is invariant under the shifts,andwhich means that\n\nPuttingin the above equation we obtain (2.5)\n\nWe perform a check of this dispersion relation using the massless box diagram in the appendixA.5.\nDespite appearances, the above representation isindependent; the role ofis to enable a truncation of the upper limit of the integral, which may be desirable in numerical bootstrap. In case, the function satisfies, the first line above vanishes and we get a representation without any additional constant pieces. Note that choosingleads to the usual fixed-dispersion relation, which is another use of the-dependence. Since the fixed-dispersion relation and crossing symmetry together lead to the so-called null constraints or locality constraints, we expect that statement of-independence is equivalent to that of null or locality constraints. Similarly, takingleads to the fixed-dispersion relation, while takinggives the fixed-dispersion relation. Curiously, settinggives us back the (nonlocal) crossing-symmetric dispersion relation (up to the locality constraints) whilegives the local crossing-symmetric dispersion relation[18]. These limits give the web in fig.18. It also makes it tempting to conjecture a relation to a potential worldsheet description, which we now briefly (and hesitatingly!) elaborate.\n\nThe worldsheet description enables us to visualize various individual channels as limits of different ways of pinching the worldsheet as in fig.17. However, we also know from string field theory[21]that we can also have a Feynman diagram decomposition if we put in the appropriate contact terms. The-dependent dispersion relation has precisely this flavour as is suggested by fig.18. However, we do not have a first principles proof that various corners of the web are accessible by dialingappropriately. For instance, in the string theory tree-level example, the convergence of the mass-level sum features(which for the superstring case gives the condition). Could convergence issues rule out the interpolation between various corners? This is related to establishing a rigorous analyticity domain which enables eq.(7.5) to converge. Examining how and when an underlying string worldsheet description emerges from the bootstrap is a very interesting open problem for the future.\n\nSECTION: 8Discussion\n\nWe will briefly discuss the connection between the present approach and several previous papers.\nIn the case of category I amplitudes, we noted that. This condition can be contrasted with the superpolynomial softness of the amplitude mentioned in[9], which states that there always existssuch that, for any positive integer. Here, at high energy with fixed momentum transfer, the amplitude is assumed to grow as. In our case, we will require,i.e.even in the forward limit amplitude should decay with increasing energy. The open-string amplitude which behaves asat largeand fixedis a very good example of this. In addition, our analysis is complementary to that followed in[12,14]. In our case, the spectrum is given as an input and we fix two Wilson coefficients to determine the partial-wave coefficients. The open-string S-matrix lies atin fig.(6). In[12,14]low-energy expansion of the amplitude is used and the Wilson coefficients are constrained by certain assumptions\u2014fore.g.,mass gap is present between any two states in the spectrum and the lowest massive state is a scalar. By tuning the scalar coupling to the string value, the authors could carve out narrow regions near the boundary of the allowed region in the space of Wilson coefficients, which corresponds to the open-string S-matrix. (2.12) renders a broader framework of S-matrices which do not admit duality or unsubtracted fixed-dispersion relation in general. We have classified such amplitudes as category II. In this category there exist some amplitudes which admit single-channel expansion only in certain limited domains, one class being scalar deformations to string amplitudes. These amplitudes also have infinite tower of Regge trajectories, which is an essential condition for having Regge behavior[11].\n\nThe main point of view that we have advocated in this paper is the following.\n\nIt is worth considering the dual resonance hypothesis in an approximate sense, which may enable us to make better contact with experiments.\n\nSee[19]for an old critical review of the duality hypothesis and phenomenological arguments why duality could be \u201cbadly broken\u201d in nature.\n\nOf course, this opens up several questions, some of which we list as promising future directions:\n\nIt will be interesting to re-examine the hadron scattering data using a basis of functions, motivated by the considerations in this paper. As we have summarized in the introduction, if one supplements the so-called interference models of the 1960s with contact terms, one would get a basis that would manifest crossing symmetry but still maintain the desirable features of the string model. The single channel analysis of[1]could only capture the asymptotic features of the amplitude; perhaps with our new basis, the Breit-Wigner type features could also be captured, leading to a more faithful contact with experiments.\n\nIt is extremely unlikely that the entire space of tree-level S-matrices that we have found using the techniques here, will meet all the requirements to come from consistent theories. For instance, in[69], five-particle consistency was used to rule out certain analogs of string models which satisfied the dual resonance condition[7]. It is quite likely that similar considerations will rule out swathes of theory space. It is a challenge however, how to go about considering five particle versions of the 2-2 S-matrices found in this paper.\n\nPerhaps a more do-able question is to set up a study of perturbative unitarity. In this case, we would need to consider a coupling parameter, which would give an imaginary part to the resonances. One could conceive of an analysis similar to[70,71]but starting with the S-matrices found in this paper. Perhaps demanding the emerging non-analyticity due to one-loop corrections to be of a specific kind will enable us to rule out theory space.\n\nIf entanglement minimisation is supposed to pick out the open string amplitude exactly, it will be interesting to find a good analytical explanation for the same. One can also look at the corresponding dual optimization problem to the primal problem we set up in this study. We did this and found that the dual problem converges much faster and matches the primal bounds to high numerical accuracy. Since the duality gap is zero in this case, one can also reconstruct the corresponding extremal primal solution from the dual solution using the complementary slackness conditions.\n\nWe demonstrated that Machine Learning methods can prove very useful while imposing non-linear constraints that traditional linear or semi-definite optimization methods fail at. In particular, the approach via PINNs may be indispensable in cases when its hard to write down an ansatz for the amplitude to set up the numerical bootstrap. Since the tree-level string amplitude has an analytic structure which is just a series of poles, there was a natural choice of ansatz for the residue at the poles in terms of Gegenbauers. Beyond tree-level however, branch cuts appear. In such cases, one can use a PINN as an ansatz for the absorptive part of the amplitude. It will be interesting to explore this direction.\n\nWhile PINNs have the advantage of being very general and therefore allow one to impose a wide variety of physical constraints, its very hard to achieve the kind of numerical accuracy that more conventional numerical methods can achieve. This could prove to be a drawback in some high-precision bootstrap studies. It will be very helpful to find ways to make PINNs more accurate without prohibitively slowing down the convergence rate.\n\nDouble copy:\nFinally, let us comment on the prospect of having the double-copy KLT type relation[72,21]level-wise.\nLet us consider the closed string amplitude in (5.4). Near the poles,and, for, the amplitude takes the form\n\nThe above expression suggests that near the massive poles, closed and open string amplitudes are related as\n\ndenotes the open string amplitude given in (1.3). We can also derive (8.2) by expanding the double copy relation[72,21],\n\naround,. We leave a detailed study of the double copy relations from the parametric series representations of the open and closed string amplitudes, reported in this paper, for future work.\n\nSECTION: Acknowledgments\n\nWe thank David Gross, Shiraz Minwalla, Hjalmar Rosengren, Ashoke Sen, Piotr Tourkine, and Spenta Wadia for useful discussions. AS thanks the participants of \u201cWhat is string theory: Weaving perspectives together,\u201d workshop at KITP, Santa Barbara and the S-matrix bootstrap 2024 conference at Reykjavik for useful discussions and encouragement. APS would like to thank the participants of \u201cFuture Perspectives on QFT and Strings\u201d conference held at IISER Pune and Eurostrings 2024 conference held at University of Southampton, where parts of this work were presented, for insightful comments. APS is grateful to Queen Mary University of London and Swansea University, and especially to Prem Kumar, Ricardo Monteiro, Carlos Nunez and Congkao Wen for warm hospitality and helpful discussions during the final stage of this work. FB thanks Sumanth Kumar for very helpful discussions about PINNs. AS acknowledges support from the SERB core grant CRG/2021/000873 and a Quantum Horizons Alberta chair professorship. APS is supported by the DST INSPIRE Faculty Fellowship (IFA22-PH 282). FB is supported by the Prime Minister\u2019s Research Fellowship (PMRF). This research was supported in part by grant NSF PHY-1748958 to the Kavli Institute for Theoretical Physics (KITP).\n\nSECTION: Appendix ATwo-channel symmetric dispersion relation\n\nWe briefly review the dispersion relation[27]for amplitudes that are symmetric in two channels, sayand. Let us first define the following variables,\n\nwhere,. We consider the amplitude to be a function ofand, so in this caseand analogouslyis held fixed.\n\nUsing the following parametrization we can mapandto complex plane,\n\nUsing the above equation we can express the amplitudeas, whereis held constant and is given by\n\nThe above equation implies\n\nUsing (A.2) withkept fixed, we find\n\nWe choose the positive sign in the above equation to denotecorresponding to. Therefore, when,lies on the unit circle. Thus, (A.2) maps the singularities of the amplitude to a unit circle in the complex plane. From (A.5) we see that whenimpliesand whenwe get. For,can be expressed as\n\nOur goal is to obtain the amplitude from its discontinuity in a particular channel. For this purpose, the above parametrization will be useful.\n\nFor later reference we defineandvariables as\n\nSECTION: A.1Dispersion relation\n\nWe make the following assumptions about.\n\nThe singularities of the amplitude are located on the unit circle.\n\nThe amplitude satisfies the complex conjugation,.\n\nSince the amplitude is symmetric inand, it must be a function of even powers of.\n\nLet us consider the following contour integration\n\nHere theis the discontinuity of the amplitude atand is expressed as\n\nThis leads us to the dispersion relation\n\nwhere we have used. Assuming\n\nwe can discard all the terms other than powers ofin the above dispersion relation. Then we obtain,\n\nIn kinematic variables we get a dispersion relation which is symmetric in two channels,and,\n\nHere,is thechannel discontinuity.\n\nSECTION: A.2Local dispersion relation\n\nThe kernel appearing in (A.13) can be expressed as\n\nNow we make a Taylor series expansion ofaround. Sincehasin the denominator, Taylor series expansion gives rise to additional singularities whenever the denominator vanishes. This means that (A.13) has spurious singularities in addition to the physical singularities of the amplitude. Here we follow the analysis of[34]to get rid of the unphysical singularities in the two-channel symmetric dispersion relation.\n\nWe can write. Note that; therefore, for any positive integer, we have. Spurious singularities appear when. If we remove terms forfrom the sum, we are left with\n\nHence effectively functional dependence ofinbecomes. For the additional constant piece in (A.14) only theorder term incontributes. This leads us to the following local dispersion relation\n\nSECTION: Massless pole\n\nWe demonstrate that the massless pole can be handled in (A.16) in a limiting way. Let us consider, which in the limitreduces to.-channel discontinuity of the amplitude is given by\n\nIt is easy to see that substituting (A.17) in (A.16) yields the required amplitude.\n\nSECTION: A.3Higher spins\n\nThe one-parameter family of representations of the tree-level amplitude given in (2.12) works for theories containing an infinite number of higher spin particles. Let us consider a four-point scattering of massless scalars where the exchanges are a massive scalar and a massive spin-one state. We can assume that massless pole and higher massive exchanges are absent for this case. Using (2.1) the amplitude that we obtain has the following form,\n\nHere,is the Wilson coefficient,andare the partial-wave coefficients for the scalar and spin-one states, respectively. If we try to apply (2.12) for the amplitude obtained above, we will get\n\nThe difference between (A.18) and (A.19) is proportional to, and thus we conclude that the presence of a single massive spin-one state is inconsistent with the shift symmetry of (2.12). This argument can be generalized for a finite number of massive higher-spin exchanges.\n\nThe above conclusion is related to the Reggeization argument[73]which requires an infinite number of subleading Regge trajectories.\n\nSECTION: A.4Convergence of individual terms\n\nThe summation in (1.3) can be divided differently into three parts,\n\nwhere\n\nare the sum of residues over poles at each channel, and\n\nis the contact term. The product inside the curly brackets, in case of the term corresponding to, contributes 1. Now, it is obvious that each individual piece cannot converge everywhere. For example,converges when.\n\nSince the open-string amplitude admits single channel expansion, therefore, when,converges toand. Similarly for,converges to the same value and. In the first quadrant, whereand, all of,andindividually diverge but their sumis a finite quantity and equal to the actual value. In the third quadrant, whereand, all of,andindividually converge.\n\nSECTION: When does the LCSDR form hold?\n\nLet us examine (2.1) for convergence. Notice here that in the absorptive part, instead of the conventional(here) dependence of the second argument, we have. Now suppose that the absorptive part in the largelimit behaved as. Hereis any arbitrary function with the condition thatandare well defined. Then in (2.1), we have the integrand going as. In the largelimit we have. In (2.1), together with the last term, we have the integrand behave as\n\nThus for convergence we need. Unlike the fixed-case, we do not have to specify the sign offor this to hold. That is why our representation also captures the unphysicalregime (unlike the fixed-case) of the string amplitude as our plots indicate (see also[18]). However, we emphasize that (2.1) needed the locality constraints to hold. As such, the partial waves should be such that these constraints hold.\n\nSECTION: A.5A check of the-CSDR\n\nWe verify the-LCSDR using the following amplitude corresponding to a massless box diagram using dimensional regularization[74],\n\nwhere the expansion has been performed. At, the amplitude blows up both due to theand thefactor. Because of this, a direct application of the (2.5) leads to \u201capparent\u201d divergences. We therefore start with an amplitude where the pole is atand the branch cut starts atas follows\n\nApplying the (2.5) to this amplitude gives\n\nHere we have separated out the contribution from the pole at. Now we can place the pole atand then bring the branch point atclose to 0 in a regulated way to recover the dispersive representation for the massless box diagram (A.23). In figure19, we show forandthat the representation converges and is independent ofas we integrate up to some large151515We can also derive other interesting relations using (2.5). For example,(A.26)where. This series is convergent everywhere except..\n\nSECTION: Appendix BChecks for Convergence\n\nSECTION: B.1Convergence withand constraints\n\nHere, we will check the convergence of the amplitudes withand the-constraints (4.14). For the purpose of demonstration, we will focus on the open superstring amplitude in fig.6and the black star amplitude in fig.9. In fig.20and fig.21we demonstrate the convergence of our numerics forinwith a grid of 180-constraints at each derivative order (see fig.2). In fig.20we show our results for the open superstring amplitude. In fig.21we show our results for the black star amplitude. We find that all of our S-matrices have converged very well with.\n\nNext, we proceed to show convergence of our amplitudes with the number of-constraints at each derivative order. We denote this number by C. The first grid that we choose is the same grid as in fig.2consisting of 180-constraints at each derivative order (C = 180). The second grid that we choose consists of 10 points in the 1st quadrant for, 60 points in the 2nd and the 4th quadrants for, 40 points in the 3rd quadrant forand 40 points in the regionandadding to a total of 210-constraints at each derivative order (C = 210) for the second grid. We useinand. We show our results for the open superstring in fig.22and for the black star in fig.23. We clearly see very good convergence with the number of-constraints.\n\nSECTION: B.2Convergence with\n\nHere, we show that the amplitudes have converged with. In fig.24and fig.25we plot the amplitudes for the open superstring in fig.6and the black star in fig.9for two different values of seed. For the open superstring we chooseand. For the black star we chooseand. We usein. We find great convergence for the respective amplitudes with the seed value of. In fig.26and fig.27we plotversusfor the open superstring and the black star with the seed value offor different values of. Again we find nice plateaus around this seed value of. This demonstrates the convergence of our numerics with.\n\nSECTION: B.3Comparison between the Fixed-dispersion relation and the LCSDR\n\nHere we will try to bootstrap the open superstring amplitude using the Fixed-dispersion relation and compare it with the results obtained from the LCSDR. For this we will use the ansatz obtained directly from the Fixed-dispersion relation for the amplitude (4.5):\n\nHere, we put the boundary termbecause we are focusing on the open superstring which satisfies the dual resonance model. We useinand impose the duality constraints:\n\nIn the () plane, the first term in thelhsof (B.2) is convergent fori.e.the 3rd and the 4th quadrants. Similarly, the second term in thelhsof (B.2) is convergent fori.e.the 2nd and the 3rd quadrants. Therefore, (B.2) is valid only in the 3rd quadrant. For the bootstrap, we choose a grid of 40 random points in the 3rd quadrant for. The tolerance we use is. We show our results in fig.28. We observe that the Fixed-dispersion relation has slower convergence compared to the LCSDR. Furthermore, we getmaxComplementarityin SDPB when we use too many duality constraints, for example if we use a grid of 80 random points instead of 40.\n\nSECTION: Appendix CWhy-constraints and not the null constraints\n\nHere we motivate the advantage of using the-constraints over the null constraints. We find that on the exact open superstring solution for the () grid in fig.(2) the variation of the null constraints has a much bigger range when compared to the variation of the-contraints. We demonstrate this in the table below. We find that the maximum value of the-constraint is larger than the maximum value of the null constraint in all four quadrants, as well as for the extra grid-points near. In general, the difference between the maximum and the minimum values is also greater for the null constraints than for the-constraints. Owing to the bigger spread of values, it is expected that with a fixed tolerance, convergence will be poorer for the null constraints, and this is precisely what we find in our numerics.\n\nSECTION: Appendix DCategory-III models\n\nA natural question to ask is that what are the quantities that can be minimized using SDPB so that we reproduce string like behavior for the S-matrices. Here we show that the natural quantity for minimization to reproduce the tree-level open superstring amplitude is\n\nMinimization of all other higher moments for exampledoes not reproduce the open superstring amplitude. The quantitycannot be minimized since it is a linear combination ofand(4.10). Again, we useinwithand impose the-constraints in the same grid as that for the open superstring. In fig.29(a) and (b) we show our results for the minimization ofand in fig.29(c) and (d) we show our results for the minimization of. We clearly see that minimization of these higher moments of the entangling power does not yield the open superstring amplitude as a solution.\n\nSECTION: Appendix EHypergeometric deformations\n\nDeformations of the open string amplitude have been recently studied in[52]. Parametric series representation for the hypergeometric deformed amplitudes is given below,\n\nFor, the amplitude approximates to(which impliesas indicated by the origin in fig.6), since in this limit the summand goes like. As,andat the leading order in their respective series expansions around. Therefore,implies. This is also indicated in fig.6.\nIn fig.30we plot the results for the bootstrap of the hypergeometric deformation of the open superstring amplitude forwithin. We use two different grid of constraints. The first grid is the same as that for the open superstring with 180-constraints at each derivative order, and the second grid has 210-constraints at each derivative order in a larger domain in theplane. We find excellent convergence with the number of constraints (C). The theories we find are not exactly the hypergeometric deformations but appear to be close cousins.\n\nSECTION: Appendix FGeneral integer-spaced spectrum with a higher-spin cutoff\n\nHere we will show that we can run our numerics using a general integer-spaced spectrum with a higher-spin cutoff and reproduce similar results for the open superstring amplitude in fig.(6) as well as for the black star amplitude in fig.(9). To demonstrate this we use the following general integer spaced spectrum with a higher spin cutoff: at mass levelthe spectrum containsin steps of. Note that the higher-spin cutoff used here matches with that of the open superstring but we have removed the restriction that only even (odd) spins contribute whenis odd (even). We useinand use the same grid of-constraints as the open superstring. The tolerance we use is. We show our results below. In fig.(31) we plot our results for the open superstring and find that both the actual spectrum and the general spectrum almost reproduce the open superstring (the general spectrum has slower convergence). In the two tables in fig.(32) we compare the results for the leading and the subleading Regge trajectories for the open superstring with the exact answer for both the actual spectrum and the general spectrum. We show that although the leading trajectory matches well with the exact answer for both the actual and the general spectrum, the subleading trajectory differs from the amplitude with the general spectrum. These discrepancies are compensated for by the news in the general spectrum. Some of the first few of these news are shown in table6. It is important to emphasise that at the level of the plots, the amplitudes appear almost indistinguishable.\n\nNext, we focus on the black star amplitude in fig.(9) and perform similar checks both with the actual spectrum and the general spectrum. The results for the black star are shown in fig.(33). Again we find very good match between between the amplitudes with the actual spectrum and the general spectrum. In fig.(34) we compare thes on the leading and the subleading Regge trajectories. We find a nice match between the two amplitudes for the leading Regge trajectory whereas the subleading Regge trajectory shows discrepancies between the two amplitudes. These discrepancies are being compensated by the news. The first few of them for the black star amplitude are given in table7.\n\nSECTION: References", "text_file": "data\\paper_texts\\2409.18259v2_content.txt"}, {"title": "Bed-Attached Vibration Sensor System: A Machine Learning Approach for\n  Fall Detection in Nursing Homes", "authors": ["Thomas Bartz-Beielstein", "Axel Wellendorf", "Noah P\u00fctz", "Jens Brandt", "Alexander Hinterleitner", "Richard Schulz", "Richard Scholz", "Olaf Mersmann", "Robin Knabe"], "published_date": "2024-12-06T11:08:47Z", "summary": "The increasing shortage of nursing staff and the acute risk of falls in\nnursing homes pose significant challenges for the healthcare system. This study\npresents the development of an automated fall detection system integrated into\ncare beds, aimed at enhancing patient safety without compromising privacy\nthrough wearables or video monitoring. Mechanical vibrations transmitted\nthrough the bed frame are processed using a short-time Fourier transform,\nenabling robust classification of distinct human fall patterns with a\nconvolutional neural network. Challenges pertaining to the quantity and\ndiversity of the data are addressed, proposing the generation of additional\ndata with a specific emphasis on enhancing variation. While the model shows\npromising results in distinguishing fall events from noise using lab data,\nfurther testing in real-world environments is recommended for validation and\nimprovement. Despite limited available data, the proposed system shows the\npotential for an accurate and rapid response to falls, mitigating health\nimplications, and addressing the needs of an aging population. This case study\nwas performed as part of the ZIM Project. Further research on sensors enhanced\nby artificial intelligence will be continued in the ShapeFuture Project.", "arxiv_id": "2412.04950v1", "html_link": "https://arxiv.org/html/2412.04950v1", "search_term": "ti:\"machine learning\"", "html_content": "SECTION: Bed-Attached Vibration Sensor System: A Machine Learning Approach for Fall Detection in Nursing Homes\n\nThe increasing shortage of nursing staff and the acute risk of falls in nursing homes pose significant challenges for the healthcare system.\nThis study presents the development of an automated fall detection system integrated into care beds, aimed at enhancing patient safety without compromising privacy through wearables or video monitoring.\nMechanical vibrations transmitted through the bed frame are processed using a short-time Fourier transform,\nenabling robust classification of distinct human fall patterns with a convolutional neural network.\nChallenges pertaining to the quantity and diversity of the data are addressed,\nproposing the generation of additional data with a specific emphasis on enhancing variation.\nWhile the model shows promising results in distinguishing fall events from noise using lab data,\nfurther testing in real-world environments is recommended for validation and improvement. Despite limited available data, the proposed system shows the potential for an accurate and rapid response to falls, mitigating health implications,\nand addressing the needs of an aging population. This case study was performed as part of the ZIM Project. Further research on sensors enhanced by artificial intelligence will be continued in the ShapeFuture Project.\n\n[idea]organization=Institute IDE+A, TH\u00a0K\u00f6ln,addressline=Steinm\u00fcllerallee\u00a01,\ncity=Gummersbach,\npostcode=51643,\ncountry=Germany\\affiliation[iam]organization=Institut f\u00fcr Allgemeinen Maschinenbau, TH\u00a0K\u00f6ln,addressline=Steinm\u00fcllerallee\u00a01,\ncity=Gummersbach,\npostcode=51643,\ncountry=Germany\\affiliation[t4k]organization=tecfor care GmbH,addressline=Fraunhoferstra\u00dfe\u00a08,\ncity=Gummersbach,\npostcode=51647,\ncountry=Germany\\affiliation[hsbund]organization=Federal University of Applied Administrative Sciences,addressline=Willy-Brandt-Stra\u00dfe\u00a01,\ncity=Br\u00fchl,\npostcode=50321,\ncountry=Germany\n\nProvides a non-intrusive fall detection system integrated into care beds.\n\nIllustrates the full development process from data collection to model training.\n\nSECTION: 1Introduction\n\nFalls in nursing homes pose substantial risks to the elderly, underscoring the critical importance of promptly identifying such incidents to mitigate immediate distress and long-term health consequences. The core objective of this research is the development of an efficient, non-intrusive fall detection system that seamlessly integrates with care beds. Extensive research from the Robert Koch Institute reveals that falls represent a significant cause of accidents, especially among the elderly, with approximately one-third of fatal accidents among those aged over 65 in the European Union attributed to falls(RKI,2024).\nNursing home residents, due to their vulnerability, are at an increased risk, often experiencing falls in close proximity to their beds.\n\nIt is noteworthy that nearly half of all falls result in injuries, with one in five necessitating medical treatment, as indicated by studies(Rubenstein,2006; Tinetti et\u00a0al.,1988; Sonnenmoser,2015). Of these falls, five percent result in bone fractures,\nwhile approximately two percent lead to hip fractures. Furthermore, the prevalence of bone fractures rises significantly with age, affecting over one-third of all accidents in individuals over 70 within the EU(RKI,2024). The consequences of falls, especially in advanced age, are indeed severe, often leading to substantial limitations in mobility. One of the most severe injuries associated with falls is the femoral neck fracture, accounting for roughly 90% of cases among the elderly(Becker and Blessing-Kapelke,2011). Approximately half of those affected are unable to navigate stairs or engage in short walks a year after sustaining such an injury. Hip fractures, however, are even more critical, resulting in the passing of one-third of the very elderly individuals and leaving 20% permanently dependent on care(Sonnenmoser,2015). Rapid detection of such injuries can help mitigate their consequences, whereas delayed detection can result in life-threatening situations.\n\nAlso the psychological consequences of falls should be taken seriously, as they can result in decreased mobility, heightened isolation, and a notable decline in the quality of life among older individuals. In addition to the immediate physical effects of falls, they may develop a fear of falling, which, regardless of their physical condition, can compromise gait stability and lead to falls(Sonnenmoser,2015). These individuals tend to restrict their mobility, decreasing their quality of life, independence, and often experiencing depression(Tinetti et\u00a0al.,1988; Sonnenmoser,2015; Jefferis et\u00a0al.,2014).\n\nConsidering these multifaceted challenges, early fall detection is of vital importance, as every minute counts in reducing suffering and long-term consequences. There is a pressing need for an alarm system capable of swiftly requesting assistance after a fall, which can alleviate the fear of falling and its associated psychological consequences. Comprehensive monitoring systems that safeguard individuals\u2019 privacy while offering timely assistance are in high demand. In the following sections, an innovative artificial intelligence (AI) based approach for the accurate detection and classification of falls occurring in proximity to beds will be implemented. This study presents a cost-effective deep learning-based fall detection system seamlessly integrated into care beds, with a focus on enhancing patient safety and ensuring privacy by avoiding the use of wearables or invasive video monitoring. It is crucial to highlight that no existing system fully satisfies these intricate requirements.\n\nThis paper is structured as follows:\nSection2provides an overview of the existing technology and approaches for fall prevention, with a particular focus on fall detection.\nSection3details the developed detection system and its constituent components.\nSection4elaborates on the model training process, including data collection, augmentation, and evaluation.\nSection5presents the model architecture and the training process.\nSection6describes lab and real-world experiments conducted to evaluate the system\u2019s performance.\nSection7presents the results of the study and discusses the implications of the findings and potential future research directions.\n\nSECTION: 2Related Work\n\nThis section provides first an overview on fall occurrence and the aftermath and then gives a brief summary of the existing technology and approaches for fall prevention.\n\nSECTION: 2.1Fall Prevention Methods\n\nAlthough various measures can be employed to prevent falls from beds, including the use of bed rails or restraints, many of these interventions are considered restrictive and can only be used under specific legal conditions, posing both ethical and safety concerns.\nBed rails, in particular, can introduce additional fall hazards and significantly impact the quality of life for those under care.\n\nFall-prevention measures can be categorized into three groups:\nFirstly, person-centered measures which involve promoting balance, exercise for those with mobility issues, strength training, and consideration of underlying health conditions that may affect mobility.\nSecondly, medication-related measures including adjusting or discontinuing medications that may increase the risk of falls.\nAnd lastly environment-centered measures involving proper footwear, training with assistive devices, and, critically, the elimination of tripping hazards.\n\nWhile these measures can reduce the risk of falls, they cannot entirely eliminate it. Therefore, additional measures are required to detect and report falls in a timely manner.\n\nSECTION: 2.2Fall Detection Methods\n\nThe methods for fall-detection can mostly be divided based on the position and type of the used sensor. Body-worn sensors are embedded in clothing, accessories, and assistive devices, collectively known as \u201cWearables\u201d. User-activated or community alarm systems are based on alarm buttons that are either implemented in a wrist band or installed stationary. Automatic wearable fall detectors remove the need to press a button independently. The latter one often employs accelerometers and tilt sensors to measure the orientation of the body or shocks that occur during falls(Zigel et\u00a0al.,2010; Litvak et\u00a0al.,2008).\n\nRoom sensors are fixed within the living space and utilize various technologies, such as cameras, radio waves, infrared, and temperature sensors. They can monitor movement patterns and interpret falls(Boehnke et\u00a0al.,2012; Koblischke,2007). Especially camera-based fall detectors are highly effective(Loharkar and Choubey,2016).\n\nFurniture-mounted sensors combine the advantages of body-worn and room-based systems. These sensors, often referred to as \u201csmart furniture\u201d, can operate without the need for users to wear them, while, depending on the method, measuring necessary data to detect falls without compromising privacy.\n\nCurrent products available on the market mainly focus on detecting whether an individual is in bed or seated.\nSome innovative developments aim to capture more extensive data for fall detection, such as weight shifts through the use of pressure sensors in bedposts.\nVibration based systems are installed stationary and detect falls by measuring vibrations in the floor, trying to detect the distinct pattern of a human fall(Alwan et\u00a0al.,2006). An extension of this approach additionally facilitates sound to further enriches the data available for the detection algorithm(Loharkar and Choubey,2016).\n\nOur study implements a vibration-based system.\n\nSECTION: 2.3AI in Fall Detection\n\nThe AI methods employed for fall detection are as diverse as the sensors used. Camera-based fall detection systems, for instance, utilize image recognition techniques, which represent a popular application domain for artificial neural networks(Foroughi et\u00a0al.,2009; Alhimale et\u00a0al.,2014; Lu et\u00a0al.,2018). In this context, a combination of conventional methods (e.g., decision trees,-nearest neighbor), more intricate machine learning approaches (e.g., support vector machine, self-organizing maps), and deep learning methods based on neural networks are utilized(Vallabh and Malekian,2018).\n\nNotably, supervised machine learning methods are frequently deployed, indicating the presence of labels for the training data, allowing for the clear differentiation between fall and non-fall data instances. Conversely, unsupervised techniques are less commonly employed, aiming to learn the normal state and classify deviations (anomalies) as potential fall events. A demonstration of this is detailed byVincenzo et\u00a0al. (2017). However, for fall detection, unsupervised methods pose challenges as other events can also lead to anomalies, thereby resulting in higher false detection rates. In contrast, supervised methods are disadvantaged as they rely on labeled data, which can be difficult to obtain due to the relative rarity of fall events(Vallabh and Malekian,2018).\n\nPrevious investigations into fall detection using vibrations, as relevant in this project, have employed techniques such as the Bayes classifier. This approach demonstrated relatively promising results in experiments with dummies(Zigel et\u00a0al.,2010), albeit relying not only on vibrations but also on the additional recording of sound signals.\n\nSECTION: 2.4Privacy in Fall Detection\n\nMany existing fall detection systems use sensors like cameras or microphones that could potentially infringe on users\u2019 privacy. These systems record data that can be used to monitor users in their private environments, raising significant privacy and security concerns. Data is typically exchanged over networks, possibly evaluated by third parties like cloud service providers. This situation poses notable risks from both unauthorized access and the use of data by authorized entities(Grunert and Heuer,2016). The Internet of Things (IoT)\nand Smart Home technology landscape intensifies these challenges, as many devices have weak security, handle sensitive data, and interact with technically inexperienced users(Kumar and Patel,2014; Zeng et\u00a0al.,2017; Psychoula et\u00a0al.,2018). To address these concerns, data minimization and avoidance should be integrated into the system design from the selection of sensors to the initial data processing, aligning with privacy regulations(Grunert and Heuer,2016).\n\nTable1divides the currently established methods into five categories. the following four criteria were highlighted to visualize the differences and similarities between the methods, but also to illustrate the problems and advantages.\n\nis characterized by the accurate detection of falls, the likelihood of false alarms, and resilience against human factors that may interfere with detection.problems with detection.\n\ndescribes how minimally invasive the intrusion into privacy is when the corresponding system is used.\n\naims at the ease of use and the effort that the user/patient needs to take to facilitate the method.\n\ntakes the investment for the nursing home or patient into account.\n\nSince user-activated or community alarm systems are dependent on the manual usage of the alarm button, depending on the fall or subsequent unconsciousness, the button might be unreachable for the patient and therefore it lacks reliability. Additionally, in case of a wearable, the patient might forget to wear it or removes it while using the bathroom, which is typically a place with a high occurrence rate of falling(Litvak et\u00a0al.,2008).\nThe later issues also apply to automatic-wearable fall detectors that additionally often produce false alarms which harms their reliability(Zigel et\u00a0al.,2010; Litvak et\u00a0al.,2008). The biggest downside of highly effective camera-based fall detectors is that they are intrusive and cause privacy concerns(Loharkar and Choubey,2016). Vibration-based systems (ours) solve the problems of body-worn sensors, function independently of the patient\u2019s discipline and allow reliable fall detection(Loharkar and Choubey,2016). Combining vibration data with additional sound measurements could improve reliability(Loharkar and Choubey,2016)but the downside is that installed microphones can also raise patient privacy concerns.\n\nSECTION: 3Methods and Materials\n\nFigure1illustrates the results of the study; the muted shade of gray indicates components still to be developed for a final application.\n\nSECTION: 3.1Developed Detection System and Component Overview\n\nThe subsequent section describes the developed detection system and its constituent components.\nThe Teensy 4.1 with an ARM Cortex-M7 processor111See:https://www.pjrc.com/store/teensy41.htmlis used as the development board.\nIt has a processor speed of 600\u00a0MHz and 7936\u00a0KB flash storage and 1024\u00a0KB RAM(Mart\u00ednez et\u00a0al.,2022).\nThe Teensy 4.1 development board is selected due to its higher clock rate and larger memory compared to alternatives such as the Arduino Uno with ATMega 328 processor(Sihombing et\u00a0al.,2018; Hidayanti et\u00a0al.,2020).\n\nThe developer board is connected to the adapter board of a MEMS accelerometer via an I2C bus connection.\nThe board stores the data measured by the MEMS acceleration sensor together with a timestamp.\nSince the board lacks a separate real time clock, it must be continuously powered keep time.\nTherefore, in addition to the wired power supply, the sensor module is also powered by a button cell to bridge short-term power failures.\nThe collected data is stored in binary format on an integrated SD card.\n\nDuring operation, the vibration measurements are intended for transmission to a central inference platform within the nursing home, aimed at facilitating fall detection through enhanced computational capabilities.\nWhile the development of this platform and its integration with Teensy 4.1 is pending, the study encompasses the detection process.\n\nThe collected measurements are filtered using a logistic regression model, which detects possible fall events.\nFor this purpose, five specific features, denoted asto, are extracted and used for training the logistic regression model.\nUpon the classification of the signal as a potential fall event, it undergoes a short-time Fourier transformation (STFT), yielding a spectrogram.\nThis spectrogram is then passed to a convolutional neural network (CNN) trained to discern human falls from other events, such as objects like water bottles.\nLeveraging the logistic regression aids in significantly reducing the computational burden required for the continuous operation of the system, as it minimizes the frequency of STFTs and CNN inference runs.\nAn additional feature that remains pending implementation is the post-detection processing of identified human falls, envisaged to trigger a signal in a human-machine interface for the nursing staff, alongside an alarm.\nSo far, this integration lays a robust foundation for a cost-effective fall detection system, promising practical applications in various contexts.\nThe components will be explained in detail in the following sections.\n\nSECTION: 3.2MEMS Vibration Sensor\n\nMEMS accelerometers can be categorized according to the different types of force measurement. The type of force measurement can be resistive or capacitive(Jin,2021). Both variants have in common that the spring element of the sensor is made of silicon. The spring element functions like a bending beam that is fixed on one side and has a floating mass at the end of the free-swinging side(Kazusuke Maenaka,2008). With resistive force measurement, the displacement of the mass is determined via strain gauges. In capacitive force measurement, the mass lies between two plate capacitors and generates a change in the capacitance ratio of the two capacitors when displaced. Capacitive MEMS sensors are small, cheap, and suitable for detecting low-frequency ground vibrations(Niu et\u00a0al.,2018; Babatain et\u00a0al.,2021). Therefore, MEMS sensors based on the capacitive principle are used in the developed system.\n\nPreliminary investigations have shown that a sampling rate of 1600\u00a0Hz is necessary for precise fall classification.\nThe capacitive MEMS sensor selected for the planned measurement system is of the type AIS2IH222https://www.st.com/resource/en/datasheet/ais2ih.pdf(Zauli et\u00a0al.,2023).\nIt has a sufficiently high sampling rate of 1600\u00a0Hz in all three dimensions compared to alternative sensors such as the BMA400333https://www.mikroe.com/accel-5-clickwith 800\u00a0Hz(Tjonck et\u00a0al.,2021).\nThe AIS2IH MEMS sensor is integrated into a STEVAL-MKI218V1 adapter board, which is connected to the Teensy 4.1 developer board via an I2C communication bus.\n\nSECTION: 3.3Logistic Regression\n\nIn the studied use case of human fall detection, Logistic regression serves as a primary mechanism to estimate the probability that a particular input data point belongs to one of two distinct categories: an event or no event. In this state of the detection, an event is anything that falls to the ground.\n\nThe data batches, each of 10 seconds in length, are manipulated by initially undergoing a zero-mean transformation and squaring, followed by the calculation of five pivotal features: maximum, median, mean, and 25th and 75th quantiles.\nThe logistic regression model, mathematically represented as\n\ninvolves optimizing the componentsof the vectorto best predict the probability of an event occurrence,, giveninput features.\nThe optimization is typically achieved through a gradient descent on a suitable loss function, often the cross-entropy loss.\nThe decision about the predicted label for an observation is then made by setting a threshold, commonly 0.5; ifis above this threshold, the observation is classified as an event, otherwise, it is not.\nIn the application described, ensuring that the logistic regression model is accurately trained and validated is of vital importance, as misclassification, especially false negatives, could result in serious events such as a person falling not being detected.\nTherefore, robust implementation and evaluation of the logistic regression model are critical to reliably filter and subsequently analyze true events(McCullagh,2019).\n\nSECTION: 3.4Short-Time Fourier Transform\n\nThe STFT is a key method in signal processing that offers a way to analyze sound or vibration data in the time-frequency domain(Shuvaev et\u00a0al.,2017). It achieves this by dividing a waveform into smaller time segments or windows and then applying Fourier transforms to each of these windows. This approach enables us to examine how the signal\u2019s frequency characteristics change as time progresses. The given signal, denoted as, can be transformed into a discrete STFT representation, labeled as. This transformation involves summing over all possible values of, withextending from negative infinity to positive infinity. In this summation,is multiplied by a window functioncentered at the time index, and also by a complex exponential term:\n\nThe resulting STFT representation,, is informative about various aspects of the waveform, such as the frequencies present in the signal and their respective amplitudes. Taking the squared magnitude of this windowed STFT representation, denoted as, and concatenating it over time, a spectrogram is obtained(Shuvaev et\u00a0al.,2017).\n\nSECTION: 3.5Convolutional Neural Network\n\nCNNs represent a category of deep neural networks that are commonly employed in image classification and analysis. CNNs facilitate end-to-end learning for categorization and feature extraction. The fusion of CNNs with STFT-generated spectrograms represents a promising approach to audio or vibration classification(Shuvaev et\u00a0al.,2017; Sejdi\u0107 et\u00a0al.,2009; Grunert and Heuer,2016). CNNs serve as feature extractors, discerning spectro-temporal patterns directly from STFT data and eliminating the need for traditional feature representations like Mel-Frequency Cepstral Coefficients (MFCCs). Advantages of this approach include simplicity and the potential for the network to learn complex and abstract concepts and features that might not be obvious to a human analyst. Nonetheless, this methodology comes with computational and memory demands, especially when processing high-resolution spectrograms(Demir et\u00a0al.,2019). Careful resource allocation is vital for real-time applications. Additionally, the risk of CNNs learning unwanted patterns, such as channel distortions, necessitates diligent preprocessing and regularization.\n\nThe primary function of the convolutional layer is to establish local connections between features from the preceding layer and map their characteristics to a feature map.\nThis mapping is achieved through the convolution operation, where the inputis convolved with the filter:\n\nThe filtercan be represented as a matrix, i.e.,\n\nTo introduce non-linearity to the feature map generated by the convolution operation, a non-linear activation function is applied(Bengio and Lecun,1997). To achieve a compression of features in connection with CNNs, max-pooling layers are often used. The max-pooling layer\u2019s purpose is to identify semantically relevant features from the previous layer. It achieves this by down-sampling the previous layer, dividing it into rectangular pooling regions, and computing the maximum value within each region(Demir et\u00a0al.,2019). In summary, CNNs ability to automatically extract complex features, while simplifying the feature engineering process, makes them a compelling choice for waveform analysis. The convolutional layer, non-linear activation, and max-pooling enhance the efficacy of this approach in extracting and classifying meaningful spectro-temporal information.\n\nSECTION: 3.6Metrics\n\nTo evaluate the performance of the trained models, mainly the Recall and Precision were facilitated. They are calculated as follows:\n\nRecall measures the model\u2019s ability to correctly identify all positive instances within the test set, making it particularly important in scenarios where missing positive cases can have serious consequences. For fall detection in nursing homes, a high Recall ensures that the system does not overlook critical fall events and was therefore of highest priority for the evaluation of the models. Precision quantifies the model\u2019s ability to correctly classify positive instances out of all instances predicted as positive, making it valuable in applications where false alarms are undesirable(Powers,2020). In the context of fall detection systems in nursing homes, high Precision means fewer false alarms, reducing the workload on healthcare staff. The objective was to attain optimal fall detection (Recall = 1.0) while minimizing false alarms (maximizing Precision). During evaluation, the threshold was dynamically adapted to achieve a Recall of 1.0 and then aimed to maximize Precision. The use of accuracy as a metric was not suitable because the dataset exhibited a significant imbalance. In Sec.3.7, the use of data augmentation methods to address this imbalance in training will be discussed in more detail.\n\nSECTION: 3.7Data Augmentation\n\nTo train the classification models, vibration measurements of human falls are necessary. To avoid injuring people, crash test dummies were used for the artificial generation of these incidents. However, this kind of data generation is substantial effort and can hardly be automated. Data augmentation is a feasible method to generate additional data based on the dummy trials. Augmenting data can involve traditional approaches like altering or permuting the data. In addition, the introduction of noise plays an important role. Modern techniques generate new, synthetic data based on information from existing data. These methods include variational autoencoders (VAE) and generative adversarial networks (GANs), which are algorithms based on neural networks. This study examines two classical augmentation methods, namely oversampling and amplification. Further research might delve deeper into more sophisticated augmentation methods.\n\nOversampling is the most trivial augmentation method and simply duplicates the dummy fall samples as often as necessary. This augmentation method serves as a baseline and allows to assess the benefits of more sophisticated augmentation methods(Shorten and Khoshgoftaar,2019). Amplification is an augmentation method that might be of high relevance for the studied problem, since during data acquisition, a single dummy was utilized, and fall data exclusively reflects this dummy\u2019s weight. Amplification and damping of the signal are assumed to simulate falls of individuals with varying weights, thereby broadening the operating range of the algorithm. The amplification process initiates with the extraction of the mean and standard deviation of the signal\u2019s noise, utilizing a statistical tool called three-sigma rule. This rule states that samples of normally distributed data that are situated outside the mean plus or minus three times the standard deviation are define as outliers(Lehmann,2013). Within this study, this method serves to identify samples associated with specific events (samples outside of the boundary range) or noise (samples inside the boundary range) in the dataset. To center the signal around zero and prevent a zero offset for the noise, the computed mean of the identified noise is subtracted from the original signal. Subsequently, a new limit range is defined by adding and subtracting three times the noise\u2019s standard deviation from zero. Values exceeding these limits are identified as indicative of fall events. The amplification factor is then randomly selected from a uniform distribution within predetermined boundaries. Signals below the limit range are scaled down, while those surpassing it are scaled up. Finally, the signal scaling is reverted. This method facilitates targeted signal amplification exclusively for values relevant to fall events.\n\nFigure3shows an example of the positive amplification of a dummy event. The orange scatters plot shows the original data, while the blue plot shows the amplified signal. It can be seen that the amplification is only carried out at points that can be assigned to an event. Noise within the data window is not amplified, as this would distort the characteristics of a fall event.\n\nSECTION: 4Model Training\n\nSECTION: 4.1Data Collection\n\nData collection is important in the development of a sensor-based fall detection system that is able to accurately detect fall events and distinguish them from other events(Loharkar and Choubey,2016).\nIts primary purpose is to gather the necessary data for training and evaluating the fall detection system.\n\nThe placement of the sensors is an essential aspect of the data collection process(Litvak et\u00a0al.,2008; Loharkar and Choubey,2016; Alwan et\u00a0al.,2006).\nThe measuring system will be integrated into the care bed in the final application.\nTo analyze the data quality depending on the measuring point, the sensor data is recorded at three different positions, see Figure4a) and Figure4b). For this purpose, a 4.7\u00a0kg test specimen is dropped from a height of 1 m at different distances from the care bed. The comparison of the maximum measured amplitudes as a function of the distance between the sensor and the impact enables the optimum placement to be determined. Measuring point I, on the upper bed frame, has the smallest maximum acceleration of 0.25 g. The acceleration of point II and point III follow following a similar curve, although the maximum acceleration at positions II is 1.01 g and III is 0.78 g.\nDue to the low acceleration and the associated low signal strength at measuring point I, for further tests the sensors are mounted at positions II and III.\n\nTo create a comprehensive data set a variety of different floor vibrations has to be measured.\nBeside a dummy fall test, different events have to be simulated, for example, falls with everyday objects such as bottles, dumbbells and personal care items, as well as dynamic actions such as jumping, tipping of chairs and furniture movements.\nThe events are carefully selected to cover different scenarios and potential fall situations.\n\nTable4shows the fall events included in the data set. In addition, each event is measured under different configurations of room size, floor level, floor material and distance of the sensor to the fall. All variants are listed in of Table2. The test setup is designed to mimic real-life conditions as closely as possible to ensure the accuracy and relevance of the data collected. The setup is visualized in Fig4. To ensure repeatable tests, a device is built that allows the dummy to fall in a defined way. The most important parameters of the dummy setup are:\n\nMaximum distance between the dummy and the floor: approx. 350\u00a0mm\n\nTriggering of the drop test with a quick-release lever\n\nTest dummy: I.A.F.F. Rescue Randy made of PVC, weighing 75\u00a0kg and 1.83\u00a0m tall\n\nThis setup enabled the systematic and controlled generation of fall events and ensures that data collection is not only comprehensive but also consistent. The data set for fall detection is created on the basis of more than 1000 fall events. Each event is measured three times with three sensors and eight settings each. In addition, around 20,000 negative events are extracted from the 24-hour measurements to strengthen the data diversity with recordings without falls. The dummy\u2019s posture is varied, resulting in different fall patterns. This variation contributes to the diversity of the dataset and allows the system to recognize and adapt to different realistic fall scenarios. This is beneficial for training AI models as it adds an additional layer of complexity and realism to the dataset(Zigel et\u00a0al.,2010; Litvak et\u00a0al.,2008).\n\nTo ensure that the AI models can also be trained with real falls, field tests are carried out over a period of 6 months in a retirement and nursing home.\n\nThe sensor unit described in Sec.3is manufactured 20 times for the field tests. The field test study is carried out in 10 rooms of the retirement and nursing home. Two sensor units are installed in each room at positions II and III. The rooms differ in size (small and large), number of residents (1 and 2) and floor (ground floor, first floor and second floor). The floor type (PVC) and the type of bed used remain the same. The measurement data is read out at intervals of 4 weeks and stored anonymously. Over the duration of the field tests, a total of 29 falls occurred in the rooms equipped with sensor units. During readout, the sensors were found to be disconnected from their power supply a total of 33 times. As a result, some sensor units were unable to record any or only partial data within a readout interval. This means that not all falls can be evaluated.\n\nSECTION: 5Model Definition and Tuning Results\n\nThe vibration signal employed for fall detection was sampled at a frequency of 1600\u00a0Hz. Utilizing a shifting window approach, the detection algorithm received 10-second segments of this signal for classification. To serve as a pre-filter, the logistic regression was implemented using the scikit-learn library. As explained in Sec.3.3, the extraction of several features from the vibration signal batches was conducted. Given these features, the logistic regression was trained to differentiate between fall events, not only limited to human falls, and the absence of any event.\n\nThe CNN implementation was carried out using Keras. The original signal batches underwent a STFT, generating spectrograms utilized for training the CNN. The architecture includes a convolutional layer with ReLU (R(z)=max(0,z)) activation function, no padding, and strides set to one. Subsequently, a max-pooling layer and a flatten layer were incorporated, followed by a fully connected output layer with a sigmoid activation, as depicted in Fig.5.\n\nThe CNN\u2019s hyperparameters were optimized using the Hyperband tuner integrated in Keras. The method is based on the successive halving algorithm and allows hyperparameter tuning by testing several settings in parallel and sorting out bad runs at an early stage(Li et\u00a0al.,2018)\n\nThe optimized features included the number of filters, kernel size width, loss function, and learning rate of the optimizer. Table5provides an overview of the hyperparameter search space. The goal of the tuning process was to optimize precision on the validation dataset while maintaining a recall value of 1. This involves setting a classification threshold that ensures all relevant instances are captured (recall of 1) while maximizing the precision of the predictions. Consequently, the hyperparameter tuner aimed to identify a configuration that accurately classifies each fall while minimizing false alarms. The tuning process was conducted for a maximum of 20 epochs per run and the validation loss as the stopping criterion. The outcomes of the tuning process are presented in Table5.\n\nFinal training was performed using the tuned settings with a stratified-fold cross validation withset to 5.\nAdditionally, as explained in Sec.3.7, oversampling and amplification augmentation were facilitated to enrich the dataset with additional fall events, thereby addressing the imbalance in the initial dataset.\nThe final training was carried out for 75 epochs, employing the Adam optimizer.\n\nSECTION: 6Experiments\n\nSECTION: 6.1Laboratory Experiments\n\nIn the pursuit of determining the optimal model architecture for the use case, a structured experiment pipeline was established.\nThis pipeline was essential in evaluating various experiments while ensuring comparability for retrospective analysis.\nCentral to our methodology was the implementation of stratified-fold cross-validation due to the imbalanced dataset to validate experiment results.\nThe experiments focus on the second phase of the two-step procedure, which is responsible for classifying different event types; the first phase uses logistic regression to differentiate event data from noise.\n\nTo validate different CNN models, a specific evaluation function was employed.\nWhen dealing with data augmentation, it was vital to integrate this process into the framework of stratified-fold cross-validation.\nTo identify an appropriate augmentation function, it was necessary to derive it solely from the training set.\nFailing to do so could result in data leakage, even when using augmentation methods that significantly alter the original data structure.\nDespite substantial changes, the fundamental basis and underlying truth of the data persist. As a consequence, this approach resulted in a computationally intensive implementation of the-fold method (see Algorithm.1).\n\nEvery data augmentation method was evaluated on four unique models:\n\na baseline model trained on original data,\n\nan all-inclusive model trained on both original and augmented data,\n\na model exclusively trained on augmented data, and\n\na two-step model initially trained on augmented data then fine-tuned on original data.\n\nThese models were assessed across various settings and with different data augmentation methods. As stated in Sec.3.6, the precision of the predictions were maximized while maintaining a recall of 1.0.\n\nSECTION: 6.2Deployment in Nursing Home\n\nIn the simulation of the application scenario in a nursing home, the focus was on simulating the actual operating conditions and the feasibility of the fall detection system. The data was collected with the sensor setup described in Sec.3.\nThe difference to the later planned implementation is that the data has so far been stored and then analyzed and not in real time, as is later planned for the real application. With the help of these authentic vibration measurements, the accuracy of the simulation and the correspondence with real signal characteristics and subtleties could be tested.\n\nThe deployment experiment replicates the intended real-time fall detection process as illustrated in Fig.6: The signal (blue) undergoes a two-stage classification. First logistic regression for initial event detection (orange) and second STFT and CNN classification (red). A spike in the orange graph marks every time the logistic regression detected an event itself and therefore a STFT was calculated. A spike in the red line marks every time the CNN classified one of those STFTs as a human fall. The signal shown in Fig.6indicates a period of 3 hours in a room. Only a rough indication of around mid-afternoon was provided as a label by employees of the nursing home.\nSimilar to the laboratory data analysis, we processed 10-second sequences individually by moving a window through the data chronologically. For each window, we calculated an input vector for the logistic regression. The frequency of this calculation depended on the step size of the shifting window. In our experiment, the window size matched the step size, resulting in non-overlapping signals. With a signal length of 3 hours, this led to a total of 1080 windows.\n\nSECTION: 7Results and Discussion\n\nSECTION: 7.1Augmentation Results\n\nThe initial experiment focused on data augmentation through duplication, which served as a baseline for the augmentation methods. It was observed that the precision of the model, which was trained on data without augmentation, fluctuated around 30%, regardless of the duplication level. The duplication augmentation involved replicating each entry in the training set by a specific value, with tested values ranging from 1 to 30. As duplication increased, models trained on augmented data showed improved precision. However, the precision plateaued beyond a duplication value of about 10, likely because of a nearly balanced dataset at this point. Despite this, a higher duplication level reduced the performance gap between the all-inclusive model and the baseline model. Further experiments for statistical smoothing were deemed necessary, but an overall improvement with data augmentation was clear, with the highest precision recorded at 65.24% for the model trained solely on augmented data at a duplication value of 20.\n\nFollowing duplication, amplification was employed as another method of data augmentation. As described in Sec.3.7, this involved isolating events and adjusting their signal amplitude, with the augmented dataset being adjusted to balance dummy and non-dummy data, as informed by the findings from the duplication experiment. A reduction in precision was observed in the all-inclusive and only augmented models when signal amplitude was decreased, indicating effective isolation of the signal during the augmentation process. Optimal performance for the augmented models was found within an amplification range of 0.7 to 1.3. This suggested that amplification might not be as significant as the addition of similar data variants. Precision values were compared using a duplication of factor 10 for dataset size comparability as seen in Fig.8.\n\nIn the final comparison of augmentation methods, each experiment\u2019s baseline model was contrasted with its all-inclusive counterpart.\nThe primary focus was on the change in precision between the unaugmented and all-inclusive models, rather than on their absolute precision.\nThe trend was unmistakable: as the dataset became more balanced with a consistent presence of unchanged dummy data, precision improved with the use of data augmentation, underscoring the importance of data balance in achieving model accuracy.\n\nThe experiments hint an inherent limitation of the underlying dataset in diversity and volume, which present a breeding ground for potential overfitting within the CNN models. If the dataset fails to encapsulate the rich spectrum of variations and nuances which are inherent in real-world scenarios, the models\u2019 ability to generalize beyond the provided data and not overfit is restricted. Additionally, data augmentation methods which enhance the model\u2019s ability to generalize cannot be fully tested if the validation is also too limited.\n\nMoreover, the effectiveness of duplication as an augmentation technique, while demonstrating notable performance improvements, also raises concerns. The success of this method may indicate that the model tends to focus on certain cases in the data, which could affect its ability to generalize to new, unseen cases. In addition, the minimum change required during amplification (about 1) to achieve optimal performance also indicates the risk of limited diversity in the amplified signals. The closer the augmented signals are to the signals of the training set the better the model performance. This could be due to the similarity between test and training data independently of the fold. Experiments done with VAEs for date augmentation have shown restricted variance within the latent space, which further supports these indications. This restriction implies clustering of points, suggesting a lack of diverse representations within the dataset and potentially hindering the model\u2019s ability to capture a wide range of characteristics essential for robust generalization. Factors inherent in the dataset, such as the consistent representation of dummy falls and uniform fall motions directed straight to the ground, likely contribute to this limited variance. Such uniformity could limit the model\u2019s exposure to different fall patterns, which could increase its tendency to overfit. However, the challenge in comparing augmentation methods arises from the significant similarity between test and training data. This similarity makes it difficult to robustly assess the generalization abilities of the model and makes it difficult to identify possible overfitting tendencies.\n\nSECTION: 7.2Deployment Results\n\nThe logistic regression identified 39 events within these windows, and the CNN pinpointed one of these events as a human fall.\n\nFigure10displays the corresponding spectrogram of this fall. Currently, validating real falls is not feasible due to imprecise time data and various environmental vibrations in actual measurements. Although our model detects potential falls, we cannot verify the accuracy of these predictions. Moreover, establishing a direct link between Lab Data and Real Data remains elusive. To address this uncertainty, we recommend rigorous testing of the model in a real-world setting post-implementation of necessary measures. While the model effectively distinguishes falls from noise in artificial scenarios, its performance in real environments remains unproven. However, the model demonstrates sensitivity in discerning fall events without triggering frequent false alarms. Even if not all falls are detected during test deployments, individual event identifications can serve as valuable data for refining the model further.\n\nSECTION: 8Summary and Outlook\n\nThe approach of utilizing vibration-based fall detection systems presents several benefits.\nFirstly, it offers a high level of reliability in detecting falls accurately, as it doesn\u2019t rely on manual activation like user-activated alarms or community alarm systems.\nThis eliminates the risk of the alarm button being unreachable during an emergency.\nAdditionally, it overcomes the issue of patients forgetting to wear or removing wearable devices, which commonly occurs with automatic wearable fall detectors.\nMoreover, vibration-based systems are minimally invasive, thus preserving privacy better compared to camera-based alternatives.\nThey function independently of the patient\u2019s actions, reducing the burden on the user and ensuring continuous monitoring, even in private settings like bathrooms where falls are prevalent. Overall, vibration-based fall detection systems offer a balance of reliability, privacy preservation and user comfort, making them a promising solution for addressing fall-related risks in various settings, including nursing homes and home care environments.\nThis study identified several challenges in developing this type of desirable monitoring system for nursing homes.\n\nA crucial issue identified is the missing variance in lab data, hindering the application of sophisticated augmentation methods and the development of unbiased models.\nTo support ongoing research efforts in this area, the comprehensive dataset compiled will be disseminated as an integral part of this investigation.\nOf particular note is the potential for synergistic merging of this dataset with other datasets in the field of fall detection, which promises to improve data fidelity and thereby advance the scientific study of the critical area of fall detection in nursing homes.\nWith only 66 instances of dummy case events and a mere 22 recorded falls, the dataset lacks both in volume and diversity.\nThis limitation raises concerns about potential bias, particularly towards nursing home residents with a weight of 75\u00a0kg, neglecting the variability in fall characteristics across different weight categories.\n\nTo mitigate this issue, we propose the generation of additional dummy data with a deliberate focus on introducing variability. Potential strategies include adjusting dummy mass through the incorporation of additional weights or modifying the distance between sensors and falling points in incremental steps. Additional data collection in real nursing home settings, possibly using a secondary device like dedicated fall detection wearables, might be of great interest as well. Enriching the dataset with diverse variations holds the promise of fostering a more robust model, enhancing its generalization capabilities, and preparing it for potential future integration of more sophisticated augmentation techniques.\n\nAlthough the two-stage AI approach, which combines a simple logistic regression model and a relatively small CNN, was successful, more challenging real-world data might require a more complex setup with many hyperparameters. Therefore, more sophisticated hyperparameter tuning algorithms are necessary(Bartz et\u00a0al.,2022). Furthermore, online-machine learning approaches could be beneficial to adapt the model to the real-world data(Montiel et\u00a0al.,2021; Bartz-Beielstein and Bartz,2023).\n\nFurther research into improving sensors using AI is being carried out as part of the ShapeFuture project. This project is focusing primarily on improving the robustness of models based on sensor data and addressing the imbalance of sensor data. These problems have already been identified as critical in the course of this case study.\n\nIn summary, the two-step approach utilizing the logistic regression demonstrates effective discrimination between events and noise. Furthermore, enhancing the performance of the CNN through techniques such as hyperparameter tuning and data augmentation proves significantly beneficial, surpassing the initial baseline performance. These findings underscore the potential of the proposed model architecture, emphasizing the need for ongoing refinement and validation in real-world environments.\n\nSECTION: Acknowledgements\n\nThis research was supported by the BMWI (ZIM) project focused on developing sensor technology and AI algorithms for fall detection in nursing bed environments. Sensor technologies enhanced by AI will be further developed in the ShapeFuture project aimed at ensuring European ECS value chain sovereignty through advancements in automotive applications. We extend our gratitude to all partners, researchers, and institutions involved for their invaluable contributions and continuous support, significantly advancing our work in fall detection systems.\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.04950v1_content.txt"}, {"title": "A Machine Learning Algorithm for Finite-Horizon Stochastic Control\n  Problems in Economics", "authors": ["Xianhua Peng", "Steven Kou", "Lekang Zhang"], "published_date": "2024-11-13T15:02:54Z", "summary": "We propose a machine learning algorithm for solving finite-horizon stochastic\ncontrol problems based on a deep neural network representation of the optimal\npolicy functions. The algorithm has three features: (1) It can solve\nhigh-dimensional (e.g., over 100 dimensions) and finite-horizon\ntime-inhomogeneous stochastic control problems. (2) It has a monotonicity of\nperformance improvement in each iteration, leading to good convergence\nproperties. (3) It does not rely on the Bellman equation. To demonstrate the\nefficiency of the algorithm, it is applied to solve various finite-horizon\ntime-inhomogeneous problems including recursive utility optimization under a\nstochastic volatility model, a multi-sector stochastic growth, and optimal\ncontrol under a dynamic stochastic integration of climate and economy model\nwith eight-dimensional state vectors and 600 time periods.", "arxiv_id": "2411.08668v2", "html_link": "https://arxiv.org/html/2411.08668v2", "search_term": "ti:\"machine learning\"", "html_content": "SECTION: A Machine Learning Algorithm for Finite-Horizon\nStochastic Control Problems in Economics\u2020\u2020thanks:Xianhua Peng is partially supported by the Natural Science Foundation of Shenzhen (Grant No. JCYJ20190813104607549) and the National Natural Science Foundation of China (Grant No. 72150003). The paper was previously entitled \u201cEM Algorithm and Stochastic Control.\"\n\n-0.2cm-0.2cm\nWe propose a machine learning algorithm for solving finite-horizon stochastic control problems based on a deep neural network representation of the optimal policy functions. The algorithm has three features: (1) It can solve high-dimensional (e.g., over 100 dimensions) and finite-horizon time-inhomogeneous stochastic control problems. (2) It has a monotonicity of performance improvement in each iteration, leading to good convergence properties. (3) It does not rely on the Bellman equation. To demonstrate the efficiency of the algorithm, it is applied to solve various finite-horizon time-inhomogeneous problems including recursive utility optimization under a stochastic volatility model, a multi-sector stochastic growth, and optimal control under a dynamic stochastic integration of climate and economy model with eight-dimensional state vectors and 600 time periods.\n\nKeywords: machine learning, deep learning, stochastic control, multi-sector stochastic growth, climate and economy model, stochastic volatility, climate change\n\nJEL classification: C61, E32, G11, L12\n\nSECTION: 1Introduction\n\nStochastic control problems are widely used in macroeconomics (e.g., the study of stochastic growth and real business cycle), microeconomics (e.g., utility maximization problem), and finance (e.g., portfolio choices and optimal execution).\nIndeed, there is a large literature on stochastic control in economics. For example,Stokey, Lucas and\u00a0Prescott (1989)describe many economic models using stochastic control, including economic growth, resource extraction, principal-agent problems, business investment, asset pricing, etc.Hansen and\u00a0Sargent (2013)give detailed discussions on stochastic control problems in which\nthe Bellman equations can be solved analytically, especially problems with quadratic objective functions and linear transition functions.Ljungqvist and\u00a0Sargent (2018)discuss dynamic programming methods and their applications to a variety of problems in economics.Miao (2020)gives a comprehensive introduction to the analytical and numerical tools for solving stochastic control problems in economics.\n\nDespite the previous efforts, three significant obstacles remain: (i) Many stochastic control problems in economics are finite-horizon time-inhomogeneous problems, which may be more difficult than the related infinite horizon problem, as the optimal control policies at different time periods are different.\n(ii) Due to the curse of dimensionality, it is generally difficult to numerically solve stochastic control problems in high dimensions and for problems with complicated stochastic dynamics.\n(iii) If the utility function in the control problem is not time-separable, then such a problem may not have the Bellman equation.\n\nTo overcome these difficulties, we propose a machine learning algorithm, the Monotonic Monte Carlo Control (MMCC) algorithm,\nto solve high-dimensional, finite time horizon, and time-inhomogeneous stochastic control problems without using dynamic programming principles. The MMCC algorithm can be implemented using a deep neural network representation of the policy functions where the parameters of the neural networks can be learned by stochastic gradient descent approach.\nIn each round of training, the algorithm first generates sample paths of the states and controls by Monte Carlo simulation, and then\nupdates the control policies in each time period sequentially in a backward direction.\nTherefore, The MMCC algorithm has a monotonicity of performance improvement in each iteration step, leading to good convergence properties.\nThe algorithm does not require the Bellman equation, does not require the utility function to be time-separable, and allows general stochastic dynamics of the evolution of states.\nWe demonstrate the effectiveness of the MMCC algorithm by solving various high-dimensional (e.g., over 100-dimensional) stochastic control problems including portfolio selection under a stochastic volatility model, multi-sector stochastic growth, and optimal control under a dynamic stochastic integration of climate and economy model with 8-dimensional state vectors and 600 time periods.\n\nSECTION: 1.1Literature Review\n\nJudd (1998)andMiranda and\u00a0Fackler (2002)provide a comprehensive treatment of traditional numerical methods such as value function iteration for stochastic control problems in economics.\nRecently, some grid point-based and grid point-free machine learning methods have been proposed for solving discrete-time dynamic economic models.\n\nFirst, for infinite-horizon and representative-agent (RA) problems,Lepetyuk, Maliar and\u00a0Maliar (2020)employ supervised neural networks to learn the policy function at grid points defined through unsupervised clustering analysis, thereby using a grid-based method.\nWithin the functional iteration framework, supervised learning is typically applied. For example,Renner and\u00a0Scheidegger (2018)andScheidegger and\u00a0Bilionis (2019)use Gaussian process regression to approximate the value function;Valaitis and\u00a0Villa (2024)use neural networks to approximate the expectation term in the Euler equation. Outside the framework of functional iteration,Maliar, Maliar and\u00a0Winant (2021)provide a unified unsupervised framework encompassing three optimization strategies: maximizing lifetime reward, minimizing equilibrium-condition error (i.e., Euler-equation error), and minimizing Bellman-equation error. In the case of Euler-equation error minimization,Pascal (2024)generalizes the Monte Carlo operator inMaliar, Maliar and\u00a0Winant (2021). Additionally, in the case of continuous-time infinite-horizon RA problems,Duarte, Duarte and\u00a0Silva (2024)use two neural networks to approximate the policy and value function and adopt supervised learning to train the two neural networks to solve the Hamilton-Jacobi-Bellman (HJB) equation.\n\nFor infinite-horizon and heterogeneous-agent (HA) problems, grid-free methods based on simulation are proposed.Maliar, Maliar and\u00a0Winant (2021)demonstrate the effectiveness of their grid-free framework by solving two representative-agent models and one heterogeneous-agent model(Krusell and\u00a0Smith,1998).\nAdditionally,Han, Yang and\u00a0E (2022)employ both supervised and unsupervised learning using two neural networks in a grid-free manner, with unsupervised learning applied to policy function approximation and supervised learning to value function approximation. They essentially aim to maximize lifetime reward and then evaluate their method based on Bellman-equation error.\nMoreover,Hall-Hoffarth (2023)minimizes the error of equilibrium conditions to solve a heterogeneous-agent New Keynesian (HANK) model. In the case of continuous-time infinite-horizon HA problems,Huang (2023)focuses on the probabilistic formulation of economic models and uses deep learning to solve the system of forward-backward stochastic differential equations corresponding to the model. Subsequently,Huang (2024)employs the probabilistic approach to solve a continuous-time version ofKrusell and\u00a0Smith (1998)and an asset pricing model with search-and-bargaining frictions.\n\nFor finite-horizon problems, existing literature focuses on solving overlapping generations models by grid-free methods. For example,Duarte, Fonseca, Goodman and\u00a0Parker (2021)solve a life-cycle model by maximizing lifetime reward;Azinovic, Gaegauf and\u00a0Scheidegger (2022)andAzinovic and\u00a0Jan \u017demli\u010dka (2023)solve an overlapping generations model by minimizing the error of equilibrium conditions. In particular,Azinovic and\u00a0Jan \u017demli\u010dka (2023)introduce a market clear layer to enforce the economic constraints.\n\nThis paper complements the existing literature in four aspects: (i) The proposed MMCC algorithm is a grid point-free (i.e., simulation-based) algorithm, which avoids the curse of dimensionality and enables us to handle high dimensional problems (e.g., 100 dimensions).\nIn this regard, our algorithm is closely related to the problem of American option pricing using simulation (see, e.g.,Longstaff and\u00a0Schwartz (2001),Tsitsiklis and\u00a0Van\u00a0Roy (2001), andBroadie and\u00a0Glasserman (1997,2004),Glasserman (2004, Ch. 8)). (ii) The MMCC algorithm differs from existing grid point-free methods in how network parameters update. In each round of iteration, the MMCC algorithm updates the neural networks of the control policies at different time periods sequentially in a backward manner. In contrast, all neural network parameters are updated simultaneously in existing grid point-free methods.\n(iii) The MMCC algorithm has a monotonicity of performance improvement at each iteration, while many existing algorithms do not have such a property. (iv) The MMCC algorithm does not use the Euler equation or Bellman equation; in contrast, many numerical algorithms and grid point-free algorithms in the literature rely on the Euler equation, Bellman equation, or their approximation.\n(v) The MMCC algorithm can solve problems with time-inseparable utility functions, which may not have Bellman equations.\n\nBesides the economic literature, there is a large body of literature on applied mathematics and applied probability in stochastic control. In particular,Yong and\u00a0Zhou (1999)andFlemming and\u00a0Soner (2006)provide an in-depth discussion on continuous time stochastic control problems and their applications.Kushner and\u00a0Dupuis (2001)give an excellent survey of numerical methods for solving continuous time stochastic control problems by using Markov chains. There have also been many studies on the numerical solutions to continuous time stochastic control problems in mathematical finance.111See, e.g.,Zhang (2004),Bouchard and\u00a0Touzi (2004),Chassagneux (2014),Chassagneux and\u00a0Richou (2016),Crisan, Manolarakis and\u00a0Touzi (2010),Gobet, Lemor and\u00a0Warin (2005),Gobet and\u00a0Turkedjiev (2016,2017),Henry-Labordere, Tan and\u00a0Touzi (2014),Henry-Labord\u00e8re, Oudjane, Tan, Touzi and\u00a0Warin (2019),Kharroubi, Langren\u00e9 and\u00a0Pham (2015),Kharroubi, Langren\u00e9 and\u00a0Pham (2014), andGuo, Zhang and\u00a0Zhuo (2015).Most of these studies focus on particular stochastic processes, e.g., discretized diffusion processes or L\u00e9vy processes, but our MMCC algorithm can be applied to general stochastic processes. Moreover, our method is a simulation-based method, suitable for high-dimensional problems.\n\nApproximate dynamic programming (ADP) has been developed222ADP has also evolved under the name of reinforcement learning in computer science (see, e.g.,Sutton and\u00a0Barto (1998)).for dealing with three sources of curses of dimensionality: high dimensionality of state space, control policy space, and random shock space; see the books byPowell (2011)andBertsekas (2012).\nADP algorithms can be broadly classified into two categories: value iteration and policy iteration.333Many ADP algorithms focus on infinite time horizon problems where the optimal value\nfunction and policy are stationary. In contrast, our MMCC algorithm focuses on finite time horizon problems where neither the optimal value function nor the optimal policy is stationary.Most ADP algorithms are value iteration algorithms,\nwhich approximate the value function by employing the Bellman equation.444Value function iteration is closely related to the duality approach for stochastic dynamic programming; see, e.g.,Brown, Smith and\u00a0Sun (2010),Brown and\u00a0Smith (2014),Brown and\u00a0Haugh (2017), andChen, Ma, Liu and\u00a0Yu (2024).As an alternative, a policy iteration algorithm keeps track of\nthe policy instead of the value function. At each period, a value function is calculated based on a policy estimated previously and then improved within the policy space.\nHowever, the value iteration and policy iteration ADP algorithms may not have a monotonic improvement of the value function at each iteration.\nThe MMCC algorithm is related to but is fundamentally different from the policy iteration ADP algorithms mainly in that: (i) The MMCC algorithm does not use the Bellman equation; (ii) The MMCC algorithm has a monotonic improvement of the value function at each iteration; (iii) The MMCC algorithm can be applied to general control problems in which the objective functions may not be time-separable.\n\nDeep neural networks were first used inHan and\u00a0E (2016)to solve stochastic control problems. They solve finite-horizon problems by approximating the time-dependent controls as feedforward neural networks at each time period; see further extensions on solving partial differential equations and stochastic differential equations inE, Han and\u00a0Jentzen (2017)andBeck, E and\u00a0Jentzen (2019). Additionally,Reppen, Soner and\u00a0Tissot-Daguette (2023)leverage this algorithm to solve high-dimensional problems in American and Bermudan option pricing.Hur\u00e9, Pham, Bachouch and\u00a0Langren\u00e9 (2021)propose solving finite-horizon stochastic control problems based on dynamic programming (i.e., the Bellman equation). They use two neural networks at each time period: one for representing the control policy and the other for representing the value function; see further numerical applications of this algorithm inBachouch, Hur\u00e9, Langren\u00e9 and\u00a0Pham (2021).\nThe MMCC algorithm differs from these papers in two aspects: (i) The MMCC algorithm leads to monotonic improvement of the value function in each iteration, while these algorithms do not.\n(ii) We provide applications of the MMCC algorithm to solve various economic problems such as multi-sector stochastic growth and the social cost of carbon emission problem.\n\nThe literature on Markov decision processes mainly concerns multi-period stochastic control problems with a finite state space or a finite control space. There are also simulation-based algorithms for Markov decision processes; see, e.g., the books byChang, Hu, Fu and\u00a0Marcus (2013)andGosavi (2015)for comprehensive review and discussion. The main differences between these algorithms and our MMCC algorithm are: (i) The MMCC algorithm has monotonicity in each iteration; (ii) The MMCC algorithm does not use the Bellman equation.\n\nThe rest of the paper is organized as follows. The algorithm is proposed in Section2, and in Section3we show that the algorithm improves the objective function monotonically in each iteration and hence has good convergence properties. In Section4, we propose an implementation of the algorithm via deep neural network approximation of the policy functions.\nTo update the policy functions, one can use stochastic gradient descent\nin each iteration. The applications of the MMCC algorithm to solve the recursive utility optimization problem under a stochastic volatility model,\nmulti-sector stochastic growth problem, and the problem of the social cost of carbon are given in Sections5,6, and7respectively.\n\nSECTION: 2The MMCC Algorithm\n\nSECTION: 2.1The Setting of the Problem\n\nWe consider a general finite time horizon stochastic control problem, withperiods fromto.\nLetbe the dimension of the control policy and letbe the dimension of the state.\nAt the-th period the decision maker observes the stateand then chooses a-dimensional control, the sigma field generated by. Hence,\nthe policyis adapted to the information available up to periodand can be represented as a function of. The initial stateis given at period.\nThe stateis determined by,, and random shock by the following state evolution equation\n\nwhereis the state evolution function andis the random vector denoting the random shock in theth period.\nPath dependence (i.e.,may depend on statesfor some) can be accommodated by including auxiliary\nvariables in.\nThe state evolution dynamics in (1) is a general one, which is not restricted to discretized diffusion processes or L\u00e9vy\nprocesses.\n\nThe goal is to find the optimal control policy.\nFor, we assume that the control policy can be represented as\n\nwhereis a function andis the vector of parameters for theth period.\n\nThe policy functionis to be determined and can be represented by a deep neural network with parameter.\nMore precisely, consider a deep neural network withhidden layers, then the functioncan be written a composite function\n\nwhereis an affine function andis a nonlinear activation function, such as the rectified linear unit.\n\nIn the case of deep neural networks, the control problem amounts to finding the affine functions,, and. This is possible by using stochastic gradient descent algorithms such as Adam(Kingma and\u00a0Ba,2015)if the stochastic gradient of the objective function can be found analytically.\n\nAt period 0, the decision maker wishes to choose the optimal controland the sequence of control parameters, which determines the sequence of controls, to maximize the expectation of his or her utility\n\nwhereis a subset ofwith;is the utility function of the decision maker in theth period. It is worth noting that the utility function in the first period can include utility at period.\n\nA control problem more general than the problem (3) is given by\n\nwhereis a general utility function that may not be time-separable as the one in (3). For simplicity of exposition, we will present our MMCC algorithm for the problem (3); however, the MMCC algorithm also applies to the general problem (5); see AppendixCfor details.\n\nFor simplicity of notation, we denoteand denote\nthe objective function of problem (3) by\n\nIn general, the expectation in (7) cannot be evaluated in closed form, and hencedoes not have an analytical form.\n\nSECTION: 2.2Description of the MMCC Algorithm\n\nThe MMCC algorithm is an iterative algorithm for solving (3), involving multiple rounds of the back-to-front updates, that updates the control policy at a given time period by optimizing the objective function with respect to the control policy at that time period only, and with\nthe control policies at all other periods fixed at their most up-to-date status in the iteration of the algorithm.\n\nMore precisely, suppose that after theth iteration, the control policy parameter is. In theth iteration, the MMCC algorithm updatesto beby the updating rule:\n\nwhereis a point-to-set map on(i.e.,maps a point into a subset of) that represents the updating rule.\nAt each time period, the algorithm updatesto beand then moves backward to update; at last, the algorithm updatesto be.\n\nNext, we specify the precise updating rule in (8). In theth iteration, before updating the control parameter at period, the control policy parameter is. Then, at period, the MMCC algorithm updatesto besuch that\n\nwhich can be easily shown to be equivalent to ((a));\nsee AppendixAfor a detailed proof.\nTherefore, suchthat satisfies (2.2) can be obtained by finding a suboptimal (optimal) solution to (12).\n\nSimilarly, at period 0, beforeis updated, the control policy parameter is. Then, the MMCC algorithm updatesto besuch that\n\nAlgorithm1summarizes the MMCC algorithm for solving problem (3).\n\nInitializeand.\n\nIterateuntil some stopping criteria are met. In theth iteration, updatetoby moving backwards fromtoas follows:\n\nMove backward fromto. At each period, updateto besuch that\n\nSuchcan be set as a suboptimal (optimal) solution to the problem\n\nwhere.\n\nAt period, updateto besuch that\n\nSuchcan be set as a suboptimal (optimal) solution to the problem\n\nwhere.\n\nTwo remarks are in order: (i) In the MMCC algorithm when we updatetoor updatetoif no improvement of the objective function can be found, we simply setor set.\n(ii) Because the MMCC algorithm does not use the Bellman equation, it\ncan be applied to general control problems.\n\nThe intuition of the MMCC algorithm is also related to\nthe block coordinate descent (BCD) algorithms, in which the coordinates are divided into blocks and only one block of coordinates is updated at each sub-step of iterations in a cyclic order. However, the details of the two algorithms differ significantly:\n(i) In essence, the MMCC algorithm attempts to update control policies in the control policy spaces (e.g., in the space of deep neural networks) rather than the Euclidean space. Consequently, all the parametersassociated with the control policyare updated simultaneously, rather than block-wise one by one as in BCD. This is similar to the relationship between the classical EM (Expectation-Maximization) algorithm555See, e.g.,Dempster, Laird and\u00a0Rubin (1977),Meng and\u00a0Rubin (1993),\nandLange (2010, Chap. 13), among others.\nFor discussion on the connection between reinforcement learning and the EM algorithm, seeDayan and\u00a0Hinton (1997).and BCD; in fact,Neal and\u00a0Hinton (1999)show that the EM algorithm can be viewed as a generalized BCD searching in the functional space of probability distribution functions rather than in the space of real numbers.\n(ii) BCD methods are used for maximizing deterministic objective functions, but the MMCC algorithm is used for maximizing the expectation of a random utility function (i.e., (7)), which generally cannot be evaluated analytically. That is why we have to employ simulation and stochastic optimization to implement the MMCC algorithm (see Section4).\n(iii) The MMCC algorithm is more flexible in the optimization requirement.\nUnlike the BCD algorithms, the MMCC algorithm does not update the control parameter based on the gradient of the objective function, mainly because in the problems solvable by the MMCC algorithm typically neither the objective function (i.e., (7)) nor the gradient of the objective function can be evaluated analytically.\n(iv) The convergence of the MMCC algorithm holds under weaker conditions. Indeed, the convergence of the BCD algorithms is obtained based on various assumptions on the objective function such as that the objective function is convex or is the sum of a smooth function and a convex separable function or satisfies certain separability and regularity conditions;666See, e.g.,Luo and\u00a0Tseng (1992),Bertsekas (2016, Chap. 3.7),Tseung (2001)andWright (2015).In contrast, the proof of convergence of the MMCC algorithm is similar to that of the EM algorithm, as inWu (1983), which does not need such assumptions on the objective function. See Section3for details.\n(v) Unlike some BCD algorithms, the MMCC algorithm\ndoes not require updating the control parameter to be the exact minimizer of the subproblem ((12) or (14)).\n(vi) The setting of the MMCC algorithm is quite different from BCD. Indeed, the MMCC is implemented via deep neural network representation of policy functions, and the parameters can be updated using stochastic gradient descent.\n\nSECTION: 3Convergence Analysis\n\nThe convergence properties of the MMCC algorithm are similar to those of the EM algorithm.\nFirst, the MMCC algorithm has monotonicity in each iteration. Second, under mild assumptions, the sequence of objective function values generated by the iteration of the MMCC algorithm converges to a stationary value (i.e., objective function value evaluated at a stationary point) or a local maximum value. Third, the sequence of control parameters generated by the iteration of the MMCC algorithm converges to a stationary point or a local maximum point under some additional regularity conditions.\n\nSECTION: 3.1Monotonicity\n\nThe objective functiondefined in (7) monotonically increases in each iteration of the MMCC algorithm, i.e. for each,\n\nSee AppendixB.1.\n\u220e\n\nSECTION: 3.2Convergence of the Value Function to a Stationary Value or a Local Maximum Value\n\nLetbe the sequence of control parameters generated by the MMCC algorithm. In this subsection, we consider the issue of the convergence ofto a stationary value or a local maximum value. We make the following mild assumptions on the objective functiondefined in (7):\n\nThe assumption (17) is needed to define stationary points of.\n\nSuppose the objective functionsatisfies (16) and (17). Then,\n\nBy (15) and (18),converges monotonically to some. However, it is not guaranteed thatis a local maximum\nofon. Indeed, if the objective functionhas several local maxima and stationary points, which type of points the sequence generated by the MMCC algorithm converges to depends on the choice of the starting point; this is also true in the case of the EM algorithm.\n\nA mapfrom points ofto subsets ofis called a point-to-set map on(Wu (1983)). Letbe the point-to-set map of the MMCC algorithm defined in (8). Define\n\n(Convergence of the value function).\nSuppose the objective functionsatisfies conditions (16) and (17). Letbe the sequence generated byin the MMCC algorithm.\n\n(1) Suppose that\n\nThen, all the limit points ofare stationary points (resp. local maxima) of, andconverges monotonically tofor some(resp.).\n\n(2) Suppose that at each iterationin the MMCC algorithm and for all,andare the optimal solutions to the problems\n(12) and (14) respectively. Then, all the limit points ofare stationary points ofandconverges monotonically tofor some.\n\nSee AppendixB.2.\n\u220e\n\nSECTION: 3.3Convergence of the Control Policy to a Stationary Point or a Local Maximum Point\n\nLetandbe defined in (19) and (20) respectively. Under the conditions of Theorem3.2,and all the limit points ofare in(resp.). This does not imply the convergence ofto a point.\nHowever, the following theorem provides sufficient conditions under which.\n\n(Convergence of the control policy).\nLetbe an instance of an MMCC algorithm satisfying the conditions of Theorem3.2, and letbe the limit of.\n\n(1) If(resp.), thenas.\n\n(2) Ifas, then, all the limit points ofare in a connected and compact subset of(resp.).\nIn particular, if(resp.) is discrete, i.e., its only connected components are singletons, thenconverges to somein(resp.).\n\nSee AppendixB.3.\n\u220e\n\nSECTION: 4An Implementation of the MMCC Algorithm\n\nSECTION: 4.1Implementing the MMCC Algorithm by Simulation\n\nIn the MMCC algorithm, we need to find a suboptimal (optimal) solution to the problems\n(12) and (14).\nIn practice, the expectation in the objective functions of these problems may not be evaluated analytically.\nWe propose solving these problems using stochastic gradient descent algorithms such as Adam for deep neural networks. At each iteration of the MMCC algorithm, sample paths are simulated using the current policy, and then a Monte Carlo optimization algorithm is applied to find updates of the control policy at each period to improve the objective function.\n\nMore precisely, at the beginning of theth iteration, we first simulatei.i.d. (independently and identically distributed) sample paths of the statesaccording to the control parameter, which are obtained at the end of theth iteration. We denote these sample paths as\n\nFurthermore, we divide thesesample paths intominibatches, each containingsample paths. Let the sample paths in theth minibatch be denoted aswhere.\n\nIn step 2(a) of Algorithm1, for theth minibatch, the expectation in the objective function of (12) is equal to\n\nwhere(see (4)) and\n\nis a simulated sample path that starts fromand then follows the control parameter. For each minibatch, the MMCC algorithm uses\n\nas a realization ofand applies Adam algorithm to update the parameteronce.\nHence, at each iterationof the MMCC algorithm, in order to update the parameter, we only need to simulatesample paths of the states during periodto period, i.e.,where.\n\nSimilarly, in step 2(b) of Algorithm1, for theth minibatch, the expectation in (14) is equal to\n\nwherearei.i.d. sample paths ofthat are simulated starting fromand then following the control parameters. The MMCC algorithm uses\n\nas a realization ofbased on theth minibatch when solving the problem (14).\n\nAt each iterationand for each time stepin the algorithm, when the Adam algorithm is used for maximizing (4.1) and (4.1), we update the parameterstimes by usingminibatches of sample. Then, the computational cost of solving the problems\n(12) and (14) are respectivelyand. Hence, the computational cost of each iteration of the MMCC algorithm is.\n\nSECTION: 4.2A Numerical Example: A Forward-Backward Stochastic Differential Equation\n\nThere are intrinsic links between recursive utilities and forward-backward stochastic differential equations (FBSDEs); in addition, there are also connections between three mathematical concepts, namely FBSDEs, stochastic control, and semi-linear PDEs. See, e.g.,El\u00a0Karoui, Peng and\u00a0Quenez (1997),Shroder and\u00a0Skiadas (1999),Kharroubi and\u00a0Pham (2015)andPham (2015).\n\nIn this subsection, we shall first\ngive a brief and nontechnical outline of the key connections between the\nthree, which will be used later when we numerically solve portfolio choice\nfor recursive utilities under stochastic volatility. Then we shall give an example where an FBSDE with 100 dimensions can be solved analytically so that we have a benchmark to demonstrate the effectiveness of the MMCC algorithm.\n\nConsider a stochastic control problem\n\nwhere the dynamics ofandare given by\n\nwheremeans transpose andis a-dimensional Brownian\nmotion. Note that this is a non-standard control problem, as the initial\nvalueis also a control variable. It can be shown that under mild\nconditions, the value of the control problem is zero; in fact, this control\nhas a natural interpretation in option pricing,is the initial option\nprice and theis the hedging strategy. Under mild conditions, the solution is given by\n\nThe optimal control () in (26) is\nlinked to a semi-linear PDE\n\nwith the terminal conditionvia\n\nwhere,, andmeans the trace, Hessian\nmatrix, and gradient operator, respectively. The optimal objective function value is 0.\n\nWe discretizeinto. The discretized control problem is\n\nsubject to the dynamics\n\nwhereapproximates;is a neural network approximation to the gradient function, andis the parameter of the network.\n\nNow take a special case in the above definition withwhereis the identity matrix,whereis a-dimensional vector with all entries equal to 0, thenin (28).\nWith a particular choice,\n(29) reduces to\n\nThe stochastic control in (26) now becomes\n\nIn particular, the It\u00f4\u2019s formula implies that the solution of the\nsemi-linear PDE is given by\n\nand the optimalis\n\nseeChassagneux and\u00a0Richou (2016)andE, Han and\u00a0Jentzen (2017)for details.\n\nIn the numerical examples, we choose,,,, and. We discretize the time intervalintoequal subintervals with ending points denoted as. The control policy at timein the discretized problem is. For each, we use a feed-forward neural network with parameterto approximate the control policy function. The neural network has six layers, where the input and output layers have 100 neurons and each of the four hidden layers has 110, 120, 120, and 110 neurons respectively. The nonlinear activation function of each layer is the rectified linear function. The minibatch size used in optimizing the neural network parameters is 64.\n\nFigure1shows the objective function values of the MMCC algorithm defined in (26). The MMCC algorithm converges after 3 iterations. It usessample paths in the simulation anditerations in the Adam algorithm. The initial learning rate of the Adam algorithm is set to be 0.01. It takes about 1.5 hours for each iteration under a Python implementation of the MMCC algorithm based on TensorFlow. The optimal objective value obtained by the MMCC algorithm is 0.0229 (with a standard error of 0.0313). The standard error is equal to the sample standard deviation of thesamples of the objective function in (26) divided by. The theoretical optimal objective function value is 0. Figure2shows the value ofin the control problem (30) of the MMCC algorithm. The optimal valueobtained by the MMCC algorithm is. The theoretically optimal valueis 4.5901.\n\nSECTION: 5Application 1: Recursive Utility with Stochastic Volatility\n\nIn this section, we consider maximizing the recursive utility inEpstein and\u00a0Zin (1989)under\nstochastic volatility models. Consider a market with two assets, a money\nmarket accountwith a fixed risk-free rate,\n\nand a stockwith a stochastic volatility modeled by\n\nwhereandare two independent standard Brownian motions.\nIn the Heston model(Heston,1993),\n\nIn the inverse Heston model(Chacko and\u00a0Viceira,2005),\n\nSupposeis the proportion of the investor\u2019s wealth invested in the stock at time, andis the consumption rate at time. Then, the dynamics of total wealthof the investor are given by\n\nConsider a recursive utilitydefined as\n\nNote that the Epstein-Zin recursive utility is given by, when,,\n\nand when,\n\nChacko and\u00a0Viceira (2005)derives an analytical solution for the casefor infinite horizon () under the inverse Heston model.Kraft, Seifried and\u00a0Steffensen (2013)solves the case\n\nwith finiteunder both the Heston model and the inverse Heston model777Some numerical methods based on a fixed point iteration are discussed inKraft, Seifried and\u00a0Steffensen (2017), though neither the Heston model nor the inverse\nHeston model is covered as their conditions (A1) and (A2) fail to hold..\n\nIn this section, we shall solve the problem completely for arbitrary, under both the Heston and inverse Heston model, using the MMCC algorithm\nby using a connection between an FBSDE and a semi-linear PDE associated with\nthe recursive utility. Indeed, by the HJB equation, it can be shown (see\nequations (4.3)-(4.5) inKraft, Seifried and\u00a0Steffensen (2013)) that the value function is given by\n\nand the optimal control policies are given by\n\nNote that whenand, then. Heresolves a semi-linear PDE\n\nwith the terminal condition, where\n\nBy the connection with FBSDE discussed in Section4.2, we know that we need\nto solve a stochastic control problem\n\nsubject to the dynamics\n\nwhereis a standard one-dimensional Brownian motion starting from 0.\nThen\n\nWe solve Heston\u2019s model forand discretizeinto 120 time periods. In the numerical example, we choose,,,,,,,. The neural network forhas four layers, where the input layer and the output layer have 1 neuron and the two hidden layers have 120 neurons.\nThe nonlinear activation function of each layer is the rectified linear function and the minibatch size in the Adam algorithm is 512.\nWe usesample paths in the simulation anditerations in the Adam algorithm.\nThe MMCC algorithm converged after 8 iterations. It takes 36 minutes for each iteration. Figure3shows the objective function value in the iteration. The optimal objective value obtained by the MMCC algorithm is 2.4145e-6 (with a standard error of 1.4150e-7). The theoretical optimal objective function value is 0.\nFigure4shows the value offorin the iteration. The valueobtained by the MMCC algorithm is. The exact value offoris 5.6150.\n\nSECTION: 6Application 2: Multi-Sector Stochastic Growth\n\nSECTION: 6.1Problem Formulation\n\nStarting fromBrock and\u00a0Mirman (1972), stochastic growth models play a fundamental role in macroeconomics, especially in\nthe models related to real business cycle\n(see e.g.,Kydland and\u00a0Prescott (1982),Long and\u00a0Plosser (1983)).\nIn the literature, this is typically studied assuming an infinite time horizon with one sector economy,\nunder which a stationary solution can be computed. In particular,\na log-linear linear-quadratic (LQ) approximation (see, e.g.,Christiano (1990)) is used to approximate\nthe objective function, which transforms the problem into a well-studied\nlinear-quadratic programming problem.\n\nBy using the MMCC algorithm, we can solve a more general multi-sector stochastic growth model with a finite time horizon, constant relative risk aversion (CRRA) utility, and general capital depreciation.\nThere are significant differences between\nthe finite time horizon and infinite time horizon problem.\n\nConsider a multi-sector model withcommodities. The control variables\nare, the labor time input allocated to the production of commodity, and, the quantity of commodityallocated for the\nproduction of commodity. The objective is\n\nwhereis the consumption of theth commodity andis the\namount of leisure time consumed, subject to the state dynamics\n\nwhere the strictly positive stochastic noiseis an observable time\nhomogeneous Markov process;,, andare given strictly positive constants;.\n\nWhenand,(i.e. all the utility functions are\nlogarithm), the model becomes the model inLong and\u00a0Plosser (1983).\nIn this case, the optimal feedback control policies are given by\n\nwhere\n\nSECTION: 6.2Numerical Results\n\nWe will compare the objective function value obtained by the MMCC algorithm with that obtained by the optimal policy for the infinite horizon problem given in (38) and (39). The control policy at periodiswhich has a dimension of,. The state variable at timeiswith dimension. For each, we use a feed-forward neural network with parameterto approximate the control policy function. The neural network has four layers, where the input layer and the output layer haveandneurons, respectively. The two hidden layers haveneurons. The nonlinear activation function of the first two layers is the rectified linear function, and the activation function of the output layer is a linear combination ofsoftmax functions, which is used to impose the constraints in (35) and (36).\n\nAs in the numerical example ofLong and\u00a0Plosser (1983), we chooseandas defined inLong and\u00a0Plosser (1983). The parameters in the model dynamics are specified as,,,, andis i.i.d. as a standard normal distribution acrossand.\n\nFigure5shows the objective function values of the MMCC algorithm for the case of. The MMCC algorithm converged after 9 iterations. It usessample paths in the simulation anditerations in the Adam algorithm. The initial learning rate of the Adam algorithm is set to be 0.01. The minibatch size used in optimizing the neural network parameters is 64. It takes 4 minutes for each iteration under a Python implementation of the MMCC algorithm based on TensorFlow. The optimal objective value obtained by the MMCC algorithm is(with a standard error of 0.024). The standard error is equal to the sample standard deviation of thesamples of the objective function in (34) divided by. The objective function value obtained by the optimal solution for the infinite horizon problem is.\n\nFigure6shows the objective function values of the MMCC algorithm for the case of.\nThe MMCC algorithm converged after 9 iterations. It takes 18 minutes for each iteration. The optimal objective value obtained by the MMCC algorithm is(with a standard error of 0.045), while the objective value obtained by the optimal solution for the infinite horizon problem is.\n\nFigure7shows the objective function values of the MMCC algorithm for the case of.\nThe MMCC algorithm converged after 3 iterations. It takes 78 minutes for each iteration. The optimal objective value obtained by the MMCC algorithm is(with a standard error of 0.029), while the objective value obtained by the optimal solution for the infinite horizon problem is.\n\nSECTION: 7Application 3: Social Cost of Carbon\n\nCai and\u00a0Lontzek (2019)develop a framework of dynamic stochastic integration of climate and economy (DSICE) and show that the uncertainty about future economic and climate conditions substantially affects the choice of policies for managing the interaction between climate and economy. The DSICE model generalizes the commonly used dynamic integrated model of climate and the economy (DICE) model(Nordhaus,2008)by allowing for economic risks and climate risks.\n\nSECTION: 7.1Problem Formulation\n\nThe DSICE model consists of the climate model and the economic model. The climate model has three parts: the carbon system, the temperature system, and other climate conditions called tipping elements.\n\nThere are two sources for carbon emissions at each time: an industrial source,, related to economic production activities, and\nan exogenous source,, arising from biological processes on the ground. The total emission at timeis The carbon concentration in the world at timeis,\nwhere the three components represent respectively the mass of carbon in the atmosphere, upper levels of the ocean, and lower levels of the ocean.\n\nThe impact of carbon emissions on carbon concentration is represented by the linear dynamical system\n\nwhereis the rate at which carbon diffuses from levelto level, whererepresent the atmosphere, upper ocean, and lower ocean, respectively.\n\nThe temperature system at timeconsists of the temperatures of atmosphereand the ocean, i.e.,The temperature system is governed by the diffusion of heat and evolves according to\n\nwhereare the heat diffusion rates;represents coefficients of heating due to radioactive forcing;is the rate of cooling arising from infrared radiation to space. The\ntotal radioactive forcing at timeis\n\nwhereis the pre-industrial atmospheric carbon concentration,is the radioactive forcing parameter, andis an exogenous process given by\n\nTipping staterepresents some irreversible change in the climate system and is modeled by a discrete state Markov chain whose transition probabilities depend on the vector of climate states. Specifically,stays at its initial value 0 until the tipping event is triggered, and thenenters one of three discrete-state Markov chainswith equal probability.\nThe transition process is represented as\n\nwhereis one serially independent stochastic process.\n\nIn the absence of climate damage, the gross world production is the Cobb-Douglas production function\n\nwhereis the world capital stock at time;is the world population in millions at time;(Nordhaus,2008);is productivity at time, decomposed into the product of a deterministic trendand a stochastic productivity state.\nThe DSICE model specifies a time-dependent, finite-state Markov chain forwith parameter values implying conditional and unconditional moments of consumption processes observed in market data. The Markov transition processes are denoted\n\nwhere, andare two serially independent stochastic processes.\n\nIn the DSICE model, the output\nis affected by the climate through the temperatureand the tipping state. More precisely, the output under the impact of climate is assumed to be\n\nin whichis the impact of tipping stateon productivity.\n\nThe social planner can mitigate emissions by choosing a mitigation factor,. Then, the industrial carbon emission at yearequals\n\nFollowingNordhaus (2008), the cost of mitigation levelis", "text_file": "data\\paper_texts\\2411.08668v2_content.txt"}, {"title": "Using Machine Learning to Discover Parsimonious and\n  Physically-Interpretable Representations of Catchment-Scale Rainfall-Runoff\n  Dynamics", "authors": ["Yuan-Heng Wang", "Hoshin V. Gupta"], "published_date": "2024-12-06T08:30:01Z", "summary": "Despite the excellent real-world predictive performance of modern machine\nlearning (ML) methods, many scientists remain hesitant to discard traditional\nphysical-conceptual (PC) approaches due mainly to their relative\ninterpretability, which contributes to credibility during decision-making. In\nthis context, a currently underexplored aspect of ML is how to develop\nminimally-optimal representations that can facilitate better insight regarding\nsystem functioning. Regardless of how this is achieved, it is arguably true\nthat parsimonious representations better support the advancement of scientific\nunderstanding. Our own view is that ML-based modeling of geoscientific systems\nshould be based in the use of computational units that are fundamentally\ninterpretable by design.\n  This paper continues our exploration of how the strengths of ML can be\nexploited in the service of better understanding via scientific investigation.\nHere, we use the Mass Conserving Perceptron (MCP) as the fundamental\ncomputational unit in a generic network architecture consisting of nodes\narranged in series and parallel to explore several generic and important issues\nrelated to the use of observational data for constructing input-state-output\nmodels of dynamical systems. In the context of lumped catchment modeling, we\nshow that physical interpretability and excellent predictive performance can\nboth be achieved using a relatively parsimonious distributed-state\nmultiple-flow-path network with context-dependent gating and information\nsharing across the nodes, suggesting that MCP-based modeling can play a\nsignificant role in application of ML to geoscientific investigation.", "arxiv_id": "2412.04845v1", "html_link": "https://arxiv.org/html/2412.04845v1", "search_term": "ti:\"machine learning\"", "html_error": "Error processing HTML: HTTP Error 404: Not Found"}, {"title": "Deep Learning and Machine Learning: Advancing Big Data Analytics and\n  Management with Design Patterns", "authors": ["Keyu Chen", "Ziqian Bi", "Tianyang Wang", "Yizhu Wen", "Pohsun Feng", "Qian Niu", "Junyu Liu", "Benji Peng", "Sen Zhang", "Ming Li", "Xuanhe Pan", "Jiawei Xu", "Jinlang Wang", "Ming Liu"], "published_date": "2024-10-04T02:50:58Z", "summary": "This book, Design Patterns in Machine Learning and Deep Learning: Advancing\nBig Data Analytics Management, presents a comprehensive study of essential\ndesign patterns tailored for large-scale machine learning and deep learning\napplications. The book explores the application of classical software\nengineering patterns, Creational, Structural, Behavioral, and Concurrency\nPatterns, to optimize the development, maintenance, and scalability of big data\nanalytics systems. Through practical examples and detailed Python\nimplementations, it bridges the gap between traditional object-oriented design\npatterns and the unique demands of modern data analytics environments. Key\ndesign patterns such as Singleton, Factory, Observer, and Strategy are analyzed\nfor their impact on model management, deployment strategies, and team\ncollaboration, providing invaluable insights into the engineering of efficient,\nreusable, and flexible systems. This volume is an essential resource for\ndevelopers, researchers, and engineers aiming to enhance their technical\nexpertise in both machine learning and software design.", "arxiv_id": "2410.03795v2", "html_link": "https://arxiv.org/html/2410.03795v2", "search_term": "ti:\"machine learning\"", "html_content": "", "text_file": "data\\paper_texts\\2410.03795v2_content.txt"}, {"title": "Multi-class heart disease Detection, Classification, and Prediction\n  using Machine Learning Models", "authors": ["Mahfuzul Haque", "Abu Saleh Musa Miah", "Debashish Gupta", "Md. Maruf Al Hossain Prince", "Tanzina Alam", "Nusrat Sharmin", "Mohammed Sowket Ali", "Jungpil Shin"], "published_date": "2024-12-06T05:55:41Z", "summary": "Heart disease is a leading cause of premature death worldwide, particularly\namong middle-aged and older adults, with men experiencing a higher prevalence.\nAccording to the World Health Organization (WHO), non-communicable diseases,\nincluding heart disease, account for 25\\% (17.9 million) of global deaths, with\nover 43,204 annual fatalities in Bangladesh. However, the development of heart\ndisease detection (HDD) systems tailored to the Bangladeshi population remains\nunderexplored due to the lack of benchmark datasets and reliance on manual or\nlimited-data approaches. This study addresses these challenges by introducing\nnew, ethically sourced HDD dataset, BIG-Dataset and CD dataset which\nincorporates comprehensive data on symptoms, examination techniques, and risk\nfactors. Using advanced machine learning techniques, including Logistic\nRegression and Random Forest, we achieved a remarkable testing accuracy of up\nto 96.6\\% with Random Forest. The proposed AI-driven system integrates these\nmodels and datasets to provide real-time, accurate diagnostics and personalized\nhealthcare recommendations. By leveraging structured datasets and\nstate-of-the-art machine learning algorithms, this research offers an\ninnovative solution for scalable and effective heart disease detection, with\nthe potential to reduce mortality rates and improve clinical outcomes.", "arxiv_id": "2412.04792v1", "html_link": "https://arxiv.org/html/2412.04792v1", "search_term": "ti:\"machine learning\"", "html_content": "SECTION: Multi-class heart disease Detection, Classification, and Prediction using Machine Learning Models\n\nHeart disease is a leading cause of premature death worldwide, particularly among middle-aged and older adults, with men experiencing a higher prevalence. According to the World Health Organization (WHO), non-communicable diseases, including heart disease, account for 25% (17.9 million) of global deaths, with over 43,204 annual fatalities in Bangladesh. However, the development of heart disease detection (HDD) systems tailored to the Bangladeshi population remains underexplored due to the lack of benchmark datasets and reliance on manual or limited-data approaches.\nThis study addresses these challenges by introducing new, ethically sourced HDD dataset, BIG-Dataset and CD dataset which incorporates comprehensive data on symptoms, examination techniques, and risk factors. Using advanced machine learning techniques, including Logistic Regression and Random Forest, we achieved a remarkable testing accuracy of up to 96.6% with Random Forest.\nThe proposed AI-driven system integrates these models and datasets to provide real-time, accurate diagnostics and personalized healthcare recommendations. By leveraging structured datasets and state-of-the-art machine learning algorithms, this research offers an innovative solution for scalable and effective heart disease detection, with the potential to reduce mortality rates and improve clinical outcomes.\n\nSECTION: IIntroduction\n\nHeart disease is a severe and urgent medical problem affecting a significant portion of the population[1]beside the other diseases. The potential severity of delayed diagnosis makes rapid diagnosis essential[2]. Current classification approaches often fall short in practical viability, and there is a need for a comprehensive framework suitable for real-life applications in detecting and classifying this condition[3,4,5,6].\n\nTo address this problem, a revolutionary software solution is being developed to help healthcare practitioners identify early-stage cardiac diseases by leveraging computer technology and machine learning approaches[7]. Early identification of heart disease by evaluating a person\u2019s present cardiac difficulties and risk factors promises to reduce mortality rates[8]. Given the complexity of heart disease detection and classification, existing approaches that rely on pre-processed datasets with few variables must be updated[9]. A comprehensive examination, including general assessments, systemic evaluations of symptoms, risk factors and detailed investigative diagnoses are required due to the disease\u2019s complexity[10].\n\nAs discussed in[11], feature selection techniques are critical in improving heart disease prediction. These techniques help identify the most relevant features from a large set of available data, thus improving the model\u2019s performance and reducing overfitting.\n\nStandard datasets often lack applicability and relevance for real-world implementation, making precise detection and classification challenging[12]. Heart disease identification heavily relies on symptoms, risk factors, and specific investigative data, which are frequently missing from traditional datasets dominated by synthetic data[13]. This work, in contrast, presents a meticulously constructed structured dataset for disease categorization, consisting of over 45,000 accurate patient records from government hospitals, diagnostic centers, and online archives, supplemented with ongoing data augmentation[14]. This extensive dataset is poised to improve model training, resulting in consistently more accurate predictions[15].\n\nThe paper\u2019s groundbreaking paradigm enables individualized disease classification and prediction by integrating cardiac disease diagnosis, categorization, and prognosis[16]. The creation of specialized datasets like HDD (Heart Disease Detection), and CD (Combined Dataset), as well as a sophisticated model using classifiers from Logistic Regression to ensemble model like Random Forest, highlights the potential for precise predictions[17].\n\nIn summary, in this paper, we make the following contributions:\n\nWe newly collected HDD and CD datasets from two different sources and classified them into different classes.\n\nTo compare the effectiveness of our suggested strategies, we carried out an experimental study.\n\nWe performed experimental evaluations to compare our proposed models with existing models.\n\nThis paper, divided into six chapters in which literature review is covered inII, dataset description inIII, proposed methodology inIV, experimental results inV, and conclusion inVI.\n\nSECTION: IILiterature Review\n\nMachine learning models are being used in various domains for their excellence[18,19,20]. There are many researches also have been done to develop various human disease recognition system using various machine learning and deep learning approaches[21,22].\nLakshmanaro et al. used feature selection and ensemble learning techniques to predict heart disease[23]. They have used ensemble learning effectively. They have also successfully used an ensemble model to forecast cardiac disease. Beulah C. et al[24]developed the ensemble classification to increase the accuracy of less robust algorithms that integrate multiple different classifiers. Jaymin Patel compares various classification methods for decision trees in his research[25]. Rani et al. proposed a hybrid system using a Support Vector Machine, Naive Bayes, Logical Regression, Random Forest, and AdaBoost classifiers in the final stage of the system\u2019s development, as Pooja Rani showed[26]. This one\u2019s accuracy of 86.6% outperforms other algorithms for predicting heart disease discovered in the scientific literature.\nLerina Aversano et al. employed machine learning for the early prediction of heart disease, showing promising outcomes in early diagnosis[27]. Dimitris Bertsimas et al. demonstrated the effectiveness of machine learning for real-time heart disease prediction[28].\n\nAdditionally, Richard Raymond Bomford and Adair Stuart Masons clinical methods have laid the foundation for many modern diagnostic techniques[29]. A. H. Chen et al. developed HIPS Heart Disease Prediction System, an early attempt at integrating data mining and heart disease prediction[30]. Abderrahmane Ed-Daoudy and Khalil Maalmi applied real-time machine learning techniques to big data for early detection of heart disease[31]. Aditi Gavhane et al. predicted heart disease using various machine learning algorithms, showcasing the diverse applicability of these techniques[32].\n\nFurthermore, M. Akhil Jabbar et al. explored lazy associative classification for heart disease prediction, providing insights into alternative classification methods[33]. Pahulpreet Singh Kohli and Shriya Arora applied machine learning in disease prediction, contributing to the growing field of AI in healthcare[34]. Jian Ping Li et al. developed a heart disease identification method using machine learning classification in e-healthcare, reflecting the integration of AI in healthcare systems[35]. Senthilkumar Mohan et al. effectively used hybrid machine learning techniques for heart disease prediction, highlighting the benefits of combining multiple algorithms[36].\nLastly, the study by Anjan Nikhil Repaka, Sai Deepak Ravikanti, and Ramya G Franklin focused on using naive Bayesian methods for heart disease prediction, adding to the array of predictive models available[37]. Priyanka S. Single et al. reviewed methodologies and techniques for heart disease classification and prediction, providing a comprehensive overview of existing approaches[38]. Vijeta Sharma et al. examined machine learning techniques for heart disease prediction, reinforcing the importance of continuous innovation in this field[39]. Archana Singh and Rakesh Kumar compared various machine learning algorithms for heart disease prediction, offering valuable insights into their comparative performance[40]. Jagdeep Singh et al. explored associative classification for heart disease prediction, presenting another dimension of classification techniques[41]. J. Thomas and R Theresa Prince utilized data mining techniques for heart disease prediction, contributing to the expanding literature on data-driven healthcare solutions[42].\n\nSECTION: IIIDataset\n\nThe dataset was collected from government hospitals, diagnostic centers, and validated online repositories. Data from 45,779 participants (aged 3-17) were used, with 5,218 children diagnosed with ADHD. The data are binary (1 for Yes, 0 for No) and identifies symptoms like chest pain, shortness of breath, and risk factors. The dataset encompasses both affected and non-affected individuals. Ethical clearance for data collection was obtained from the Ethical Review Board of the respective institutions.\n\nThe dataset can be accessed from Kaggle:Dataset Link:Heart Disease Dataset.\n\nSECTION: III-AHDD Dataset\n\nHDD stands for Heart Disease Detection, we have used heart disease symptoms, examine techniques, risk factors, investigation, and diagnosis information mentioned in[43]and[44]. Nineteen symptoms frequently occur across twenty-seven heart diseases. Risk factors are also present. This dataset exclusively represents patterns associated with heart disease patients. It can identify all twenty-seven heart disease categories. Additionally, the dataset allows for binary classification of heart disease for atypical samples.\n\nSECTION: III-BBIG-Dataset\n\nIn 2022, over 1700 people datasets with 15 attributes were collected from hospitals and 1200 from online sources. Non-affected users\u2019 data is stored in the BIG-D dataset. The sample of this dataset is shown in TableII\n\nSECTION: III-CCD Dataset\n\nCD stands for the combined dataset. We named this dataset according to its action. This dataset is developed from combining the HDD and BIG dataset. In this dataset, data for affected people and non-affected people are combined. We have developed this dataset to train advanced models for better accuracy and to make them usable in advanced technology. This dataset consisted of two labels, including a normal person and a person with heart disease. According to the dataset HDD, which was created with 27 diseases, there are 19 symptoms and four risk factors. The 19 symptoms have two levels: some are major symptoms, and some are minor symptoms. The correlation matrix of the features are shown in Fig.2.\n\nSECTION: III-DDataset Limitations\n\nWhile comprehensive, the dataset exhibits certain limitations, such as a bias toward specific population groups, which could potentially impact generalization to other demographics. Future studies should consider incorporating more diverse data.\n\nSECTION: IVProposed Methodology\n\nIn this study, we introduced a new heart disease dataset and a model for classification of heart disease. Unlike previous research using existing datasets, our framework is unique, utilizing real-life data from government hospitals, diagnostic clinics, and online sources. The workflow of our proposed methodology is shown in Fig.3\n\nSECTION: IV-AFeature Engineering\n\nFeatures were selected based on clinical significance, including 19 symptoms and 4 risk factors based on[43,44]. Logistic Regression was chosen for its interpretability, while Random Forest was employed for its ability to handle complex, non-linear relationships. The ensemble approach of random forest mitigates overfitting and ensures robust predictions.\n\nSECTION: IV-BClassification\n\nIn the study, we employed logistic regression to classify the disease and normal people, as well as the severity classification of the patient.\nLogistic regression is a supervised classification algorithm commonly used for predictive analysis based on probability. This technique evaluates the relationship between the dependent variable and independent variables or risk factors by calculating probabilities using the logistic function, also known as the sigmoid function[45].\nThe logistic function is expressed as:\n\nWhererepresents the probability that the dependent variable is 1 (e.g., the presence of heart disease),is the intercept, andare the coefficients of the independent variables.\n\nThe logistic function constrains the output between 0 and 1, which is essential for binary classification problems.\nLogistic regression is particularly suitable for predicting binary outcomes (e.g., diseased vs. not diseased). The primary applications of logistic regression include prediction and estimating the probability of success. In this context, logistic regression reveals a statistically significant link between the risk factors and the likelihood of developing heart disease.\n\nThe cost function used in logistic regression is derived from the sigmoid function and is expressed as:\n\nwhereis the cost function,is the number of training examples,is the actual output for the-th example, andis the predicted probability for the-th example.\n\nIn the study, we also employed Random Forest to classify the disease and normal people, as well as the severity classification of the patient.\nThis algorithm designed to mitigate the overfitting issue commonly associated with single decision trees. Single decision trees often learn from only one pathway of decisions, resulting in poor generalisation to new datasets[46,47,22]. Random Forest addresses this by constructing multiple clusters of decision trees with controlled variation, enhancing overall prediction accuracy. The Random Forest algorithm operates by merging random selections of input features and using Breimans bagging (Bootstrap Aggregating) sampling techniques. The bagging algorithm improves robustness by drawing observations with replacements from the training data and randomly splitting nodes to achieve the best split within a random subset of features. The decision function in Random Forest can be expressed as:\n\nwhereis the predicted class,is the prediction from the-th decision tree, andis the total number of trees in the forest.\nIn our case, we optimized the hyperparameters of the Random Forest algorithm using a randomized search algorithm with cross-validation. This process involved tuning parameters such as the number of trees, maximum depth, and the number of features considered for splitting at each node to achieve the best performance on the validation set. The Random Forest can also help reduce the noise and overfitting of the system[48].\n\nSECTION: VExperimental Results\n\nWe used Random Forest and Logistic Regression on the HDD, BIG dataset and CD datasets.\n\nSECTION: V-APerformance With HDD dataset and BigD Dataset\n\nTableIIIdemonstrates the performance accuracy of the HDD dataset using different models. Random Forest achieved 92.77% training accuracy and 91.90% testing accuracy, while Logistic Regression achieved 95.00% training accuracy and 93.87% testing accuracy. Besides, the Random forest for the BigD dataset shows 90.80% accuracy.\n\nSECTION: V-BPerformance With CD Dataset\n\nSimilarly, TableIVdemonstrates the performance accuracy of the CD dataset, with Random Forest achieving 99.21% training accuracy and 96.66% testing accuracy, and Logistic Regression achieving 98.66% training accuracy and 95.67% testing accuracy.\n\nSECTION: V-CComparative Analysis\n\nThe proposed model achieves a testing accuracy of 96.66%, outperforming existing studies. TableVcompares our results with prior research:\n\nSECTION: V-DDiscussion\n\nOur main contribution to the study is creating real-world heart disease datasets, namely HDD, BIG, and CD.\n\nThe HDD dataset, designed for recognizing patterns in 27 heart disease categories, achieved high accuracy, with Logistic Regression at 93.87% and Random Forest at 91.90%. The BIG dataset, including unaffected individuals, supports binary classification. Random Forest achieved 90.80% accuracy, slightly lower than HDD, likely due to added complexity from non-affected samples. This highlights BIG\u2019s value in real-world scenarios with healthy populations.\n\nThe CD dataset, combining affected and unaffected cases, achieved the highest performance, with Random Forest at 96.66% and Logistic Regression at 95.67%. Its balanced representation enables Random Forest to capture intricate interactions, making it ideal for both binary and multi-class tasks.\n\nAcross all datasets, Logistic Regression performed well with structured data, while Random Forest excelled in handling complexity, emphasizing the importance of matching models to dataset characteristics.The performance analysis of classification model are shown in Fig.4.\n\nSECTION: VIConclusion\n\nThis research assessed machine learning algorithms for cardiac disease identification utilizing the HDD, BIG, and CD datasets.\n\nThese findings underscore the importance of dataset diversity in enhancing model efficacy. The HDD dataset is superior for disease-specific classification, while the BIG and CD datasets facilitate wider real-world applicability. Random Forest exhibited versatility in handling complex datasets, rendering it appropriate for clinical applications.\n\nFuture research may augment these datasets with supplementary features and investigate explainability frameworks to improve diagnostic precision and reliability. This study advances the creation of scalable, dependable instruments for cardiac disease detection.\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.04792v1_content.txt"}, {"title": "Machine learning algorithms to predict the risk of rupture of\n  intracranial aneurysms: a systematic review", "authors": ["Karan Daga", "Siddharth Agarwal", "Zaeem Moti", "Matthew BK Lee", "Munaib Din", "David Wood", "Marc Modat", "Thomas C Booth"], "published_date": "2024-12-06T03:25:01Z", "summary": "Purpose: Subarachnoid haemorrhage is a potentially fatal consequence of\nintracranial aneurysm rupture, however, it is difficult to predict if aneurysms\nwill rupture. Prophylactic treatment of an intracranial aneurysm also involves\nrisk, hence identifying rupture-prone aneurysms is of substantial clinical\nimportance. This systematic review aims to evaluate the performance of machine\nlearning algorithms for predicting intracranial aneurysm rupture risk.\n  Methods: MEDLINE, Embase, Cochrane Library and Web of Science were searched\nuntil December 2023. Studies incorporating any machine learning algorithm to\npredict the risk of rupture of an intracranial aneurysm were included. Risk of\nbias was assessed using the Prediction Model Risk of Bias Assessment Tool\n(PROBAST). PROSPERO registration: CRD42023452509. Results: Out of 10,307\nrecords screened, 20 studies met the eligibility criteria for this review\nincorporating a total of 20,286 aneurysm cases. The machine learning models\ngave a 0.66-0.90 range for performance accuracy. The models were compared to\ncurrent clinical standards in six studies and gave mixed results. Most studies\nposed high or unclear risks of bias and concerns for applicability, limiting\nthe inferences that can be drawn from them. There was insufficient homogenous\ndata for a meta-analysis.\n  Conclusions: Machine learning can be applied to predict the risk of rupture\nfor intracranial aneurysms. However, the evidence does not comprehensively\ndemonstrate superiority to existing practice, limiting its role as a clinical\nadjunct. Further prospective multicentre studies of recent machine learning\ntools are needed to prove clinical validation before they are implemented in\nthe clinic.", "arxiv_id": "2412.04749v1", "html_link": "https://arxiv.org/html/2412.04749v1", "search_term": "ti:\"machine learning\"", "html_error": "Error processing HTML: HTTP Error 404: Not Found"}, {"title": "Driving Thermoelectric Optimization in AgSbTe2 via Design of Experiments\n  and Machine Learning", "authors": ["Jan-Hendrik P\u00f6hls", "Chun-Wan Timothy Lo", "Marissa MacIver", "Yu-Chih Tseng", "Yurij Mozharivskyj"], "published_date": "2024-12-06T01:21:47Z", "summary": "Systemic optimization of thermoelectric materials is arduous due to their\nconflicting electrical and thermal properties. A strategy based on Design of\nExperiments and machine learning is developed to optimize the thermoelectric\nefficiency of AgSb1+xTe2+y, an established thermoelectric. From eight\nexperiments, high thermoelectric performance in AgSb1.021Te2.04 is revealed\nwith a peak and average thermoelectric figure of merit of 1.61 +/- 0.24 at 600\nK and 1.18 +/- 0.18 (300 - 623 K), respectively, which is over 30% higher than\nthe best literature values for AgSb1+xTe2+y. Ag-deficiency and suppression of\nsecondary phases in AgSb1.021Te2.04 improves the electrical properties and\nreduces the thermal conductivity (~0.4 W m-1 K-1). Our strategy is implemented\ninto an open-source graphical user interface, and it can be used to optimize\nthe methodologies, properties, and processes across different scientific\nfields.", "arxiv_id": "2412.04699v1", "html_link": "https://arxiv.org/html/2412.04699v1", "search_term": "ti:\"machine learning\"", "html_error": "Error processing HTML: HTTP Error 404: Not Found"}, {"title": "Classification of Fermi-LAT unassociated sources with machine learning\n  in the presence of dataset shifts", "authors": ["Dmitry V. Malyshev"], "published_date": "2024-12-06T00:00:00Z", "summary": "About one third of Fermi Large Area Telescope (LAT) sources are unassociated.\nWe perform multi-class classification of Fermi-LAT sources using machine\nlearning with the goal of probabilistic classification of the unassociated\nsources. A particular attention is paid to the fact that the distributions of\nassociated and unassociated sources are different as functions of source\nparameters. In this work, we address this problem in the framework of dataset\nshifts in machine learning.", "arxiv_id": "2412.04675v1", "html_link": "https://arxiv.org/html/2412.04675v1", "search_term": "ti:\"machine learning\"", "html_content": "SECTION: Classification ofFermi-LAT unassociated sources with machine learning in the presence of dataset shifts\n\nAbout one third ofFermiLarge Area Telescope (LAT) sources are unassociated. We perform multi-class classification ofFermi-LAT sources using machine learning with the goal of probabilistic classification of the unassociated sources. A particular attention is paid to the fact that the distributions of associated and unassociated sources are different as functions of source parameters. In this work, we address this problem in the framework of dataset shifts in machine learning.\n\nSECTION: 1Dataset shifts\n\nThe basic assumption of classification with machine learning is that the joint distributions of input featuresand output features, i.e., classes, are the same for the training and target datasets:\n\nIn the presence of a dataset shift the training and target distributions are different.\nThe joint distribution can be written as a product of conditional probability times a prior distribution in two different ways:\n\nCorrespondingly, there are two special cases of the dataset shift(MorenoTorres2012AUV,):\n\nCovariate shift:, but;\n\nPrior shift:, but.\n\nIn this paper, we use the 4FGL-DR4 catalog, version \u201cgll_psc_v34.fit\u201d2022ApJS..260\u202653A;2023arXiv230712546B.\nAn example of a dataset shift is shown in the top two plots in Fig.1.\nThe distribution of sources in the training dataset (associated sources) on the top left plot is different from the distribution of\ntarget dataset on the top right plot (the unassociated sources).\nThis difference can be due either to covariate shift or to prior shift.\nIn this note we perform classification of unassociated sources using both covariate and prior shift assumptions\nand discuss the relation between the two approaches.\n\nSECTION: 2Data selection\n\nIn the analysis we use a similar definitions of classes as in Ref.2024arXiv240104565M.\nThe physical classes are grouped into four sets dominated by FSRQs, BL Lacs, pulsars, and millisecond pulsars:\n\u201cfsrq+\u201d: fsrq, nlsy1, css;\n\u201cbll+\u201d: bll, sey, sbg, agn, ssrq, rdg;\n\u201cpsr+\u201d: snr, hmb, nov, pwn, psr, gc;\n\u201cmsp+\u201d: msp, lmb, glc, gal, sfr, bin\n(the definitions of class acronyms can be fround in Ref.2022ApJS..260\u202653A).\nWe note that in these classes we do not take into account bcu and spp sources.\nWe use seven input features in the covariate shift case2024arXiv240104565M:\nlog10(Energy_Flux100), log10(Unc_Energy_Flux100), log10(Signif_Avg), LP_index1GeV, LP_beta, LP_SigCurv, log10(Variability_Index), where LP_index1GeV is the index of the log-parabola spectral fit at 1 GeV, while the other features are transformations of the features in the 4FGL-DR4 catalog.\nThe classification is performed with the random forest algorithm implemented in\nscikit-learn(scikit-learn,).\nFor the prior shift model we use the following three features: log10(Energy_Flux100), LP_beta, log10(LP_EPeak), where\nLP_EPeak is the peak energy of the spectral energy distribution modeled with the log-parabola function.\nWe consider only sources with 10 MeVLP_EPeak1 TeV.\n\nSECTION: 3Prior shift model\n\nIn the prior shift model, we assume that the distribution of sources in different classes as functions of input features is the same for\nassociated and unassociated sources,\nwhile the difference in the distributions of associated and unassociated sources comes from the differences in class prevalences.\nThe overall probability distribution function of unassociated sources is represented as:\n\nwhereis the frequency (prevalence) of classamong unassociated sources.\nThe unknown coefficientsare determined by maximizing the log-likelihood\n\nOne of the caveats of the classical prior shift model is that the distributions may not be the same\nfor associated and unassociated sources in the different classes.\nFor example, the distribution of extragalactic associated sources in Fig.1bottom left has a large density atfor\nintermediate and large fluxes, while for small fluxes the distribution inis rather wide.\nThe top right plot of Fig.1shows that in the high LP_EPeak regime (largely extragalactic from the bottom left plot), nearly all unassociated sources are at low fluxes.\nIn order to account for the possible flux dependence of the distributions of sources, we introduce flux-dependent prior shifts, parameterized by sigmoid functions plus a constant:\n\nwherelog10(Energy_Flux100). This model has four (instead of one) free parameters for each of the classes.\nThe flux-dependent prior shifts are shown in Fig.2top left panel.\nAs expected, the contribution of extragalactic sources (fsrq+ and bll+ classes) is suppressed at large fluxes.\nHowever, the model does not fit the data well, as one can see, e.g., in the example of the log10(LP_EPeak) distribution on the top right panel of\nFig.2.\n\nOne of the advantages of the prior shift model, is that it allows one to introduce new classes.\nIn addition to the flux-dependent prior shifts, we introduce a new population modeled as a 3-dimensional Gaussian distribution in the 3 input features of the prior shift model.\nThe flux-dependent prior shifts are shown on the bottom left panel of Fig.2.\nThe corresponding model and the distribution of sources as a function of log10(LP_EPeak) are shown on the bottom right panel of Fig.2.\nNow the model fits the data relatively well at the expense of a new Gaussian component parameterized with 7 parameters.\n\nSECTION: 4Prior vs covariate shift models\n\nAlthough covariate shift models cannot accommodate a new class, one can indirectly see the presence of a possible new population of sources.\nIn the region of parameter space, where the new population may be present, the members of the new population would be proportionally distributed among the known classes.\nIn Fig.3on the left, we show a comparison of the predictions for the distribution of the classes in the prior shift and covariate shift models as a function of log10(LP_EPeak).\nThe models agree relatively well outside of the grey vertical lines at 200 MeV and 1 GeV, which approximately show the \u201cboundaries\u201d of the Gaussian component.\nHowever, inside the vertical grey dotted lines the covariate shift model systematically predicts more sources for the four classes compared to the prior shift model. This effect can be due to a new population of sources modeled as a Gaussian in the prior shift case.\nIn order to qualitatively asses the contribution of the new component to the four classes in the covariate shift case, we scale the distributions of associated sources to fit the distributions of unassociated sources outside of the grey dotted lines and subtract the scaled distributions of associated sources from the unassociated ones.\nThe corresponding differences are shown as dashed lines on the right panel of Fig.3.\nThe sum of the differences is shown as the grey solid line.\nIt has a similar distribution as the distribution of sources in the Gaussian component shown by the purple dash-dotted line.\nThis similarity shows that there is possibly a new population of sources modeled with a Gaussian distribution in the prior shift case. Given the relatively low Epeak values, this population should have a big overlap with the population of soft Galactic unassociated sources introduced in Ref.2022ApJS..260\u202653A.\n\nAcknowledgments.The author would like to thank Jean Ballet, Aakash Bhat, Toby Burnett, and Benoit Lott for valuable discussions and comments and\nto acknowledge support by the DFG grant MA 8279/3-1.\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.04675v1_content.txt"}, {"title": "Towards Performance-Aware Allocation for Accelerated Machine Learning on\n  GPU-SSD Systems", "authors": ["Ayush Gundawar", "Euijun Chung", "Hyesoon Kim"], "published_date": "2024-12-05T19:26:30Z", "summary": "The exponential growth of data-intensive machine learning workloads has\nexposed significant limitations in conventional GPU-accelerated systems,\nespecially when processing datasets exceeding GPU DRAM capacity. We propose\nMQMS, an augmented in-storage GPU architecture and simulator that is aware of\ninternal SSD states and operations, enabling intelligent scheduling and address\nallocation to overcome performance bottlenecks caused by CPU-mediated data\naccess patterns. MQMS introduces dynamic address allocation to maximize\ninternal parallelism and fine-grained address mapping to efficiently handle\nsmall I/O requests without incurring read-modify-write overheads. Through\nextensive evaluations on workloads ranging from large language model inference\nto classical machine learning algorithms, MQMS demonstrates orders-of-magnitude\nimprovements in I/O request throughput, device response time, and simulation\nend time compared to existing simulators.", "arxiv_id": "2412.04569v1", "html_link": "https://arxiv.org/html/2412.04569v1", "search_term": "ti:\"machine learning\"", "html_content": "SECTION: Towards Performance-Aware Allocation forAccelerated Machine Learning on GPU-SSD Systems\n\nThe exponential growth of data-intensive machine learning workloads has exposed significant limitations in conventional GPU-accelerated systems, especially when processing datasets exceeding GPU DRAM capacity. We propose MQMS, an augmented in-storage GPU architecture and simulator that is aware of internal SSD states and operations, enabling intelligent scheduling and address allocation to overcome performance bottlenecks caused by CPU-mediated data access patterns. MQMS introduces dynamic address allocation to maximize internal parallelism and fine-grained address mapping to efficiently handle small I/O requests without incurring read-modify-write overheads. Through extensive evaluations on workloads ranging from large language model inference to classical machine learning algorithms, MQMS demonstrates orders-of-magnitude improvements in I/O request throughput, device response time, and simulation end time compared to existing simulators.\n\nSECTION: 1Introduction\n\nThe exponential growth in machine learning workload complexity and dataset sizes has exposed critical limitations in conventional GPU-accelerated systems, particularly for data-intensive applications such as graph neural networks (GNNs) and large-scale recommender systems. Current GPU-accelerated systems rely heavily on CPU-mediated data access patterns, creating a significant performance bottleneck due to the I/O and synchronization overhead stemming from data propagation over PCI-e links between components in the interaction model[10]. This architectural limitation becomes particularly pronounced when processing datasets exceeding GPU DRAM capacity, with data propagation overhead accounting for more than 80% of total processing latency in GNN applications[10]. Current storage interfaces struggle to accommodate these unpredictable access patterns and dense I/O request bursts efficiently.\n\nDirect GPU-SSD systems have emerged as a promising architectural solution, offering the potential to bypass CPU mediation in storage operations and leverage the full parallel processing capabilities of modern SSDs. However, current implementations face several critical challenges that prevent optimal performance. Enterprise SSDs employ sophisticated internal architectures with multiple channels, complex garbage collection mechanisms, and dynamic wear-leveling algorithms[6]. Yet, these components are typically treated as a black box by GPU interfaces in GPU-SSD systems. This abstraction leads to missed optimization opportunities, manifesting in reduced bandwidth utilization, poor data locality, and suboptimal channel traffic distribution[7]. Additionally, the smaller access granularity required by many machine learning workloads often conflicts with the underlying SSD architecture\u2019s optimal access patterns[10].\n\nTo address these fundamental limitations, we propose a novel in-storage GPU architecture that maintains awareness of internal SSD states and operations. Our approach introduces mechanisms for intelligent scheduling and address allocation while maintaining full compatibility with host CPU I/O operations. We evaluate our proposed architecture through experimentation using an augmented version of the MQSim-MacSim simulator, called MQMS, which combines SSD emulation capabilities with cycle-accurate GPU simulation. Through MQMS, we assess system performance across multiple metrics\u2014including I/O per second (IOPS) and device response time\u2014on various workloads ranging from LLMs to classical learning algorithms.\n\nSECTION: 2Enterprise-Grade SSDs\n\nEnterprise SSDs must satisfy stringent I/O requirements for performance-driven data-intensive applications, such as big data analytics services. These workloads demand high IOPS for parallel request processing, high device endurance to handle intensive write operations, and sustainable QoS to maintain consistent application performance. Meeting these requirements necessitates additional hardware resources and advanced internal functionalities compared to client SSDs[7].\n\nPerformance analysis of current SSD simulator submodules reveals significant limitations in modeling enterprise-grade performance characteristics. Testing with 4KB random request workloads demonstrates that client SSD simulators, even when configured with enterprise-class parameters such as channel counts, die size, and page size, achieve only a fraction of real enterprise SSD performance[7]. While the Samsung PM9A3 enterprise SSD datasheet shows near-linear IOPS scaling with I/O queue size until saturation, simulators like SimpleSSD and MQSim scale at an asymptotic, non-linear rate, performing an order of magnitude worse[12].\n\nWhile current simulators allow configuring physical specifications like die/block/page counts and read/write latencies, their IOPS is limited by the lack of resource management used in enterprise storage systems[6]. To address this, we implement two key optimizations inspired by enterprise SSD design in MQMS, leveraging core primitives like NVMe interface support inherited from the SSD simulator submodule, MQSim[14].\n\nSECTION: 2.1Dynamic Address Allocation\n\nEnterprise SSDs maximize internal parallelism through numerous units such as channels, dies, and planes[13]. However, existing SSD simulators fail to fully exploit these units, particularly plane-level parallelism, resulting in reduced I/O request throughput potential[15].\n\nThe primary cause of this inefficiency is thestatic behaviorof the address allocation strategy employed by these simulators. Physical addresses for write I/O data are determined based on logical addresses using fixed rules. This approach limits opportunities to utilize underlying planes in parallel due to constraints associated with plane-level parallelism[7]. For instance, when physical addresses are statically assigned, multiple write I/O requests that could otherwise be processed in parallel may be forced to wait because they are mapped to the same plane[15]. This results in I/O requests stalling while available planes remain idle, leading to underutilization of the SSD\u2019s potential parallelism.\n\nTo address this issue, modern enterprise SSDs employdynamicaddress allocation, allowing write I/O data to be stored anywhere by dynamically assigning physical addresses. This strategy enables the allocation of the same physical address\u2014across different planes\u2014to multiple arbitrary write I/O requests, thereby creating opportunities to exploit plane-level parallelism[13]. Figure1depicts an example where a vector in memory has its four contiguous elements dynamically allocated to the same address across four flash pages in parallel. By dynamically distributing write requests across all available planes, the SSD controller scales I/O request throughput as, whereis the write request count andis the plane count.\n\nIt is important to consider that dynamic address allocation trades off plane-level data locality for higher request throughput. By allocating data across multiple planes, workloads that rely on plane-level locality may experience performance penalties. This trade-off is negligible for workloads with high data isolation. In the context of in-storage GPU systems, where workloads tend to be highly concurrent, maximizing SSD request throughput is generally a larger factor in minimizing workload latency[15].\n\nWe have implemented dynamic address allocation in MQMS, aiming to maximize internal parallelism for data-intensive workloads. Our experiments indicate that this approach outperforms restricted dynamic allocation methods in terms of device IOPS and response times for tested enterprise workloads.\n\nSECTION: 2.2Fine-Grained Address Mapping\n\nModern enterprise SSDs increasingly handle large numbers of small I/O requests, a trend exacerbated by the growth in flash page sizes (up to 16 KB) aimed at improving disk capacity and bandwidth[6]. However, current SSD simulators inefficiently process small I/O requests, generating redundant read and write operations that result in significantly poorer performance metrics relative to real devices.\n\nThis inefficiency stems from the coarse-grained address mapping schemes typically used in these simulators, where logical and physical addresses are mapped at the flash page level[14]. Under page-level mapping, servicing a small write request involves three steps: reading the entire page containing the data to be updated, modifying the specific portion, and then writing back the entire page\u2014a process of transactions known as read-modify-write (RMW) operations. Figure2illustrates a scenario involving coarse-grained address mapping, where servicing four small write requests (,,, and) requires reading two flash pages, modifying their contents, writing the modified pages back, and marking the original pages as invalid.\n\nTo address this issue, modern enterprise SSDs implement fine-grained address mapping, where logical and physical addresses are mapped at sub-page sizes, such as sectors or sub-sectors[6]. This method allows small write requests to be serviced immediately by writing only the new data and invalidating the corresponding old data, without reading or writing the unaffected portions of the page. Figure3illustrates an example of fine-grained address mapping, where servicing four small write requests (,,, and) involves writing only a single flash page and invalidating the corresponding original data.\n\nWhile fine-grained mapping increases the size of the mapping table and the frequency of table accesses, especially for large I/O requests, these overheads are mitigated in modern enterprise SSDs equipped with large internal DRAM caches that store the entire mapping table. The additional memory and computational overhead are generally negligible compared to the performance benefits gained from reduced RMW operations for small I/O requests[7].\n\nSECTION: 3Workload Evaluation\n\nSECTION: 3.1Kernel Sampling\n\nTo evaluate MQMS against the baseline MQSim-MacSim simulator, we require representative traces from large-scale machine learning workloads. Modern machine learning models exhibit repeated kernel patterns derived from their block structure. For instance, ResNet-50 contains 48 identical convolutional layers, while transformer models iterate through self-attention and fully connected layer blocks[4]. Analysis reveals that these kernels exhibit independently and identically distributed execution times with negligible inter-kernel cache dependency, as evidenced by consistent L1 and L2 hit rates under cache flush conditions[2]. This characteristic enables statistical kernel sampling for trace generation.\n\nWe leverage Allegro to generate workload traces for LLM inference, clustering GPU kernels by name, grid size, and block size before applying 1-D k-means clustering () recursively[2]. The clustering process uses the central limit theorem to determine optimal group sizes, splitting groups until homogeneous execution time distributions within each cluster are achieved. For any kernel groupwithkernels, meanand variance, the sampled execution timesconverge to a normal distribution with meanand varianceasapproaches infinity.\n\nPer-group sampling useskernels, whereis derived from execution time variance relative to the mean to maintain error bounds. With kernel groupshavingkernels and sampled mean execution timesfollowing normal distributions, the total predicted execution timemaintains specified error boundswith 95% confidence[2].\n\nWe have implemented this sampling algorithm into MQMS\u2019s trace generation tool for GPU simulation, which creates SASS-Assembly traces for simulating NVIDIA GPU workloads. The algorithm dramatically reduces trace sizes while maintaining the essential workload characteristics needed for accurate comparative analysis. Implementation-wise, Allegro requires just a single upfront cost\u2014while the initial hardware profiling and sampling do add some processing time, these steps occur only once during trace generation. After this preprocessing phase, the actual simulation runs with minimal additional performance overhead[2].\n\nSECTION: 3.2Evaluation Results\n\nTo evaluate the performance characteristics of MQMS against the baseline MQSim-MacSim simulator, we conducted experiments using representative workload traces generated from three data-intensive LLM inference workloads. LLM inference presents an ideal test case for storage system evaluation due to its data-intensive nature and the large-scale memory access patterns required to load model weights and process inputs. Table1provides descriptions of these sampled workloads.\n\nBoth simulators were evaluated under identical configurations tailored to emulate enterprise SSDs[7]. Key parameters, such as channel count, chips per channel, planes per die, and page size, were set to reflect enterprise SSD specifications. Timing configurations, such as read and write latencies, were adjusted to match specification performance as well. Both simulators were set to use the round-robin scheduling policy for GPU kernel execution and the CWDP (Channel-Way-Die-Plane) scheme for SSD page allocation[9].\n\nFigure4compares the IOPS achieved by both simulators for each LLM inference workload. Across all workloads, MQMS exhibits I/O request throughput orders of magnitude beyond that of MQSim-MacSim. This improvement is particularly pronounced for BERT, where the performance gap between MQMS and MQSim-MacSim reaches its maximum, showing an IOPS differential that is an order of magnitude larger than in other workloads. This stark difference stems from BERT\u2019s bidirectional architecture, which generates more frequent I/O requests when loading attention weights across multiple layers simultaneously[3]. MQMS\u2019s ability to exploit plane-level parallelism is particularly effective for such access patterns, as it can distribute these concurrent small requests across multiple planes without the overhead of RMW operations.\n\nImproved IOPS performance catalyzes improvements in both device response time and simulation end time. Device response time, measured as the interval between enqueueing a request in the I/O submission queue and its removal from the I/O completion queue, particularly shows dramatic improvement for MQMS. Figure5shows MQMS maintaining average device response times multiple orders of magnitude lower than MQSim-MacSim across all workloads.\n\nWhile metrics like cache/memory statistics and page footprint characterize the benchmark behavior, the simulation end time provides a comprehensive view of system performance by capturing the cumulative effect of improved IOPS, reduced latency, and better chip utilization. Figure6indicates MQMS completes simulations up to four orders of magnitude faster than MQSim-MacSim, illustrating how improvements in I/O request servicing latency aggregate to accelerate system-level execution.\n\nSECTION: 4Policy Maxima\n\nThe performance characteristics of GPU-SSD systems can vary significantly based on the interplay between scheduling policies and page allocation schemes. Different workloads exhibit unique patterns of I/O requests, memory access, and computational requirements that may be better served by specific policy combinations. By analyzing these policy combinations across various workloads, we identifypolicy maxima\u2014optimal configurations where the combined properties of scheduling and allocation schemes particularly complement specific workload characteristics.\n\nTwo primary scheduling policies govern GPU resource allocation in MQMS: round-robin and large chunk. Round-robin scheduling trivially rotates through all active workloads, allocating GPU resources to one kernel from each workload in a circular sequence. Large chunk scheduling emerges as a fallback policy when the number of blocks per kernel falls below the product of block stride and available cores, making fine-grained scheduling inefficient[9]. Rather than adhering to strict rotation, this policy processes larger consecutive workload segments before switching. The policy is triggered when, whererepresents the number of blocks in the current kernel,is the configured stride length, andis the number of available GPU cores[9]. Large chunk scheduling can also be explicitly selected for workloads where maintaining GPU context is prioritized over fairness, such as in batch processing scenarios with multiple independent tensor operations that share common weights.\n\nThree page allocation schemes are primarily considered in MQMS: CWDP, CDWP (Channel-Die-Way-Plane), and WCDP (Way-Channel-Die-Plane). These schemes determine how logical addresses are mapped to physical flash resources. CWDP prioritizes channel-level parallelism, allocating pages across channels first before considering ways, dies, and planes, which provides lower latencies but potentially underutilizes flash-level resources[8]. CDWP modifies this approach by prioritizing die interleaving over way pipelining, offering better flash-level parallelism at the cost of increased system-level resource conflicts[8]. WCDP takes a contrasting approach by prioritizing way-level resources first, favoring way pipelining over channel striping, which can achieve improved resource utilization but may incur higher request servicing latencies[8].\n\nSECTION: 4.1Evaluation Results\n\nFigure7shows that backprop exhibits the most pronounced performance variation across policy combinations, where large chunk and WCDP delivers a 128% improvement over round-robin and CDWP due to its regular access patterns and high data locality. In contrast, hotspot shows larger but more erratic variations, with a 92% performance difference between configurations. Figure8indicates well-matched policy combinations can dramatically reduce device response times, with backprop showing an \u00a085% reduction when using large chunk and CWDP versus round-robin and CDWP. Figure9reveals similar patterns in simulation end times, with lavaMD achieving a 21% reduction in execution time under round-robin and CDWP compared to large chunk and WCDP.\n\nSECTION: Code Availability\n\nThe source code and additional materials for the MQMS simulator are available on GitHub:github.com/ayushgun/mqms.\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.04569v1_content.txt"}, {"title": "Machine learning approach for mapping the stable orbits around planets", "authors": ["Tiago F. L. L. Pinheiro", "Rafael Sfair", "Giovana Ramon"], "published_date": "2024-12-05T19:23:05Z", "summary": "Numerical N-body simulations are commonly used to explore stability regions\naround exoplanets, offering insights into the possible existence of satellites\nand ring systems. This study aims to utilize Machine Learning (ML) techniques\nto generate predictive maps of stable regions surrounding a hypothetical\nplanet. The approach can also be extended to planet-satellite systems,\nplanetary ring systems, and other similar configurations. A dataset was\ngenerated using 10^5 numerical simulations, each incorporating nine orbital\nfeatures for the planet and a test particle in a star-planet-test particle\nsystem. The simulations were classified as stable or unstable based on\nstability criteria, requiring particles to remain stable over a timespan\nequivalent to 10,000 orbital periods of the planet. Various ML algorithms were\ntested and fine-tuned through hyperparameter optimization to determine the most\neffective predictive model. Tree-based algorithms showed comparable accuracy in\nperformance. The best-performing model, using the Extreme Gradient Boosting\n(XGBoost) algorithm, achieved an accuracy of 98.48%, with 94% recall and\nprecision for stable particles and 99% for unstable particles. ML algorithms\nsignificantly reduce the computational time required for three-body\nsimulations, operating approximately 100,000 times faster than traditional\nnumerical methods. Predictive models can generate entire stability maps in less\nthan a second, compared to the days required by numerical simulations. The\nresults from the trained ML models will be made accessible through a public web\ninterface, enabling broader scientific applications.", "arxiv_id": "2412.04568v1", "html_link": "https://arxiv.org/html/2412.04568v1", "search_term": "ti:\"machine learning\"", "html_content": "SECTION: Machine learning approach for mapping the stable orbits around planets\n\nContext.Numerical N-body simulations are typically employed\nto map stability regions around exoplanets,\nproviding insights into the potential presence\nof satellites and ring systems.\n\nAims.Use Machine Learning (ML) techniques to generate predictive maps of stable\nregions surrounding hypothetical planet, and\nthis approach can also be applied to planet-satellite systems,\nplanetary ring systems, and other similar systems.\n\nMethods.From a set ofnumerical simulations, each\nincorporating nine orbital features for the planet and\ntest particle, we created a comprehensive dataset of three-body\nproblem outcomes (star-planet-test particle).\nSimulations were classified as stable or unstable,\nconsidering the stability criteria that a\nparticle must remain stable over a time span of 104orbital periods of the planet.\nVarious ML algorithms were compared and fine-tuned through\nhyperparameter optimization to identify the most effective predictive\nmodel, and all the tree based algorithms demonstrated comparable accuracy\nperformance.\n\nResults.The optimal model, employing the Extreme Gradient Boosting (XGBoost) algorithm, achieved\nan accuracy of 98.48% ,\nwith 94% recall and precision for stable particles and 99%\nfor unstable particles.\n\nConclusions.ML algorithms significantly reduce computational\ntime in three-body simulations, achieving speeds approximatelytimes faster than traditional numerical simulations.\nFrom saved training models, predictions of entire stability maps are made in less than a second,\nwhile an equivalent numerical simulation can take up to a few days.\nOur ML model results will be accessible\nthrough a forthcoming public web interface,\nfacilitating broader scientific application.\n\nSECTION: 1Introduction\n\nThe population of known exoplanets has grown year-over-year,\nprompting extensive research into the stability of extrasolar planetary systems.\nTo investigate the potential existence of satellites or ring systems around\nthese exoplanets, numerical N-body simulations are frequently employed.\nThese simulations map stability regions around a planet by utilizing\na grid of orbital parameters as initial conditions\nfor the N-body integration.\n\nSeveral studies have expanded this analysis to various contexts in\ndifferent gravitational settings.Hunter (1967)examined the stability of satellites orbiting Jupiter under\nthe influence of solar gravity. Their findings indicate that the stable zone\nfor prograde motion extends to 0.45 of the Hill radius (), while for\nretrograde motion, it extends to 0.75.Neto & Winter (2001)investigated the gravitational capture in the Sun-Uranus-satellite system.\nThey employed a diagram of initial conditions of semi-major axis versus eccentricity\nto identify stable regions around Uranus, considering particles with various\ninitial inclinations and arguments of pericenter.\n\nHolman & Wiegert (1999)numerically investigated the stability of S-type\nand P-type planet orbit around a\nbinary star system\nwhere the planet is treated as a test particle.\nTheir work resulted in an empirical expression\nfor the critical semi-major axis of a stable particle initially in a circular\nand planar orbit, relating it to binary distance, mass ratio, and eccentricity.\nThe authors analyzed a range of binary mass ratios= [0-0.9] and\neccentricities e\u00a0\u00a0=\u00a0[0-0.8].\n\nDomingos et\u00a0al. (2006)examined the stability of hypothetical satellites\norbiting extrasolar planets with a mass ratio. They derived an\nempirical expression for the stability boundary in both prograde () and\nretrograde () orbits, formulated in terms of the Hill radius and the\neccentricities of the planet and the satellite.\n\nRieder & Kenworthy (2016)conducted N-body simulations to analyze the stability of a\npotential ring system around the candidate planet J1407b, thereby imposing constraints\non both the orbital and physical parameters of this system. For the PDS110b system,Pinheiro & Sfair (2021)performed millions of numerical simulations of the three-body problem,\naiming to refine parameters that remain undefined based on observational data, including the\nmass and eccentricity of unseen secondary companion,\nas well as the ring inclination and radial size.\n\nWhile the aforementioned studies have provided valuable insights, conducting\nsuch numerical simulations for many systems incurs substantial computational\nexpense. To address this challenge, a novel method to expedite this process involves the\nuse of ML algorithms.\nIn this approach, N-body simulations generate a dataset\nfor training ML models, which can\nthen predict outcomes for unseen data more efficiently.\n\nThe identification of stability regions using ML methods has\nalready been demonstrated in previous studies.Tamayo et\u00a0al. (2016)applied\nML algorithms to predict the stability of tightly packed planetary\nsystems, consisting of three planets\norbiting a central star. Their models achieved\na predictive accuracy ofover the test set,\nand were three orders of\nmagnitude faster than direct N-body simulations.\n\nTamayo et\u00a0al. (2020)developed SPOCK\n(Stability of Planetary Orbital Configurations Classifier),\na ML model capable of predicting the stability\nof compact systems with three or more planets.\nThis approach is 105times faster than numerical simulations.\n\nCranmer et\u00a0al. (2021)introduced a probabilistic ML model that predicts the stability of\ncompact multi-planet systems with three or more planets,\nincluding when these systems are likely to become unstable.\nThis model demonstrates over two orders of magnitude greater\naccuracy in predicting instability times compared to analytical estimators.\n\nExpanding on this approach,Lam & Kipping (2018)employed deep neural\nnetworks to predict the stability of initially coplanar, circular P-type orbits\nfor circumbinary planets. Their method achieved an accuracy of at leastfor the test set,\nfurther demonstrating the potential of ML in analyzing orbital\nstability.\n\nBuilding upon the successes of previous ML\napplications in orbital stability analysis,\nwe propose using this approach to predict a stable region map\nsurrounding a single-planet system.\nThis method can also be extended to analyze the stability of other analogous systems,\nsuch as planet-satellite pairs, planetary rings, and binary minor planets,\nprovided they lie within the trained parameter space of the ML model.\n\nWe conducted a series of dimensionless numerical\nsimulations involving an elliptical three-body problem\n(star, planet, and a particle orbiting the planet).\nEach simulation represents a distinct initial condition\nwithin our dataset, used to train and evaluate machine\nlearning algorithms.\nExpanding on the work ofDomingos et\u00a0al. (2006),\nour study explores a wide range of orbital and physical\nparameters, including the mass ratio of the system and\nseveral orbital elements of both the particle and planet.\n\nThis comprehensive approach could be\ncomputationally demanding if solved purely through\nnumerical simulations, especially when increasing\nthe number of grid parameters to be analyzed.\nRunning a single three-body problem may be relatively fast,\nbut a broader set of initial conditions may significantly\nincreases computational costs, highlights the\nadvantages of our ML method.\n\nThis paper is structured as follows:\nSection2introduces key concepts and methodologies for our analysis\nand introduce ML algorithms used in this study.\nSection3presents a description of\nour numerical model and dataset preparation. Section4describes how to tackle with imbalanced classes.\nSection5presents the performance evaluation of our\nML algorithms.\nSection6compares the predictions made by our\nbest performance ML model with the results fromDomingos et\u00a0al. (2006)andPinheiro & Sfair (2021), as well as with a stability map of Saturn\u2019s\nsatellites. Finally, Section7presents our concluding remarks and\nimplications of this study.\n\nSECTION: 2Supervised learning\n\nThis section presents fundamental concepts and\ntechniques for our study.\nThese definitions and approaches will be referenced and\ncompared throughout the subsequent sections, particularly in our\nresults and discussion.\n\nThe Supervised learning includes\nin the training data the desired solution (labels).\nThese labels can be composed of continuous numerical\nvalue (regression task) or discrete values\nwhich represent different classes (classification task).\n\nClassification tasks are techniques that use\ntraining samples (or instances) during the learning process\nto develop a generalization of classes and\nmake a prediction given unknown new instances.\nHere, we address binary classifications\nstable and unstable particles to\npredict stability maps within a specific planetary system.\n\nThe dataset was split into three subsets: training, validation, and test data.\nThe training data is used by the algorithm to find the best generalization\nfeatures for different classes. The validation data provides an unbiased\nevaluation of model effectiveness, and is used to tune model hyperparameters\nand thresholds, select the best\nML algorithm, and detect\nunderfitting and overfitting(Shalev-Shwartz & Ben-David,2014).\n\nThe cross-validation technique involves partitioning the training data into\nsubsets. Specifically, in this paper, we split the training data into 5\nsubsets. Subsequently, we train and evaluate the model on distinct subsets,\nfollowed by averaging the outcomes to estimate the\nunbiased error on the training set of the model(Fushiki,2011).\n\nAfter training several models with different configurations multiple times on\nthe reduced training set and evaluating them through cross-validation, we\nidentified the best-performing model. Subsequently, we retrained the selected\nmodel using the complete training dataset and evaluated\nfinal performance of ML model\nusing holdout data (test dataset).\n\nWe divided our work into five main steps, as shown in Figure1,\nwhich sketches the workflow of our methodology. The process\nbegins with generating a dataset from numerical simulations. We then address\nthe issue of imbalanced classes through resampling techniques. Using\ncross-validation, we perform hyperparameter tuning and explore optimal\nthreshold values to establish the best configuration model. The final step\ninvolves assessing the performance of the best model.\n\nSECTION: 2.1Hyperparameter tuning\n\nHyperparameters are specific algorithm parameters that must be set prior to the\ntraining process, and the model performance directly depends on them(Weerts et\u00a0al.,2020). The search for the best hyperparameters was carried\nout in two different ways: Grid and Random Search, both validated using\ncross-validation.\n\nThe Grid Search systematically explores all possible combinations of\nhyperparameters. In contrast, Random Search randomly selects some\ncombinations to evaluate, which is useful when dealing with an extensive\nnumber of potential combinations.\n\nSECTION: 2.2Threshold tuning\n\nIn general, in binary classification tasks, classifier algorithms employ\na default threshold value of 0.5. This means that if the probability of an\ninstance being in a positive class is0.5, it is assigned to the\npositive class; otherwise, it is classified as negative. However, for an\nimbalanced dataset, using 0.5 as the threshold value is generally\ninappropriate(Zou et\u00a0al.,2016).\n\nThe Receiver Operating Characteristic (ROC) curve is a frequently employed tool\nfor determining the optimal threshold value(G\u00e9ron,2022). The ROC\ncurve is a probability curve, which plots the True Positive Rate (hereafter\nreferred to as recall) versus the False Positive Rate for different\nthreshold probabilities from 0 to 1. Figure2provides an\nillustrative example of ROC curves for three hypothetical models\nwith varying performance levels, and also the\nROC curve of our best performance ML model (see sec.5).\n\nThe True Positive Rate is the ratio of positive instances that are\ncorrectly predicted, while the False Positive Rate represents the fraction of\nnegative instances that are incorrectly predicted as positive.\nThe ROC curve always begins at the point (0,0), where the threshold is 1,\nsignifying that all instances are predicted as the negative class. It ends at\n(1,1), where the threshold is 0, indicating that all instances are predicted as\nthe positive class.\n\nThe Area Under the Curve (AUC) represents the area under the ROC curve,\nproviding a measure of the model\u2019s ability to differentiate classes. In a\nperfect model (exemplified by the black curve in figure2), the\npositive class is always predicted withaccuracy regardless of the\nthreshold parameter, resulting in an AUC of 1.\n\nGood predictors tend to have an AUC value close to. For instance,\nthe green curve in figure2has an AUC of, indicating that the model has aprobability of\nsuccessfully distinguishing between positive and negative classes.\nThe dashed red line represents the ROC curve of our\nbest-performing ML model with an AUC of 0.998.\nThis model will be discussed in section5.\n\nThe reference line (dashed gray line) represents an AUC of 0.5, signifying that the\nmodel cannot differentiate between classes, resulting in random guesses for\npredictions. Conversely, the blue curve represents a poorly performing model\nwith an AUC of 0.23, where the majority of positive instances are predicted as\nnegative.\n\nThe optimal threshold (the black dot in Figure2)\ncan be determined as the point closest to the upper-left\ncorner of the ROC curve (0,1),\nor the point that maximizes the distance\nfrom the reference line\nwhere the true positive rate is high and\nthe false positive rate is low.\n\nThe concepts and methods outlined in this section provide the framework for\nour subsequent analysis. We will refer back to these definitions and compare\ntheir effectiveness in later sections, particularly when evaluating our\nresults and discussing their implications.\n\nSECTION: 2.3Classification algorithms\n\nWe tested eight distinct ML algorithms\nto find the optimal approach:\nNearest Neighbors, Naive Bayes, Artificial Neural Network, and tree-based methods\nDecision Tree, Random Forest,\nXGBoost, Light Gradient Boosting Machine (LightGBM), and\nHistogram Gradient Boosting. Lately, we ran a Genetic algorithm to verify our findings.\n\nGiven that all tree-based algorithms demonstrated\nbetter and comparable performance, we will focus our analysis on these methods.\nWith the exception of Decision Tree,\nall other tree-based classifiers are ensemble methods,\nwhich utilize a group of weaker learners\n(in this case, an ensemble of decision trees) to enhance their predictions.\nThe collective perspective of a group of predictors yields better predictions than a\nsingle predictor(G\u00e9ron,2022).\nWe give further details of\nthe fundamental concepts of each of these\ntree-based algorithms in AppendixA.\n\nSECTION: 3Dataset preparation\n\nIn this section, we detail the methodology for investigating the orbital\nstability of a three-body problem (star, planet, and particle) through an\nensemble of dimensionless numerical simulations. Each simulation represents a\ndistinct initial condition for a hypothetical planetary system.\n\nWe begin by defining the mass parameteras\n\nwhereandare the\nmasses of the planet and star, respectively.\n\nA survey including 4,150 confirmed exoplanets\nfromSchneider et\u00a0al. (2011),\nreveals that 95.71of them\nhave an equivalent,\nand 2.05 % of these have\u00a1.\nThus, we restricted our analysis to this range of\nmass parameter, and within it we\nadopted a random uniform distribution.\n\nThroughout all numerical simulations,\nthe star and planet masses were set\nbyand, respectively.\nThe planet semi-major axis was set toand its collision radius () was\ndefined asof the Hill radius ()\ncomputed at the pericentre:\n\nThis choice forrepresents\n55.6% of planets in the catalogue ofSchneider et\u00a0al. (2011)have a collision radius that is less than 5% of their Hill radii,\nas shown in the histogram in figure3.\n\nThe planet eccentricity () and true anomaly ()\nwere randomly uniformly sampled within the specified ranges:and.\nThe remaining orbital elements for the planet\ninclination (), arguments of pericenter (),\nand longitude of the node () were set to\nzero.\n\nThe test particle was modeled orbiting a\nplanet and gravitationally disturbed by the star.\nWe adopted a random uniform distribution to select\nparticle orbital initial conditions for semi-major axis\nfrom 1.1to 1,\neccentricity from 0 to 0.99,\ninclination betweenand, and\narguments of pericenter, longitude of the node, and true anomaly\nranging fromto.\n\nWe numerically integrated usingReboundpackage\nand IAS15 integrator(Rein & Spiegel,2015),\nfor a time span oforbital period of the planet.\nWe recorded every time when particle collided with the planet\nor was ejected from the system, which occurred when\nthe particle reaches a hyperbolic orbit (eccentricity)\nor when its semi-major axis is.\n\nIn total, we rannumerical systems\nand the overall outcome was as follows:of the particles remained stable\nthroughout the entire integration,\nwhilewere unstable.\nAmong the unstable particles,\n53.56% of them collided with the planet,\nand 46.44% were ejected from the system.\n\nThe outcomes of the numerical simulations were utilized\nto build a dataset for training ML algorithms.\nThis dataset consisted of 9 features,\nwhich are the initial conditions of each numerical simulation:\nthe mass ratio of the system,\nsemi-major axis, inclination, argument of pericenter,\nand longitude of the node of the particle,\nand eccentricity and true anomaly of both the planet and the particle.\nWe labeled each initial condition with the numerical\nresults as either stable or un unstable system.\n\nFigure4shows four histograms\nof the number of numerical simulations in relation to the\nparticle semi-major axis (upper left panel),\neccentricity (upper right panel),\ninclination (lower left panel),\nand planet eccentricity (lower right panel).\n\nAs the eccentricity of the planet increases, the number\nof stable systems is slightly reduced. Nevertheless, we can find\nstable particles even for highly eccentric planets.\nThe particle eccentricity has a more distinct effect on the number of stable systems,\nwith no stable orbits found for. This is expected since\nparticle orbits experience stronger\ngravitational disturbances at their pericenters, which\nincreases the quantity of particles that collided.\n\nThe number of stable systems is also strongly\ninfluenced by the inclination of particles\nrelative to the orbital plane of the planet.\nIn our set, there are no stable systems for inclination betweenand, while retrograde systems tend to have more stable orbits compared to\nprograde ones.\n\nCloser to the planet there is a\nsignificant number of collisions, and\nthe number of stable systems increases with the initial particle distance,\nreaching a peak at.\nAfter this peak, the quantity of stable systems declines, and\nonly a few systems remain stable beyond,\nwith the majority of particles being ejected.\n\nOther features, such as the mass ratio of the system,\nargument of pericenter, longitude of the ascending node, and\ntrue anomaly, showed a regular distribution\nacross the range of their parameters.\n\nSECTION: 4Imbalanced class\n\nThe numerical results demonstrate that the classes are\ndisproportionate in the dataset by a factor 1:10, with the unstable\nclass being much larger than the stable class.\nThis imbalance could lead to biases in classification and impair\nthe effectiveness of the various classifier algorithms\nwhen trying to generalize(Zheng et\u00a0al.,2015).\nML algorithms typically assume\nthat classes are evenly distributed,\nwhich can causes the minority class to be\nclassified poorly(Kumar & Sheshadri,2012; Carruba et\u00a0al.,2023b).\n\nResampling the training data is one strategy that\ncan be used to tackle this problem.\nOne option is the undersampling, which is a technique that removes instances\nfrom the majority class.\nThe weakness of this method is that it may lose\ninformation by removing part of the data(Mohammed et\u00a0al.,2020).\nAnother resampling process is the oversampling,\nwhich involves increasing the size of\nthe minority class until the classes are balanced.\nWe tested the performance of four\ndifferent types of oversampling: random oversampling,\nSynthetic Minority Over-sampling Technique\n(SMOTE), Borderline SMOTE, and Adaptive Synthetic Sampling (ADASYN).\nIn AppendixB, we provide additional details on the\nkey concepts of each of these resampling techniques.\n\nAccording toCarruba et\u00a0al. (2023a), an severe imbalanced dataset\ncan occur when the class ratio is 1:100, and in this scenario resampling methods may be necessary.\nAfter testing the performance of our algorithms with various resampling techniques,\nwe observed a marginal improvement, except for the LightGBM that increased of 13% in identifying stable particles,\nachieved solely through the resampling technique, without hyperparameter or threshold tuning.\nWith this improvement, the LightGBM performance\nis almost as good as the\nbest-performing model without any resampling.\n\nSECTION: 5Performance evaluation\n\nEvaluation metrics enable the assessment of classifier performance.\nThe confusion matrix is a statistical table that maps prediction\nresults, showing the quantity of correctly and incorrectly predicted data(Stehman,1997). While accuracy measures the rate at which a model\ncorrectly predicts, this metric may not be reliable for imbalanced datasets.\nA confusion matrix displays the actual number of instances for each class\nin its rows, and the predicted number of instances for each class in\nits columns. It is divided into four categories: True Positive (TP) and\nTrue Negative (TN), where the algorithm correctly predicts positive and\nnegative classes; and False Positive (FP) and False Negative (FN),\nwhere the algorithm incorrectly predicts positive and negative classes.\n\nThese four categories are used to measure recall, specificity, precision,\nF1-score, and accuracy. Accuracy is the ratio of instances correctly\npredicted by the algorithm, given by\n\nSpecificity or true negative rate is the ratio of correctly predicted\ninstances within the actual negative class:\n\nand the False Positive rate is given by.\n\nRecall or the true positive rate is the ratio of correctly classified positive instances\nto the total number of actual positive instances:\n\nPrecision is the ratio of actual positive instances within the\npredicted positive class:\n\nThe F1-score combines precision and recall by calculating their harmonic mean:\n\nWhile accuracy measures the rate at which a model correctly predicts, this\nmetric may not be reliable for imbalanced datasets. F1 scores measure the\noverall quality of a model, ranging from 0 to 1, where 1 signifies a model\nthat perfectly classifies instances and 0 indicates a model that fails to\nclassify any instance correctly.\n\nSECTION: 5.1Results and Comparison of Different Algorithms\n\nThe ML outcomes were evaluated using a dataset generated from\nnumerical simulations (see Sec.3).\nOur binary approach involves classifying systems as either\nstable or unstable. The final dataset consists of 100,000 simulations, divided into\nthree subsets: 80% for training and validation data and 20% for testing data.\nIn the presence of imbalanced classes and to achieve better performance in\nclassifier algorithms, we tested five distinct resampling methods.\nWe utilized\ncross-validation techniques to identify the best hyperparameters, resampling\nmethod, and the optimal threshold value for each algorithm.\n\nThe best performance among the five algorithms was achieved by XGBoost, without\nusing any resampling techniques, with a threshold value of 0.4, and setting some\nhyperparameters as follows:\n\nbooster = gbtree.Booster is a hyperparameter for\nthe type of boosting model used during the training stage. Gbtree\nmeans gradient boosted trees.\n\neta = 0.2. Eta is the learning rate and mathematically,\nit scales the contribution of each tree added to the model.\n\nscale_pos_weight = 0.4. Controls the balance of positive\nand negative class weights.\n\nmax_depth = 8. The maximum depth for each tree.\n\nThis result was also verified using the Genetic Algorithm. Genetic Algorithms\nmimic the process of genetic evolution and are capable of selecting the most\noptimal algorithm and the best hyperparameters for a given dataset(Chen et\u00a0al.,2004).\n\nFigure5shows the performance of the five algorithms using four\ndistinct metrics. The upper left panel displays accuracy, while precision for each\nclass is presented in the upper right panel. The lower left panel shows the\nrecall, and the lower right panel shows F1-scores. Stable classes are\nrepresented by orange bars, while unstable classes are\nshown with blue bars.\n\nAll algorithms demonstrated comparable accuracy performance.\nXGBoost achieved the highest accuracy at 98.48%, while Decision Tree exhibited\nthe lowest score at 97.43%, showing a marginal difference of 1.05%.\n\nAn important consideration here is that accuracy measures how correctly the model\nclassifies whether the system is stable or unstable across all predicted data.\nTherefore, this favorable result may be influenced by the proportion of\nimbalanced classes, with the quantity of unstable instances being\napproximately 7.5 times larger than stable ones in our dataset. The algorithms\nachieved a recall and precision of 99% for all classes, except for the\nprecision of Random Forest and the recall of the Decision Tree\nalgorithm, which were 98%.\n\nIn terms of stable class performance, both LightGBM and Random Forest achieve the\nhighest precision value, correctly classifying 95% of their predictions. They\nidentify 92% and 88% of actual stable instances (recall), respectively. XGBoost\nattains the same percentages for precision and recall, leading to the highest\nF1-score of 94%, while LightGBM and Random Forest achieve 93% and 92%,\nrespectively.\n\nThe area under the ROC curve (AUC) provides a measure of overall model performance,\nwith a value of 1 representing perfect classification (Figure2).\nTable1compares the AUC values for each algorithm. XGBoost\noutperforms other models, closely followed by LightGBM and Histogram Gradient Boosting.\nThe Decision Tree algorithm shows slightly lower\nperformance compared to the ensemble methods.\n\nFigure6shows the confusion matrices for each algorithm,\nproviding statistical results from the testing dataset of 20,000 instances.\nClass 0 represents a stable system, while class 1 is unstable. All algorithms\ndemonstrate satisfactory performance, with even the lowest-performing model\n(Decision Tree) correctly predicting at least 19,485 instances.\n\nXGBoost, the highest accuracy algorithm, misclassified only 303 instances,\nwith a nearly balanced distribution of errors between stable and unstable classes\n(150 and 153 instances, respectively).\nIt also correctly identified the highest number of stable instances (2,211).\nRandom Forest and LightGBM achieved the highest precision for the stable class\n(0.95), slightly outperforming XGBoost in this metric.\n\nRandom Forest showed the best performance in correctly classifying unstable\ninstances (17,536). However, it also had the highest number of false positives,\nmisclassifying 285 stable instances as unstable \u2013 nearly double the error rate\nof XGBoost for this class.\n\nSECTION: 5.2Feature importance\n\nAn interesting outcome from the XGBoost algorithm is the feature importance,\nwhich estimates how effectively each feature contributes to reducing impurity\nthroughout all the decision tree splits. The feature importance is determined by\naveraging the importance of each feature across all decision trees,\nand Figure7illustrates the\nrelative importance of features in our classification task.\n\nParticle orbital elements dominate the top three positions, with semi-major\naxis (), eccentricity (), and inclination () scoring 0.3567, 0.2783,\nand 0.2351, respectively. These findings align with the distribution patterns\nobserved in Section3, where we noted significant\nreductions in stable systems beyondand the absence of\nstable particles with inclinations nearor eccentricities.\nPlanet eccentricity ranks fourth in importance (0.0564), followed by particle\nargument of pericenter (, 0.0274). The remaining features - particle\ntrue anomaly (), mass ratio (), planet true anomaly (),\nand particle longitude of node () - contribute less significantly,\nwith scores ranging from 0.0119 to 0.0110.\n\nThese results quantify the relative importance of orbital parameters in\ndetermining system stability, that most significantly influence dynamical outcomes.\n\nSECTION: 6Comparative analysis\n\nWe now apply our best ML model, XGBoost, to predict four\ndistinct diagrams of initial conditions () with the aim of\nreproducing the results ofDomingos et\u00a0al. (2006),Pinheiro & Sfair (2021), and those\nfor a particular Saturn moon group known as Inuit.\nIn each diagram the test particles are uniformly distributed within\na range of initial semi-major axes from 1.1to 1with a step size of, and initial eccentricities\nranging from 0 to 0.5, with.\n\nThe XGBoost model, which demonstrated superior performance in our\nearlier analysis with an accuracy of 98.48%, is now tasked with predicting\nstability in these specific planetary systems. This approach allows us to\nevaluate the model\u2019s generalization capabilities across diverse orbital\nconfigurations and compare its predictions directly with established numerical\nresults.\n\nTo create an entire stability map with10,000 different initial conditions,\nnumerical simulations required between two and\nfour days on a single core of an Intel i7-1165G7 processor,\nwhile ML predictions, with saved training, generate a map in 0.5 seconds.\n\nSECTION: 6.1Comparison withDomingos et\u00a0al. (2006)\n\nDomingos et\u00a0al. (2006)numerically simulated multiple stable maps\nto delineate the boundaries of stable regions surrounding\na close exoplanet orbiting its star at a semi-major axis of 0.1 au.\nThey derived an analytical expression for the critical exosatellite\nsemi-major axis as a function of the eccentricity of both the satellite and\nthe planet, for two different orbital inclinationsand.\n\nWe reproduce () diagrams of two different systems explored\nbyDomingos et\u00a0al. (2006), one in prograde and the other in retrograde orbit.\nThe mass ratio between planet and star was defined as,\nwith a planet radiusand orbiting\na Sun-like star in a circular orbit.\n\nFigure8compares the numerical simulation results with machine\nlearning predictions, with the left panels representing the results\nfor a prograde system and right panels for retrograde ones. The performance\nof our model predictions are summarized in figure9,\nwhere the dashed lines refers to a prograde system and the\nsolid lines to the retrograde case.\nThe dashed lines represent the analytical expression\nderived byDomingos et\u00a0al. (2006).\n\nIn both scenarios, the model accurately classified the main structure of the\nstable region (orange points), but showed some discrepancies at the\nboundaries. For the prograde case, the model closely replicated the stable\nregion, with only minor differences at the edges. The stable region extends to\nabout 0.49in both the numerical and ML results, aligning\nwell with Domingos et al. (2006). The model achieved an overall accuracy of\n95.86%, a precision of 94% for unstable particles and 100% for stable\nparticles, a recall of 100% for unstable\nparticles and 89% for stable particles,\nand a F1-score of 97% for unstable particles and 94% for stable particles.\n\nThe performance comparison between the analytical expression\nfor the critical semi-major axis proposed byDomingos et\u00a0al. (2006)and our ML model reveals similar overall results.\nWhileDomingos et\u00a0al. (2006)\u2019s equations exhibit marginal higher accuracy (96.05%)\nand recall for the stable and unstable classes (91.80% and 98.70%, respectively),\nour ML model outperforms in precision for both classes.\nSpecifically,Domingos et\u00a0al. (2006)achieves a precision of 97.78%\nfor stable particles and 95.08% for unstable particles,\nwhereas our model achieves 100% precision in both classes.\nThe F1-score for both methods have the same percentage.\n\nFor the retrograde case, the model captured the overall larger stable\nregion, extending to about 0.93. However, it slightly\nunderestimated the stable region\u2019s extent for eccentricities above 0.3. The ML\nmodel also missed some stable particles with semi-major axes beyond\n0.8, particularly at lower eccentricities.\n\nDespite these limitations, the ML model achieved an accuracy of 86.24%,\noutperforming the analytical expression byDomingos et\u00a0al. (2006),\nwhich had an accuracy of 85.04%.\nFurthermore, the ML model attained a precision of 99% for stable particles and\n62% for unstable particles, a recall of 83% for stable particles and 98% for unstable particles.\nThese results yielded F1-scores of 90% for stable particles and 76% for unstable particles.\nIn comparison,Domingos et\u00a0al. (2006)\u2019s equations achieved slightly lower F1-scores,\nwith 89.26% for stable particles and 74.85% for unstable particles,\nalong with a recall of 80.81% for the stable class and a precision of 59.94%\nfor the unstable class.\nHowever, their precision and recall for stable and unstable particles\nwere marginally better, with values of 99.83% and 99.54%, respectively.\n\nThese results demonstrate the model\u2019s ability to accurately predict the\ngeneral structure of stable regions for both prograde and retrograde orbits,\nwhile highlighting areas for potential improvement, particularly in\ncapturing fine details at stability boundaries and for highly eccentric\norbits in retrograde systems.\nEven the analytical expression byDomingos et\u00a0al. (2006),\nwhich applies only to=and inclinations of 0\u2218or 180\u2218failed to classify many stable particles beyond the black line.\nFurther refinement of the model, possibly\nthrough an expanded dataset or enhanced feature engineering, could address\nthese minor discrepancies.\n\nSECTION: 6.2Comparison withPinheiro & Sfair (2021)\n\nPinheiro & Sfair (2021)studied the stability of a possible ring\nsystem around the candidate planet PDS110b.\nThey rannumerical simulations of\nthe three-body problem, each representing a different initial condition.\nWe evaluated the performance of our model for the PDS110b system,\nspecifically considering a scenario where the planet\nhas a mass equivalent to 6.25 Jupiter masses, an eccentricity of 0.2,\nand a ring inclination of.\nThis scenario corresponds to one of the possible parameters for\na retrograde ring system around PDS110b.\n\nFigure10shows the numerical and predicted stability maps around\nPDS110b. Although this system is retrograde, in this case\nthe model performed much better than in the previous example,\nachieving an accuracy of 93.68%\nover20,000 predicted initial conditions.\nHere, the number of stable and unstable instances is almost\nequivalent, with 5,722 actual stable and\n14,373 actual unstable instances, respectively.\n\nFigure11shows\nprecision, recall and F1 scores for both classes,\nranging between 85% and 98%.\nThe slight difference is attributed to the model\u2019s generalization\nat the boundary of the stable region,\nwhere it misclassified 286 instances as unstable\nand 984 instances as stable.\nSome of these misclassifications belong\nto outlier particles outside the stable region.\nOur numerical simulations cover a time span oforbital periods;\nhowever, some of these stable particles might become unstable\nwith longer integration times.\n\nSECTION: 6.3Stability map of Saturn\u2019s Inuit satellites\n\nSome irregular Saturn moons are classified into three different groups\ndistinguished by their inclination range.\nWe generated a stability map for the Inuit group, where\nthe range of inclination is betweenand.\nTable2summarizes the orbital\nelements of the moons belonging to this group.\n\nAs another test, we run20,000 numerical simulations\nof three body problem representing Sun, Saturn\n(corresponding to) and test particle.\nThe test particle\u2019s initial orbital inclination was set to,\nwith the argument of pericenter, true anomaly, and longitude of node being randomly\nselected fromtousing a uniform\ndistribution.\n\nFigure12shows the numerical and machine\nlearning predictions for this example.\nThe black squared points represent the satellites\nbelonging to the Inuit group.\n\nThis example produced our best performance with an\naccuracy of 97.57%, along with a precision and\nrecall for the stable class of 97% and 93%,\nrespectively, and with a precision and recall\nfor the unstable class of 98% and 99% (Fig.13).\nIn total, the ML model incorrectly\nclassified only 470 instances out of20,000,\nwith most of them being spread particles at the\nboundary of the stable region.\nThis good result is reflected in the F1-score,\nwhich for both classes reached high values,\nwith 95% for the stable class and 98% for the unstable one.\n\nSECTION: 7Final remarks\n\nThis study demonstrates the efficacy of ML techniques in predicting\norbital stability for hypothetical planet.\nWe utilized a comprehensive dataset derived fromnumerical simulations of\nthree-body systems, encompassing a wide range of orbital and physical parameters.\nThese parameters included the mass ratio of the system, semi-major axis,\ninclination, argument of pericenter, longitude of the node, eccentricity, and\ntrue anomaly for both the planet and test particles.\n\nOur numerical simulations revealed that 11.83% of particles remained stable\nthroughout the integration period, while 47.22% collided with the planet and\n40.95% were ejected from the system. This imbalanced class distribution\nnecessitated the application of resampling methods to enhance model performance.\nWe evaluated five ML algorithms: Random Forest, Decision Tree,\nXGBoost, LightGBM, and Histogram Gradient Boosting,\nall algorithms demonstrated comparable accuracy performance.\nThrough rigorous hyperparameter tuning and threshold optimization, we identified XGBoost\nas the best-performing model, achieving an accuracy of 98.48%.\nTo validate our model\u2019s generalization capabilities, we applied it to reproduce\nthe numerical results ofDomingos et\u00a0al. (2006),Pinheiro & Sfair (2021), and a\nstability map for Saturn\u2019s Inuit group of satellites. The model demonstrated\nrobust performance, achieving an accuracy of 97.57% across these diverse\nscenarios.\nEven though our dataset covers inclinations fromto, near i=the stable region\nexhibits significant sensitivity to minor inclination variations,\nresulting in decreased performance.\nAdditionally, we also tested the performance of our model\nin the case of Earth (= 3)\nand obtained an accuracy of 95.17%.\n\nOur results show the potential of ML algorithms as powerful\ntools for reducing computational time in three-body\nsimulations in ordertimes faster than traditional numerical simulations.\nThe ability to generate stability maps covering a wide range of orbital and\nphysical parameters\nwithin seconds represents a significant advancement in\nefficiency compared to\ntraditional numerical methods.\nThe implementation of our model will be made accessible\nthrough a public web interface, facilitating its use by the broader scientific community.\n\nAs future work we are refining the model\u2019s performance,\nparticularly in capturing fine details at stability\nboundaries and for highly\neccentric orbits, and also highly\nretrograde system.\n\nAdditionally, we plan to expand the dataset to include more diverse\nplanetary system configurations could enhance\nthe model\u2019s generalization\ncapabilities and broaden its applicability to a wider range of\nastrophysical scenarios.\n\nSECTION: References\n\nSECTION: Appendix AMachine learning algorithms\n\nHere we present the main concepts and\ncharacteristics of the tree-based ML algorithms\nutilized in this paper.\n\nSECTION: Decision Tree\n\nDecision Tree, first introduced byQuinlan (1986), classifies instances by\nsplitting them into leaf nodes(Mitchell1997). This method builds a tree\nformed by a sequence of several nodes, each with a specific rule\n(), whereis a threshold value of\na single feature () in thenode(Shalev-Shwartz & Ben-David2014).\nThe tree starts with the root node, where the data are split into two\nnew subsets and nodes, which can be either a leaf node or a decision node.\nThe leaf or terminal node is where the instance is classified, while the\ndecision node splits the sample again into two new subsets and\nnodes. This process repeats until the end of the branch.\n\nThe choice of feature\nand best threshold value for each node depends on the impurity of the\nsubsequent two subsets, which can be calculated by estimating the class\ndistribution before and after the split(G\u00e9ron2022).\n\nSECTION: Bagging classifier: Random Forest\n\nGenerating a set of classifiers can improve accuracy and robustness. When\nthis approach uses the same training algorithm for every predictor to train\ndifferent random subsets, the method is called bagging(G\u00e9ron2022).\nThe Bagging classifier creates random subsets of the dataset and trains a\nmodel on each subset(Breiman1996). This method returns a\ncombination using a voting scheme of predictions from all predictors.\n\nThe bagging algorithm for trees works by iteratively selecting a subset of\ninstances from a training set through bootstrapping, then fitting a\ntree to these selected instances. Predictions for unseen instances are\nmade by averaging the predictions from all individual trees(Cutler et\u00a0al.2011).\n\nRandom Forest, introduced byBreiman (2001), is an example of a\nbagging classifier and an ensemble of Decision Trees. Random Forest\nreduces the risk of overfitting by decreasing the correlation between\ntrees(Shalev-Shwartz & Ben-David2014). It trains numerous decision trees using\ndifferent subsets of the training data. The final prediction is determined\nby the most voted outputs among these individual decision trees.\n\nSECTION: Boosting classifiers\n\nBoosting aims to improve the accuracy of any given learning algorithm.\nThis method combines several weak learners into a strong learner and\ntrains predictors sequentially, with each one attempting to rectify the errors\nof its predecessor(G\u00e9ron2022). It uses the residuals of the\nprevious model to fit the next model. The following subsections present an\noverview of XGBoost, LightGBM, and Histogram Gradient Boosting.\n\nSECTION: XGBoost\n\nXGBoost (Extreme Gradient Boosting) is a decision tree ensemble developed\nbyChen & Guestrin (2016)based on the idea of Gradient-Boosted Tree (GBT) byFriedman (2001). It computes the residual of each tree prediction,\nwhich are the differences between the actual values and the model\npredictions. These residuals update the model in subsequent iterations,\nreducing the overall error and improving predictions(Wade & Glynn2020). Unlike sequentially built Gradient-Boosted Trees,\nXGBoost splits the data into subsets to build parallel trees during\neach iteration.\n\nXGBoost differs from Random Forest in its training process. It\nincorporates new trees predicting the residuals of previous trees, and\ncombines these predictions, assigning varying weights to each tree, for\nthe final prediction(Chen & Guestrin2016).\n\nLight Gradient Boosting Machine (or LightGBM) is a variant of the\nGradient-Boosted Tree released in 2016 as part of Microsoft\u2019s Distributed\nMachine Learning Toolkit (DMTK) project(Ke et\u00a0al.2017). The method employs\nhistograms to discretize continuous features by grouping them into\ndistinct bins. LightGBM introduces the leaf-wise growth strategy, which\nconverges quicker and achieves lower residual.\n\nLightGBM provides different characteristics compared to XGBoost,\noffering alternative benefits that may be more suitable depending on the specific use case.\nThese include improved memory usage,\nreduced cost of calculating the gain for each split, and reduced\ncommunication cost for parallel learning, resulting in lower training time\ncompared to XGBoost. The algorithm also utilizes the new Gradient-Based\nOne-Sided Sampling (GOSS) and Unique Feature Bundle (EFB) techniques(Ke et\u00a0al.2017). GOSS creates the training sets for building the base\ntrees, while EFB groups sparse features into a single feature(Bent\u00e9jac et\u00a0al.2021).\n\nHistogram Gradient Boosting is a boosting model inspired by\nLightGBM. For training sets larger than tens of thousands of instances, this\ntechnique, with its histogram-based estimators, proves very efficient\nand faster than previous boosting methods(Wade & Glynn2020). Traditional\ndecision trees require long processing times and heavy computation\nfor huge data samples, as the algorithm depends on splitting all\ncontinuous values and characteristics(Ibrahim et\u00a0al.2023).\n\nHistogram Gradient Boosting streamlines this process by using histograms to bin the continuous\ninstances into a constant number of bins. This approach reduces the number of\nsplitting points to consider in the tree, allowing the algorithm to\nleverage integer-based data structures(Wade & Glynn2020). Consequently,\nthis improves and speeds up the tree implementation.\n\nSECTION: Appendix BImbalanced class\n\nRandom oversampling involves replicating\ninstances randomly within\nthe dataset to achieve a balance in class distribution.\nThis approach may lead to overfitting,\nand an alternative solution to avoid this\nis to use the SMOTE.\n\nChawla et\u00a0al. (2002)proposed oversampling the\nminority class by generating \u201dsynthetic\u201d\ninstances.\nFirstly, this method calculates the\ndifference in feature vectors between\nan instance and its nearest neighbors.\nIn the second step, it multiplies this\ndifference by a random number ranging from 0 to 1,\nand a new instance is subsequently\ncreated by adding this result to the\nfeatures of the original instance.\n\nAnother oversampling technique based on SMOTE\nis Borderline-SMOTE(Han et\u00a0al.2005).\nThis method attempts to identify the borderline of\neach class and avoids using the\noutliers (noise points) of the minority\nclass to resample them. The algorithm works with the following steps:\n\nFor each instance of the minority class, check\nwhich class its nearest neighbors belong to.\n\nCount the number of nearest instances\nthat belong to the majority class.\n\nIf all neighbors are from other classes,\nthis point is classified as a noise\npoint, and it is ignored when resampling the data.\n\nIf more than half of the neighbors are from other classes,\nthis is a border point.\nIn this case, the algorithm identifies thenearest neighbors that are of the same class\nand generates a synthetic instance between them.\n\nIf more than half of the neighbors are from the minority class,\nit is a safe point and the SMOTE technique is applied normally.\n\nThe last oversampling approach implemented\nin this work is ADASYN,\nwhich generates synthetic instances for\nthe minority class by considering\nthe weighted distribution of this particular class.\n\nThe methodology employed byHe et\u00a0al. (2008)involves an estimation of the\nimpurity of the nearest neighborsfor each instance in the minority\nclass, given by\n\nwhereis the number of non-minority andthe number of neighbors.\n\nThe subsequent step normalizes thevalue as\n\nwhereis the size of the minority class data. This normalized value\nis then multiplied by the total number of synthetic instances to be\ngenerated. This procedure proportions the number of synthetic instances to\nbe created for each minority instance.", "text_file": "data\\paper_texts\\2412.04568v1_content.txt"}, {"title": "Stabilizing and Solving Inverse Problems using Data and Machine Learning", "authors": ["Erik Burman", "Mats G. Larson", "Karl Larsson", "Carl Lundholm"], "published_date": "2024-12-05T18:31:14Z", "summary": "We consider an inverse problem involving the reconstruction of the solution\nto a nonlinear partial differential equation (PDE) with unknown boundary\nconditions. Instead of direct boundary data, we are provided with a large\ndataset of boundary observations for typical solutions (collective data) and a\nbulk measurement of a specific realization. To leverage this collective data,\nwe first compress the boundary data using proper orthogonal decomposition (POD)\nin a linear expansion. Next, we identify a possible nonlinear low-dimensional\nstructure in the expansion coefficients using an auto-encoder, which provides a\nparametrization of the dataset in a lower-dimensional latent space. We then\ntrain a neural network to map the latent variables representing the boundary\ndata to the solution of the PDE. Finally, we solve the inverse problem by\noptimizing a data-fitting term over the latent space.\n  We analyze the underlying stabilized finite element method in the linear\nsetting and establish optimal error estimates in the $H^1$ and $L^2$-norms. The\nnonlinear problem is then studied numerically, demonstrating the effectiveness\nof our approach.", "arxiv_id": "2412.04409v1", "html_link": "https://arxiv.org/html/2412.04409v1", "search_term": "ti:\"machine learning\"", "html_content": "SECTION: Stabilizing and Solving Inverse Problems using Data and Machine Learning\n\nWe consider an inverse problem involving the reconstruction of the solution to a nonlinear partial differential equation (PDE) with unknown boundary conditions. Instead of direct boundary data, we are provided with a large dataset of boundary observations for typical solutions (collective data) and a bulk measurement of a specific realization. To leverage this collective data, we first compress the boundary data using proper orthogonal decomposition (POD) in a linear expansion. Next, we identify a possible nonlinear low-dimensional structure in the expansion coefficients using an auto-encoder, which provides a parametrization of the dataset in a lower-dimensional latent space. We then train a neural network to map the latent variables representing the boundary data to the solution of the PDE. Finally, we solve the inverse problem by optimizing a data-fitting term over the latent space.\n\nWe analyze the underlying stabilized finite element method in the linear setting and establish optimal error estimates in theand-norms. The nonlinear problem is then studied numerically, demonstrating the effectiveness of our approach.\n\nSECTION: 1Introduction\n\nTechnological advances have led to measurement resolution and precision improvements, shifting the paradigm from data scarcity to abundance. While these data can potentially improve the reliability of computational predictions, it still needs to be determined how to consistently merge the data with physical models in the form of partial differential equations (PDE). In particular, if the PDE problem is ill-posed, as is typical for data assimilation problems, a delicate balancing problem of data accuracy and regularization strength has to be solved. If the data is inaccurate, the PDE problem requires strong regularization; however, if the data is accurate, such a strong regularization will destroy the accuracy of the approximation of the PDE. Another question is how to use different types of data. Some large data sets, consisting of historical data of events similar to the one under study, can be available. In contrast, a small set of measurements characterizes the particular realization we want to model computationally. In this case, the former data set measures the \u201cexperience\u201d of the physical phenomenon, while the latter gives information on the current event to be predicted.\n\nThis is the situation that we wish to address in the present work. The objective is to construct a computational method that combines machine learning techniques for the data handling parts and hybrid network/finite element methods for the approximation in physical space. First, the large data set is mapped to a lower dimensional manifold using an auto-encoder or some other technique for finding low dimensional structures such as singular value decomposition or manifold learning. Then, we train a network to reproduce the solution map from the lower dimensional set to the finite element space. Finally, this reduced order model solves a non-linear inverse problem under the a priori assumption that the solution resides in a neighborhood of the lower dimensional manifold.\n\nTo ensure an underpinning of the developed methods, we consider the case of a unique continuation problem for a non-linear elliptic operator. That is, given some interior measurement (or measurements on the part of the boundary), a solution is reconstructed despite lacking boundary data on the part of the boundary. Such problems are notoriously ill-posed, and using only the event data set, it is known that the accuracy of any approximation in the whole domain cannot be guaranteed due to the poor global stability[1]. Indeed, in general, stability is no better than logarithmic. This means that for perturbations of order, the error must be expected to be of orderwith. In interior subdomains stability is of H\u00f6lder type, meaning that the same perturbation gives rise to anerror. Computational methods can have, at best, rates that reflect this stability of the continuous problem[8]. A convenient a priori assumption is that the missing data of the approximate solution is in a-neighbourhood of a finite-dimensional space,, whereis the smallest distance from the solution toin some suitable topology. In this case, it is known that the stability is Lipschitz; that is, the problem has similar stability properties to a well-posed problem, and finite element methods can be designed with optimal convergence up to the data approximation error. For linear model problems discretized using piecewise affine finite element methods with mesh parameter, one can prove the error bound[9],\n\nHere,is a constant that depends on the dimension of the data set, the geometry of the available event data, and the smoothness of the exact solution. In particular,typically grows exponentially in.\n\nSince the size ofmust be kept down, there is a clear disadvantage in using the full large dataset. Indeed, forsufficiently large, the experience data will have no effect. Instead, we wish to identify a lower-dimensional structure in the high-dimensional dataset, a lower-dimensional manifold such that the data resides in a-neighbourhood of the manifold. For this task, one may use proper orthogonal decomposition in the linear case or neural network auto-encoders in the general case.\n\nIn the linear case, the data fitting problem reduces to a linear system; however, an ill-conditioned optimization problem has to be solved in the nonlinear case, leading to repeated solutions of linearized finite element systems. To improve the efficiency of this step, we propose to train a network to encode the data to an FE map, giving fast evaluation of finite element approximations without solving the finite element system in the optimization.\n\nThe approach is analyzed in the linear case with error estimates for a stabilized FEM using the reduced order model.\n\nWe prove that the inverse problem with boundary data in a finite-dimensional setis stable and design a method that reconstructs the solution using the reduced order basis with the same dimension as. We proved optimal error bounds in the-norm for this method, where the constant of the error bound grows exponentially with the dimension of.\n\nIn the situation where a large set of perturbed random data,, from the setis available, we develop a practical method for the solution of the severely ill-posed inverse problem of unique continuation, leveraging the large dataset to improve the stability properties. In order to handle non-linearity in the PDE-operator and data efficiently we adopt machine learning algorithms. The machine learning techniques are used for the following two subproblems:\n\nIdentification of the latent spacefromto find the smallest possible space for the inverse identification.\n\nConstruction of the solution map\n\nwhich approximates the Galerkin approximation of\n\nwhereis the non-linear PDE in question.\n\nThe performance of the combined finite element/machine learning approach is assessed against some academic data assimilation problems.\n\nThe inverse problem we consider herein is of unique continuation type. There are many types of methods for this type of problem. In the framework we consider the earliest works considered quasi-reversibility[3]. The stabilized method we consider for unique continuation was first proposed in[4,5]and[6]. More recent works use residual minimization in dual norm[11,13,7]. The optimal error estimates for unique continuation with a trace in a finite dimensional space was first considered for Dirichlet trace in[9]and for Neumann trace in[10]. The idea of combining unique continuation in finite dimensional space with collective data was first proposed in[19,2]using linear algebra methods for the compression and direct solution of the linear unique continuation problem. Low rank solvers for the solution of inverse problems have also been designed in[26]using proper orthogonal decomposition.\n\nIn recent years, significant advancements have been made in utilizing machine learning for solving PDEs and efficiently representing the solutions or deriving reduced order models[16,20,29,23,27,30]. These developments are very useful in the context of inverse problems, where they have been utilized in both data- and model-driven inverse problems[24,25,21,18,17,15,12].\n\nIn Section 2, we introduce the model problem and the finite element discretization; in Section 3, we present and prove stability and error estimates; in Section 4, we develop a machine learning-based approach for solving the inverse problem; in Section 5, we present several numerical examples illustrating the performance of the method for various complexity of the given\nset of boundary data; and in Section 6 we summarize our findings and discuss future research directions.\n\nSECTION: 2Inverse Problem and Finite Element Method\n\nSECTION: 2.1Inverse Problem\n\nLetbe a domain in,a subdomain, and consider the minimization problem\n\nwhereis a nonlinear second order differential operator andis an observation of the solution in the subdomain.\nNote that we do not have access to boundary conditions for the partial differential equation; we only know thatin, and\nthus, the problem is, in general, ill-posed.\n\nAssume that we have access to a dataset\n\nof observed Dirichlet data at the boundary. The datasetmay have different properties, but here we will assume\nthat is of the form\n\nwhereare bounded intervals and. Below we will also consider access to a finite setof samples from,\n\nIncludingas a constraint leads to\n\nA schematic illustration of a problem of form (2.5) is given in Figure1.\n\nSECTION: 2.2Finite Element Method\n\nLetbe a finite element space on a quasi-uniform partitionofinto shape regular elements with mesh\nparameterand assume that there is an interpolation operatorand a constant such that for all,\n\nfor. Hereis the union of all elements that share a node with.\n\nThe finite element discretization of (2.5) takes the form\n\nwhere.\n\nSECTION: 3Analysis for a Linear Model Problem\n\nIn this section, we present theoretical results for a linear model problem. We show that the finite dimensionality leads to a well-posed continuous problem, which may, however, have insufficient stability that may cause problems in the corresponding discrete problem. We, therefore, introduce a stabilized formulation that retains the stability properties from the continuous problem, and then we prove error estimates.\n\nSECTION: 3.1The Continuous Problem\n\nConsider the linear model problem\n\nwhereand\n\nwhere the functionsare linearly independent\non. Then with\n\nwe may expressas the linear combination\n\nwhereis the coefficient vector. The inverse problem (2.1) is then equivalent to\ncomputing the-projection ofon,\n\nThis is a finite-dimensional problem, and therefore, existence follows from uniqueness.\nTo prove uniqueness consider two solutionsand, we then have\n\nand takinggives\n\nBy unique continuation for harmonic functions, we conclude thatis zero on the boundary and\nthereforesince the setis linearly independent on. It follows thatis linearly independent onand by\nfinite dimensionality, there is a constant such that\n\nNote, however, that the constant may be huge, reflecting the often near\nill-posed nature of an inverse problem.\n\nSECTION: 3.2The Discrete Problem\n\nIn practice, only an approximation of the basisis available,\nsince we observe data on the boundary and must solve for an approximate basis.\nAssuming that we compute an approximate basisusing\nNitsche\u2019s method with continuous piecewise linears, defined on a triangulationof,\n\nwhere the forms are defined by\n\nwiththe given Dirichlet data on, we have the error estimates\n\nprovided the regularity estimateholds, which is the case for convex or smooth domains.\n\nNext, we define the operators\n\nto represent linear combinations given coefficient vectors. By composingandwith the coefficient extraction operator, we note thatforandfor. We also note thatis the Galerkin\napproximation defined by (3.9) of, sinceis the Galerkin approximation\noffor, and we have the error estimate\n\nThe estimate (3.15) follows directly using the Cauchy-Schwarz inequality and\nthe error estimates (3.12) for the approximate basis\n\nwith.\n\nNow if we proceed as in (3.5) with the modesreplaced by the approximate\nmodes, we can not directly use the same argument as in the continuous case to show that there\nis a unique solution since the discrete method does not possess the unique continuation property, and it doesn\u2019t\nappear easy to quantify how small the mesh size must be to guarantee that the bound (3.8) holds\nonas discussed in Remark3.1.\n\nNote that the constant in (3.8) is\ncharacterized by the Rayleigh quotient\n\nand for the corresponding discrete estimate\n\nwe instead have the constant\n\nUsing the triangle inequality and the error estimate (3.12) we have\n\nand thus we may conclude that\n\nforwithsmall enough. Thus forsmall enough the discrete bound (3.19) holds but we note that the\nprecise characterization of how smallhas to be appears difficult.\n\nTo handle this difficulty, let us instead consider the stabilized form\n\nwhere\n\ncontrols the error at the boundary, andis the standard\nnormal gradient jump penalty term\n\nwhereis the interior faces in the mesh. In the implementation, we estimate\nthe-norm by\n\nwhere, withthe unit normal tois the tangent derivative at.\n\nSECTION: 3.3Error Estimates\n\nOur first result is that the additional stabilization terms inensure that we have stability for the discrete problem\nsimilar to (3.8) that holds for the exact problem.\n\nLetbe defined by (3.23). Then, there is a constant such that\n\nProof.Forwe get by using the stability (3.8) on, adding\nand subtracting, and employing the triangle inequality,\n\nwhere we finally used the identity, which holds since. Next, we bound the\nthe second term using the stabilizing terms in. To that end, we observe that we have the orthogonality\n\nsince the discrete basis is, a Galerkin projection (3.9) of the exact basis with respect to the Nitsche form.\nUsing the dual problem\n\nwe obtain\n\nwhereis the interpolation operator and we used the standard trace inequalityforon element.\nFinally, using the elliptic regularity, we get\n\nwhich combined with (3.28) directly gives the desired estimate.\n\u220e\n\nDefine the stabilized projection,\n\nWe then have the following error estimate for the stabilized projection with approximate basis functions.\n\nLetbe defined by (3.5) andbe defined by\n(3.37). Then, there is a constant such that,\n\nProof of Proposition3.1.Using the triangle inequality\n\nHere the first term can be directly estimated using (3.15),\n\nsince forwe haveand using the stability estimate (3.8) followed by\n(3.5) we get\n\nFor the second term, we first note that the stabilization termsandvanish onso that\n\nWe then have for any,\n\nwhere we used (3.42) in (3.42); the definition (3.5)\nofto subtractin (3.46); the stability (3.27);\nthe boundsand.\nThus, we conclude that\n\nwhich combined with (3.39) and (3.40) concludes the proof.\n\u220e\n\nWe finally prove the following global result,\n\nLetbe defined by (3.5) andbe defined by\n(3.37). Then, there is a constant depending on higher order Sobolev spaces ofsuch that,\n\nProof.With, we have\n\nBy norm equivalence on discrete spaces we have\n\nSincethere holds using (3.8),\n\nBy Proposition3.1there holds\n\nFor the second term we have using (3.15),\n\nSimilarly we have\n\nWe conclude the proof by using the bound\n\n\u220e\n\nObserve that the stabilization is never explicitly used in order to obtain error estimates. Indeed its only role is to ensure the boundwithout condition on the mesh. On the other hand, due to the stabilization, it appears hard to obtain-error estimates. If indeed the mesh is so fine that the optimization problem is well posed without stabilization, then we obtain error bounds in a similar fashion, both for the-norm and the-norm.\n\nLetbe the unit disc. Then the solutions toare of the form\n\nwhereare the standard polar coordinates. Letbe the disc centered at the origin with radius. We note that whenbecomes large, the modes become small in the disc, and therefore,\nthe inverse problem becomes increasingly ill-posed, for instance, the constant in an estimate of the type\n\nscales like\n\nand thus becomes arbitrarily large whenbecomes large. But, if we, from observations, can conclude that only modes withfor someare present, then the stability is controlled. Note also that the stability is directly related to where the discis placed. If it is located close to the boundary, the stability improves.\n\nSECTION: 4Methods Based on Machine Learning\n\nWe develop a method for efficiently solving the inverse problem (2.5) with access to sampled datausing machine learning techniques. The main approach is:\n\nConstruct a parametrization of the data set by first approximately expanding the samples in a finite series of functions, for instance, using Proper Orthogonal Decomposition, and secondly using an autoencoder to find a possible nonlinear low dimensional structure in the coefficients.\n\nUse operator learning to construct an approximation of the finite element solution operator that maps the expansion coefficients to the finite element solution.\n\nComposing the decoder, which maps the latent space to expansion coefficients with the solution network, we obtain a differentiable mapping that can be used to solve the inverse problem efficiently.\n\nSECTION: 4.1Processing the Boundary Data\n\nTo assimilate the data setin a method for solving the extension problem, we seek to construct a differentiable parametrization of. To that end, we first use Proper Orthogonal Decomposition (POD) to represent the data in a POD\nbasis,\n\nwhere. We introduce the mapping\n\nwhere. We also need the reconstruction operator\n\nWe have\n\nand we note that the operatoris invertible\nand differentiable.\n\nNext, we seek to find a possible nonlinear low dimensional structure in the POD coefficients using an autoencoder\n\ntrained to minimize the loss\n\nSee Figure2(a)for a schematic illustration.\nHereis the latent space with dimension. If there is a low dimensional structure, we may often takesignificantly lower than.\n\nSECTION: 4.2Operator Learning\n\nNext, we discretize the problem using finite elements\nand train a network\n\nwhich approximates the Galerkin approximation of\n\nsee Figure2(b). The output of the network is the finite element degrees of freedom (DoFs). For the training of the network we use the energy functionalcorresponding to the differential operatoras the foundation for the loss function. Lettingdenote the expectation operator andan arbitrary probability distribution, the loss function that we minimize during training is\n\nIf there is no corresponding energy functional, one can instead minimize the residual of the finite element problem. It should be noted though, that assembling the residual instead of the energy has a greater computational cost and that the residual is not as easily and naturally decomposed into its local contributions as the energy. For technical details about network architecture and training used in this work, we refer to the example section5.2.\n\nSECTION: 4.3Inverse Problem\n\nFinally, composing the maps, we get a solution operator\n\nthat maps the latent space into approximate finite element solutions to the partial differential equation\n\nsee Figure2(c).\n\nThis mapping is differentiable and can be directly used to\nrewrite the optimization problem as an unconstrained problem\nin the form\n\nwhere we note that the constraint is fulfilled by construction.\n\nSECTION: 5Examples\n\nWe consider three examples of the inverse minimization problem ordered in increased nonlinearity. The first is a fully linear case with a linear differential operator and linear boundary data. In the second example, we consider a nonlinear operator with linear data. The final example is a fully nonlinear case with both operator and data being nonlinear. The examples demonstrate how each introduced nonlinearity may be treated with machine learning methods.\n\nThe implementation used for the examples is based on the code presented in[28]which is publicly available athttps://github.com/nmwsharp/neural-physics-subspaces. The GPU computations were performed on the Alvis cluster provided by NAISS (See Acknowledgements).\n\nSECTION: 5.1Linear Operator with Linear Data\n\nWe start with the fully linear case which we will build upon in the later examples. To construct a linear synthetic data set, we may pick a\nset of functionsand consider\n\nwhere. Note that we require the boundary data to be bounded. Alternatively, we can also consider taking the\nconvex hull of the basis functions, which corresponds to requiring that\n\nGiven nodal samples of such functions, we may apply principal component analysis (PCA) to estimate a set of basis functions and use them to\nparametrize the data set. More precisely, assume we observe the boundary\ndata in the nodal points at the boundary. Letbe the matrix where each observation forms a row. Then, computing the eigenvectors to the symmetric\nmatrixprovides estimates of the basis.\n\nHere, we consider two-dimensional examples. We letbe the unit square centered at the origin and generate four structured uniform triangular meshes of varying sizes: 10x10, 28x28, 82x82, and 244x244. The synthetic data set of boundary nodal values is in turn generated from the perturbed truncated Fourier series\n\nwhereis the circumference ofandis the counter-clockwise distance along the boundary starting from the point where the boundary crosses the first coordinate axis. We sample unperturbed coefficientsand perturbations. For each of the four meshes, we consider two values of the number of coefficients used to describe the boundary conditions;and. We generate 1000 functions of the type (5.3) for each of the eight cases. Then, for every case, we compute a POD basisfor the boundary using PCA on the data set. Unsurprisingly, the number of significant singular values turns out to be the numberused in each case.\n\nWe use the POD boundary basis to compute an interior basis by solving Laplace\u2019s equation with the finite element method (FEM). We take the discrete spaceto simply be the space of piecewise linear finite elements on the triangle mesh considered. The FEM interior basisis computed by: For each, findsuch thatand\n\nIn Figure3, the significant POD boundary basis functions, together with their corresponding FEM interior basis functions, are presented for the case withand the 82x82 mesh.\n\nWe may now use the fact that Laplace\u2019s equation is linear to superpose the FEM interior basis functions in a linear combination. This enables us to solve a linear inverse minimization problem over the coefficients in the linear combination. We present a demonstration of this process for the case withand the 82x82 mesh in Figure4.\n\nSECTION: 5.2Nonlinear Operator with Linear Data\n\nWe again consider the linear data sets from the previous section, but here together with anonlineardifferential operator. Because of the nonlinearity, we cannot use the FEM interior basis and the superposition principle as in the fully linear case. Instead, we use a neural network to approximate the solution operator, i.e., the inverse of the nonlinear differential operator. The solution is still in the form of a finite element function, so the output of the network gives an approximation of the finite element solution. The input to the network is POD-coefficients corresponding to the same POD boundary basis functions as in the linear case in the previous section. We use the following nonlinear energy functional as the foundation for the loss function during training of the network.\n\nThis functional corresponds to the nonlinear differential operator whose inverse (the solution operator) we want to approximate with the neural network. We use a simple multilayer perceptron (MLP) network architecture with 4 hidden layers of the same width X and an output layer of width O representing the finite element DoFs. For standard P1 elements considered here it is simply the finite element function\u2019s nodal values. We use the exponential linear unit (ELU) as the activation function in the 4 hidden layers and no activation function in the last layer. A schematic illustration of this network is provided in Figure2(b).\n\nIn each iteration during the training, we pick a fixed number (referred to as the batch size) of randomly selected coefficient vectors and use them to compute an average loss. The coefficient values are picked from. The optimization is performed with the Adam optimizer where we performiterations with a decreasing learning rate. The learning rate starts at 1e-4, and after every 250k iterations, it is decreased by a factor of\u00a00.5.\n\nTo measure the well-trainedness of the network, we, as an initial guiding measure, use the zero energy, i.e., the value of the computed energy using the output from the network when an all zero vector is given as input. This, of course, corresponds to homogeneous Dirichlet boundary conditions and gives that the solutionand thus that. We also perform more rigorous studies of well-trainedness by computing the actual finite element solution with FEniCS[22]and comparing it to the network approximation. This is done by computing their average norm difference over 1000 problems, where for each problem we randomly select a coefficient vector with values from. The difference is computed in both the-norm (-seminorm) and the-norm. We also compute both the absolute and the relative norm differences, where the relative norm difference is the absolute difference divided by the norm of the finite element solution.\n\nFor the numerical examples we have again considered the two different coefficient vector lengths (9 and 21) and the four meshes from the linear case in the previous section. The network architectures and batch sizes used during training are given in Table1.\n\nThe hyperparameter values for the problem sizes have been obtained by trial and error. In Table2, we present training info for the four mesh sizes for coefficient vector lengthand. The training has been performed on a single A100 GPU. For the largest mesh case (244x244), we have not been able to train with all elements present in the energy functional loss function (It has resulted in a NaN loss function value). To make it work, we have employed the trick of randomly selecting a fixed number of elements for every input vector during training, and only considering the energy functional contribution from those elements. The number of elements used is denoted \u201cEls\u201d in Table2.\n\nWith these neural networks we may solve the inverse minimization problem over the coefficient space. In Figure5, a demonstration of this process is presented for the case of 21 input coefficients and the 244x244 mesh, i.e., the neural network whose training info is presented in the last row of Table2(b).\n\nSECTION: 5.3Nonlinear Operator with Nonlinear Data\n\nWe consider the same nonlinear differential operator with the same neural networks as in the previous section but here we add complexity by introducing an underlying nonlinear dependence on the input coefficients to the network. To construct such a nonlinear dependence we may pick a smooth function, whereis a parameter domain inand\nconsider boundary data of the form\n\nwhereis some small probabilistic noise andis a set of samples from the parameter spaceequipped with a probability measure. In this case, we expect an autoencoder with a latent spaceof at least the same dimension asto perform well.\n\nWe consider a simple polynomial example where the coefficientsdepend on the parameter variablesas\n\nHere the matricesand their entries are randomly sampled from a uniform distribution. The perturbationsare sampled from a normal distribution.\n\nFor the numerical results we take,and sample matrix entries fromwhich are then held fixed. To generate coefficient vectors, we sample parameter variablesand perturbations. We consider two cases: the linear case withand the quadratic case with. To get a sense of what the data look like, we plot the coefficients as functions of the parameters for both cases in Figure6.\n\nWe analyze data generated for the linear case with PCA and data generated for the quadratic case with both PCA and autoencoders. The results are shown in Figure7.\n\nWe consider a more advanced nonlinear example where the coefficientsdepend on the parameter variablesas\n\nHere we havenumber of equidistant Gaussian bell curves indexed bywhere each coefficient is assigned exactly one bell curve with midpointand exactly one parameteraccording toand, respectively. The perturbationsare sampled from a normal distribution.\n\nFor the numerical results we takeand sample perturbations. We consider four cases:\n\nwithand\n\nwithand\n\nwithand\n\nwithand\n\nThe coefficients, PCA results and autoencoder results are shown in Figure8. For the autoencoder results we have used MLPs with 5 layers with the middle layer being the latent layer. The latent layer width has been varied and the hidden layer widths have all been fixed at 64. The activation function ELU has been applied to all layers except the last.\n\nFrom the Gaussian data examples presented in Figure8we note something interesting. If the number of bell curvesis divisible by the latent dimension, the PCA suggests that the underlying structure has dimension. Ifisnotdivisible by, the PCA instead suggests that this dimension is. In the third example with results presented in Figure8(c), we have. Here the PCA suggests that the underlying dimension is 21, whereas the corresponding autoencoder study suggests that a reduction down to 9 dimensions could provide the same improvement as a reduction down to 17.\n\nIn light of the above, we may take the autoencoder with latent layer width = 9 from this case and connect its decoder to the input of the operator network for the 244x244 mesh with 21 input coefficients. We may thus solve the inverse minimization problem over a 9 dimensional latent space instead of a 21 dimensional coefficient space. We present a demonstration of this process in Figure9.\n\nSECTION: 6Conclusions\n\nThe regularization of severely ill-posed inverse problems using large data sets and stabilized finite element methods was considered and shown to be feasible both for linear and non-linear problems. In the linear case, a fairly complete theory for the approach exists, and herein, we complemented previous work with the design and analysis of a reduced-order model. In the linear case, a combination of POD for the data reduction and reduced model method for the PDE-solution was shown to be a rigorous and robust approach that effectively can improve stability from logarithmic to linear in the case where the data is drawn from some finite dimensional space of moderate dimension. To extend the ideas to non-linear problems we introduced a machine learning framework, both for the data compression and the reduced model. After successful training, this resulted in a very efficient method for the solution of the non-linear inverse problem. The main observations were the following:\n\nThe combination of analysis of the inverse problem, numerical analysis of finite element reconstruction methods, and data compression techniques allows for the design of robust and accurate methods in the linear case.\n\nMeasured data can be used to improve stability, provided a latent data set of moderate size can be extracted from the data cloud.\n\nMachine learning can be used to leverage the observations in the linear case to non-linear inverse problems and data assimilation and results in fast and stable reconstruction methods.\n\nThe main open questions are related to how the accuracy of the machine learning approach can be assessed and controlled through network design and training. For recent work in this direction, we refer to[14].\n\nThis research was supported in part by the Swedish Research\nCouncil Grants No. \u00a02021-04925, and the Swedish\nResearch Programme Essence. EB acknowledges funding from EP/T033126/1 and EP/V050400/1.\n\nThe GPU computations were enabled by resources provided by the National Academic Infrastructure for Supercomputing in Sweden (NAISS), partially funded by the Swedish Research Council through grant agreement no. 2022-06725.\n\nSECTION: References\n\nAuthors\u2019 addresses:\n\nErik Burman, \u2003 Mathematics, University College London, UKe.burman@ucl.ac.uk\n\nMats G. Larson, \u2003 Mathematics and Mathematical Statistics, Ume\u00e5\u00a0University, Swedenmats.larson@umu.se\n\nKarl Larsson, \u2003 Mathematics and Mathematical Statistics, Ume\u00e5\u00a0University, Swedenkarl.larsson@umu.se\n\nCarl Lundholm, \u2003 Mathematics and Mathematical Statistics, Ume\u00e5\u00a0University, Swedencarl.lundholm@umu.se", "text_file": "data\\paper_texts\\2412.04409v1_content.txt"}, {"title": "Reversible molecular simulation for training classical and machine\n  learning force fields", "authors": ["Joe G Greener"], "published_date": "2024-12-05T17:49:44Z", "summary": "The next generation of force fields for molecular dynamics will be developed\nusing a wealth of data. Training systematically with experimental data remains\na challenge, however, especially for machine learning potentials.\nDifferentiable molecular simulation calculates gradients of observables with\nrespect to parameters through molecular dynamics trajectories. Here we improve\nthis approach by explicitly calculating gradients using a reverse-time\nsimulation with effectively constant memory cost. The method is applied to\nlearn all-atom water and gas diffusion models with different functional forms,\nand to train a machine learning potential for diamond from scratch. Comparison\nto ensemble reweighting indicates that reversible simulation can provide more\naccurate gradients and train to match time-dependent observables.", "arxiv_id": "2412.04374v1", "html_link": "https://arxiv.org/html/2412.04374v1", "search_term": "ti:\"machine learning\"", "html_content": "SECTION: Reversible molecular simulation for training classical and machine learning force fields\n\nThe next generation of force fields for molecular dynamics will be developed using a wealth of data.\nTraining systematically with experimental data remains a challenge, however, especially for machine learning potentials.\nDifferentiable molecular simulation calculates gradients of observables with respect to parameters through molecular dynamics trajectories.\nHere we improve this approach by explicitly calculating gradients using a reverse-time simulation with effectively constant memory cost.\nThe method is applied to learn all-atom water and gas diffusion models with different functional forms, and to train a machine learning potential for diamond from scratch.\nComparison to ensemble reweighting indicates that reversible simulation can provide more accurate gradients and train to match time-dependent observables.\n\nSECTION: Introduction\n\nMolecular dynamics (MD) simulations have given us insight into how atoms move, from biomolecules to materials[1].\nKey to the accuracy of a MD simulation is the accuracy of the force field used to describe how the atoms interact.\nFor classical molecular mechanics, force field development has largely been manual with parameters tuned to give the best possible match to quantum mechanical (QM) data (bottom-up) and condensed phase properties (top-down)[2,3].\nThere have been automated approaches, including ensemble reweighting methods[4,5,6,7]like the popular ForceBalance[8,9,10], and graph neural networks to avoid discrete atom typing[11], but much work is still done manually[12].\nThe recently emerging and promising machine learning interatomic potentials (MLIPs)[13,14]are typically trained bottom-up on QM data alone[15], though this can give a distorted view of the utility of these models[16].\nWhilst MLIPs can be validated on other data[17], using non-QM data during training has proved challenging.\nThis puts a heavy emphasis on generating large and diverse QM datasets and neglects other available data.\n\nOne approach to training force fields with experimental data is differentiable molecular simulation (DMS), in which automatic differentiation (AD)[18]is used to obtain the gradients of a loss value with respect to the parameters over a simulation.\nThis has had a number of recent applications[19,20,21,22,23,24,25,26,27,28,29,30]with dedicated software available[31,32,33,34,25].\nIt is appealing due to the variety of possible loss functions and because the gradients are exact with respect to the forward simulation.\nThere are, however, three main problems with DMS.\nFirstly, the memory required is linear in the number of simulation steps meaning that gradient checkpointing is required for longer simulations and that larger neural networks may be incompatible.\nSecondly, performance is considerably slower than standard simulation due to the overhead of reverse mode AD (RAD).\nFinally, the gradients are prone to explosion due to the numerical integration.\nDespite this, DMS holds promise particularly for training on time-dependent observables where ensemble reweighting approaches are not generally applicable[35].\nExamples of these include diffusion coefficients, autocorrelation functions, relaxation rates, thermal conductivity and reaction rates, where available data is challenging to use during training.\n\nHere we take inspiration from reversible differential equation solvers[36,37]and reversible neural networks[38,39]and ask if DMS can be done without storing intermediate states, i.e.\u00a0by explicitly deriving gradients rather than using conventional AD.\nThis is motivated by three features of molecular simulations: they consist of the same step repeated many times, the algorithm does not contain branching, and they are reversible in certain situations.\nWe find that identical gradients to DMS with RAD can be obtained with effectively constant memory cost and a computation count comparable to standard simulation, and explore gradient truncation as a way to avoid gradient explosion.\nThis reversible simulation approach is demonstrated with three examples: learning molecular mechanics water models with different functional forms, training to match gas diffusion data, and learning a MLIP for diamond from scratch.\n\nSECTION: Results\n\nA molecular simulation is run using a force field with parameters.\nWe wish to improveto better match experimental data.\nWhilst it is possible to do this using gradient-free approaches, this scales poorly with parameter number and both molecular mechanics force fields and MLIPs can have thousands or more parameters.\nConsequently, we wish to calculatewhere the loss functionrepresents the match of the simulation to experiment.\nExisting gradient-based approaches to parameterise force fields are summarised in Table1and Figure1A.\n\nHere we show (see the Methods) that:\n\nwhereare the coordinates at step,are the forces on each atom at step,is the force function,is the snapshot step, and the angle brackets represent the average over snapshots of the simulation.can be calculated at each time step.\nBy calculating a series of intermediate values,can be accumulated by stepping back in time.\nThis equates to the same operations as DMS with RAD but coded explicitly, and requires running the simulation back in time, hence the name reversible simulation.\n\nArbitrary trajectories back in time will diverge in the NVT (canonical) ensemble, hence an initial simulation forwards in time must be run for the length of the simulation to ensure we obtain a valid trajectory.\nGiven the tendency of the reverse-time integrator to gradually diverge over time from the corresponding forward simulation due to not being bitwise reversible[42,43], snapshots also need to be stored every 1 ps to reset the coordinates and velocities.\nApart from this storage, which is cheap, the method is constant in memory for any simulation length.\nConceptually it is similar to the adjoint method[41,30], with a comparison in the Methods, though the adjoint method solves a different equation back in time[44,45,46,47].\n\nTo test reversible simulation and compare to ensemble reweighting, we train a 3-point molecular mechanics water model to match experimental data.\nParameterising water models is a common challenge where the fit to various properties has to be balanced.\nIn this case enthalpy of vapourisation and radial distribution function (RDF)[50]data were used as a proof of principle, though other studies have used more properties[8].\nStarting from the popular TIP3P water model[51]we train the Lennard-Jonesandparameters, the partial charge on oxygen (and hence on hydrogen, since the overall molecular charge is zero), and the equilibrium values and force constants for the harmonic bonds and angles.\nAs can be seen in Figure1B, the gradients are numerically identical to DMS with RAD for small step numbers as expected.\nThe gradients from reversible simulation correlate surprisingly well with those from ensemble reweighting, which are compared to in Figure1C.\nIt is encouraging that these two distinct approaches give similar gradients.\nThe gradients vary much less for reversible simulation over repeats with different random seeds used for the thermostat (Figure1D).\nThis is possibly due to the increased number of steps contributing to the gradient as discussed in the Supplementary Methods.\nPlotting the loss values against the gradients shows that the loss is minimised when the gradient is zero, indicating that the gradients are accurate and that optimising with the gradients will minimise the loss (Figure1E).\nThe correlation of loss to gradient magnitude is better for reversible simulation, suggesting that it may provide a smoother optimisation surface.\n\nAs shown in Figure2A both reversible simulation and ensemble reweighting provide gradients that improve the match to experiment for the chosen properties over training with simulations of 50 ps using a box of 895 water molecules.\nThey follow similar optimisation pathways through parameter space, shown in Figure2B for two parameters, with reversible simulation taking steps in a more consistent direction than ensemble reweighting as suggested by Figure1E.\nLonger validation simulations with the learned potentials show an improved match to the enthalpy of vapourisation across multiple temperatures and to the RDF (Figure3), though ensemble reweighting does not match the enthalpy of vapourisation as well.\nOther properties are also shown.\nThe match to density is made worse as it was not used during training, though the match to the self-diffusion coefficient is improved.\nRather than fit to all available properties here, we aim to demonstrate that reversible simulation is able to match chosen experimental properties for all-atom models.\n\nSince reversible simulation is independent of the functional form used to calculate the forces, we also demonstrate that it can optimise parameters for other functional forms of the non-charge non-bonded potential.\nThe double exponential, Buckingham and Lennard-Jones soft core potentials have all been proposed as improvements over the Lennard-Jones potential, in which the repulsion term is not physically motivated.\nBy starting from sensible parameters and training on the same properties as before, parameters can be learned that better fit the experimental data.\nAs can be seen in Figure2C-D and Figure3these flexible functional forms give potentials of a similar shape with the learned parameters and are able to match the enthalpy of vapourisation and RDF well.\nThis indicates that reversible simulation could be useful in developing the next generation of force fields that go beyond Lennard-Jones.\n\nAs discussed in the Methods, the run time of reversible simulation is similar to that of the forward simulation if the required gradients can be calculated explicitly.\nFor water training the run time was 2.7 ms per simulation step on CPU for Lennard-Jones, compared to 2.3 ms for a single forward step.\nIn comparison the run time of OpenMM on the same system was 1.2 ms per step on CPU for a standard simulation, so reversible simulation can approach the simulation speed of mature software.\nOptimisation for GPU is left to further work.\nThe alternative functional forms add less than 10% to the run time of Lennard-Jones.\n\nGiven that ensemble reweighting gives similar gradients to reversible simulation (Figure1C) and is often easier to set up, it will be the preferred choice for many properties of interest.\nHowever, reversible simulation is distinguished by its ability to target time-dependent properties.\nHere we show how this can be useful by learning parameters that match the experimental diffusion coefficientof the oxygen diatomic molecule in water.\nFor Lennard-Jones we use TIP3P starting parameters for the water and oxygen parameters from Wang et al.\u00a02021[53].\nBy training on simulations of 50 ps with 10 oxygen molecules randomly placed in 885 water molecules and calculatingusing the slope of the mean squared displacement (MSD) against time, reversible simulation can learn parameters that reproduce the experimental value ofm2s-1forfrom a starting value ofm2s-1(Figure4A-B).\n\nSimilar to the water models discussed previously, we learn parameters for alternative functional forms.\nThese are also able to reproduce the experimental value of, indicating that reversible simulation can train to match time-dependent properties for a variety of functional forms.\nLonger simulations with the learned parameters reproduce improvedvalues, as shown in Figure4B.\n\nThe water molecules described above have fewer than 10 parameters each.\nIn order to demonstrate that reversible simulation can train neural networks with many more parameters from scratch, we train the MLIP model for diamond used in Thaler and Zavadlav 2021[5]on GPU using the experimental elastic stiffness tensor.\nThe model consists of a Stillinger-Weber prior with starting parameters for silicon[54]and the DimeNet++ neural network[55].\nThe virial stress tensor and stiffness tensor calculated via the stress fluctuation method were used to define the loss function, with only three distinct stiffness moduli in the stiffness tensor due to symmetries in the diamond cubic crystal.\nAll parameters of the model were trained over increasing numbers of simulation steps of 1000 carbon atoms, with 1 ps of simulation used by the end of training.\nThis was sufficient to train the model, as shown in Figure4C-D.\nThe learned model maintains a low loss over longer 100 ps validation simulations, indicating stability, with stress and stiffness values showing good agreement with the target values.\nThe DimeNet++ model used has 121,542 parameters, demonstrating that reversible simulation can effectively train models with large numbers of parameters.\nAs long as one force evaluation can fit in memory, reversible simulation should be applicable to even larger models whereas DMS would struggle even with gradient checkpointing.\n\nSECTION: Discussion\n\nThe number of computations required to calculate gradients with reversible simulation is similar to that of a standard simulation due to gradient truncation, as described in the Methods.\nIn addition, reversible simulation uses effectively constant memory for any number of simulation steps, is applicable to many loss functions and gives accurate gradients that numerically match those from the forward simulation, unlike the adjoint method.\nThe high degree of control over the gradients, not available in general with AD, means that gradient truncation can be easily implemented.\nThese improvements over DMS should make it applicable to larger systems and systems where the potential has a significant memory cost such as MLIPs.\nThe ability to train three different systems using the Adam optimiser with gradients from reversible simulation shows its wide applicability.\n\nOne drawback of the method is that it requires implementing the algorithm whereas ensemble reweighting can largely make use of existing software.\nHowever, implementing the algorithm is not particularly difficult and can mostly be achieved using fast components of existing software.\nAnother drawback is that the loss and force functions need to be differentiable with respect to the atomic coordinates, which can be challenging for losses such as density.\nSecond order AD may be required to calculate the force gradients for MLIPs, but this is supported in many frameworks.\nSome loss functions involving combinations of averages are also hard to implement with reversible simulation.\n\nRecent work has used neural networks for continuous atom typing[11].\nThese methods could be trained end-to-end with reversible simulation to target condensed phase properties.\nIt should also be possible to train on binding free energy data directly[33,56]with reversible simulation by differentiating through the appropriate estimator.\nOne surprise from this work is the similarity between gradients arising from reversible simulation and ensemble reweighting.\nThis is encouraging given that they are computed in different ways.\nFor many applications, ensemble-based approaches are sufficient.\nHowever, reversible simulation allows time-dependent properties to be targeted and here gives gradients with less variance.\nIt could be used in combination with ensemble reweighting to target multiple properties, alongside force matching to QM data[57].\nA variety of approaches will be important for training the next generation of molecular mechanics force fields, MLIPs, and everything in-between[58].\n\nSECTION: Methods\n\nConsider the widely used Langevin integrator for running molecular simulations in the NVT (canonical) ensemble:\n\nwhere foratoms at step,are the atomic coordinates,are the velocities,are the accelerations,are the masses,is the force function arising from the interaction potential,are the force field parameters,is the collision frequency,is the Boltzmann constant,is the temperature andis a stationary Gaussian process with zero-mean.\nOne popular implementation is the Langevin middle integrator from OpenMM[59,60], which has been used successfully for DMS[25].\nThe integration step at stepfor this integrator is:\n\nwhereare the forces arising from the interaction potential,is the time step,are random velocities generated from the Boltzmann distribution at temperatureeach step and\u2032denotes intermediate computation values.\nThe velocities are offset by half a time step from the coordinates.\nIf the match to experiment after a simulation ofsteps is represented by a loss functionthen according to the multi-variable chain rule:\n\nsinceonly appears inandduring the integration step (Equation\u00a02).\nIn the case that multiple snapshots contribute to the loss, then:\n\nwhereis the step number of the snapshot and the angle brackets represent the average over the snapshots.can be calculated at the point of calculating.can be calculated each step, shown in the Supplementary Methods for the example of the Lennard-Jones potential, meaning that the challenge is to calculate theterms.\nThis can be rewritten:\n\nThe terms can be derived using Symbolics.jl[61]from an unrolled simulation (see the Supplementary Methods).\nThe first two terms are:\n\nNoting thataccumulates terms for each step backwards in time, this suggests an efficient approach to calculatingby running a reverse-time simulation.\nThis is mathematically equivalent to RAD.\nThe concept is similar to using a reversible differential equation solver[36,37]and reversible neural networks[38,39], with a discussion in Section 5.3.2 of Kidger 2021[36].\nFor the Langevin middle integrator, the time step is reversible provided that the random velocities from the previous step,, are known:\n\nNote that this integrator is not bitwise reversible[42,43]since the order of floating point operations is different to the forward step.\nConsequently, coordinates and velocities are stored every 1 ps and reset during the reverse simulation to prevent drift.\nThis incurs a small memory cost proportional to the number of simulation steps.\nA series of accumulation vectors is required to update.\nThe starting values at stepare:\n\nAt each time step, the accumulation vectors,and the growingare updated:\n\nwhereis the contribution tofrom stepandis the contribution tofrom all steps fromto.\nThere are two gradient calls, in lines 3 and 4 of Equation\u00a05.\nThese are vector-Jacobian products, as expected for an equivalent scheme to RAD, and consequently are efficient to compute via AD[18].\nFor the simple functional forms of molecular mechanics potentials they can be coded explicitly, and hence AD is not required at all.\nThis is shown for the Lennard-Jones potential in the Supplementary Methods.\nFor MLIPs that compute potential energy and use AD to calculate the forces, second order AD can usually be used to calculate the two required gradients.\n\nWhilst this form of the algorithm is specific to the Langevin middle integrator, the leapfrog Verlet integrator corresponds to the special case whereps-1.\nIn this case the leading bracketed term inincreases to 2, 4, 6, 8, etc.\u00a0as further steps are taken back in time (Equation\u00a04).\nThis demonstrates what is known practically[19,62,63], that gradients can explode even for a stable forward simulation.\nFor typical values ofps-1andfs the leading terms increase to 1.999, 3.996, 5.991, 7.984, etc., so gradient explosion is still a problem.\nThis motivates the use of gradient truncation[64,30], whereis not accumulated beyond a certain number of reverse steps.\nHere truncation was found to give more accurate gradients than gradient norm clipping[65,25].\nThe effect of gradient truncation on the accuracy of gradients is shown in FigureS1.\nTruncation after 200 steps was used throughout the results as it appears to balance preventing gradient explosion with using information from as many steps as possible.\nAs described below, truncation also increases the speed of reversible simulation since reversible steps only need to be carried out whilst gradients are being accumulated.\nSteps can be skipped by loading from the stored coordinates and velocities.\n\nSo far we have considered that the loss depends only on the coordinates and velocities at one point in time.\nOne advantage of reversible simulation over ensemble reweighting is that the loss value can take in multiple time points, for example to calculate diffusion coefficients.\nIn this case, additional terms are added to Equation\u00a03 and calculated with a different set of accumulation values.\nTruncation is applied separately for each.\nThe ability to control the gradients explicitly at every step is useful for allowing gradient truncation for losses that consider multiple time points, which would be challenging with AD software.\n\nBy carrying out the gradient calculation this way we have alleviated the problems with using RAD for DMS.\nThe memory cost is reduced, and hence no gradient checkpointing is required, since no intermediate values apart from the vectors in Equation\u00a05 and occasional coordinate and velocity copies need to be stored.\nThe typical 5-10x compute overhead of RAD is reduced since we code everything explicitly.\nThe calculation ofandeach step typically takes a similar amount of time to the calculation of, suggesting a slowdown of around 3x over the forward simulation, though for molecular mechanics force fields it is often possible to share calculations when computing the three values explicitly as shown in the Supplementary Methods.\nIn the absence of gradient truncation, the cost is one forward simulation followed by the reverse simulation consisting of one standard and two RAD calls to the force function.\nHowever truncating every 200 steps, in addition to preventing gradient explosion, means that the reverse simulation only needs to take a fraction of the steps of the forward simulation depending on how often snapshots contribute to the loss.\nWhen training the water model snapshots are taken every 2000 steps, so reversible simulation only needs to be done for a tenth of steps.\nConsequently, the computation count is similar to the forward simulation and ensemble reweighting.\nConcretely, on 32 CPU cores (Intel Xeon Gold 6258R) the water model with 2685 atoms runs at 2.3 ms per forward step, 3.9 ms per reverse step, and 2.7 ms per step for a 50 ps training run.\nOpenMM[60]on the same machine runs at 1.2 ms per step for a standard simulation with the same parameters.\n\nThe above derivation will change for different integrators and thermostats.\nHere we avoid the complexities of constant pressure simulation, constrained bonds and angles, virtual sites and Ewald summation for long-range electrostatics, though the approach should extend to include them.\n\nWe implemented reversible simulation in the Julia language[66]due to its flexibility, speed and growing use in science[67].\nThe Molly.jl MD package[25]was used for standard MD components such as neighbour lists and periodic boundary conditions.\nLoopVectorization.jl and Polyester.jl were used to improve performance.\nDouble floating point precision was used throughout to increase numerical precision (see Figure1B).\nInteger random seeds were stored from the forward simulation and used to generate the same random velocitiesduring the reverse simulation.\nGradients were computed using Zygote.jl[68]and Enzyme.jl[69,70].\nMDAnalysis[71]and BioStructures[72]were used for analysis.\nEnsemble reweighting was implemented following ForceBalance[8]with AD used to calculate the requiredandgradients for improved speed and accuracy.\nThe same number of snapshots were used to calculate the loss for reversible simulation and ensemble reweighting.\nFor the molecular mechanics models, the required force gradients were explicitly derived and implemented for bonded and non-bonded terms for all functional forms.\n\nTo train the water models we used a cubic box with 3 nm sides containing 895 water molecules.\nThe Langevin middle integrator withps-1, a temperature of 295.15 K, a time step of 1 fs, no bond or angle constraints, a 1 nm cutoff for non-bonded interactions and the reaction field approximation for long range electrostatics were used.\nEach epoch an equilibrium simulation of 10 ps was followed by a production simulation of 50 ps, with the loss computed from snapshots taken every 2 ps.\nA Monte Carlo barostat was used to set the pressure to 1 bar during equilibration but not during the production run.\n\nThe enthalpy of vapourisation was calculated following the procedure in OpenFF Evaluator[73].\nThe gas potential energy was pre-computed once before training.\nSince bond and angle constraints were not used during training but were used for validation simulations, 2.8 kJ/mol was added to the liquid potential energy during training as tests in OpenMM with TIP3P water indicated that not using constraints leads to this difference.\nA mean squared error (MSE) loss with an experimental value of 44.12 kJ/mol was used.\nThe RDF was calculated for O-O and O-H distances using the differentiable procedure from Wang et al.\u00a02023[24]and experimental data from Soper 2013[48].\nIn addition to the Lennard-Jones or alternative parameters described below, the TIP3P starting parameters[51]of O partial charge -0.834, O-H bond distance 0.09572 nm, O-H bond force constant 462750 kJ mol-1nm-2, H-O-H angle 1.824 radians and H-O-H angle force constant 836.8 kJ/mol were used.\nThe Adam optimiser with a learning rate ofwas used, parameter values were divided by their starting values for optimisation to account for their different sizes, and a maximum gradient magnitude of 1000 per parameter was applied.\nTraining was carried out on 32 CPU cores for a week or around 1000 epochs.\n\nValidation simulations were carried out using OpenMM[60].\nAt each temperature from 260 K to 365 K at 5 K intervals, a 120 ns simulation was run with the first 20 ns being discarded as equilibration.\nThe Langevin middle integrator withps-1, the Monte Carlo barostat with a pressure of 1 bar, a time step of 2 fs, constrained bonds and angles, a 1 nm cutoff for non-bonded interactions and particle mesh Ewald for long range electrostatics were used.\nSnapshots were saved for analysis every 50 ps.\nFor the self-diffusion coefficient, 5 short 5 ns equilibration simulations were run as above followed by 5 100 ps simulations in the NVE ensemble using the Verlet integrator with a time step of 1 fs.\nThe diffusion coefficient was calculated as described in the later section on gas diffusion.\nThe dielectric constant was calculated following the procedure in OpenFF Evaluator[73].\nThe RDF was calculated using MDAnalysis[71].\n\nHere we outline the potential energy functions used for the alternative functional forms.\nThese were only applied to the oxygen atoms in each molecule by settingkJ/mol or similar for hydrogen.\nThe starting O partial charge and bonded parameters are always those from TIP3P.\nIn each caseis the interatomic distance.\nThe Lennard-Jones potential is standard and has parametersand:\n\nThe TIP3P starting parametersnm andkJ/mol were used, and also where relevant for other functional forms.\n\nThe double exponential potential has parameters,,and:\n\nwhere.\nThe starting valuesandfrom Horton et al.\u00a02023[74]were used.\n\nThe Buckingham potential has parameters,and:\n\nThe starting valueskJ/mol,nm-1andkJ/mol nm6were used after a fit of the three parameters to the TIP3P Lennard-Jones potential curve.\n\nThe Lennard-Jones soft core potential has parameters,,and:\n\nwhere.\nThe starting valuesandwere used.\n\nUnless otherwise stated, the same simulation and training options as the water model were used.\nOnly the non-bonded parameters were trained.\nNo barostat was used during equilibration.\nThe same box of 895 water molecules was used except 10 water molecules were randomly replaced each epoch with oxygen molecules followed by an energy minimisation.\nSnapshots were taken every 200 fs.\nThe MSD of oxygen gas molecules was calculated, accounting for the periodic boundary conditions, across multiple time segments spanning half the simulation time. This was divided by 6 times the segment time to obtainfrom Einstein\u2019s relation.\nTraining simulations were carried out using the Langevin middle integrator withps-1and a time step of 1 fs.\nTraining in the NVT ensemble was found to give better results than the NVE ensemble and represents a likely use case.\nConsequently, the validation simulation were also run in the NVT ensemble.\nThe loss was the MSE to an experimentalvalue ofm2s-1[52], multiplied by.\nStarting parameters for the oxygen gas ofnm andkJ/mol were taken from Wang et al.\u00a02021[53].\nThe Adam optimiser with a learning rate ofwas used.\nFor validation, 5 simulations of 100 ps were run after separate 10 ps equilibration runs and thevalue averaged.\n\nThe Stillinger-Weber prior[54]was implemented in Julia.\nThe starting parameters were those for silicon with modified length and energy scalesnm andkJ/mol to account for the smaller carbon atom[5].\nRather than implement the DimeNet++ model[55]in Julia, PythonCall.jl was used to call the Jax code from Thaler and Zavadlav 2021[5,32]on GPU.\nIn this section the notation from that paper is matched.\nA cubic box with 1.784 nm sides containing 1000 carbon atoms was used, representing 5 diamond unit cells in each direction.\nThe Langevin middle integrator withps-1, a temperature of 298 K and a time step of 0.5 fs were used.\nThe loss was defined as:\n\nwherekJ-2mol2nm6,kJ-2mol2nm6,GPa,GPa andGPa.\nThe crystal is assumed to have zero stress for vanishing strain.\nThe virial stress tensoris calculated[75]as:\n\nwhereis the number of atoms,is the outer product,are the atomic masses,are the atom velocities,is the atomic coordinate array (),is the atomic force array (),is the potential energy,is the lattice tensor describing the simulation box andis the box volume.\nThe isothermal elastic stiffness tensorwas calculated at constant strainvia the stress fluctuation method:\n\nwithand Kronecker delta.\nSecond order AD was used to calculate, meaning that third order AD was used to calculate the gradient of the loss function.,andwere calculated from[5].\nThe Born contribution to the stress tensor is omitted as it is difficult to calculate with reversible simulation and it is a considerably smaller term than the others.\nThe loss was computed from snapshots taken every 250 fs.\nAD was used in Julia or Jax to compute the required derivatives.\nThe training simulation time was scaled up over epochs, and was set to 0.5 fs multiplied by the epoch number with no equilibration.\nBy the end of training at 2000 epochs the simulation time was 1 ps, which was found to be sufficient for learning.\nThe Adam optimiser with a learning rate offor the DimeNet++ parameters andfor the Stillinger-Weber parameters was used.\nThe validation simulations with the learned model were 100 ps.\nTraining and validation were carried out on one A100 GPU.\nOther details are the same as Thaler and Zavadlav 2021[5].\n\nSECTION: Availability\n\nTraining and validation scripts are available under a permissive licence athttps://github.com/greener-group/rev-sim.\nMolly.jl is available athttps://github.com/JuliaMolSim/Molly.jl.\n\nSECTION: Conflict of interest\n\nThe author declares no competing interests.\n\nSECTION: Acknowledgements\n\nI thank the Sjors Scheres group, Stephan Thaler, Josh Fass, Yutong Zhao, Yuanqing Wang, Daniel Cole, Joshua Horton and Kresten Lindorff-Larsen for useful discussions; all contributors to Molly.jl; William Moses and Valentin Churavy for support with Enzyme.jl; and Jake Grimmett, Toby Darling and Ivan Clayson for help with high-performance computing.\nThis work was supported by the Medical Research Council, as part of United Kingdom Research and Innovation (also known as UK Research and Innovation) [MC_UP_1201/33].\nFor the purpose of open access, the MRC Laboratory of Molecular Biology has applied a CC BY public copyright licence to any Author Accepted Manuscript version arising.\n\nSECTION: References\n\nSECTION: Reversible molecular simulation for training classical and machine learning force fields\n\nSECTION: Joe G Greener\n\nSECTION: Supplementary Methods and Data\n\nSECTION: Reversible molecular simulation derivation\n\nConsider a single simulation step of the Langevin middle integrator as shown in Equation\u00a02.\nOne operation at a time can be explicitly computed with constants represented by values ofand intermediate vectors represented by values of:\n\nThe multi-variable chain rule can then be used to compute:\n\nThis is the first term in Equation\u00a04.\nFollowing a similar process with assistance from Symbolics.jl[61]further terms, which quickly increase in complexity, can be derived.\nExamining the relationship between these terms manually leads to the relations in Equation\u00a05.\n\nSECTION: Force gradients\n\nThe Lennard-Jones potential between two atoms is defined by potential energyfor interatomic distanceand atom pair parametersand.\nThe magnitude of the forceand the gradients required for reversible simulation are given by:\n\nSignificant computation can be reused when calculating these quantities.\nNote that whenthe power 12 term will approach zero andandwill have the same sign.\n\nSECTION: Comparison to ensemble reweighting\n\nConsider for example the ForceBalance approach[8,9,10].\nIfis a generic thermodynamic average property then:\n\nwhereis the number of microstates,is the probability of state,is the potential energy of state,is the partition function and the angle brackets represent the average over microstates.\nBy differentiating this[8,9,10]we obtain:\n\nThis can be compared to Equation\u00a01.\nFinite differences can be used to calculateand[8], but AD provides a way to do this faster and with higher accuracy[5].\nTypically, one or more simulations are run and the snapshots sampled are taken as representative of the microstates.\nThis assumes sufficient sampling of low energy regions and requires enough time between snapshots to reduce correlation.\nThe first term is the same as in Equation\u00a01 and represents the direct dependence ofon the parameters.\nThe second term represents how a change in the parameters affects the weighting of states in the ensemble.\nReversible simulation does this by differentiating through a simulation, whereas the ensemble reweighting approach reweights the snapshots based on how the potential energy depends on the parameters.\nEnsemble reweighting therefore only consider snapshot states, whereas reversible simulation considers a number of steps prior to each snapshot state depending on gradient truncation.\nSince reordering states does not change the gradients arising from ensemble reweighting, observables that depend on multiple time points such as diffusion coefficients are not directly applicable to this scheme.\nDiffTRe extends the above approach by using thermodynamic perturbation theory to reuse states, allowing for more efficient training[5].\n\nSECTION: Comparison to the adjoint method\n\nThe adjoint method differentiates an ordinary differential equation (ODE) before discretising it[41,36].\nConsider a loss functionwhose input is the result of an ODE solver acting on hidden state:\n\nThe adjointdetermines the gradient of the loss with respect to:\n\nIt can then be shown[41]that:\n\nThe required integrals for solving,andcan be computed in a single call to an ODE solver.\nThis steps back through time starting from the final state, similar to reversible simulation.\nThe two vector-Jacobian products above are similar to the two in Equation\u00a05.\nHowever, reversible simulation discretises the differential equation before differentiating it[36].\nThis means that the gradients match those of the forward simulation to within numerical error.\nBy contrast, the adjoint method solves a different equation to obtain the gradients, which can cause problems[44,45].\nIt can be unclear how to best solve this adjoint equation.\nThe forward simulation is stable for conventional MD cases, but this is not guaranteed for the adjoint equation[46], so it makes sense to use the gradients of the forward simulation if possible.\nThere has also been work on second order neural ODEs[47].", "text_file": "data\\paper_texts\\2412.04374v1_content.txt"}]], ["ti:\"transformers\"", [{"title": "Fast Tree-Field Integrators: From Low Displacement Rank to Topological\n  Transformers", "authors": ["Krzysztof Choromanski", "Arijit Sehanobish", "Somnath Basu Roy Chowdhury", "Han Lin", "Avinava Dubey", "Tamas Sarlos", "Snigdha Chaturvedi"], "published_date": "2024-06-22T16:05:34Z", "summary": "We present a new class of fast polylog-linear algorithms based on the theory\nof structured matrices (in particular low displacement rank) for integrating\ntensor fields defined on weighted trees. Several applications of the resulting\nfast tree-field integrators (FTFIs) are presented, including (a) approximation\nof graph metrics with tree metrics, (b) graph classification, (c) modeling on\nmeshes, and finally (d) Topological Transformers (TTs) (Choromanski et al.,\n2022) for images. For Topological Transformers, we propose new relative\nposition encoding (RPE) masking mechanisms with as few as three extra learnable\nparameters per Transformer layer, leading to 1.0-1.5%+ accuracy gains.\nImportantly, most of FTFIs are exact methods, thus numerically equivalent to\ntheir brute-force counterparts. When applied to graphs with thousands of nodes,\nthose exact algorithms provide 5.7-13x speedups. We also provide an extensive\ntheoretical analysis of our methods.", "arxiv_id": "2406.15881v2", "html_link": "https://arxiv.org/html/2406.15881v2", "search_term": "ti:\"transformers\"", "html_content": "SECTION: Fast Tree-Field Integrators: From Low Displacement Rank to Topological Transformers\n\nWe present a new class of fast polylog-linear algorithms based on the theory of structured matrices (in particularlow displacement rank) for integrating tensor fields defined on weighted trees. Several applications of the resultingfast tree-field integrators(FTFIs) are presented, including (a) approximation of graph metrics with tree metrics, (b) graph classification, (c) modeling on meshes, and finally (d)Topological Transformers(TTs)(Choromanski et\u00a0al.,2022)for images. For Topological Transformers, we propose new relative position encoding (RPE) masking mechanisms with as few asthreeextra learnable parameters per Transformer layer, leading to1.0-1.5%+accuracy gains. Importantly, most of FTFIs areexactmethods, thus numerically equivalent to their brute-force counterparts. When applied to graphs with thousands of nodes, those exact algorithms provide5.7-13xspeedups. We also provide an extensive theoretical analysis of our methods.\n\nSECTION: 1Introduction\n\nMatrix-vector multiplication remains a key computational block of virtually all modern machine learning (ML) algorithms.\nFor this reason, decades of research have been dedicated towards making this fundamental operation more efficient.\nOne approach to achieve this goal is through efficient hardware design, e.g., using modern GPU and TPU accelerators(Abadi et\u00a0al.,2016; Yu et\u00a0al.,2022,2020).\nThe alternative method involves developing algorithms for efficient matrix-vector multiplication by leveraging either (1) sparse matrices(Wang,2021; Beniamini et\u00a0al.,2020), or (2) structured dense matrices(Thomas et\u00a0al.,2018; Chandrasekaran et\u00a0al.,2018).\nThese algorithms can be applied in modern neural network systems, where weights are pruned to encourage sparsity(Blalock et\u00a0al.,2020)or they can be parameterized with structured matrices(Sindhwani et\u00a0al.,2015).\n\nIn this work, we aim to accelerate multiplications with a\nlarge class of matrices, that we refer to as-distance matrices, which play an important role in several ML algorithms.\nConsider a matrix, wherestands for the shortest-path distance between the-th and-th vertex of an undirected graph.\nHerestands for the set of vertices (nodes),denotes the set of edges,maps them to their positive weights,\nand.\nWe calla-distance matrix in. Note that if, thenis the Shortest Path Kernel matrix.\n\nThe product(where) represents a scalar field onobtained by discretely integrating the field defined by. In this integration, a new field value at a vertexis calculated by averaging the old field values at all vertices, weighted according to the function. This integration can be extended to general tensor fields by replacing vectorwith a tensor:\n\nWe refer to the above procedure as the-integration of a fieldon. We will use the termsgraph field integration(GFI) andmultiplication with-distance matricesinterchangeably throughout the paper. When the graph,, is a tree, we call this procedure (Eq.1)tree field integration.\nNext, we highlight several applications that rely on multiplications with-distance matrices,.\n\nInterpolation on manifolds:This task involves predicting unseen values on a manifold from a set of known values. For example, predicting the velocities of all points on a flag with known velocities for a few points(Pfaff et\u00a0al.,2021). For a discretized manifold, the interpolated values can be obtained using a weighted average using graph field integration (Eq.1).\n\nOptimal Transport (OT):A popular method used to solve the entropic OT problem(Peyr\u00e9 and Cuturi,2019)is the Sinkhorn algorithm(Eckstein and Nutz,2022). Sinkhorn relies on multiplications withcost matrices, which are special cases of-distance matrices for metric spaces induced by shortest-path distances in graphs. This can be efficiently solved using graph field integration.\n\nTopological Transformers (TTs):Topological Transformers(Choromanski et\u00a0al.,2022)are extensions of traditional Transformers(Vaswani et\u00a0al.,2017)for graph inputs. TTs modify the 1-D relative positional encoding (RPE) using \u201cmask matrices\", which are-distance matrices. We show how these matrices can be efficiently integrated into the attention mechanism (Sec.4.4).\n\nIn the above applications, apart from the graph field integration step, the bottleneck lies in the process of explicitly materializing the-distance matrix. Naively performing the integration in Eq1consists of two steps:(a)computing the-distance matrix,, which requirestime in the worst case (which we callpreprocessing), and(b)performing the multiplication takestime. This is prohibitively expensive while using large graphs.\n\nIn this paper, we introduce a new class of fast polylog-linear algorithms for graph field integration that uses low displacement rank (LDR) matrices(Thomas et\u00a0al.,2018; Chandrasekaran et\u00a0al.,2018). To summarize, our primary contributions are given below:\n\nWe provide the firstexactpolylog-linear multiplication algorithms calledFast Tree-Field Integrators(FTFIs), for general weighted trees and\na rich class of maps, including rational, trigonometric, exponential and exponentiated quadratic functions (Sec.3.2).\n\nWe show how Fast Tree-Field Integrators can be applied to support fast computations on general graphs by approximating graph metrics with tree metrics (Sec.4).\n\nWe show that FTFIs are5.7-10xfaster than baseline graph field integration methods for large-scale graphs (Sec.4.1and4.2).\n\nWe showcase the efficacy of FTFIs in several applications including graph classification (Sec.4), interpolation on meshes (Sec.4.2), and Topological Vision Transformers (TVTs) (Sec.4.4). For TVTs, we propose new relative position encoding (RPE) masking mechanisms by introducing onlythreeextra learnable parameters, which leads to1.0-1.5%accuracy gains. We provide an exhaustive evaluation on Vision Performers (25models on multiple datasets). Some of our best models use exponentiated quadratic functions, which has not been applied in this context before.\n\nFor completeness, we also propose approximate FTFI extensions viaNon-Uniform FFT(NU-FFT)(Kircheis et\u00a0al.,2023)and random Fourier features (RFFs)(Rahimi and Recht,2007)(Sec.A.2).\n\nSECTION: 2Related work\n\nEfficient graph field integration (Eq.1) has been studied by prior works for different classes of matrices. For example,Al-Mohy and Higham (2011)considered exponentiated adjacency matrix-vector multiplication,Spielman and Teng (2012)targeted symmetric diagonally dominant matrices (e.g., Laplacian),Arrigo et\u00a0al. (2018)analyzed matrices that are power series of random walk kernels. In contrast to these approaches,Saad and Schultz (1986)proposed general iterative methods for solving certain linear systems using Arnoldi\u2019s iterations. However, These iterative methods can suffer from convergence issues.Williams (2007)showed that it is possible to pre-process any boolean matrix to achieve sub-quadratic matrix-vector multiplication.\n\nThe general problem of computing the action of a matrix on a vector, where the matrix is the graph kernel, in sub-quadratic time is intractable, except for a few special cases(Al-Mohy and Higham,2011; Choromanski et\u00a0al.,2023). In this work, we embed the graphunder consideration in a tree (replacing the graph metric by the underlyingtree metric). Then, we leverage the tree structure to approximate the action of the kernel on a given vector by providingexactintegration on a tree.\n\nPrevious works(Bartal et\u00a0al.,2022,2019; Abraham et\u00a0al.,2008; Bartal,1998)have used the theory oftree metrics(TMs) in several applications in mathematics and computer science.\nTMs are widely used to embed a complex metric space (e.g., a Riemannian manifold) into a more tractable one, while approximately preserving (all or most of the) pairwise distances. They find applications in distributed & online algorithms(Khan et\u00a0al.,2008; Bubeck et\u00a0al.,2018), biology(Mossel,2007), vision, robotics(Athitsos and Sclaroff,2003), and ML (e.g., metric spaces\u2019 regression(Gottlieb et\u00a0al.,2011)).\n\nApplying tree metrics (TM) to compute approximateis a natural approach to scale up matrix multiplications.\nIf a TM approximates the metric space well, then the derived embeddings should have low distortion. However, in the worst-case scenario, this is not true for deterministictree embeddings.\nA natural alternative is to sample trees from probabilistic distributions, which are shown to provide logarithmic distortion in expectation(Fakcharoenphol et\u00a0al.,2004b; Bartal et\u00a0al.,2022).\nThis can be further improved to constant distortion for certain classes of metrics, e.g., celebratedsnowflake metics(Leeb,2016). For graph metrics defined by shortest-path distances, there exist spanning trees providing constant average distortion (over all pairs of nodes). These spanning trees can be constructed asnear minimum weight spanning trees(Bartal et\u00a0al.,2016).\nUnfortunately, explicit application ofanytree metric still requirestime (impractical for large) to:(1)compute all shortest-path distances via the breadth-first-search algorithm (BFS), even if sub-quadratic methods were used to construct a tree (e.g. minimum spanning tree),(2)store the matrix, and(3)perform matrix-vector multiplications. We provide more details about work related to graph field integration in AppendixB.\n\nSECTION: 3Fast Tree-Field Integrators (FTFI)\n\nIn this section, we present our approach for performing efficient field integration on a tree, which we callfast tree field integrator. We begin by introducing the concept of integrator trees (ITs), which is a specialized decomposition of a tree using the theory ofbalanced separators(Sec3.1). Subsequently, we leverage these integrator trees to execute efficient integration on a tree via adivide-and-conquer algorithm(Sec3.2).\n\nSECTION: 3.1IntegratorTrees (ITs) - preliminaries\n\nTo support fast integration for various tensor fieldsdefined on a given input tree, we first design a special data structure that we refer to as anIntegratorTree(IT). An object of this type is constructed only once per, regardless of the number of tensor fields used.\nAn IT is a rooted binary tree. To avoid confusion, we will refer to its vertices asnodes, reserving termverticesfor those of. Each node of IT corresponds to the induced sub-treeof. For every non-leaf node corresponding to some, apivotpointalong with two sub-trees:andare constructed. The following needs to be satisfied:\n\nfor,\n\n(denotes the number of vertices).\n\nThe next lemma shows that every treewithhas the above decomposition and it can be efficiently found.\n\nIfis a tree with, thenadmits a decomposition\n(given above and it can be constructed inlineartime.\n\nThe algorithmic proof is provided in AppendixA.1and uses standard tools from the theory of balanced separators.\n\nTheleft childof the non-leaf node forcorresponds toand theright childto. In addition to these two pointers, a non-leaf node also contains eight extra fields, partitioned into two groups, one corresponding to its left child and one to its right children.\nThe fields corresponding to the left child are as follows:\n\nLeft-ids:an array of the ids (in) of those vertices that are in, mapping the ids of vertices into the original ids in(each sub-tree uses consecutive numbers fromas ids locally).\n\nLeft-d:an array of different shortest-pathdistances from the pivot point to the vertices in.\n\nLeft-id-d:an array mapping the ids of vertices (in) to the indices in left-d of their corresponding distances from the pivot point.\n\nLeft-s:a corresponding array of the ordered sub-sets of ids (in) of vertices within a particular distance from the pivot point.\n\nFields corresponding to the right child are defined similarly.\nThe leaf nodes of the IT consist only of the-transformed (element-wise) distance matricesfor their corresponding sub-trees (see: Fig1). In principle, the leaf nodes of IT correspond to sub-trees with less thanvertices each. In practice, we choose higher, for more efficient integration (see: discussion in Sec.4.1).\n\nFrom what we have said so far, it is clear that an IT can be constructed by applyingbreadth first search(BFS) and the linear algorithmic procedure for constructing the decomposition from Lemma3.1. Note that every vertex of the input tree appears in the logarithmic number of nodes in the IT since the size of the sub-tree is at mostthe size of its parent in IT. We conclude that IT for the given input treecan be computed intime, wherestands for the number of verticesof.\n\nSECTION: 3.2Integrating with IntegratorTrees\n\nWe are ready to explain how ITs allow us to efficiently integrate any given tensor fielddefined onfor a wide class of function. We will apply adivide-and-conquerstrategy.\n\nWe start in the root node of IT. If that node is a leaf then the-transformed distance matrix is stored and can be directly used for matrix-tensor multiplication. If this node is not a leaf, then it encodes the decomposition. Take some. Note that the valueof the new field inafter-integration is given as follows for:\n\nTo compute the new values of the field for nodes, one needs to:\n\nCompute the contribution to it from(-terms). This can be done simply by applying Eq.2recursively for, which\nmeans traversing to the left child of the root.\n\nAdd the so-calledcross-termscontributions coming from the vertices of(-terms).\n\nThe key observation is that the latter (cross-term) contributions can be retrieved simply by computing, where: (1)withandbeing the sizes of the node\u2019s left-d and right-d arrays respectively., and (2) Letwhererefers to the size of the subset. Thenis defined as follows:\n\nGiven the structure of IT, tensorcan be computed in linear time. Note that the following holds:\n\nwhere.\nAnalogous analysis can be derived for, with matrixreplacing.\nThus the overall time complexity of the cross-terms computations is determined by the algorithm for matrix-tensor multiplications with matricesand.\n\nMatricesare of the form:for some sequences,and.\n\nA functionis-cordial(or:cordialifis not specified), if there existssuch that matrix-vector multiplication with a matrixcan be conducted in timefor every,.\n\nNext, we demonstrate the importance of cordial functions in our FTFI framework.\n\nIfis-cordial then-integration for the general weighted tree ofvertices can be conducted in time.\n\nDenote bytime complexity for running FTFI on the-vertex tree. We have the following recursive formula for, where:\n\nThis is implied by the fact that: (1) the size of each sub-tree is at mostthe size of its parent, (2) the computation across left and right children is dominated by multiplications with matricesand. The solution of this recursion leads to the statement.\n\u220e\n\nNext, we show some practical implications of Lemma3.3,\nwhere tree weights arecompletely arbitrary.\nAdditional results are given in Sec.A.2.3.\n\nWe claim that every rationalis-cordial for any. We will use Lemma 1 from(Cabello,2022)stating that: given any set ofrational functionsand, one can compute thevaluesin time(by applying FFT). For a given vector, it thus suffices to define:and that lemma can be applied to efficiently compute. We conclude that for any,-integration can be conducted intime for-vertex weighted trees and any rational(see also: Sec.4.3, Sec.4.2, Sec.4.4).\n\nThe above result on rational functions clearly applies also to polynomial, but here we can do better. We show thatis-cordial. Assume that. We have:, where matrixis defined as an outer-product of two vectors:and. Thus eachsupports linear matrix-vector multiplication (via associativity property). The proof is completed, sinceis a constant. We conclude that-integration can be conducted intime for-vertex weighted trees and any polynomial(see: Fig.2and Fig9).\n\nTake. Thenis an outer-product of two vectors:and. The remaining analysis and conclusion is thus the same as for the polynomial case (see also: Sec.4.4).\n\n(is a constant) We claim thatis-cordial. In that setting, matrixsatisfies:and thus is aCauchy-likeLDR, supporting fastmatrix-vector multiplication(Victor Y.\u00a0Pan,2000). We conclude that-integration can be conducted intime for-vertex weighted trees and(see: Fig.2).\n\nNow matrixcan be re-written as, whereandare diagonal, with diagonal entries given by sequencesandrespectively, and furthermoreis thegeneralized Vandermonde matrix(GVM) (using arbitrary nonnegative integers as exponents). It is defined as:, whereand. As in the previous case, the embedding trick can be applied, but we will use it only for columns. That effectively leads to the completion of the set of exponentsto the set of consecutive integers starting fromand a regular Vandermonde matrix, that supportsmatrix-vector multiplication, replacing GVM. The benefit of this embedding, as compared to the previous one, is that even though it still increases the number of columns by a multiplicative factor of, the number of rows does not change. Therefore, for, substantial computational speedups are achieved (see: Sec.4.4).\n\nSECTION: 4Experiments\n\nIn this section, we outline the experimental setup and report the performance of FTFI across various settings. For all the experiments, we only consider minimum spanning tree (MST) as an approximation of our graph. Specifically, we design experiments to answer these research questions:\n\nHow efficient are FTFIs for tree field integration?\n\nHow does the approximation quality of FTFI compare to other integration algorithms?\n\nHow can we further improve the approximation quality in FTFI?\n\nHow can we use FTFI in real-world large-scale settings?\n\nSECTION: 4.1Runtime Efficiency of FTFI\n\nThe main goal of this experiment is to evaluate the speedups obtained by FTFI as compared to brute-force tree field integrator\u00a0(BTFI) i.e. the explicit calculation of Eq1on a tree.\nWe consider two classes of graphs:(a)synthetic, obtained from a path-graph by adding random edges and(b)mesh graphsfrom\nThingi10K(Zhou and Jacobson,2016)dataset. For BTFI, we compute the MST and then integrate a random scalar fieldon the vertices of the MST. Since BTFI & FTFI are numerically equivalent, we report the pre-processing time and integration as a function of vertex count () in Fig.3. We observe that FTFI achieves up to13xspeedups for 20K-vertex meshes and5.7x+ for synthetic graphs with over 10K vertices compared to BTFI.\n\nSECTION: 4.2Approximation Quality of FTFI\n\nWe evaluate the approximation quality achieved by FTFI across a wide range of graph-based tasks.\n\nInterpolation on meshes.We compare the efficiency of FTFI with baselines on thenormal vector prediction task.\nEvery node of the considered meshwith a vertex-set, is associated with a locationand a vertex normal. For each mesh, we randomly select a subsetwithand mask out their vertex normals (set as zero vectors). The interpolation task involves predicting the vertex normals of each masked nodeas:where, withbeing the shortest path distance between nodeand, andis a rational function.\nWe perform a grid search to set hyperparameterfor each mesh and report the result with the highest cosine similarity between predicted and ground truth vertex normals, averaged over all the nodes.\nWe run tests on40 meshesof the 3D-printed objects with a wide range of sizes from theThingi10Kdataset (details inSectionD.3).\nWe compare FTFI with BTFI, low-distortion tree-based algorithms such as Bartal Trees(Bartal,1996)and FRT trees(Fakcharoenphol et\u00a0al.,2004a)alongside the state-of-the-art method for graph-field integration, the Separator Factorization (SF) algorithm(Choromanski et\u00a0al.,2023). We also compare against the baseline BGFI which entails explicitly materializing the kernel matrix ofand then performing matrix tensor multiplication with a tensor fielddefined by the\u2019s.\n\nPreprocessing involves building specific tree structures (FRT, Bartal), calculating the kernel matrices (BGFI, BTFI), or creating specialized data structures (SF, FTFI) for efficient later use. The first two plots in\nFig.4shows the pre-processing time and cosine similarity for various algorithms applied to meshes of different sizes.\nFTFI is the fastest in terms of pre-processing time and achieves competitive performance in terms of cosine similarity (between predicted and actual vertex normals) when compared with the SF algorithm while being numerically equivalent to BTFI.\nFTFI is a few orders of magnitude faster than BTFI and the tree-based methods while maintaining accuracy.\n\nGraph classification.Graph kernels have been widely used for graph classification tasks in previous works(Kriege et\u00a0al.,2020; Nikolentzos et\u00a0al.,2021).\nWe compare the classification results obtained using the approximate kernel from FTFI with those from the exact SP kernel.\nIn this setting, we use the Shortest Path (SP) kernel,.\nWe perform experiments on a wide range of bioinformatics and social networks datasets likeD&D,Mutag,Reddit,Imdb, among others. We follow(de\u00a0Lara and Pineau,2018)and construct the graph feature for both kernels by using the smallesteigenvalues (is a hyperparameter). This feature set is then used for classification, using a random forest classifier. We observe that FTFI achieves significant speed improvements while achieving similar accuracy compared to its brute-force counterpart, BGFI (see Fig.5). We provide more details about the experimental setup and baselines AppendixD.4. We also report additional experiments on meshes and point clouds in AppendixD.1.\n\nSECTION: 4.3Improving approximation quality with learnable-distance matrices\n\nWe propose to further improve the approximation quality of FTFI by learning a-distance matrix on metrics derived from the MST. As an application, we choosegeneral graph metrics, where our goal is to learn the shortest-path distancebetween a given pair of nodesin a graph. Given a-distance matrix and tree-derived metricthe objective is to learn a mapping to minimize\n\nRather than using a fixed, we parameterize and train it. We consider rational function:\n\nwhereare trainable parameters.\n\nTraining dataset. For a graph, we randomly sample vertices. The training dataset consists of tuples of the form:, whereare randomly sampled vertices. Each data point can be constructed in time, or evenif weights are in(Thorup,1997).\n\nFinal evaluation. To evaluate the quality of the approximation, we compute the relative Frobenius norm error:, wherestands for theFrobenius norm,is a tree for a given graphandis an identity function (see: our notation from Sec.1). It quantifies how closely the distance matrix ofis approximated by the-distance\nmatrix of. Computingis expensive and our training does not rely on it. Our empirical results show that the relative error,, can be substantially improved by using the light-weight MSE training loss (defined in Eq.6).\n\nWe report the evaluation error for these experiments in Fig.6(with additional results in Fig.8in the Appendix). We observe that a rational function with quadratic numerator and denominator provides strong performance across different graphs. We notice that increasing the training set todata points does not have a substantial impact on the final error. Estimating the coefficients ofprovides approximation improvements across all graphs in as few as40 training steps.\n\nThese above results show that tree-based estimators are expressive enough to emulate integration on arbitrary graphs. This expressive power can be further enhanced by pairing them with \u201cnonlinear\" functions. Thus, they explain why the presented techniques are relevant for general graphs.\n\nSECTION: 4.4Large Scale Transformer Experiments using FTFI\n\nFor large-scale applications of FTFI, we select Topological Vision Transformers (TopViT),(Choromanski et\u00a0al.,2022), and leverage it for efficient\nincorporation of masking within ViTs. We provide detailed description of masked Transformers in AppendixC.\n\nTopological Vision Transformers with trees :We propose an extension to TopViT that seamlessly integrates FTFI. In this extension, we model the mask matrix as an-distance matrix (with learnable) defined on the minimum spanning tree (MST) obtained from the 2D grid graph image encoding, where vertices correspond to different patches.\nWe parameterizeas.\nWe use the linear attention mechanism introduced in Performers(Choromanski et\u00a0al.,2021), where the attention kernel is written as:for a deterministic, applied element-wise.\nWe experiment with different values of hyperparameters,,and cross-heads parameter sharing strategies as shown in Table1(synced indicates that RPE-parameters are shared across different attention heads).\n\nWe run experiments on ImageNet and Places365 datasets using ViT-B/16 (see Table1).\nFor all the kernels, our variants beat the baselines. For, the best variant applies an exponentiated quadratic function, for which we apply Vandermonde matrices (see: discussion in Sec.3.2.1). Our best variant across all kernels (78.79%) provides2%accuracy gains over the best baseline (76.76%). In the synced setting, we use onlythreeextra learnable parameters per layer (shared in all attention heads across all layers) and obtain1-1.5%accuracy gains. In the asynced setting, we use a small set ofextra learnable parameters per layer (3 extra parameters per head).\nOverall, we observe that FTFI improves the approximation quality within Transformers with a minimal number of parameters. We provide additional discussions on the ViT results for ImageNet in AppendixD.5.1and for Places365 in AppendixD.5.2.\n\nAdditional results on the I-Naturalist dataset, where we outperform various low-rank attention baselines, are provided in AppendixD.5.3.\n\nWe scale our experiments to run on the larger ViT-L architectures and evaluate on ImageNet. In this setting, we use RPE mechanism withand(that provided strong performance in previous experiments) and asynced strategy. We observe that FTFI provides7%accuracy improvement (see: Fig.7).\n\nFurther results on Video Transformer (ViViT)(Arnab et\u00a0al.,2021)are provided in AppendixD.6. We also provide additional experiments including Gromov-Wasserstein distance computation(Vayer et\u00a0al.,2018)(see Sec.D.2), along with code pointers (AppendixD).\n\nSECTION: 5Conclusion\n\nWe provided a new class of algorithms for fast and exact integration of tensor fields defined on weighted trees, relying on the theory of structured (in particular low displacement rank) matrices. We showed how those algorithms can be applied for accurate integration on general graphs, in particular via their minimum weight spanning trees. We presented several applications of the presented methods, from graph classification and interpolation on meshes, through graph metric approximation to Topological Vision Transformers. Our methods provide significant (5-13x) speedups while maintaining the quality of their exact counterparts.\n\nSECTION: 6Author Contributions\n\nKC conceived the idea behind FTFI, proved the theoretical results, implemented FTFI algorithm and ran the vision experiments in this paper. AS integrated the FTFI algorithm in the GW style algorithms and ran some graph and point cloud classification tasks. SBRC ran graph classification experiments as well as experiments on the CUBES dataset. HL ran the experiments on the meshes. AD helped develop methods, and along with TS and SC acted as senior advisors for the project. All authors contributed to the writing of the manuscript.\n\nSECTION: References\n\nSECTION: Appendix ATheoretical results\n\nIn this section, we provide proofs of all theoretical results in the paper.\n\nSECTION: A.1Proof of Lemma3.1\n\nWe will apply Lemma 7.19 from[Cygan et\u00a0al.,2015](that we provide also below for reader\u2019s convenience) and its algorithmic proof. We refer toCygan et\u00a0al. [2015]for a definition of the related graph terms.\n\nAssume thatis a graph of treewidth at most, and consider a nonnegative weight functionon the vertices of. Then inthere exists a-balanced separatorof size at most.\n\nNote first that for each rooted tree, we can compute the size of each of its rooted sub-trees (and store it in the root of the sub-tree) in the linear time, simply by applying dynamic programming. We can now apply the above lemma for the treewith the weight function that assigns weightfor each vertex. By following its algorithmic proof (and using breadth first search for tree exploration), we can obtain a nodeand sub-treesrooted in vertices connected with, with the following properties:\n\n,\n\nforand wherestands for the set size.\n\nWe then choose the first indexsuch that. Note that such an indexexists andbecause of the above and the fact that our tree has at least six vertices. We define asa sub-tree ofinduced by the set:and bya sub-tree ofinduced by the set:.\nNote that the triplesatisfies the requirements of Lemma3.1. That completes the proof.\n\u220e\n\nSECTION: A.2Fast Approximate Tree-Field Integrators\n\nIf matricesfrom Sec.3.2.1do not support fast matrix-vector multiplication, the question arises whether fast approximate procedures can be applied.\n\nAssume that the Fourier Transform (FT) ofexists and denote it by. Note thatis the inverse FT ofand can be re-written asTherefore, the following holds:\n\nWe conclude that for any probabilistic distributiononwith pdf,can be re-written as:, where randomis given as:forand. Thus matrixcan be unbiasedly approximated as:for,with rows given byandrespectively. Matrix-vector productcan be then unbiasedly approximated asand computed in time. For, substantial computational gains are obtained. In particular, if, the approximate-integration is conducted in time. Note thatcontrols estimator\u2019s variance, thus decreasingincreases the error.\n\nWe will now propose a closely-related method, that relies on the non-uniform FFT (NU-FFT).111See[Greengard and Lee,2004]for an excellent introduction.\n\nDenote:for a given. Define:, whereis given as:, and furthermore: (1)is adelta-Diracfunction, (2). Our goal is to efficiently evaluate functionin points:.\n\nAssume that the inverse FT ofexists and denote it by. Note thatis the FT ofand can be written as:. Sinceis also a convolution ofand,is a product of the inverse FTs:andrespectively. Therefore, we can write:, where. Now, functioncan be evaluated foras follows: (1) a quadrature method is applied to obtain points:(and corresponding weights) for the approximate computation of the integral defining, (2) the NU-FFT is applied to computesimultaneously in those points in polylog-linear time, (3) given pre-computed(and the quadrature weights), NU-FFT is applied again to compute quadrature-based approximation of.\n\nThe-integration process applying this method runs in polylog-linear time since the computation oftakes polylog-linear time. A prominent application isgiven as:, withbeing a renormalized indicator of belonging to interval.\nIn this setting, the integral definingis thus limited to. Interestingly, forwe can also apply methods from Sec.3.2.1(see: our discussion below on the trigonometric case).\n\nNote that a Hadamard (element-wise) product of two outer-product matrices is itself an outer-product matrix. Using the analysis from the polynomial and exponential cases, we conclude thatis a sum of a constant number of terms, each being an outer-product matrix. Thus the same conclusion follows.\n\nIfthen it can be re-written as:. Observe that the cordiality property is preserved under linear combination of the finite number of cordial functions. We can thus conclude that analogous results as the above forcan be derived for. That is also the case forthat can be re-written as:. In both cases, we extend the domain fromto, but this does not affect the analysis.\n\nSo far we have not put any restrictions on the tree weights.\nIf we restrict all weights to be the same (without loss of generality, equal to one),\nthen the problem becomes easier. In this case for any function, matricesandare Hankel[Brent,2010](constant on each anti-diagonal and belonging to LDR class).\nThen, matrix-vector multiplication can be done in.\nThe analysis from the proof of Lemma3.3forcan be repeated. We conclude that-integration can be conducted intime for-vertex unweighted trees and any. This was already proven in[Choromanski et\u00a0al.,2022].\n\nAssume that tree weights take values of the form:for some. Then, matricesanddo not need to be Hankel, but can be embedded into Hankel matrices with rows/columns corresponding to distancesfrom the pivot, whereandis the largest distance between a vertex and the pivot.\nTensorcan also be padded into a larger one with extra rows/columns (corresponding to unrealized distances) set to zero. Ifis constant, the asymptotic time complexity remains the same as in the previous case, but the algorithm might not be practical since the number of rows and columns grows by a multiplicative factor of. For certain non-cordial, the algorithm can be modified for potential gains.\n\nSECTION: Appendix BAdditional Related Work\n\nIn this section we provide additional related works. One of the methods to tackle this problem is via iterative methods[Koutis et\u00a0al.,2012]like Arnoldi iteration[Arnoldi,1951], Conjugate Gradient[Shewchuk,1994]and the celebrated Spielman-Teng algorithm[Spielman and Teng,2012]for symmetric diagonally dominant (SDD) matrices. There are a number of extensions and variations of the above methods[Blelloch et\u00a0al.,2011, Boman et\u00a0al.,2008, Christiano et\u00a0al.,2010, Koutis and Miller,2007, Spielman and Teng,2008, Daitch and Spielman,2008, Koutis and Miller,2008].They mainly take into account the structure of the matrix (SDD)[Koutis et\u00a0al.,2010,2011a,2012], embedding of a graph into low stretch spanning trees[Elkin et\u00a0al.,2005], graph sparsification[Spielman and Teng,2010]and the choice of a goodpre-conditioner[Maggs et\u00a0al.,2003, Koutis et\u00a0al.,2011b].\nWe want to emphasize that the research on low stretch trees for general graphs is orthogonal to the main topic of this work. In our manuscript, we show in particular how to conduct efficient integration on arbitrary trees. Thus our work can be naturally combined with those algorithms to leverage all the above low stretch tree constructions for a better approximation of the graph\u2019s metric.\n\nThe other class of method comes from the celebrated work of[Al-Mohy and Higham,2011]and there are a number of extensions of this work[Kloster and Gleich,2023, Al-Mohy and Higham,2010, Moore,2011, Moler and Van\u00a0Loan,2003, Auckenthaler et\u00a0al.,2010].\n\nAnother class of methods is via sampling, where one samples a subset of a large matrix, which is then used to approximate the matrix-vector multiplication (i.e. Monte Carlo sampling) methods[Drineas et\u00a0al.,2006, Drineas and Kannan,2001, Acebron,2019, Acebron et\u00a0al.,2019, Benzi et\u00a0al.,2017, Martinsson,2019].\n\nWe note that none of these methods are directly applicable in our cases as our-matrix is neither Hermitian or SDD. The randomized algorithms are harder to use in the setting of training of a neural network. Moreover our method isexactontrees, where all the above methods are approximations.\n\nSECTION: Appendix CTopological Transformers\n\nInput:Query/key matrices:, value matrix, mask, procedurecalculating(or its approximation) for the input, kernel feature map:.denotes vectorization.Output:Masked low-rank attention embeddings using.1. Compute matrices,with rows defined as:,, where/stands for the ith row of/.2.,fordenoting ith column of.3. Output the embeddingof the ith tokens as:, whereis the ith row ofanddevectorizes its input back to.\n\nWe now recall the formulation of general masked transformers.\n\nLet us denote bythe number of input tokens. The attention used in a regular Transformer linearly projects their representations into three learnable matrices,calledqueries,keysandvaluesrespectively.\n\nGeneral masked attentionis given by the following equation, whereis themask matrix, andis the so-calledmasked attention matrix(MAM):\nwhich is defined as:\n\nwheredenotes the element-wise (Hadamard) matrix product,is some kernel function andis a kernel matrix defined as:for therowofand the jth rowofrespectively.\nWe callthe unmasked attention matrix (UAM). Note that whenis the softmax function, we recover the well-known attention mechanism in Transformers.\n\nHereis the all-ones vector of length, andis a diagonal matrix with the input vector as the diagonal. The time complexity of computing (9) is.\n\nIf the kerneladmits (at least in expectation) a dot-product decomposition, i.e.for some mapping:(and some).is called a(random) feature map(RFM) for.\nForwith rows given asandrespectively,\nRFM-based kernel linearization leads directly to the efficient unmasked attention mechanism of the form:\n\nHerestands for the approximate attention and brackets indicate the order of computations. Such a mechanism is characterized by time complexityas opposed tofor regular attention. If, computational gains are obtained.\n\nThe central question in[Choromanski et\u00a0al.,2022]was how to incorporate the masking in the linear attention as above. Note that in this caseis never materialized. Building on the work of[Luo et\u00a0al.,2021], the authors[Choromanski et\u00a0al.,2022]propose a general algorithm that efficiently implements masked linear attention.\n\nIn this work, we use different mappings(see Table1). Our key contribution in this work is to propose a novel mask matrixand the implementation of a fast matrix multiplication by. The above result then allows us to construct novel classes of Topological Transformers.\n\nSECTION: Appendix DExperimental Details and Additional Experiments\n\nIn this section, we provide additional details regarding the experimental setup and present additional results from our experiments. Our code is available athttps://github.com/brcsomnath/FastTreeIntegrator. Specifically, we provide there the code for: (1) our algorithm leveraging IntegratorTree data structure (depicted in Fig1), (2) adaptation to the Gromov-Wasserstein-type computation, (3) graph classification and (4) experiments on interpolation on meshes.\n\nSECTION: D.1Additional experiments for graph metric approximation with-distance matrices\n\nWe present additional results for the training loss, relative Frobenius Norm Error (), for more samples from the Thingi10K dataset (to complement the results in Fig.6). In Fig.9, we observe that in most cases having rational functions with higher polynomial degrees results in lower training loss.\n\nWe also perform similar experiments for graph classification on the CUBES datasetHanocka et\u00a0al. [2019]. Specifically, we investigate how the polynomial degree affects the graph classification performance in Fig.9(left). We observe that increasing the polynomial degree improves the classification accuracy up to a certain degree. For the same dataset, we also compute the training loss for different polynomial degrees in Fig.9(right). Similarly, we observe that higher-degree rational functions achieve lower training loss for fitting the polynomial coefficients.\n\nMoreover, we benchmark FTFI on ModelNet10[Wu et\u00a0al.,2015], a dataset for 3D Point Cloud (PC) classification. For each PC, we create an-neighborhood-graph and use FTFI for graph classification\nThe Shortest Path kernel achieves an accuracy of, whereas our FTFI with the degree-2 polynomial improves the accuracy to% (% relative improvement over the baseline), similar to the observation in9.\n\nSECTION: D.2Integration of FTFI into GW-style algorithms\n\nWasserstein distance has found many uses in ML, particularly due to it\u2019s principled approach to compare probability distributions. Gromov WassersteinM\u00e9moli [2011]discrepancy is an extension of Wasserstein distance to graph structured data, with a lot of downstream applications like graph clustering and classification. Inspired by the work of[Choromanski et\u00a0al.,2023], we follow the exact same procedure in the integration of FTFI in the conditional gradient algorithm. The FTFI can be injected seamlessly in place of the Fast Matrix Multiplication (FMM) algorithms in Algorithm 2 and Algorithm 3 (see[Choromanski et\u00a0al.,2023]).\n\nOur method GW-FTFI run consistently-x faster than the baseline methods using the Shortest Path kernel, withno dropin accuracy in computing the associated costs (Figure10). The plots shown are obtained by averaging overseeds and random trees of various sizes. For the baseline experiments, we use the implementation from the POT library[Flamary et\u00a0al.,2021].\n\nSECTION: D.3Interpolation on Meshes\n\nIn this section, we present implementation details for the mesh interpolation experiments inSection4.2. All experiments were run on a computer with an i9-12900k CPU and 64GB memory.\n\nIn the vertex normal prediction task inSection4.2, we choose 40 meshes for 3D-printed objects with a wide range of size from the Thingi10K[Zhou and Jacobson,2016]dataset with the File IDs:\n\n[60246, 85580, 40179, 964933, 1624039, 91657, 79183, 82407, 40172, 65414, 90431, 74449, 73464, 230349, 40171, 61193, 77938, 375276, 39463, 110793, 368622, 37326, 42435, 1514901, 65282, 116878, 550964, 409624, 101902, 73410, 87602, 255172, 98480, 57140, 285606, 96123, 203289, 87601, 409629, 37384, 57084]\n\nFor both our FTFI and the baseline BFFI methods, we do a grid-search over the hyper-parameterfor each mesh and report the pre-processing time associated with the hyper-parameter(s) that give(s) us the best cosine similarity.\n\nSECTION: D.4Additional Details on Graph Classification\n\nWe conduct graph classification experiments on a wide range of benchmark datasets. We report the dataset statistics for the graph classification datasets in Table2. More details about the datasets are available inMorris et\u00a0al. [2020]. To evaluate the performance of the different kernels, we employ the\nframework proposed by[Errica et\u00a0al.,2020]. In particular, 10-fold cross-validation is used\nto obtain an estimate of the generalization performance of our method and the baseline method. We repeat this cross validation experiment 5 times to get a robust estimation and report the standard deviation for each setup.\n\nTo obtain graph features, we follow the approach presented in[de\u00a0Lara and Pineau,2018]. In this setting, we obtain the-smallest eigenvalues from the approximated kernel from FTFI and forward these features to a random forest classifier for classification. For BGFI, we perform the same process obtaining the-smallest eigenvalues from the exact shortest kernel. FTFI achieves similar performance to the BGFI while being significantly faster. We tune the hyperparameterindependently for each method.\n\nIn Table4, we report the results for a wide range of baselines and compare FTFI. We observe that FTFI achieves competitive performance among various strong kernel-based classification baseline approaches. Note that FTFI results are not directly comparable with other approaches, as FTFI constructs an intra-graph kernel while other methods use inter-graph kernels. Despite the aforementioned considerations, we contend that positioning our results within the broader framework of alternative methodologies demonstrates that FTFI remains a compelling approach, owing to its speed and comparable classification accuracy.\n\nSECTION: D.5Additional details on experiments for Topological transformers\n\nIn this subsection, we provide additional training details for our image classification tasks. Table5and table6present the architectural as well as the training details.\n\nWe train the ViT models starting from their pretrained checkpoint (pretrained on ImageNet-21k). We replace the dense attention in ViT by the Performer attention (see Equation10). We use Algorithm1to efficiently incorporate the mask matrixin the attention mechanism.\n\nWe have already provided comparison with SOTA efficient-attention methods: low-rank attention Transformers in Sec 4.4, quality-wise. On standard ImageNet benchmark, our best Transformer with FTFI provide 78.15accuracy, as compared to 76.37of the best low-rank -attention variant (obtained by testing three different linear variants). That gives 1.78accuracy improvement with only 3 extra trainable parameters per head (36 extra trainable parameters per layer). We have also run the experiments with cosFormer. It achieved 76.3accuracy (consistent with what is reported in the literature, see [8]), lower than both: our method and the best tested low-rank attention variant. The RF-Gate-Gaussian achieved 76.35accuracy, which is is still lower than both: FTFI and the best tested low-rank attention variant.\n\nWe have also conducted tests on another challenging dataset: Places365. In the paper, we report 1.71accuracy improvement over low-rank attention Transformer (56.51accuracy vs 54.8accuracy). For the rebuttal, we also run the experiment with cosFormer which achieved 55.4accuracy (consistent with what is reported in the literature, see: [8]). This is still 0.93behind our method. The RF-Gate-Gaussian achieved accuracy 55.1, lower than this of cosFormer.\n\nI-naturalist is yet another challenging dataset, with 10K classes, diverse image quality and significant class imbalance. Transformer with FTFI provides 1accuracy improvement over its regular low-rank attention counterpart and the cosFormer. Furthermore, FTFI achieved 0.8improvement over RF-Gate-Gaussian. The convergence of the FTFI variant is 20-23faster than this of its regular low-rank attention counterpart, the cosFormer and RF-Gate-Gaussian.\n\nSECTION: D.6Video Vision Transformer\n\nViViT ([Arnab et\u00a0al.,2021]) is a novel architecture that adapts the Vision Transformer (ViT) for video processing. It efficiently handles the spatiotemporal dimensions of video data by factorizing the input and applying attention mechanisms across both space and time. This allows ViViT to capture complex motion patterns and long-range dependencies in videos.\n\nApplying FTFI with a topological masking mechanism to the ViViT architecture (factorized Transformer model variant, trained from scratch, as described inArnab et\u00a0al. [2021]) results in aabsolute improvement on the Kinetics dataset ([Kay et\u00a0al.,2017]). The experimental setup followsArnab et\u00a0al. [2021]. To the best of our knowledge, this is the first application of Topological Transformers to video data.\n\nSECTION: Appendix EBroader Impact\n\nWe do believe that the potential impact of this work is significant, as providing both: (a) theoretical advancements in structural graph theory as well as (b) practical applications in (1) designing computationally efficient Transformers leveraging topological inductive priors, (2) graph classification and (3) interpolation on manifolds. The core problem of fast multiplication with-distance matrices plays an important role in various fields: physical sciences, chemistry, and network sciences. Our main contributions are algorithmic, with no clear negative side effects. While used in the context of Transformers, they should be though applied cautiously due to the nontrivial carbon emission footprint associated with training large Transformer models.\n\nSECTION: Appendix FLimitations\n\nCurrently, FTFI can be applied on general graphs via certain classes of trees defined on these graphs (e.g. spanning trees), with low-distortion trees being more preferable. It would be interesting to see whether the main concepts used in the FTFI algorithm (such as the theory of balanced separators) can be directly incorporated into efficient and exact algorithms operating on general graphs (or general sparse graphs that appear in most machine learning applications). Determining general conditions on the classes of graphs and functionsunder consideration that are sufficient for exact sub-quadratic time integration is yet another important problem for future work.\n\nSECTION: NeurIPS Paper Checklist\n\nClaims\n\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope?\n\nAnswer:[Yes]\n\nJustification: We give detailed explanations of our contributions in the introduction (page 2).\n\nGuidelines:\n\nThe answer NA means that the abstract and introduction do not include the claims made in the paper.\n\nThe abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.\n\nThe claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.\n\nIt is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.\n\nLimitations\n\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\n\nAnswer:[Yes]\n\nJustification: The limitations are clearly explained in AppendixF\n\nGuidelines:\n\nThe answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.\n\nThe authors are encouraged to create a separate \"Limitations\" section in their paper.\n\nThe paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.\n\nThe authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.\n\nThe authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.\n\nThe authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.\n\nIf applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.\n\nWhile the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.\n\nTheory Assumptions and Proofs\n\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?\n\nAnswer:[Yes]\n\nJustification: We introduce the notion of our algorithm Fast Tree Field Integrator in section3. We describe the main algorithm in detail and introduce the technical (theoretical) results. The proofs of these results can be found in AppendixA.\n\nGuidelines:\n\nThe answer NA means that the paper does not include theoretical results.\n\nAll the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.\n\nAll assumptions should be clearly stated or referenced in the statement of any theorems.\n\nThe proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.\n\nInversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.\n\nTheorems and Lemmas that the proof relies upon should be properly referenced.\n\nExperimental Result Reproducibility\n\nQuestion: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?\n\nAnswer:[Yes]\n\nJustification: Training details to replicate each experiment are in the AppendixD.\n\nGuidelines:\n\nThe answer NA means that the paper does not include experiments.\n\nIf the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.\n\nIf the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.\n\nDepending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.\n\nWhile NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example\n\nIf the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.\n\nIf the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.\n\nIf the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).\n\nWe recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.\n\nOpen access to data and code\n\nQuestion: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?\n\nAnswer:[Yes]\n\nJustification: We provide the code as well as details to run our experiments in AppendixD.\n\nGuidelines:\n\nThe answer NA means that paper does not include experiments requiring code.\n\nPlease see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n\nWhile we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).\n\nThe instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n\nThe authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.\n\nThe authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.\n\nAt submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).\n\nProviding as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.\n\nExperimental Setting/Details\n\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?\n\nAnswer:[Yes]\n\nJustification: All details are presented in Sec4and AppendixD.\n\nGuidelines:\n\nThe answer NA means that the paper does not include experiments.\n\nThe experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.\n\nThe full details can be provided either with the code, in appendix, or as supplemental material.\n\nExperiment Statistical Significance\n\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?\n\nAnswer:[Yes]\n\nJustification: All experiments in the paper except for the ones using large Transformer models have been run multiple times using various random seeds and we report the relevant statistics. The experiments using Transformers are too expensive to run multiple times as the experiments are run on a huge dataset like ImageNet.\n\nGuidelines:\n\nThe answer NA means that the paper does not include experiments.\n\nThe authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.\n\nThe factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).\n\nThe method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)\n\nThe assumptions made should be given (e.g., Normally distributed errors).\n\nIt should be clear whether the error bar is the standard deviation or the standard error of the mean.\n\nIt is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.\n\nFor asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).\n\nIf error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.\n\nExperiments Compute Resources\n\nQuestion: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?\n\nAnswer:[Yes]\n\nJustification: We report the compute resources used in AppendixD.\n\nGuidelines:\n\nThe answer NA means that the paper does not include experiments.\n\nThe paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.\n\nThe paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.\n\nThe paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper).\n\nCode Of Ethics\n\nQuestion: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethicshttps://neurips.cc/public/EthicsGuidelines?\n\nAnswer:[Yes]\n\nJustification: All authors have reviewed the NeurIPS code of ethics and the research conform to the code.\n\nGuidelines:\n\nThe answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n\nIf the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.\n\nThe authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).\n\nBroader Impacts\n\nQuestion: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?\n\nAnswer:[Yes]\n\nJustification: The broader impacts of our work is detailed in AppendixE.\n\nGuidelines:\n\nThe answer NA means that there is no societal impact of the work performed.\n\nIf the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.\n\nExamples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.\n\nThe conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.\n\nThe authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.\n\nIf there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).\n\nSafeguards\n\nQuestion: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?\n\nAnswer:[N/A]\n\nJustification: Our paper is theoretical in nature and we are not releasing any new models or data.\n\nGuidelines:\n\nThe answer NA means that the paper poses no such risks.\n\nReleased models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.\n\nDatasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.\n\nWe recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.\n\nLicenses for existing assets\n\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?\n\nAnswer:[Yes]\n\nJustification: We have properly cited all the papers that introduced various algorithms and data that are used in this work.\n\nGuidelines:\n\nThe answer NA means that the paper does not use existing assets.\n\nThe authors should cite the original paper that produced the code package or dataset.\n\nThe authors should state which version of the asset is used and, if possible, include a URL.\n\nThe name of the license (e.g., CC-BY 4.0) should be included for each asset.\n\nFor scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.\n\nIf assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets,paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.\n\nFor existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.\n\nIf this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators.\n\nNew Assets\n\nQuestion: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?\n\nAnswer:[Yes]\n\nJustification: We release the code for the main algorithm. The usage is detailed in the anonymous github repo.\n\nGuidelines:\n\nThe answer NA means that the paper does not release new assets.\n\nResearchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.\n\nThe paper should discuss whether and how consent was obtained from people whose asset is used.\n\nAt submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.\n\nCrowdsourcing and Research with Human Subjects\n\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?\n\nAnswer:[N/A]\n\nJustification: We do not conduct any research that involves crowd sourcing or with human subjects.\n\nGuidelines:\n\nThe answer NA means that the paper does not involve crowdsourcing nor research with human subjects.\n\nIncluding this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.\n\nAccording to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.\n\nInstitutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects\n\nQuestion: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?\n\nAnswer:[N/A]\n\nJustification: Our paper does not involve crowd sourcing nor research with human subjects.\n\nGuidelines:\n\nThe answer NA means that the paper does not involve crowdsourcing nor research with human subjects.\n\nDepending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.\n\nWe recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.\n\nFor initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.", "text_file": "data\\paper_texts\\2406.15881v2_content.txt"}, {"title": "Uncertainty Quantification for Transformer Models for Dark-Pattern\n  Detection", "authors": ["Javier Mu\u00f1oz", "\u00c1lvaro Huertas-Garc\u00eda", "Carlos Mart\u00ed-Gonz\u00e1lez", "Enrique De Miguel Ambite"], "published_date": "2024-12-06T18:31:51Z", "summary": "The opaque nature of transformer-based models, particularly in applications\nsusceptible to unethical practices such as dark-patterns in user interfaces,\nrequires models that integrate uncertainty quantification to enhance trust in\npredictions. This study focuses on dark-pattern detection, deceptive design\nchoices that manipulate user decisions, undermining autonomy and consent. We\npropose a differential fine-tuning approach implemented at the final\nclassification head via uncertainty quantification with transformer-based\npre-trained models. Employing a dense neural network (DNN) head architecture as\na baseline, we examine two methods capable of quantifying uncertainty:\nSpectral-normalized Neural Gaussian Processes (SNGPs) and Bayesian Neural\nNetworks (BNNs). These methods are evaluated on a set of open-source\nfoundational models across multiple dimensions: model performance, variance in\ncertainty of predictions and environmental impact during training and inference\nphases. Results demonstrate that integrating uncertainty quantification\nmaintains performance while providing insights into challenging instances\nwithin the models. Moreover, the study reveals that the environmental impact\ndoes not uniformly increase with the incorporation of uncertainty\nquantification techniques. The study's findings demonstrate that uncertainty\nquantification enhances transparency and provides measurable confidence in\npredictions, improving the explainability and clarity of black-box models. This\nfacilitates informed decision-making and mitigates the influence of\ndark-patterns on user interfaces. These results highlight the importance of\nincorporating uncertainty quantification techniques in developing machine\nlearning models, particularly in domains where interpretability and\ntrustworthiness are critical.", "arxiv_id": "2412.05251v1", "html_link": "https://arxiv.org/html/2412.05251v1", "search_term": "ti:\"transformers\"", "html_content": "SECTION: Uncertainty Quantification for Transformer Models for Dark-Pattern Detection\n\nThe opaque nature of transformer-based models, particularly in applications susceptible to unethical practices such as dark-patterns in user interfaces, requires models that integrate uncertainty quantification to enhance trust in predictions. This study focuses on dark-pattern detection, deceptive design choices that manipulate user decisions, undermining autonomy and consent. We propose a differential fine-tuning approach implemented at the final classification head via uncertainty quantification with transformer-based pre-trained models.\nEmploying a dense neural network (DNN) head architecture as a baseline, we examine two methods capable of quantifying uncertainty: Spectral-normalized Neural Gaussian Processes (SNGPs) and Bayesian Neural Networks (BNNs). These methods are evaluated on a set of open-source foundational models across multiple dimensions: model performance, variance in certainty of predictions and environmental impact during training and inference phases. Results demonstrate that integrating uncertainty quantification maintains performance while providing insights into challenging instances within the models. Moreover, the study reveals that the environmental impact does not uniformly increase with the incorporation of uncertainty quantification techniques. The study\u2019s findings demonstrate that uncertainty quantification enhances transparency and provides measurable confidence in predictions, improving the explainability and clarity of black-box models. This facilitates informed decision-making and mitigates the influence of dark-patterns on user interfaces. These results highlight the importance of incorporating uncertainty quantification techniques in developing machine learning models, particularly in domains where interpretability and trustworthiness are critical.\n\n123\n\nSECTION: 1Introduction\n\nThe field of NLP was revolutionized with the arrival of transformer models, a groundbreaking architecture introduced by Vaswaniet al.in their seminal work, \"Attention is All You Need\"[42]. Prior to this, NLP relied heavily on Convolutional Neural Networks (CNN), which were useful in analyzing the spatial features of the data but lacked semantic awareness and nuances. Later, Recurrent Neural Networks (RNN) were used, which processed data sequentially and struggled but faced issues with long-range dependencies within text and stability during training due to the vanishing gradient.\n\nTo address this, gated RNNs like Long Short-Term Memory (LSTM) were introduced, which mitigated the vanishing gradient problem but were not parallelizable and required high computational demand for training[12]. With their unique self-attention mechanism, transformers enabled parallel processing of entire data sequences, offering a substantial leap in efficiency and effectiveness. This architecture\u2019s ability to capture complex relationships across distant parts of a text significantly enhanced performance across a myriad of NLP tasks, setting new benchmarks in machine translation, sentiment analysis, and beyond. Subsequent iterations, such as BERT[8]and the GPT series[6,34], further refined and extended the transformer\u2019s capabilities, embedding it as the cornerstone of modern NLP research and applications. The transformative impact of these models is not just limited to their superior performance; they have also democratized access to high-quality NLP tools, fostering innovation and expanding the field\u2019s frontiers[22].\n\nSpecifically, transformer models have been widely used for sequence classification tasks, from text sentiment analysis[46]to DNA classification[13]. Despite all their advantages, transformers suffer from similar limitations to other neural network-based models,i.e.their black-box nature that makes their understanding difficult[39]. This aspect can be critical in tasks such as autonomous driving[9]or medical diagnosis[49], where there is a need to obtain a measure of certainty in the model predictions before committing to any action.\n\nGiven the black-box nature of transformer models and the critical importance of reliable predictions in high-stakes applications, integrating uncertainty quantification into these models becomes paramount[39]. However, transformer architectures\u2019 complexity and pre-trained nature present significant challenges in modifying their internal components to accommodate uncertainty measures. As a result, focusing on the final classification head offers a practical and effective approach to introduce uncertainty quantification[38].\n\nThe interpretability and reliability of transformer-based models can be improved by integrating classification heads with predictions and measures of confidence or uncertainty. This is particularly important in applications where errors can have high costs, and understanding the model\u2019s confidence can lead to better decision-making processes[1,27]. One example of a pervasive issue requiring such knowledge is the use of dark-patterns in user interfaces. These deceptive design strategies compromise user autonomy and challenge the ethical integrity of digital services. Therefore, understanding a model\u2019s confidence can help prevent such issues and promote fair and transparent digital practices.\n\nIn this paper, to address the inherent opacity of neural networks and meet the growing demand for more transparent and trustworthy AI systems, three approaches are explored to improve the interpretability and reliability of transformer-based models for dark-pattern detection: (1) dense neural networks (DNNs), (2) Bayesian neural networks (BNNs), and (3) spectral-normalized neural Gaussian Processes (SNGPs) classification heads. We examine the performance of BNNs and SNGPs in quantifying uncertainty across various deep learning models, analyze the impact of these uncertainty quantification techniques on the performance and environmental sustainability of AI models during both training and inference phases, and explore the practical implications of uncertainty modelling through real-world examples of high and low uncertainty cases.\n\nThis research enhances the detection of dark-patterns by integrating uncertainty quantification techniques in transformer models to improve the transparency and reliability of predictions, which is crucial for applications where deceptive design practices compromise user autonomy. By providing a quantitative analysis of how these models perform, the research highlights the enhanced ability to identify dark-patterns while addressing potential impacts on environmental sustainability. The goal is to develop AI systems that are both ethically responsible and environmentally considerate, aligning with the growing demand for transparency and trust in AI applications.\n\nSECTION: 1.1Classification Heads\n\nDense layers, also known as fully connected layers, are the most basic form of a neural network layer, where each input neuron is connected to every neuron in the next layer[21]. In classification tasks, a dense layer typically serves as the final layer that maps the learned representations to the target classes. The primary advantage of dense layers lies in their simplicity and effectiveness in learning complex patterns through these direct connections. However, they do not inherently provide measures of uncertainty in their predictions, treating all inputs with equal certainty.\n\nBayesian dense layers[19]extend the concept of dense layers by incorporating Bayesian inference into the network\u2019s architecture. Unlike traditional dense layers with fixed weights after training, Bayesian dense layers treat weights as distributions, This approach allows the network to simulate multiple possible models of parameterswith an associated probability distribution, enhancing the model\u2019s ability to express and quantify uncertainty in its predictions.\nThis is crucial for critical applications like drug discovery and fair AI systems, where decision-making relies heavily on the reliability of the model\u2019s output[1,27].\n\nDuring training, BNNs utilize a prior distribution for weights instead of fixed values, reflecting initial beliefs which are updated via a likelihood function assessing the model\u2019s fit to the data. Bayesian inference computes a posterior distribution combining these elements, often approximated through variational inference for practical implementation. After training, the BNN generates multiple predictions by randomly sampling different sets of weight values from the posterior distribution of weights. This process offers insights into into their uncertainty and impacting the predictive uncertainty. By comparing these multiple predictions, the degree of uncertainty can be assessed, where low variability among predictions indicates low uncertainty, while high variability suggests greater uncertainty. This is uncertainty quantified using multiple forward passes to generate a distribution of predictions.\n\nHowever, the major challenge with Bayesian dense layers is their computational complexity and the need for more sophisticated training techniques to manage the probabilistic nature of the weights.\n\nSpectral-normalized Neural Gaussian Process (SNGP)[25]is a relatively recent approach that combines the ideas of spectral normalization and Gaussian Processes (GPs) with deep learning to enhance a deep classifier\u2019s capacity to measure the distance between the test example and the training data. Spectral normalization[30]is a technique used to stabilize the training of neural networks by normalizing the weight matrices, ensuring that the Lipschitz constant of the function (represented by the network) is constrained. This helps maintain the model\u2019s generalization ability. On the other hand, GPs provide a principled, probabilistic approach to learning in kernel machines, offering a powerful tool for uncertainty quantification. By integrating GPs with deep learning, SNGP layers aim to preserve the deep neural network\u2019s capacity for feature extraction and representation learning while enhancing the model\u2019s ability to provide meaningful uncertainty estimates for its predictions. This makes SNGP particularly appealing for tasks requiring a careful balance between performance and interpretability, such as safety-critical applications.\n\ndark-patterns, defined as deceptive user interface designs that manipulate online users into unintended actions have emerged as significant threats to privacy, fairness, and transparency in digital environments. These designs exploit psychological vulnerabilities, leveraging mechanisms such as obfuscation, false urgency, and social proof to influence user decisions in favour of the companies implementing them[5]. The importance of detecting these manipulative strategies cannot be overstated, as they compromise user autonomy and undermine the ethical foundation of digital services[16,43]. Machine learning and deep learning algorithms have demonstrated great accuracy in identifying deceptive practices in digital ecosystems[47]. This offers a solution to enhance user protection and promote transparency and fairness. However, to ensure effective detection, it is important to utilize state-of-the-art architectures and incorporate certainty in the detection process[10]. Thus, combating dark-patterns is crucial, aligning with the growing demand for digital accountability and protection of consumers in complex online landscapes. This relaionship highlights the ongoing efforts and significant impact of research in tackling unethical practices in digital user interfaces.\n\nSECTION: 2Related work\n\nTransformers have been widely used for text classification tasks, showcasing their versatility and efficacy across a wide array of applications. One notable application is in the domain of customer feedback analysis[31], where the authors demonstrate transformers\u2019 ability to handle complex multi-label classification tasks. This research highlights how transformers, with their deep contextual understanding, can effectively categorize customer reviews into multiple relevant categories, thus providing valuable insights into customer sentiment and preferences.\n\nThe ability of transformer models to handle noisy data is crucial for maintaining their performance in real-world applications. A study titled \"Transferable Post-hoc Calibration on Pretrained Transformers in Noisy Text Classification\"[50]proposes post-hoc calibration techniques to fine-tune pretrained transformer models, enabling them to classify texts accurately even in the presence of noise and variability. This study demonstrates the adaptability of transformers to diverse and challenging datasets and states the importance of managing certainty in the predictions of black-box deep learning models.\n\nAnother study[33]further illustrates the effectiveness of transformer-based approaches in sentiment analysis. This study explores the nuanced capabilities of transformer models in detecting sentiment, leveraging their deep learning architecture and attention mechanism to comprehend and interpret the nuances of human emotions expressed in text. The research suggests that transformer models can accurately classify texts that express a clear opinion. However, they struggle with ambiguous and ambivalent linguistic patterns. Therefore, identifying the data presented in such situations is crucial for improving the model\u2019s performance.\n\nAdditionally, text classification can be challenging due to the lack of labelled data, which can significantly reduce model performance. Zhanget al.[40]have proposed a solution to this problem by incorporating knowledge graphs with transformational models. The research emphasises the importance of data quality. In data-scarce scenarios, it is essential to have diverse and challenging data to help the model learn and improve further. There is a need for future research in this area, particularly regarding the application of certainty to identify points where data can be strengthened.\n\nTogether, these studies paint a comprehensive picture of the state-of-the-art in the application of transformer models to text classification, highlighting their flexibility, efficiency, and effectiveness across varied contexts and challenges.\n\nIn the development of Bayesian Neural Networks (BNNs), significant strides have been made across various domains, demonstrating their versatility and effectiveness in enhancing classification tasks through uncertainty estimation. A notable advancement is presented by Bensenet al.[2], where a hierarchical structure within BNNs is tailored for convolutional networks. This approach capitalizes on the inherent uncertainty estimation capabilities of BNNs to improve classification outcomes, particularly in complex visual tasks.\nSimilarly, Milaneset al.[29]showcase the application of BNNs in the biomedical field. Here, BNNs are leveraged to classify motor imagery tasks, proving particularly adept at managing the inherent noise in electroencephalogram (EEG) data, thus underscoring the robustness of BNNs in handling data with significant variability.\nIn the realm of image classification, the effectiveness of BNNs is further highlighted in[3].\nThis research emphasizes how uncertainty estimation intrinsic to BNNs can bolster prediction confidence, thereby enhancing the reliability of image classification systems. Extending this integration of Bayesian methods into convolutional neural networks (CNNs), Ferranteet al.[11]explore the incorporation of uncertainty directly into the network architecture. This integration not only improves the performance in image-based classification tasks but also provides a clearer understanding of the model\u2019s decision-making process, making it more interpretable.\n\nGaussian Processes (GPs) are a powerful, flexible and widely used Bayesian non-parametric framework for modeling and inference in a wide range of domains, from machine learning and computer science to physics, engineering, and the natural sciences[35]. GPs are particularly well-suited for modeling complex, nonlinear, and multi-modal systems, as well as for handling small sample sizes and high-dimensional input spaces. At a high level, GPs provide a prior distribution over functions, which can be combined with data to obtain a posterior distribution over functions. This posterior distribution is also a GP, which can be used for a variety of tasks, such as function interpolation, extrapolation, optimization, and uncertainty quantification. One of the key advantages of GPs is that they provide a principled way to quantify and propagate uncertainty, which is especially important in applications where the data is noisy or the underlying system is not well understood.Wanget al.[44]use GPs to focus on distinguishing educational content from non-educational materials, using word embeddings generated with Word2Vec[28]. Jayashreeet al.[17]use GPs with convolutional kernels to benchmark the performance of GPs in text classification tasks for different datasets.Yeet al.[48]use SNGPs combined with focal loss for reliable dialog response retrieval.\n\nComplementing the focus on certainty quantification, the principles of GreenAI underscore the need for environmentally sustainable practices in AI research and development[15,37]. As models grow in complexity and size, substantial computational resources are required, leading to significant energy consumption and environmental impact. Aligning with the objectives of GreenAI, the ability to assess the certainty of model outputs accurately allows for more efficient allocation of computational resources. Resources can be optimized when the model exhibits high confidence, reducing unnecessary energy consumption. Conversely, in cases of high uncertainty, the model requires additional processing or data.\n\nThe integration of certainty quantification techniques, such as BNNs or SNGPs, can enhance the interpretability and reliability of AI systems. By accurately quantifying uncertainty, these techniques can identify instances where the model is highly confident, minimizing the need for excessive computational resources and associated energy consumption. These techniques will ensure that the integration of sustainability into AI research and development aligns with pressing environmental objectives while also fostering the development of AI systems that are both reliable and energy-efficient.\n\nIn this context, the ability to quantify uncertainty in AI predictions becomes an invaluable asset. Accurately assessing certainty can optimize computational resources, reduce energy consumption, and contribute to realizing Green AI principles, enhancing the interpretability and trustworthiness of AI systems and promoting sustainable practices in AI development.\n\nSECTION: 3Methodology\n\nThis section describes the components of the dark-pattern classification with uncertainty quantification task. We begin by describing the dark-patterns dataset used for this study. Then, we provide an overview of the model and head selection for the task.\n\nSECTION: 3.1Data\n\nThe dataset used is the dark-patterns dataset developed by Yadaet al.[47], containing 2356 examples scraped from different websites.\n\nIt consist of a binary problem equally balanced for English language. No pre-processing is applied maintaining the case of raw text. The texts consists of examples of dark-patterns and normal patters with maximum length of 857 characters, median length 463 and min length 1 characters, with currency, Han and Hiragana symbols <1% each.\n\nSECTION: 3.2Models\n\nThe choice of models for this comparison is grounded in their innovative contributions and varied approaches to NLP and machine learning challenges:\n\nDolphin-Llama2-7B-AWQ111https://huggingface.co/cognitivecomputations/dolphin-llama2-7b: An advanced model originating from the LLaMA2 architecture[41], renowned for its natural language understanding and generation capabilities, especially in conversational contexts. It is enhanced by training on the Dolphin dataset222https://huggingface.co/datasets/cognitivecomputations/dolphinto eliminate bias and alignment issues, making it particularly effective for dark-pattern detection. This model incorporates AWQ technology[24]for 4-bit weight quantization, optimizing efficiency and speed without sacrificing accuracy, highlighting its potential for rapid and precise analysis in identifying manipulative digital interfaces.\n\nbert-large-uncased: BERT[8]is a foundational model that significantly advanced the understanding of context in language, as the first model to successfully apply Transformers at scale. The selection of its largest variant, for our study serves a dual purpose: to benchmark the evolution of model architectures over time and to ensure a comprehensive analysis by employing the most capable version.\n\nMistral-7B-OpenOrca-AWQ333https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-AWQ: Mistral model version quantized with AWQ method trained on Q&A OpenOrca Microsoft Dataset[23]augmented with GPT-4 and GPT-3.5. The foundational Mistral[18]model focuses on balancing computational efficiency with performance, suitable for diverse application scenarios.\n\nmamba-370m444https://huggingface.co/state-spaces/mamba-370m-hf: Mamba[13]leverages new architecture based on selective state-space models (SSMs). Unlike Transformers, Mamba selectively retains or discards information based on the current token without attention, significantly reducing complexity from quadratic to linear with sequence length. This architectural innovation is especially relevant for analyzing complex webpage elements in dark-pattern detection, offering a promising approach.\n\nnomic-embed-text-v1555https://huggingface.co/nomic-ai/nomic-embed-text-v1: Nomic Embed[32]innovates in embedding techniques to provide more dynamic, context-aware representations, surpassing leading models as of February 2024. Its top 25 ranking on the MTEB leaderboard666https://huggingface.co/spaces/mteb/leaderboardfor tasks critical to our project, like Semantic Search and Summarization, underscores its suitability for analyzing web content. With a compact size, a low memory usage footprint and advanced training methods, Nomic Embed efficiently processes up to 8192 tokens, making it ideal for identifying manipulative elements in extensive online materials.\n\nIt is important to note that all these models are open-source, contributing to the democratization of advanced artificial intelligence tools and fostering innovation across the field.\n\nThe quantized LLM models are quantized with Activation-aware Weight Quantization (AWQ)[24]. AWQ focuses on low-bit weight quantization (INT3/4) recognizing that all weights are not equally important. AWQ\u2019s selective quantization preserves essential model performance while ensuring that models remain lightweight and fast, making it a key technology for the effective and efficient application of advanced AI in real-world scenarios.\n\nOur model development strategy employs the \"pre-train and fine-tune\" paradigm[7,14,20], utilizing pre-trained models for fine-tuning. This stage is crucial for enhancing the models\u2019 performance and certainty in the downstream task of identifying dark-patterns. Through this methodology, we aim to deepen our understanding of these models\u2019 capabilities in specific real-world scenarios, focusing on the critical intersection of performance and certainty in detecting dark-patterns.\n\nBNNs use different methods for variance reduction, two of them being the reparametrization[4]and flipout methods[45]. The flipout method has emerged as a preferable variance reduction technique over the reparameterization trick. While reparameterization effectively reduces variance for models with continuous latent variables by transforming stochastic variables into deterministic functions, its application is limited to tractable distributions. Flipout, on the other hand, introduces random perturbations to gradients within mini-batches, mimicking the effects of larger batches to stabilize training without additional computational costs. This approach not only broadens its applicability, including to discrete variables and complex distributions but also reduces intra-batch interference, making it particularly suitable for the vast and varied parameter spaces of LLMs.\n\nIn BNNs, weights are represented by probability distributions, which encode beliefs about the possible values those weights can take based on the data and prior information. To make a prediction, one must sample from these distributions, resulting in a different set of weights for each prediction. These varying sets of weights lead to a range of possible outputs for a given input, reflecting the model\u2019s uncertainty about the most appropriate weights to use. For this reason, it is necessary to compute multiple predictions for the same inputs in order to obtain a confidence interval.\n\nThe flowchart for the methodology of this work is shown in Figure1. The DNN, SNGP and BNN classification heads are added to each model for the dark-patterns binary classification tasks. The DNN head outputs single-point predictions while the SNGP head outputs a probability distribution and the BNN head outputs multiple predictions which form a confidence interval.\n\nSECTION: 4Results and Discussion\n\nThis study extensively explores the trade-off between performance, certainty, and sustainability of Transformer models, focusing on Dense Neural Networks (DNNs) as a baseline, Bayesian Neural Networks (BNNs), and Spectral-normalized Gaussian Processes (SNGPs) for uncertainty quantification.\n\nFor the experiments, every model is fine-tuned on the dark-patterns dataset with the three different classification heads: DNN, BNN and SNGP. We compare the model size, accuracy, F1, inference time and train and test carbon emissions measured with Codecarbon[26]. For the model tuning, 20% of the data is reserved as test, and the remaining 80% is divided into 20% validation and 80% train. The training parameters are defined in Table2.All experiments are conducted in a cloud server with an Nvidia RTX 5000 GPU with 16G VRAM.\n\nEven though the Mistral and Llama 2 models are quantized, they remain frozen during fine-tuning since the GPU does not have enough VRAM to fit all the weights. For the remaining models, all weights are fine-tuned. Regarding the hyperparameters of the classification heads, the SNGP head has 1024 inducing points, based on the default value on the original paper[25]. For fairness of results, the DNN and BNN classification heads have a hidden layer of 1024 units, with the number of output neurons of the base model as inputs and one output neuron for the binary classification logits. For the BNN, the weight initialization parameters areandand the number of predictions used for obtaining the confidence interval is 10.\n\nIt is important to note that the main objective of this paper is not to improve the baselines on dark-pattern detection, but to use the dark-pattern classification task as a way to compare different classification heads for uncertainty quantification in transformer models.\n\nSECTION: 4.1Trade-off Performance Analysis\n\nAfter evaluating the performance of different types of models used in this study, we discovered that each model has its unique advantages and limitations based on specific accuracy and inference time metrics (see Table3). These performance metrics significantly impact determining the suitability of each model type for various practical applications.\n\nDNNs displayed consistent accuracy across all tests, making them a reliable choice for applications where stable and predictable performance is needed. Due to their simpler architecture, DNNs also had the fastest inference times among the models tested. This makes them particularly valuable in scenarios where rapid decision-making is critical, such as in real-time systems where delay can result in inefficiencies or safety concerns.\n\nBNNs, while offering the advantage of quantifying uncertainty, showed variability in performance, particularly in accuracy. This variability originates from their probabilistic nature, which, while providing a deeper insight into the model\u2019s confidence levels, can lead to less predictability in outcomes. The inference times for BNNs were significantly longer than those for DNNs due to the computational overhead of managing probabilistic weights and multiple sampling to estimate uncertainty. This model type is best suited for applications where prediction confidence is more critical than prediction speed, such as in strategic decision-making environments where incorrect decisions could have high consequences.\n\nIn the case of DNNs vs BNNs, DNNs have half the number of parameters of BNNs, as BNNs require a mean and standard deviation for each weight, and can be trained easily with a cross entropy loss, while probabilistic models usually use Kullback-Leibler divergence, which is not as straigthforward. In the case of SNGPs, they use a simpler Laplace approximation to the original Gaussian process, as exact and even approximate GPs suffer from a high computational cost as they need to invert the covariance matrix to obtain the predictive Gaussian distribution. DNNs are also a good choice when the inference time requirements don\u2019t allow for the multiple predictions of BNNs or the slightly slower predictions of SNGPs.\n\nSNGPs were designed to balance accuracy and the quantification of uncertainty. However, their integration with larger models like Mistral resulted in reduced performance and higher variability in accuracy, as indicated in Figure3. Despite this, SNGPs maintained moderate inference times and offered valuable insights into the uncertainty of predictions, making them suitable for use in environments where reliability and detailed probabilistic understanding are necessary but where the extreme computational demands of BNNs are prohibitive.\n\nOne of the most significant findings from the performance analysis was the impact of model size on accuracy and inference times. Larger models such as Llama and Mistral had slower inference times and also showed decreased accuracy in the case of Mistral using SNGP. This result indicates the greater challenge of modelling certainty in larger models. This aspect is crucial when selecting a model for practical applications, as the benefits of larger, more complex models must be weighed against the increased resource demands and potential decrease in performance.\n\nSECTION: 4.2Sustainability and Environmental Impact\n\nThe study demonstrates a direct correlation between the size of the Transformer models and their carbon emissions, even when LLMs have frozen layers except for the head, such as Mistral and Llama. These larger models require significantly more computational power, translating to higher energy consumption and increased carbon emissions, as depicted in Figure3. This relationship is crucial for organizations that balance performance with sustainability, as opting for smaller, more efficient models like Nomic could significantly reduce their environmental impact.\n\nThe environmental impact varies significantly across different model types during the training and inference phases. Traditional DNNs, while less computationally intensive than BNNs or SNGPs, do not offer uncertainty quantification, which might necessitate retraining or additional computational overhead in uncertain scenarios. Although BNNs provide valuable insights into model certainty, they also have a much higher energy cost due to the need to process multiple samples to estimate uncertainty. This is evident from the study\u2019s findings, where BNNs consumed up to ten times more energy than DNNs under similar conditions.\n\nSECTION: 4.3Comparing Uncertainty Quantification Approaches\n\nThrough the detailed review of model performance, stability, and energy consumption, we can better understand the strengths and limitations of each BNN and SNGP approach.\n\nAs shown in Table4, the analysis reveals significant differences in certainty and accuracy between SNGP and BNN models. For instance, BERT and Llama models equipped with SNGP heads showed exceptional stability with identical scores of 95.745 for the top and bottom 10% of predictions, accompanied by a near-zero mean variance (0.005). This indicates highly stable predictions across the dataset, suggesting that SNGP models maintain consistent performance even in varying data conditions. In contrast, the Mistral model with an SNGP head displayed less stability, with a notable discrepancy between the highest and lowest 10% of predictions (68.085 vs 57.447) and a significant mean variance (0.931).\n\nWhen coupled with an SNGP, Nomic showcased high certainty, as both the top and bottom 10% of predictions scored very high (97.872), coupled with low variance (0.005). This reflects a robust model architecture or particularly effective training data, emphasizing the potential of SNGP to provide reliable and consistent outputs.\n\nIn general, SNGP models tend to show lower mean variances than their BNN counterparts for the same set of models, indicating more stable predictions. While BNNs may occasionally reach higher peaks of certainty or confidence in certain predictions, their performance across the dataset is comparatively less stable, marked by higher mean variances. This variability could influence the choice between SNGP and BNN depending on the need for stability in application outputs.\n\nFrom an energy consumption perspective, the SNGP models generally incur similar costs to traditional models without uncertainty quantification heads, whereas BNNs can be significantly more energy-intensive. For example, the BNN classification head\u2019s test emissions were ten times higher than those of DNNs. This higher energy demand is primarily because BNNs require multiple samples to measure certainty effectively. Note that BNN models could require even more computational resources if more points are needed to enhance certainty and reliability, potentially affecting their scalability and practical application in energy-sensitive environments.\n\nThe use of an SNGP head implies an additional energy cost of approximately 1.2% on average, which is minimal compared to the substantial ten times increase associated with BNN heads. This analysis suggests that while SNGP provides a balance between performance uncertainty stability and energy efficiency, BNNs pose challenges regarding higher operational costs and environmental impact.\n\nSECTION: 4.4Practical Implications\n\nOne of the study\u2019s objectives is the model\u2019s ability to quantify uncertainty in its predictions. Table5presents high and low uncertainty instances from the Nomic SNGP model. The semantic clarity of the text examples is evident in the low uncertainty cases, where the model\u2019s predictions are made with confidence. These examples are straightforward, containing complete phrases that convey a clear message, related to consumer behaviour or stock levels, such as \"Only a few more left!\" or \"Hurry! Limited Quantity Available.\".\n\nOn the other hand, the high uncertainty cases consist of single words like \"Arthritis Aids\" or \"Irwin\", which are semantically ambiguous without further context. This ambiguity translates into a higher variance in the model\u2019s confidence. The presence of these high-variance cases in the table serves a critical function; it underscores the capability of the Nomic SNGP model to introspect and evaluate its certainty.\n\nThis model\u2019s \"self-awareness\" has practical implications that add a layer of interpretability to the AI\u2019s decision-making process compared to the traditional dense head layer model counterparts. This interpretability is crucial for end-users, enabling them to discern when a model\u2019s output is reliable and when it should be treated with scepticism. This discernment is critical when considering the identification of dark-patterns. The certainty measure allows the model to signal which cases are clear-cut and ambiguous, thus avoiding the pitfall of overgeneralizing or making unwarranted assumptions based on uncertain predictions, a real-world relevance that should resonate with data scientists and AI developers.\n\nIt is important to note that the ability to measure certainty is a tool that end-users could use, and it also plays a crucial role in guiding the development of the model itself. As discussed in a recent study by Schmarjeet al.[36], having high-quality data and addressing label ambiguity is crucial for data scientists to identify gaps in the training data or areas where the model require further improvement. Knowing the level of certainty in the model\u2019s predictions can significantly improve the learning process by analyzing instances of high uncertainty that indicate the model\u2019s difficulties and which predictions need reevaluation or additional context.\n\nSECTION: 5Conclusions and Future Work\n\nThis research paper focuses on enhancing the interpretability of transformer models by integrating uncertainty quantification, aimed specifically at detecting dark-patterns in user interfaces as an example of risky situations where certainty is valuable. We demonstrate that this approach can make AI systems more trustworthy without significantly compromising performance. Our study uses dense layers, Bayesian dense layers, and spectral-normalized neural Gaussian processes to achieve this goal. The evaluations across various metrics\u2014model size, accuracy, inference time, and environmental impact\u2014indicate that while there are trade-offs, particularly regarding computational demand and carbon footprint, the benefits of increased reliability and accountability in model predictions are profound.\nWhile we have made progress, there are still areas to explore, particularly in detecting and mitigating bias in text-based AI applications, where dark-patterns can skew outcomes unfavourably. We suggest that uncertainty quantification methods can be adapted to identify and correct biases in training data or model predictions. Additionally, we propose using conformal prediction and distance awareness to establish confidence intervals around predictions, providing a clear statistical guarantee about their accuracy. Applying these methods to transformer models can further enhance their usability in risk-sensitive environments.\n\nThe funding for this work was provided by Funditec.\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.05251v1_content.txt"}, {"title": "BEExformer: A Fast Inferencing Transformer Architecture via Binarization\n  with Multiple Early Exits", "authors": ["Wazib Ansar", "Saptarsi Goswami", "Amlan Chakrabarti"], "published_date": "2024-12-06T17:58:14Z", "summary": "Large Language Models (LLMs) based on transformers achieve cutting-edge\nresults on a variety of applications. However, their enormous size and\nprocessing requirements make deployment on devices with constrained resources\nextremely difficult. Among various efficiency considerations, model\nbinarization and Early Exit (EE) are common effective solutions. However,\nbinarization may lead to performance loss due to reduced precision affecting\ngradient estimation and parameter updates. Besides, the present early-exit\nmechanisms are still in the nascent stages of research. To ameliorate these\nissues, we propose Binarized Early Exit Transformer (BEExformer), the\nfirst-ever selective learning transformer architecture to combine early exit\nwith binarization for textual inference. It improves the binarization process\nthrough a differentiable second-order approximation to the impulse function.\nThis enables gradient computation concerning both the sign as well as the\nmagnitude of the weights. In contrast to absolute threshold-based EE, the\nproposed EE mechanism hinges on fractional reduction in entropy among\nintermediate transformer blocks with soft-routing loss estimation. While\nbinarization results in 18.44 times reduction in model size, early exit reduces\nthe FLOPs during inference by 54.85% and even improves accuracy by 5.98%\nthrough resolving the \"overthinking\" problem inherent in deep networks.\nMoreover, the proposed BEExformer simplifies training by not requiring\nknowledge distillation from a full-precision LLM. Extensive evaluation on the\nGLUE dataset and comparison with the SOTA works showcase its pareto-optimal\nperformance-efficiency trade-off.", "arxiv_id": "2412.05225v1", "html_link": "https://arxiv.org/html/2412.05225v1", "search_term": "ti:\"transformers\"", "html_content": "SECTION: BEExformer: A Fast Inferencing Transformer Architecture via Binarization with Multiple Early Exits\n\nLarge Language Models (LLMs) based on transformers achieve cutting-edge results on a variety of applications. However, their enormous size and processing requirements make deployment on devices with constrained resources extremely difficult. Among various efficiency considerations, model binarization and Early Exit (EE) are common effective solutions. However, binarization may lead to performance loss due to reduced precision affecting gradient estimation and parameter updates. Besides, the present early-exit mechanisms are still in the nascent stages of research. To ameliorate these issues, we propose Binarized Early Exit Transformer (BEExformer), the first-ever selective learning transformer architecture to combine early exit with binarization for textual inference. It improves the binarization process through a differentiable second-order approximation to the impulse function. This enables gradient computation concerning both the sign as well as the magnitude of the weights. In contrast to absolute threshold-based EE, the proposed EE mechanism hinges on fractional reduction in entropy among intermediate transformer blocks with soft-routing loss estimation. While binarization results in 18.44 times reduction in model size, early exit reduces the FLOPs during inference by 54.85% and even improves accuracy by 5.98% through resolving the \"overthinking\" problem inherent in deep networks. Moreover, the proposed BEExformer simplifies training by not requiring knowledge distillation from a full-precision LLM. Extensive evaluation on the GLUE dataset and comparison with the SOTA works showcase its pareto-optimal performance-efficiency trade-off.\n\nKeywordsEarly Exit (EE)Binarized Neural Network (BNN)Selective Learn-Forget Network (SLFN)Textual InferenceTransformers\n\nSECTION: 1Introduction\n\nThe last decade has witnessed substantial progress in the field of Natural Language Processing (NLP). Current state-of-the-art (SOTA) comprises pre-trained Large Language Models (LLMs) involving transformers. However, such models have exorbitant computational complexity, which limits their applications to resource-constrained setups[30]. This necessitates the need for efficient modeling practices. Efficient modeling reduces memory as well as computational power requirements. It also lowers the training and inference time of the model. Moreover, it opens the door to deployment on edge devices[1].\n\nVarious modeling aspects have been considered towards enhancing the efficiency of models, including pruning[2], knowledge distillation[3], low-rank approximation[4], and quantization[5]. Out of these, quantization has become quintessential for deploying NLP models on edge devices[6]. While the majority of efficiency considerations target reduction in the number of layers, nodes, or parameters, quantization targets efficiency by reducing the precision of the model\u2019s memory representation. In neural networks, quantization can be segregated as Post Training Quantization (PTQ)[7,8,9]and Quantization Aware Training (QAT)[10,11]. In PTQ, the model is quantized post-training, hence its name. It can be static, wherein both weights as well as activation are quantized[8]. Otherwise, it can be dynamic, i.e. weights are quantized before and activations are quantized dynamically at inference[9]. While QAT integrates quantization during the training process itself. Here, quantization is performed on the weights and/or activations during the forward pass. Whereas it uses pseudo gradients during backpropagation to deal with the zero gradient problem due to binarization. This is accomplished using full-precision latent weights. These latent weights are actually quantized to get the actual weights of the QNN[11]. Existing quantization techniques convert to reduced precision floating-point (FP) or integer (Int) notations like FP16, FP8, Int8, Int4, and binary (1-bit). The class of neural networks employing 1-bit quantization is referred to as Binarized Neural Networks (BNN). It is the extreme form of quantization through which the memory consumed to store the weights and activations is 32x lower compared to full-precision FP32 notation[12].\n\nTargeting only one aspect of achieving efficiency does not suffice. Besides, dynamic variation in model architecture helps to customize processing and adjust computational requirements based on input during inference. The current LLMs are composed of several layers of transformer blocks stacked together. Conventionally, every input sequence undergoes processing through all the layers to generate the final inference. However, heterogeneity can be observed in the inputs due to varying sequence lengths as well as sentence complexities[13]. Given a network, the lower layers capture the shallow features while more intricate features are captured by higher layers. For some inputs, the features from the lower or intermediate layers might render predictions beyond a certain level of confidence. While some complex inputs might require processing through all the layers. Thus, allowing an exit from certain break-points based on certain conditions can significantly bring down computation[13,14]. Additionally, early exit (EE) provides a remedy to the \"overthinking\"[15,13,16]problem in LLMs having millions and even billions of parameters. In such models, while predictions based on lower layers are correct, they might become incorrect after undergoing transformations in the higher layers due to overcomplicated features. These unnecessary features lead to wasteful computation or \u201coverthinking.\u201d Thus, it is observed that EE not only enhances the efficiency, but it can even improve efficacy[14].\n\nEven though BNN seems to be a promising efficiency enhancement approach, a handful of research works have only been able to effectively implement it in the field of NLP[17,18,19]. Most of them have attempted BNN through knowledge distillation from a full-precision LLM like BERT[17,18]. Even though knowledge distillation leads to a lightweight model, the training process itself is computationally as well as memory intensive[1,20]. Also, none of the works have attempted to devise a transformer architecture from scratch through BNN. Similarly, previous research on EE mechanisms for NLP has been on models with weights initialized from a pre-trained LLM. The EE criterion in the majority of works is a set threshold that needs to be calibrated based on the task as well as the input distribution. This leads to faltering efficacy when inputs deviate during inference. Furthermore, none of the previous works have attempted to combine BNN with the EE mechanism utilizing transformers for textual inference.\n\nIn this paper, we propose a first-of-its-kind architecture for textual inference having multiple binarized transformer blocks with selective learning capabilities stacked upon each other interleaved with decision blocks for EE. The binarization technique involves an intuitive Binarization-Aware Training (BAT) mechanism utilizing real-valued latent weights for gradient updates. We apply a piecewise polynomial binarization function closely approximating the non-differentiable impulse function having a piecewise linear function as its derivative. This ensures the gradient is aware of the sign and the magnitude of the weight values. For EE, we estimate the fractional change in entropy of the logits after each transformer block during inference. If the entropy does not reduce beyond a threshold percentage, the exit condition is satisfied and the final logits are returned. This aids in the reduction of computation by eliminating the execution of subsequent blocks beyond the exit point. Additionally, it resolves the performance loss due to the \"overthinking\" problem inherent in complex networks. During training, we apply soft-routing loss, combining the loss from all exits to effectively update the weights and enhance the decision-making capability of each transformer block. Moreover, the transformer has a binarized version of the Selective Learn-Forget Network (SLFN)[21]integrated into it, leading to the elimination of insignificant information and better comprehension of long-range dependencies. The proposed model learns its parameters from scratch. This reduces any additional computation and memory requirement associated with using a full-precision LLM as a starting point. Despite this, the proposed architecture exhibits pareto-optimal performance-to-complexity trade-off compared to the SOTA on the GLUE benchmark evaluation on tasks like SST-2, CoLA, MRPC, and RTE. The principal contributions of this paper are as follows:\n\nWe binarize both weights as well as activations of the transformer architecture through a piecewise approximation to the impulse function to make it differentiable and ensure gradient-based weight updates consider both the magnitude as well as the sign of the weights.\n\nWe devise an EE mechanism on the basis of fractional changes in entropy of the logits between subsequent transformer blocks. This alleviates the need to set an absolute threshold entropy value besides resolving the \"overthinking\" problem.\n\nAdditionally, a binarized SLFN is integrated in each transformer block to enable selective learning with the elimination of insignificant information. This is accompanied by a soft-routing loss estimation during training to enhance the decision-making capability of each transformer block.\n\nThe compendium to this paper has been presented herein. Section2gives an overview of the related works in the domain. Section3describes the proposed BEExformer architecture. Section4contains the experiment setup details along with data-set information. Section5presents the results obtained through extensive evaluation on multiple tasks. Finally, the conclusions are drawn in Section6.\n\nSECTION: 2Related Works\n\nIn this section, a commentary on the related works has been presented. Since the proposed BEExformer is the first model combining binarization with EE for textual inference, we separately review works implementing BNN and EE for textual content.\n\nSECTION: 2.1Binarized Neural Networks (BNN)\n\nBai et al.[17]implemented weight binarization in a BERT model. To tackle sharp loss in performance, they apply knowledge distillation along with ternary-weight splitting to initialize their model. However, they experience hindrances in binarizing activations and were not able to implement a fully binarized network due to a sharp performance drop due to loss of precision. Qin et al.[18]proposed a fully binarized BERT model applying binarization subject to information entropy maximization along with direction-matching distillation inferring from similarity matrices to deal with direction mismatch. Liu et al.[10]formulated a closer approximation to the impulse function and updated the weights based on a magnitude-aware gradient. Liu et al.[19]further proposed a multi-step distillation approach to gradually distill the model into lower precision before obtaining the final BNN from a full-precision BERT model. They utilize an elastic binary activation with parameters inferred while training. All of the above-mentioned works adopt knowledge distillation from a full-precision model to counter performance loss. However, executing both student and teacher models simultaneously can be memory-intensive. Furthermore, additional fine-tuning on downstream tasks is often required for optimal results. been presented. Since the proposed BEExformer is the first model combining binarization with EE for textual inference, we separately review works implementing BNN and EE for textual content.\n\nSECTION: 2.2Early Exit (EE)\n\nZhou et al.[22]developed an early-exit mechanism with the layers of a Pre-trained Language Model (PLM) interlaced with internal classifiers to exit from the network. They determined the change in predictions of successive internal classifiers, and the exit criterion was fulfilled if no change could be observed for a given number of steps. In the same context, Xin et al.[15]calculated the entropy of predictions from the internal classifiers, and if it fell short of a predetermined threshold, the exit criterion was fulfilled. However, determining the optimal threshold value is a daunting task. Liu et al.[23]put forth a pre-trained transformer model with dynamic exits having loss calculated at each layer as the sum of Masked-Language Modeling (MLM) and Sentence-Order Prediction (SOP) losses. Mangrulkar et al.[13]proposed a modified version of the switch transformer, formulating a switch layer to assess the complexity of a sentence and optimally route it to an expert model having fewer layers. Here, the expert models comprise a set of BERT models with a varying number of encoder layers followed by a prediction layer. Here, expert routing augments the model complexity, translating into additional resource requirements. Moreover, the effectiveness of expert routing hinges on the capability of the experts to be specialists for varying input patterns.\n\nSECTION: 3Proposed Architecture\n\nThe proposed BEExformer architecture embeds an input sequence and processes it through a cascade of binarized transformer blocks with the provision to exit early after each block. Each transformer block possesses binarized multi-head attention (MHA) with parameters updated via both sign as well as magnitude-aware gradient. Additionally, a selective learn-forget network binarized in a similar manner replaces the feed-forward layer in conventional transformer architecture for precise estimation of context. For EE, an intuitive criterion monitoring the fractional reduction in entropy between consecutive transformer blocks has been devised. Once the exit condition is satisfied, the processing is routed to an auxiliary block comprising binarized feed-forward layers to obtain the final predictions. This enables a two-fold efficiency enhancement due to a binarized architecture along with a provision to reduce the amount of computation dynamically during inference by terminating before the execution of all transformer blocks. The proposed architecture has been illustrated in Figure1, while its components have been elucidated herein-below.\n\nSECTION: 3.1Input Representation\n\nThe input sequences in a given corpus are at first tokenized, and a lookup tableis constituted as a one-to-one mapping between the unique tokensin the vocabularywith a unique integer,. For each sentence, a tokenized sequenceis constructed consulting the lookup table as shown in Algorithm1. Besides, padding is appended to ensure uniform sequence length.\n\nThe tokenized sequenceis converted into a series of embeddings, where each tokenin the sequence is substituted by an embedding vector. The embeddings are randomly initialized and optimized throughout training, much like other network parameters. Position embeddingis used on the sequence to determine the token ordering for contextualized representation. For a given positionand dimension,is computed as shown in equation (1).\n\nFinally, the token embedding sequenceand the position-encoded sequenceare element-wise concatenated (denoted by) to obtain the embedded sequencesuch thatas shown in equation (3).\n\nSECTION: 3.2Overall Architecture\n\nThe backbone network consists of a sequence oftransformer blocks with an exit provision after each block as portrayed in Figure1(a). It can be functionally represented astaking inputand having a cascade ofdifferentiable functions (each representing a transformer block) as follows:\n\nWhere,denotes composition operator. The hidden representations from theintermediate block is given as:\n\nTo accomplish the objective of EE, a criterion is defined. In case the criterion is fulfilled at exit, the logitsreturned by theexit are fed to its connected auxiliary blockand the probability distributionis returned. This has been explained in Section3.4.\n\nAdditionally, the BEExformer utilizes BAT (described in Section3.3) to binarize the weights and activations of all the transformer blocks as well as the auxiliary blocks for EE. Generically, a binarized layer is defined as follows:\n\nWhere,,,,,,anddenote the computed activation, activation function, layer operator, binarization function, weights, incoming activation, and bias, respectively.\n\nSECTION: 3.3Binarization\n\nConventionally for binarization,(equation (7)) is applied. However, being not differentiable, it cannot be applied to compute the gradient. To ameliorate this, we apply magnitude-aware approximation ofas in Bi-Real Net[10]for the first time to the best of our knowledge in a transformer encoder. Denoted by, this approximation is expressed in equation (8). From the comparison betweenandalong with their derivatives in Figure2, it can be seen thatclosely approximatesequipped with the capability of being differentiable. Given, its derivative is computed as in equation (9).\n\nwhereis a differentiable piecewise polynomial function being a second-order approximation of the impulse function. Whileis a piecewise linear function approximating the derivative ofduring gradient calculation.\n\nConventional gradient descent cannot be applied to BNN due to the gradient of lossconcerning binarized weights being insufficient to enable bit-flips. To mitigate this issue, we perform BAT wherein the gradient computed for theblock concerning binarized weightsfor theiteration is used to update real-valued weightsfor theiteration. This has been shown in the following equation:\n\nwhere,denotes element-wise differentiation anddenotes the learning rate. Finally, the updated binarized weightsare frozen asapplying equation (7). The details of implementation of the above-mentioned binarization technique in the transformer blocks are presented in Section3.5.\n\nSECTION: 3.4Early Exit\n\nWe propose an EE criterion that estimates proportionate entropy reduction over exits, i.e. fractional reduction in entropy concerning the previous exit. When this value falls below a certain threshold, the exit criterion is satisfied. This has been portrayed in Algorithm2. Here, the entropy of logitsis computed as in equations12and13. Initially, the value ofis taken as, which signifies the case when logits are spread as evenly as possible acrossclasses.\n\nFollowing this, an auxiliary blockis defined to processand predict the outputas follows:\n\nHere,belongs to the set of binarized feed-forward networksas shown in Figure1(d). It processesinto a discrete probability distributionas the output for theexit. To ensure minimal computational overhead,has a minimal number of parameters compared to the backbone network.\n\nThe proposed EE criterion is robust against diversity in inputs during inference as only the fractional reduction in entropy is noted. This alleviates the limitation of previous works with absolute entropy thresholds to deal with diverse inputs. Also, determining the absolute threshold is a cumbersome process in such models. Compared to it, settingis pretty straightforward based on the computational budget and output confidence requirements. Moreover, the same value ofcan be used across multiple tasks.\n\nSECTION: 3.5Transformer Blocks\n\nEach transformer block binarizes the multi-head attention (MHA)module put forth by Vaswani, A.[24]as depicted in Figure1(b).is comprised ofbinarized self-attentionheads, which in turn relies on queries, keys, and valuesfor computation as follows:\n\nwhere,, andhave been obtained through binarized-linear (bi-linear) transformation (explained in Section3.3) on inputwheredenotes the sequence length anddenotes the dimension of hidden representations. The computations have been shown in equations (16-18).\n\nwhere. Finally,is calculated in equation (19) through projection ofconcatenatedthroughafter binarization.\n\nTo augment the efficacy of residual connection, a binarized SLFN is used to replace the feed-forward layer in the conventional transformer architecture. This is the modified version of SLFN proposed by Ansar et al.[21]. In SLFN, there are two forget gates,and, which aid in the elimination of insignificant information in the previous hidden state and the total incoming activations, respectively. Besides, it has a selective learning gatefor rational estimation of dependencies. All the gates incorporate binarization of incoming activations as shown in equation (8). The architecture has been illustrated in Figure1(c), while the formulation of the gates has been provided in the following equations.\n\nwhereanddenote full-precision input atstep andhidden state, respectively. Whereas,,, anddenote weight matrices for inputs as well as hidden states, respectively. Here, we applyon all incoming weights and activations for binarization. Finally, the updated hidden state is calculated as follows:\n\nSECTION: 3.6Loss Function\n\nThe loss function L calculated at the ith exit is given by the following equation:\n\nWhereis the input data containing sequence-label pairs,,is the probability distribution returned by theexit,denotes trainable parameters, andis the cross-entropy function. The proposed model uses a soft-routing loss objective, i.e. combining loss from all exit layers to enhance the decision capability. This aids in optimal assessment of the exit based on the predictions from all the exit blocks while reducing the loss. The total lossis calculated as the mean loss across all exits given by:\n\nwhere C is the total no. of exits.\n\nSECTION: 4Experimental Setup\n\nIn this section the details of the experiment conducted along with the data-set information have been provided.\n\nSECTION: 4.1Data-Sets\n\nThe proposed methodology has been tested on various data-sets from the GLUE benchmark[25], such as Stanford Sentiment Treebank (SST-2)[26], Corpus of Linguistic Acceptability (CoLA)[27], Microsoft Research Paraphrase Corpus (MRPC)[28], and Recognizing Textual Entailment (RTE)[25]. These data-sets cover a wide range of tasks such as sentiment analysis, linguistic acceptability, semantic similarity between texts, paraphrase detection, and entailment detection. The metrics for evaluation are F1 score for MRPC, Matthews correlation for CoLA, and accuracy for the remaining tasks.\n\nSECTION: 4.2Implementation Details\n\nThe proposed BEExformer has been implemented on a Python 3 Compute Engine with the Nvidia L4 GPU, 22.5GB VRAM, 53GB System RAM, and 201.2GB Disk in Google Colab111https://colab.research.google.com. The details of the hyperparameters of the BEExformer have been presented in Table1.\n\nSECTION: 5Results and Discussion\n\nSECTION: 5.1Comparison with Related Works\n\nTo the best of our knowledge, combining binarization with early-exits is unprecedented for textual content. Thus, we compare with related works on solely focused on binarization as well as EE in neural networks for NLP. For ease of comprehension, the results of works on binarization and early exiting have been demarcated. This is followed by results from the proposed BEExformer and its ablations. Table2presents the comparison with related works on the basis of model precision, size and performance metrics for various tasks. Among the quantized models, BEExformer excels over the binarized models on all tasks. It even gives better results than the 8-bit quantized versions on some tasks. During comparison with full-precision EE models, it can be noticed that despite having 46 times lower model sizes on an average, the BEExformer gives comparable performance.\n\nSECTION: 5.2Pareto Optimality\n\nFigure3gives a clear picture of the pareto-optimality in terms of performance versus efficiency trade-off. Here, the proposed BEExformer gives the Pareto-optimal solution for both comparisons with quantized models as well as EE models. This can be attributed to the effectiveness of the proposed BAT approach along with the SLFN-based transformer blocks, which are potent in selectively capturing significant context in sequences. Moreover, the EE mechanism solves the \"overthinking\" problem by providing correct inference with minimal processing. This translates into reduced latency during inference.\n\nSECTION: 5.3Study of Model Ablations\n\nTable2also presents the results from various ablations of the proposed BEExformer. Here, BEExformer (WEE) without EE is the most lightweight version, while the full precision version, i.e. BEExformer (FP), gives the best results among the ablations. Despite being 18.44 times smaller than BEExformer (FP), i.e. the full-precision version with EE, the proposed BEExformer experiences a minimal performance drop of 3.58% across tasks. However, the proposed BEExformer leads over the full-precision version when EE is not applied, i.e. BEExformer (WEE-FP) with a 2.66% improvement in spite of being 7.18 times smaller in size. Additionally, the difference in performance between BEExformer (FP) and BEExformer (WEE-FP) is 6.24%, together with a difference of 5.98% between the proposed BEExformer and BEExformer (WEE). This highlights the efficacy of the EE mechanism in countering the effect of \"overthinking\" on performance. Given the performance versus efficiency trade-off, the proposed BEExformer gives the best results compared to all its ablations.\n\nSECTION: 5.4Distribution of Exit Points\n\nFor a deeper insight, we plot the exit point distribution along with the number of parameters saved, i.e. not computed due to EE during inference over all the tasks in Figure4. It appears that the majority of the observations exit after the second transformer block, while negligible samples need to go through the last transformer block. This highlights savings in terms of compute during inference compared to traditional models, where all samples get processed by all the layers in the neural network. The total number of parameters saved from computation during inference is proportional to how early the exit condition is satisfied. Its maximum effect is noticed at the second exit and starts decreasing thereafter. For subsequent exits, the plots indicate that despite having a higher frequency in some cases, their impact on saving parameters might not be that pronounced. An interesting observation is that none of the samples exit from the first block during inference. It can be attributed to the substantial reduction in entropy from the initialized valueafter the execution through the first transformer block. It exhibits that each transformer block possesses the potency to extract substantial information from the input and make inferences based on it. Furthermore, Table3presents the percentage reduction in overall FLOPs during inference over all tasks compared to the variant without EE. On average, the EE mechanism in BEExformer saves around 54.85% FLOPs during inference accompanied with 5.98% increase in performance. It affirms our motivation behind EE to utilize fewer transformer blocks than present in the architecture during inference.\n\nSECTION: 5.5Effect of EE Threshold\n\nWe observe that the EE threshold () plays a pivotal role in determining the trade-off between efficacy of results and reduction in FLOPs during inference. Figure5shows comparison of performance of the value ofproposed in this paper, i.e. 0.0001 with. Although largervalue reduce the inference time with 4.59% average reduction in FLOPs, the confidence in results decreases translating into a drop in performance by 3.53%. Alternatively, for smaller values of, the confidence in outputs increases with a slight delay in inference. Thus, settingproves to be a win-win situation with a rise in performance along with reduced FLOPs for inference. It tackles the case of \"overthinking\" wherein the entropy of logits starts rising instead of decreasing. On the other hand, EE lets the inputs with even a slight reduction in entropy to be processed further while terminating execution otherwise.\n\nSECTION: 6Conclusion\n\nIn this paper, BEExformer\u2014a binarized transformer architecture with selective learning and EE for textual content\u2014has been proposed. It incorporates a differentiable piecewise approximation to binarization during the training process to ensure gradient computation takes into account both the magnitude and the sign of real-valued weights to update the final binarized weights. This enables a manifold reduction in memory requirements with performance at par to full-precision models, enabling deployment on resource-constrained edge devices. Furthermore, it has fast inferencing capability, allowing it to exit from intermediate transformer blocks based on an intuitive technique monitoring entropy changes in logits. This reduces the overall FLOPs during inference, translating into reduced latency. Besides, the EE offers a solution to the \"overthinking\" problem intrinsic to LLMs. Extensive evaluation on a range of tasks in the GLUE data-set, showcases its ability to deliver pareto-optimal results concerning both efficacy as well as efficiency. However, the proposed architecture has certain limitations too. The current version is based on the transformer encoder and is limited to inferencing tasks only. Additionally, being a dynamic architecture, it is difficult to predict the inference time and power consumption once the model is deployed. In the future, we aim to overcome these issues by exploring how to modify the proposed BEExformer architecture to accomplish generative tasks. Furthermore, a procedure can be devised to accurately predict the power consumption and time taken for inference for a given input.\n\nSECTION: Acknowledgments\n\nNone. The author(s) received no financial or any other kinds of support for the research, authorship, and/or publication of this article.\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.05225v1_content.txt"}, {"title": "Transformers Meet Relational Databases", "authors": ["Jakub Pele\u0161ka", "Gustav \u0160\u00edr"], "published_date": "2024-12-06T17:48:43Z", "summary": "Transformer models have continuously expanded into all machine learning\ndomains convertible to the underlying sequence-to-sequence representation,\nincluding tabular data. However, while ubiquitous, this representation\nrestricts their extension to the more general case of relational databases. In\nthis paper, we introduce a modular neural message-passing scheme that closely\nadheres to the formal relational model, enabling direct end-to-end learning of\ntabular Transformers from database storage systems. We address the challenges\nof appropriate learning data representation and loading, which are critical in\nthe database setting, and compare our approach against a number of\nrepresentative models from various related fields across a significantly wide\nrange of datasets. Our results demonstrate a superior performance of this newly\nproposed class of neural architectures.", "arxiv_id": "2412.05218v1", "html_link": "https://arxiv.org/html/2412.05218v1", "search_term": "ti:\"transformers\"", "html_content": "SECTION: Transformers Meet Relational Databases\n\nTransformer models have continuously expanded into all machine learning domains convertible to the underlying sequence-to-sequence representation, including tabular data. However, while ubiquitous, this representation restricts their extension to the more general case ofrelational databases. In this paper, we introduce a modular neural message-passing scheme that closely adheres to the formal relational model, enablingdirectend-to-end learning of tabular Transformers from database storage systems. We address the challenges of appropriate learning data representation and loading, which are critical in the database setting, and compare our approach against a number of representative models from various related fields across a significantly wide range of datasets. Our results demonstrate a superior performance of this newly proposed class of neural architectures.\n\nSECTION: IIntroduction\n\nWhile the approaches to mathematical modeling of complex systems, ranging from control theory to machine learning (ML), evolved in various independent ways, one aspect remained almost universal \u2014 thedata representation. Irrespective of the used models, from decision trees to neural networks, virtually all ML libraries expect input samples in the form of fixed-size numeric tensors, most often just (feature) vectors. Assuming the data samples as independent points in-dimensional spaces is extremely convenient and allows for building directly upon the elegant foundations of linear algebra and multivariate statistics[1]. However, actual real-world data is not stored in numeric vectors or tensors but mostly in the interlinked structures of internet pages, knowledge graphs, and, particularly,relational databases. Indeed, while there are numerous data storage formats, the traditional relational database management systems (RDBMS) arguably dominate the industry, from medicine and engineering to enterprise application domains[2].\n\nIn recent years, we have witnesseddeep learningto quickly dominate all perceptual domains, from vision and speech to language. Nevertheless, it remains very rare to encounter neural models on the classictabular datawith heterogeneous features, where standard statistical models, mainly various decision tree ensembles[3], still appear to lead the benchmarks[4]. Improving the performance of the neural models, primarily the omnipresentTransformerarchitecture[5], on tabular datasets gains increasing amounts of attention, sometimes quoted as the \u201clast unconquered castle\u201d for deep learning[6]. Nevertheless, generalizing Transformers from the tabular to the fullrelationaldata model posits arguably an even bigger challenge.\n\nIn this paper, we introduce a new class of such deep learning architectures aimed directly at relational database representation while utilizing insights from the established field ofrelational learning[7], which is concerned with such generalizations of statistical models.\n\nThe core contribution of our work, put into context of related work in Sec.II-F, is the design of a new neural message-passing scheme following the formal relational model while deeply integrating the existing (tabular) Transformer architectures. The implementation of the proposed framework is readily available at Github.111https://github.com/jakubpeleska/deep-db-learning\n\nSECTION: IIRelated Work\n\nWhile the body of work on using deep learning with relational databases themselves is extremely scarce, there are established machine learning areas that either use neural models on simpler data structures or address relational structures with other (non-neural) models. In this section, we first briefly review these fields, often overlooked in deep learning, to properly position the contribution of our work (Sec.II-F).\n\nSECTION: II-ATabular models\n\nTabular neural models[8]are concerned with transferring deep learning strategies into the (classic) tabular data setting, currently still largely dominated by standard statistical models, such as gradient-boosted trees[3]. These commonly aim to amend the Transformer architecture[5]to better fit the complex, often heterogeneous and discrete, attribute structure of the tabular data.\nSome notable models in this category include the TabNet[9], which uses a custom-modified transformer-based architecture; TabTransfomer[10], which focuses on categorical values while utilizing the original Transformer Encoder structure; SAINT[11], which introduced the concept of inter-sample attention; and Trompt[12], which takes inspiration from prompt learning of language models.\nWe note that these tabular Transformers are sometimes (confusingly) referred to as \u201crelational.\u201d However, they do not follow the actual relational (database) model and cannot be (directly) used as such.\n\nSECTION: II-BStatistical relational learning\n\nFor decades[13], proper learning with actual relational representations has been the concern of the little-known field of Relational machine learning[14]. It builds heavily on the formalism of first-order logic (FOL)[15], in which the tabular representation and the corresponding models are effectively viewed aspropositional, while the database representation, corresponding formally to a subset of FOL, requiresrelationalgeneralization(s) of such models.\nMany such FOL-based methods have been proposed, mostly following the paradigm of Inductive Logic Programming (ILP)[16], later extended with probabilistic methods under the umbrella of Statistical Relational Learning (SRL)[7]. The most appropriate SRL works capable of learning from database representations then follow the paradigm of \u201clifting,\u201d[17]referring to the generalization of classic statistical models into the relational setting.\nHowever, building on the FOL foundations, the SRL models typically do not scale well and, importantly, do not offer the latent representation learning capabilities of neural networks.\n\nSECTION: II-CPropositionalization\n\nFrom the SRL view, the Tabular Transformers address the exact same representation expressiveness as their classic tree-based counterparts they aim to surpass. The tabular, also known as \u201cattribute-value,\u201d data format\nis an established ML representation perpetuating the whole field.\nWhile much of the real-world data structures, such as relational databases, donotfit into this representation, a natural urge arises to transform such structures into the expected format and proceed with the standard models. This practice, generally referred to aspropositionalization[18], is the traditional method of choice that has dominated the industry[19,20]. Propositionalization is essentially a data preprocessing routine where relational substructures get extracted and aggregated into standard statistical (tabular) attributes corresponding to various select-join-aggregate (SQL) routines in the database setting.\nBuilding on decades of practice, the resulting (statistical) models using the resulting attribute vectors typically perform very well. However, their representation learning capabilities are principally limited, as the preprocessing (denormalization) stepnecessarilyintroduces an information loss.\n\nSECTION: II-DNeuro-symbolic models\n\nAn interesting area on the intersection of proper relational (logical) representations and deep learning is known as Neural-Symbolic Integration[21]. There is a (small) number of neuro-symbolic frameworks that operate with some (subset of) FOL representation, effectively covering the relational databases while marrying the principles of neural networks through deep integration, such as Neural Theorem Provers[22], Logic Tensor Networks[23], or Lifted Relational Neural Networks[24]. These methods are, in theory, capable of actual deep learning from relational databases. However, to the best of our knowledge, none of these methods scales to real-world database sizes due to the complexity associated with their FOL-based foundations, except for those that follow some form of the propositionalization scheme under the hood, such as[25].\n\nSECTION: II-EDeep relational models\n\nThe closest related work consists of extending standard neural models towards relational representations.\nThe most prominent models in this category are Graph Neural Networks (GNNs)[26]designed for end-to-end learning with graph-structured data. There are currently hundreds of the original GNN model[27]variants, some of which are close in spirit to our proposal, particularly some of the hyper-graph[28]and multi-relational[29]extensions towards knowledge-graph applications[30]. Nevertheless, the graph-based view adopted within this stream of research is generally not concerned with the salient features specific to relationaldatabases, particularly with the rich inner structure of the individual records.\n\nThere have been only very few works that address (some of) the database-specific aspects. Particularly, the original work of[31]followed by an (unsuccessful) pre-training procedure in[32], and the work of[33], which further incorporated feature engineering and random architecture search to improve its performance.\nA different line of work has been to utilize techniques from pre-training (large) language models while treating related database tuples as sentences, similarly to the tabular models[34], such as in[35].\n\nIn a similar spirit, the authors of[36]presented a (draft) vision for foundational database models, later shifting focus to scaling up GNNs for the task in[37]by leveraging symmetries. Likewise, a recent position paper of[38]aimed to establish \u201crelational deep learning\u201d as a new machine learning subfield while introducing a framework for benchmarking the GNN models,222focusing heavily on the temporal dimension of database records, which we explore experimentally in App.Csuch as[39],[40], and[41].\n\nSECTION: II-FOur Contributions\n\nOur work can be seen as a continuation of these deep relational learning efforts, most notably the work of[41]that this paper directly expands. Particularly, we extend the existing GNN paradigm by tightly integrating the Transformer architecture into the relational message-passing scheme.\nThus, apart from proper treatment of the inter-relational structure, we also incorporate, in the spirit of the tabular Transformers, theintra-relationalstructure of the attributes, embedded end-to-end within the same learning scheme. Covering the GNN efforts as a special case, we introduce the most complete framework for deep learning withactualrelational (SQL) databases, demonstrating superior results over the widest range of available benchmark datasets reported thus far.\n\nSECTION: IIIBackground\n\nSECTION: III-ARelational Databases\n\nThe principles of relational databases are formally based on therelational model[42], rooted in FOL[15], providing a unified declarative specification for managing structured data, irrespective of the particular software implementation. This abstraction allows the definition of any database as a collection of-ary relations defined over the domains of their respective attributes, managed by the RDBM system to ensure consistency of the data with the integrity constraints of the logical database schema.\nThe key concepts to be used in this paper are as follows.\n\nFormally, an-ary relationis a subset of the Cartesian product defined over the domainsof itsattributesas, where. Each relationconsists of a heading (signature), formed by the set of its attributes, and a body, formed by the particular attribute values, which is commonly viewed as atableof the relation.\n\nAttributesdefine the terms of a relation, corresponding to thecolumnsof the respective table. Each attribute is a pair of the attribute\u2019s name and atype, constraining the domain of each attribute as. An attributevalueis then a specific valid value from the respective domain of the attribute.\n\nAntuplein a relationis a tuple333the ordering is instantiated through the naming of the attributesof attribute values, whererepresents the value of the attributein. The relation can thus be defined extensionally by theunorderedset of its tuples:, corresponding to therowsof the table.\n\nBesides the domain constraints, the most important integrity constraints are the primary and foreignkeys. Aprimarykeyof a relationis a minimal subset of its attributesthat uniquely identifies each tuple:\n\nAforeignkeyin relationthen refers to the primary keyof another relationas\n\nThis constitutes the inter-relations in the database, with the RDBMs managing thereferential integrityof.\n\nSECTION: III-BDeep Learning\n\nDeep learning[43]is a paradigm characterized by the use ofgradient descentto optimize parameters of nested functions, commonly viewed through their computation graphs, referred to asneural networks. The main conceptual idea lies in learninglatent representationsof the data corresponding to the inner layers of the networks, generally constrained to the form of fixed-size numeric tensors, which restricts directly applying deep learning to relational databases. While passing beyond that limitation, we will generalize upon concepts known from two neural architectures that address two forms or related (simpler) structured representations ofsequencesandgraphs.\n\nThe Transformer[5]is a popularsequence-to-sequencemodel, relying primarily on the \u201cattention\u201d mechanism for inter-relating the given sequence tokens. Each input tokenhere isembeddedinto a continuous vector representation:, and combined with a \u201cpositional encoding\u201d capturing its positional role:.\nTheself-attentionmechanism then inter-relates all pairs of the input tokens to update their values as\n\nwhere,, andare the so-called \u201cquery\u201d, \u201ckey\u201d, and \u201cvalue\u201d matrix projections (\u201croles\u201d) of the input embeddings. This efficient matrix computation can (optionally) be further repeated in parallel with separateprojection matrices (multi-head attention).\n\nIn addition to the self-attention, Transformers employcross-attentionfor tasks involving two distinct streams of sequences. In cross-attention, the query matrixis derived from the target () sequence decoder\u2019s input, while the keyand valuematrices are derived from the source () sequence encoder\u2019s output as.\nIn either case, the updated valuesthen position-wise pass through two standard feed-forward network (FNN) layers:, followed by layer normalization to reduce internal covariate shift, and residual connections for improved gradient propagation.\n\nGNNs are a general class of neural models aimed atgraph-structureddata using the concept of (differentiable)message-passing[26]. Given an input graph, with a set of nodesand edges, letbe the vector representation (embedding) of nodeat layer.\nThe general concept of GNNs can then be defined through the following sequence of functions:\n\nMessagefunctioncomputes messages for each edge:\n\nAggregationfunctionaggregates the messages for each:\n\nUpdatefunctionupdates representation of each:\n\nThe particular choice of the message, aggregation, and update functions then varies across specific GNN models, which are commonly composed of a predefined numberof such layers, enabling the message-passing to propagate information across-neighborhoods within the graph(s). Note that the attention module of the Transformer follows the same schema while assuming a fully connected graph.\n\nSECTION: IVProposed Architecture\n\nIn this section, we describe the proposed learning representation and the relational message-passing architecture designed for end-to-end deep learning of Transformers from databases.\n\nSECTION: IV-AData and Learning Representations\n\nIn order to directly follow the inductive bias of the relational database model (Sec.III-A), we consider the learning representation of a database as atwo-level multi-relational hypergraph, where (i) each relationforms-ary hyperedges corresponding to the-tuplesintra-relatingits attributes, and (ii) each pairof such relationsinter-relatedthrough the foreign key constraintsforms another set of hyperedges from the respective tuple pairs. Note we consider all the tuple attributesto form the link, and not just their keys, as these may also be composite, possibly spanning the whole tuple as a corner case of, hence the forming ofhyperedgesinstead of just edges here.\n\nAdditionally, for each such foreign-key tuple pair, we also consider the \u201creverse\u201d hyperedgeto be able to fully propagate learning representations throughout the database, irrespective of the (ad-hoc) ordering choices of the database designer.\n\nWe then use the tuple pairs ofandto build a bi-directional bi-partite hypergraph, connecting the tuples of the individual relations, for each foreign key constraint in the database schema.\n\nWe aim at direct deep learning from raw database storage systems with as little preprocessing as possible while retaining the proper relational model semantics[42], for which we consider the relations\u2019 attribute valuesas the minimal processing unit, building on the formal assumption ofatomicity[42].\nHowever, the current RDBMSs do not preserve the respective attribute type semantics required for deep learning. For instance, for integer-type (\u201cint\u201d) columns, the information on whether the data contained are of nominal, ordinal, or cyclic nature is missing. Similarly, string-type (\u201cvarchar\u201d) columns may either contain actual text or encode discrete categories. However, such information is crucial to properly process the data with the neural models.\n\nA distinction must also be made about attributes that form the key constraints as to whether they convey actual information or serve merely the referential purpose. To resolve such issues while avoiding manual data preprocessing, we have built an automated procedure that attempts to determine all such information from the database schema based on a combination of simple heuristics and selected data statistics.\nOnce the schema (Sec.III-A) is detected with all the attributetypesdetermined, we first proceed with theirencodingto numerical values. Notably, we (optionally) transform the textual types with a pre-trained language model, particularly Sentence-BERT[44](App.C).\n\nWe then continue withembeddingof the attributes in an appropriate fashion. Particularly, following methods from the tabular Transformers (Sec.II), we use a simple lookup table that stores embeddings of the detected categorical types, and \u201cstack\u201d or \u201clinear\u201d embedding of the numeric types (see App.C-Afor details). Additionally, we (optionally) include the cyclic (\u201cdate/time\u201d) types with a special embedding respecting the periodic structure of the timestamp[45]. Importantly, each attribute has its own embedding function to allow for separate latent spaces.\n\nFor machine learning, we need to establish what constitutes the learning samplesin the given relational setting. In this paper, we consider the standard (self-)supervised scenario where a single attributeof a single target relationforms the output labels. Nevertheless, in contrast to the (classic) tabular setting, the input examplescan no longer be considered as i.i.d. tuples.\n\nThere are generally two cases: either (a) the database contains separate relational samples where each rowof the target tablebelongs to a single learning instance, or (b) the database cannot be split into such separate components, withpossibly spanning the whole hypergraph structure. To extract batches of the learning samples, irrespective of the structure, we follow a simple breadth-first-search (BFS) procedure, starting from each rowof the target tableand expanding over all the tables related through the foreign key constraints, in both the referenced and the referencing directions, while checking for loops.444Due to the possible interdependence between the samples, care must be taken to prevent information leakage about the labels, for which we mask out all target labels from the target columnofwhen processing the samples. Overlooking this precaution led to some inappropriate accuracy reports in some of the related works.\n\nA salient feature of relational databases is that they can be very large, for which we optionally allow to run the loading nativelyin-databasethrough recursive SQL (self-)joins\nwith which minibatches of the hypergraph samplesmay be fetched into memory in a lazy fashion (with caching) from the, possibly remote, RDBMS. To make sure that the resulting hypergraph samples fit into memory, particularly in the (b) case, we (optionally) bound the BFS with a depth limit.555In inductive learning settings, this limit can be set to correspond to the perimeter of the relational receptive field of the subsequent neural message-passing, corresponding e.g. to the number of layers in GNN models (Sec.III-B), without loss of information.\n\nNevertheless, in the (most) cases where the whole database simply fits into memory, the whole hypergraph structure can be conveniently loaded and accessed with the more flexible neighborhoodsamplingtechniques[46]. Particularly, we utilize the heterogeneous graph sampling routine introduced in[47], which proved most suitable for our relational setting.\n\nSECTION: IV-BNeural Architecture Space\n\nTo natively facilitate deep learning on the two-level hypergraph structure of the relational model (Sec.IV-A), we introduce a general two-level neural message-passing scheme composed of modular differentiable parameterized operations defined on the levels of (i) individualattributes(ii) and (sets of) relatedtuples. We further divide these operations w.r.t. their input-output characteristics into three categories:\n\nstandardTransformations\n\n-aryCombinations\n\npermutation-invariantAggregations\n\nwhere,andmay refer to either the attributesor the tuples.\nNote that this can be seen as an extension of the \u201cmessage-aggregate-update\u201d paradigm of the GNNs (Sec.III). An instance of the proposed scheme is outlined in Fig.1.\n\nEvery instantiation of the scheme starts with the embedding (Sec.IV-A) transformation (\u201cEmbedder\u201d) of the individual relationattribute values, resulting into an-tuple of vectorsper each original tuplefrom.666The attribute embedding dimensionswithin and across the relations may generally differ, so as to accommodate the possibly varying information loads in the tables, but in this paper we set them to be the same for simplicity.Each such tuplethen undergoeseither(i) an attributecombination\n\nthat merges the attribute embeddings into a joint tuple embeddingor (ii) a tupletransformation\n\nthat keeps the attribute embeddings separate as.\nIn either case, the resulting tuple representationsubsequently enters the second level of neural computation where it gets combined with all the tuples related through the second type of hyperedges (Sec.IV-A). Particularly, eachundergoes a tuplecombination\n\nwith each, where, resulting into a set ofrepresentations for each such pair ofand the related.\nEach such set of the combined representations then undergoes a tupleaggregation\n\nwhere, to obtain onerepresentation.\nFinally, weaggregateall such tuple representations\n\nfrom all thelinked relations back into a single final tuple representationfor each.\nImportantly, the same computation is performed simultaneously foreachrelationin the database, and the resulting representations may be used again as input into subsequent layers of the same computation scheme in the classic spirit of deep learning.\n\nAdditionally, the scheme allows for optional intermediate blocks (dashed borders in Fig.1).\n\nFirst and foremost, this includes a \u201cpost-embedding\u201d block that addresses the outlined division into the two options of (i) attribute combinationand (ii) transformationin the first step of the scheme. Notably, combining the attributes in the (i) case\ndisposes of the original column structure of, reducing the data dimensionality fromto, and turning the remainder of the scheme into a largely standard single-level heterogeneous GNN computation[48], as explored in some of the related works (Sec.II). Such operation can range from a simple concatenation toTabular Transformersthat themselves combine columns into a single row embedding, such as Trompt[12]or TabNet[9]. Opting for the (ii) transformation then retains the original tabular structure throughout the scheme, for which we utilize operations ranging from simple positional encoding to tabular Transformer blocks retaining the columns, such as the SAINT[11]and TabTransformer[49].\n\nThe subsequent (optional) tupletransformationthen follows the same logic while beingrepeatedlyapplied at the beginning of each layer of the scheme, for which the chosen model has to comply with the respective interface.\nFinally, the scheme allows for a closing (optional) tuple combination, facilitating a residual connection stream in the overarching relational part.\n\nSECTION: IV-CTheDBFormer\n\nTechnically, any differentiable parameterized operations that satisfy the corresponding input-output interface of the transformation, combination, and aggregation operators can be used in their respective places within the scheme, some of which are presented in our experiments (Sec.V).\nNevertheless, we highlight one particular instantiation that we deem to most closely integrate the essence of the original Transformer architecture[5]with the relational database model (Sec.III), which we further refer to as theDBFormer, depicted in Fig.1.\n\nFirstly, the model instantiates a Transformer Encoder in place of the tupletransformation, facilitatingself-attentionover the relations\u2019 attributes in the standard spirit of the tabular Transformers[8], but repeated across the database and over the layers, as part of the relational scheme. Secondly, the model also usescross-attentionin place of the tuplecombinationas\n\nessentially forming a Transformer Decoder from the remaining part of the scheme per each pair of interrelated relations.\n\nWe hypothesize that the cross-attention module used in this place might be able to extract the necessarylatentrelational features, as exploited with the successful propositionalization methods (Sec.II), but in a fully end-to-end fashion through gradient descent. Based on the notable expressiveness of Transformers[50], the select-join-aggregate operations normally used to construct such relational features should be well within the hypothesis space of the resulting architecture, in which we assume the query, key, and value roles of the input tokens to correspond to the foreign-key, primary-key, and column-value roles of the individual attributes, respectively.\nThe idea is that the self-attention firstly transforms the tuple attributes w.r.t. each other within the tables, the cross-attention then learns their contextual interactions with attributes from the referenced tuples, and the attention-sum finally weights all their importance w.r.t. the referencing tuples.\n\nSECTION: VExperiments\n\nWe test777The source code for the experiments can be found athttps://github.com/jakubpeleska/deep-db-learningand the web server serving the database datasets is made publicly available athttps://relational.fel.cvut.cz/a number of instantiations of the proposed scheme against representative models from the distinct related work categories (Sec.II) through standard supervised classification and regression tasks across a wide range of diverse relational database datasets.\n\nSECTION: V-ADatasets\n\nWhile RDBMs are some of the most widespread data storages, publicly available relational database benchmarks are considerably scarce. There are numerous collections of classic\ntabular[52]and structured datasets[53,54], including graphs[55,56], some of which are conceptually close to the database setting[57,58]. Nevertheless, none of these provideactualrelational database representations.888Instead, they present simplified CSV, JSON, or XML files that do not fully represent the RDBM setting.Thus, as part of this work, we have re-established the most complete resource collection in this area, originally created by[51], where we currently maintain overof actual (SQL) database datasets from various domains, together with historical\nscoreboards and additional statistics.999This collection also covers most of the previous benchmarks from the domain of relational learning (Sec.II).In this paper, we narrow these down to 19 classification (Tab.III) and 16 regression (Tab.IV) datasets, filtering out (uninteresting) databases that are either too small or too trivial to fit.\nThe remaining datasets are of highly diverse characteristics w.r.t. their sizes, schemas, structures, and application domains, as further detailed in App.B.\n\nSECTION: V-BRelated work models\n\nAs a baseline instance of the scheme, we consider a simpletabularFNN model[6]operating solely on the target table, i.e., ignoring all the inter-relations. This naive strategy is useful in revealing whether the given dataset task is indeed relational in nature or not.\nFrom the statisticalrelational learning(Sec.II), we choose the state-of-the-art RDN-boost[59], which, following the lifting strategy, can (very roughly) be seen as a relational generalization of the popular gradient-boosted trees[3].\nAs thepropositionalizationrepresentative, we select the FastProp algorithm followed by XGBoost[60]\u2013 a battle-proof combination as promoted in[20], which leads a number of the relational dataset scoreboards[51].\nTo cover theneuro-symbolicarea, we further emulate the popular CILP++ method[25]by connecting propositionalization with a FNN model in a similar fashion.\nWe were unable to put any of the few recent deep relational learning proposals (Sec.III) into operation, but some of the closest GNN-based works can be viewed as conceptually close to the reduced (attribute combination) variants of the scheme (Sec.IV-B).\n\nSECTION: V-CScheme instantiations\n\nAs the space of all the possible neural models within the proposed scheme is very large, we tested only a few selected instantiations. This means selecting some particular parameterized differentiable operations in place of the initial Embedder module, and the attributeand tupletransformations, combinations, and aggregations(Sec.IV-B).\n\nThis model, already detailed in Sec.IV-Cand Fig.1, consists oflayers where each can be defined as\n\nWith this instantiation, we further tested extending the initial baseline Embedder (Sec.IV-A), transforming merely thecategoricalandnumericalvalues to embedding vectors with the use of lookup tables and linear transformations respectively, with a number of ablations described in detail in App.C.\n\nThis model can be seen as a \u201creduced\u201d version of the proposed scheme for its use of the attribute-combination function that flattens the columns\u2019 dimension as, whereand. The reduced dimensionality then allows for the use of standard graph convolution modules. Particularly, we employed the SAGE[61]convolution, with which therepeating layers can be described as\n\nThe model uses the baseline Embedder, and the residual combination module is skipped.\n\nThis instance is designed to closely follow the tabular architecture of Trompt, as introduced in[12]. The Trompt Encoder is used once at the beginning as the \u201cpost-embedding\u201d (Sec.IV-B) module to transform the data. Therepeating layers then have a simple definition of\n\nwhere the tuple transformation and closing combination modules are skipped, and\n\nNotably, the model utilizes the Trompt Decoder as a prediction head and has a custom Embedder that extends the baseline by following the categorical embeddings with Layer Normalization[62]. It also uses linear transformation of numerical values followed by a ReLU activation and Layer Normalization.\n\nAnother tested instance based on a tabular Transformer is the DB extension of TabNet[9]. The TabNet encoder is formed by a series of repeated Feature Transformers, each followed by the Attention Transformer.101010For further description of the Feature and Attention Transformers, we refer to the original article[9].\n\nSimilarly to the DB GNN, TabNet belongs to the \u201creduced\u201d category. Its Embedder processes only thecategoricalvariables through the embeddings lookup table, and thenumericalvariables are duplicated to the target dimension by the Stack Embedder (App.C). Itsrepeated layers can be defined as\n\nwhereis defined in Equation1.\n\nThe SAINT instance refers to the tabular model introduced in[11]. The model takes a Transformer Encoder layer and extends it by a second block that uses \u201cIntersample Attention,\u201d the details of which can be found in the article[11].\n\nThe scheme\u2019s instance utilizes the \u201cSAINT Encoder\u201d layer as the tuple transformation operation in a mixture with thecross-attentionfor the tuple combination. The model also uses the baseline Embedder with an extension that a ReLU activation function follows the linear transformation. Therepeated layers can be defined as\n\nThe last experimental instance is based on the TabTransformer[10]model. The TabTransformer architecture preprocesses only thecategoricalattributes, whilenumericalattributes are simply passed through Layer Normalization. Thecategoricalcolumns are then passed through a Transformer Encoder block.\n\nSimilarly to the TabNet instance, the Embedder uses lookup table embeddings forcategoricalattributes and a Stack Embedder fornumericalattributes to avoid transformations of the values. The rest of therepeating layers are defined as follows\n\nwithdefined in Equation1.\n\nSECTION: V-DParameterization\n\nWe follow a largely standard parameterization routine across all the methods. For the propositionalization-based related work, the number of relational features ranges around, depending on the depth of a custom BFS procedure that we implemented to improve their default performance, and the boosting works with the optimized default ofandbase estimators. For the neural methods, including the baseline tabular FNN, we follow a standard deep learning setup of tuning the embedding dimensions, learning rate, and batch size, detailed further in App.A-B.\n\nSECTION: V-EResults\n\nOur classification and regression results with the models (Sec.V-B,V-C) are summarized in TableIand TableII, respectively. Firstly, we see that many of the datasets are simply not accessible (N/A) to the tabular models (Tabular), in cases where the target table does not contain any informative attributes. Nevertheless, in the few cases where it does, even simple tabular models (FNN) perform very well, in accordance with[6]\n\nThe RDN-boost is a sophisticated SRL (Relational) method that does capture the relational inter-dependencies for which it, however, needs to set up \u201cmodes,\u201d[59]which we implemented in a rather straightforward fashion, possibly explaining its generally weaker performance. We note that we were unable to put the method into operation in the regression setting; hence, it is missing from the respective table.\nMore importantly, the method does not scale well to larger datasets, reported (also) with the missing values. This issue was partially shared with the other relational methods, too.\nThe getML (Fastprop+XGBoost) system[20], on the other hand, performed very well out-of-box, validating the strength of the propositionalization (Propos.) practice[63]. Similarly, the propositionalization-based neuro-symbolic (Ne-Sy) approach of CILP++[25]performed very strongly, too.\n\nFinally, instantiations of the proposed scheme generally displayed superior performances, with a small number of exceptions where the propositionalization shone. The overall best results were displayed by the proposedDBFormermodel (Sec.IV-C), demonstrating the strength of the close integration between the original Transformer architecture and the relational model. Nevertheless, the GNN instantiations, as well as the Tabular Transformer integrations with Trompt[11]and TabNet[9], exhibited strong performances, too.\n\nSECTION: VIConclusions\n\nWe introduced a general scheme that extends Transformers for deep learning from relational databases, utilizing a custom message-passing mechanism that adheres to the relational model of the common RDBMS. Our experiments with various instantiations of the scheme demonstrate its viability and superior performance as compared to commonly used methods from the associated fields of relational learning.\n\nTo improve the performance even further, incorporating self-supervised pre-training, in the spirit of the tabular models (Sec.II), for domain transfer across differentdatabasesseems like a promising avenue for future work.\n\nSECTION: Appendix AExperimental Setup\n\nThe output of the last layer, produced for the target table, is flattened if necessary and processed by a FNN prediction head withlayers, with each of the hidden layers followed by ReLU activation and, optionally, Batch Normalization[64]. For the standard gradient descent training in the classification tasks, the FNN output feeds into cross-entropy loss and MSE loss for the regression tasks, respectively.\n\nFor the metrics used in the results reporting, we simply leverage accuracy for the classification tasks and, to provide a somewhat comparable metric, a \u201cNormalized Root Mean Squared Error\u201d (NRMSE) is used across the regression tasks. Thefunction is defined as\n\nwhereis the mean of all the training target values, andfunction is defined as\n\nSECTION: A-AEnvironment\n\nAll the executed experiments discussed in SectionVused a simple hyperparameter optimization pipeline. The pipeline consisted of Ray[65], used for the distribution of resources and model training management; Optuna[66], used for searching over the hyperparameter space; and MLFlow[67], used for aggregating the parameters and metrics.\n\nAs for hardware, the training runs were split into two categories based on the dataset size, more precisely based on the number of rows in the target table (App.B). The runs on the datasets with less than or equal to 10,000 rows were trained on a single core of theAMD EPYC 7742 64-CoreProcessor and runs on larger datasets were executed onNVIDIA A100-SXM4 40GBGPU with a maximum of 4 runs sharing a single GPU.\n\nSECTION: A-BHyperparameters\n\nThere were 16 runs per model and dataset executed as part of the hyperparameter search, each running for 4000+ training steps111111With an exception of models that reached a hard training limit of 2 hours, however, this limit was surpassed on only the most extensive datasets such as \u201ctpcd\u201d (App.B) with large models. Nevertheless, extending this limit possibly allows for future improvements.on a standard 70:30 training-validation split. All the neural models used vanilla Adam[68]optimizer with a learning rate set as a hyperparameter on a logarithmic space within. The heterogeneous graph sampling routine (HGSampling), as described in SectionIV-B), facilitated the data sampling where the batch size was parametrized by the dataset size, with a hyperparameter scale factor from an exponential space in the interval, and limited to a value of, whereand; hence the batch size always remained in the interval of. The embedding dimensionwas also a hyperparameter in the search space, defined as a choice from the set of. The number of layersinside the scheme\u2019s instances was set as a random integer from.\nThe decision-making decoder FNN head was parametrized by the number of linear layersthat was 1, 2, or 3, where each hidden layer hadchannels and a flag whether to use the \u201cBatch Normalization.\u201d\n\nSECTION: Appendix BDatasets\n\nThe database datasets[51]used for the classification and regression tasks can be viewed in TablesIIIandIV, respectively. The tables contain statistics about the relational databases that they represent: \u2018Num. Rels.\u2019 - number of relations inside the database, \u2018Num. Edge. Types\u2019 - number of primary, foreign key pairs, \u2018Num. Targ. Cols.\u2019 - number of non-key columns in the target table, \u2018Avg. Targ. Edges\u2019 - the average number of references from a single target table row to other tables, \u2018Total Num. Rows\u2019 - the overall number of rows in all tables of the database, such as \u2018Total Num. Edges\u2019 - the overall number of primary, foreign key pairs between all tables of the database, \u2018Text Col.\u2019 - whether the database contains non-key text attribute, and \u2018Time Col.\u2019 - whether the database containsdatetimeattribute.\n\nSECTION: Appendix CDBFormerablation studies\n\nIn this appendix section, we report the ablations performed with the mainDBFormermodel. The ablations are aimed to assess the sensitivity of the results w.r.t. (i) the selection of the initial embedding and (ii) the selection of the hyperparameters.\n\nSECTION: C-AEmbedders\n\nThe initial processing of data can often significantly influence the effectiveness of a model. Building on the work done in the field of tabular models (Sec.II), there is a variety of possible approaches. The categorical variables are almost always encoded with a simple embedding lookup table, with the exception of the models that do not use categorical variables at all, e.g., Excelformer[69]. Nevertheless, for the other variable types, several options may be considered.\n\nStack Embedder: the simplest option to increase the dimensionality of thenumericattributes is to copy the valuetypes in the embedding vector, whereis the target dimension of the embeddings.\n\nLinear Embedder: a linear layer withno activationfunction, one input channel, andoutput channels is another common way to create the embedding vectors out ofnumericvariables.\n\nText Embeddings Transcoder: as discussed in SectionIV-B, plain text data from the database can be processed by a pre-trained language model. While it is unlikely that the language model embedding dimension will match the set-out dimension, a linear layer with no activation can again be leveraged to address the dimensionality difference.\n\nTimestamp Embedder: the most sophisticated embedding we considered is to account for the possible periodical information that might be encapsulated by the year, month, day, etc., of the timestamp attributes, for which the embedder first uses cyclic encoding with a combination of positional encoding to dimension, where, and only then puts the output through the linear layer to get embeddings of dimension.\n\nThe classic tabular Transformer models usually only take the opportunity to combine simple embedding for thecategoricalvariables with either the Stack or Linear Embedder for thenumericalvariables. However, usage of the text and timestamp attributes can potentially lead to performance gains.\nTheDBFormer, representing the leading model of this paper, was thus further tested with an additional list of such embedding options as follows:\n\nBaseline (base): the embedder uses onlycategoricalandnumericalvariables with a simple embeddings lookup table and a Linear Embedder.\n\nWith Text (text): extends the baseline embedder withtext embeddingstransformed by the Text Embeddings Transcoder.\n\nWith Time (time): extends the baseline embedder with datetime attributes transformed by the Timestamp Embedder.\n\nTableVcompares the performance of the baselineDBFormersetting to the one leveraging the textual embeddings. As can be seen, the textual embeddings significantly improve the model performance, confirming the usefulness of the information present in the often overlooked textual attributes.\n\nThe recently proposed work of[57]heavily emphasized the time dimension in the\nrelational database setting. To experimentally evaluate its importance, TableVIshows the comparison of theDBFormermodel utilizing the time attributes with the Timestamp Embedder to its baseline version. As can be seen, the Timestamp Embedder strongly improves the performance on almost all relevant datasets, again validating the importance of the information present in the time attributes. Employing both text and time attributes thus showed significant improvements in performance.\n\nSECTION: C-BHyperparameter sensitivity\n\nAll the previous experiments were carried out with the utilization of the reported hyperparameter optimization (App.A-B). To test the robustness of the mainDBFormerarchitecture, we also present results without the hyperparameter tuning over three versions of the model listed below.\n\nLarge: embedding dimension = 64, schemelayers = 4, attention heads = 4, decoder hidden layers = 2, decoder hidden channels = 64\n\nMedium: embedding dimension = 32, schemelayers = 3, attention heads = 4, decoder hidden layers = 2, decoder hidden channels = 64\n\nSmall: embedding dimension = 16, schemelayers = 2, attention heads = 2, decoder hidden layers = 2, decoder hidden channels = 32\n\nAll three models were trained with a learning rate of 0.0001 using the vanilla Adam optimizer. The dropout rate inside the attention modules was set to 0.1, and all the decoder heads utilized the Batch Normalization. The initial Embedder module did extend the baseline with both the Text Embeddings Transcoder and the Timestamp Embedder in all cases, with all the remaining settings (App.A) being fixed.\n\nThe results in TableVIIshow that theDBFormermodel keeps displaying superior results, even without the hyperparameter tuning, and demonstrates the robustness of the architecture. Notably, theLargemodel is within 3% of the accuracy of the optimizedDBFormer*model (highlighted in bold) on the majority of the classification datasets. TheMediumandSmallmodels then performed adequately well, even outperforming theDBFormer*in a few cases where the hyperparameter optimization apparently did not find the best settings (highlighted by underlining).\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.05218v1_content.txt"}, {"title": "Global Optimization with A Power-Transformed Objective and Gaussian\n  Smoothing", "authors": ["Chen Xu"], "published_date": "2024-12-06T17:33:43Z", "summary": "We propose a novel method that solves global optimization problems in two\nsteps: (1) perform a (exponential) power-$N$ transformation to the\nnot-necessarily differentiable objective function $f$ to obtain $f_N$, and (2)\noptimize the Gaussian-smoothed $f_N$ with stochastic approximations. Under mild\nconditions on $f$, for any $\\delta>0$, we prove that with a sufficiently large\npower $N_\\delta$, this method converges to a solution in the\n$\\delta$-neighborhood of $f$'s global maximum point. The convergence rate is\n$O(d^2\\sigma^4\\varepsilon^{-2})$, which is faster than both the standard and\nsingle-loop homotopy methods. Extensive experiments show that our method\nrequires significantly fewer iterations than other compared algorithms to\nproduce a high-quality solution.", "arxiv_id": "2412.05204v1", "html_link": "https://arxiv.org/html/2412.05204v1", "search_term": "ti:\"transformers\"", "html_content": "SECTION: Global Optimization with A Power-Transformed Objective and Gaussian Smoothing\n\nWe propose a novel method that solves global optimization problems in two steps: (1) perform a (exponential) power-transformation to the not-necessarily differentiable objective functionto obtain, and (2) optimize the Gaussian-smoothedwith stochastic approximations. Under mild conditions on, for any, we prove that with a sufficiently large power, this method converges to a solution in the-neighborhood of\u2019s global maximum point. The convergence rate is, which is faster than both the standard and single-loop homotopy methods. Extensive experiments show that our method requires significantly fewer iterations than other compared algorithms to produce a high-quality solution.\n\nSECTION: 1Introduction\n\nIn this work, we consider the global optimization problem of\n\nwhereis a continuous and non-concave function with a global maximum, andis a positive integer. The minimize-version of this problem is often encountered in machine learning, such as model training and adversarial attack in computer vision. The gradient-based algorithms, such as the (stochastic) gradient descent, are commonly used, which only guarantee to approximate a locally optimal solution in a general case.\n\nHomotopy, also called graduated continuation, is a class of methods for finding a global solution to (1), with many successful applications in machine learning (e.g.,[]). It converts the original problem to\n\nwhereis called the scaling coefficient andis a random variable with a pre-selected distribution, such as a standard multivariate Gaussian distribution (Gaussian Homotopy, GH) or a uniform distribution in a unit sphere. Based on the observation thatapproaches111Note thatif.asdecreases to 0, the homotopy methods admits a double-loop mechanism: the outer loop iteratively decreases, and for each fixed value of, the inner loop solves, with the solution found in the current inner loop as the starting search point in the next inner loop.\n\nThe double-loop mechanism of the homotopy methods is costly in time. To tackle this issue,[9]propose a single-loop Gaussian homotopy (SLGH) method that iteratively performs one-step update ofand, which reduces the convergence rate fromto. However, in theory SLGH only guarantees to approximate a local optimum222Theorem 4.1 in[9]shows that SLGH approximates a solutionsuch that., which is not necessarily a global one. A time-efficient algorithm that aims at the global maximum is still to be found.\n\nTherefore, in this work, we propose a new method, namely the Gaussian Smoothing with a Power-transformed Objective (GSPTO), for solving the optimization problem of (1). According to Corollary1, GSPTO converges to a neighborhood ofwith the rate of. It indicates that GSPTO is faster than the standard homotopy and SLGH, ifis pre-selected to lie in. This point is verified by experiments in Section5, which show that the GSPTO-based algorithms (PGS and EPGS, introduced later) are significantly faster than other algorithms to produce high-quality solutions.\n\nUnder the condition ofand an additional one, there is a thresholdsuch that whenever, the Gaussian-smoothed objectiveis concave in(see[16, Main Result (Corollary 9)]). Hence, Gaussian smoothing converts the original possibly non-concave maximization problem to a concave one, if the maximum pointcoincides with. Although this condition is not true in general333This is why smoothing alone is insufficent for optimization, and is typically used in conjunction with iteratively reducing the scaling parameter, which becomes the homotopy algorithm., we can modify the objective to makeclose to(global maximum point of the original objectivebefore modification).\n\nIntuitively, if we modiftyto put sufficiently large weight on its global maximum, the global maximumshould get close enough to. One way of such modification is by taking powers of, if. The differenceis positively related with the power, which indicates that more weight is put onasincreases. Figure 1 (a) verifies this intuition with an example, and Figure 1(b) illustrates the effects of taking exponential powers.\n\nFrom the above intuition, we propose GSPTO for solving the global optimization problem (1), which is a new method that places more weight on the objective\u2019s maximum value (by increasing the gap between the global and local maximum values) before performing Gaussian smoothing. Based on GSPTO, we design two algorithms, Power Gaussian Smoothing (PGS) and Exponential Power Gaussian Smoothing (EPGS), which are featured with replacing the original objectivewith a (exponential) power transformation. Specifically, withandas two hyper-parameters, PGS solvesand EPGS solves, both using a stochastic gradient ascent algorithm derived in this paper, which does not require the differentiability of. Here,denotes a multivariate Gaussian distribution anddenotes an identity matrix of dimension.\n\nThe homotopy methods, firstly proposed in[2, Chapter 7], are intensively studied in the field of machine learning for global optimization problems.[15]derives a bound for the worst scenario of the GH algorithm in a deterministic setting (i.e., the expectationis not approximated with samples), while[8]provides a convergence analysis in a stochastic setting (i.e.,is estimated with samples). Specifically, the latter proves that with a probability greater than, the solutionproduced by their proposed homotopy algorithm is-optimal (i.e.,) aftersteps of solution-update.[6]changes the distribution of the perturbationfrom the commonly used Gaussian or uniform to the distribution that minimizes the estimation error of the gradient.[12]proposes an algorithm for learning the whole solution path produced by the homotopy. Specifically, their algorithm learns a modelthat predicts (for any) the solution to, whereis the set of model parameters to be trained.\n\nThe smoothing and homotopy methods have a large number of successful applications in machine learning, such as neural network training ([8]), adversarial attack on image classification models ([9]), solving-regularized least-square problems ([18]), neural combinatorial optimization ([6]), improving the optimization algorithms of stochastic gradient descent and Adam ([17]), and so on.\n\nThere are two existing studies,[5]and[3], that replace the originalwith a surrogate objective,, which also involves the exponential transformationbefore smoothing (andare pre-selected and fixed). But their works are different from ours. In[5], the proved result444is a convex problem given thatis positive definite andis convex ([5, Theorem 3.1]).that justifies their surrogate objective requires that, andbe selected so thatis positive semi-definite. This indicates that EPGS, for whichand, is not a special case of theirs, sinceis negative definite and violates their requirement. Moreover, their theory on the distance between the optimum point of the new surrogate andis incomplete (see[5, Section 3.2]).[3]focus on the case whereis close to 0 (see the setence below their Eq. (7)), which is very different from GSPTO\u2019s requirement thatis sufficiently large.\n\nSECTION: Contribution\n\nThis paper introduces a novel method, GSPTO, for solving global optimization problems, with the contributions summarized as follows.\n\nTo our knowledge, for global optimization problems, this is the first work that proposes the idea555Although the surrogate objective in[5]can be viewed as a special way of achieving the effect, they have not mentioned this idea.of putting more weight on the global maximum values of the objective, to decrease the distance between the optimum point before and after Gaussian smoothing (i.e.,). PGS and EPGS are two ways of realizing this idea, and future studies are motivated for finding better ones.\n\nGSPTO is faster than the homotopy methods (which also apply smoothing) both in theory and practice. According to Corollary1, GSPTO has a convergence rate of, which is faster than the standard homotopy method (,[8, Theorem 5.1]), and SLGH (,[9, Theorem 4.1]), ifis pre-selected to lie in. Extensive experiments show that it is significantly faster than all other compared algorithms (including the homotopy ones) to produce solutions of quality.\n\nOur convergence analysis does not require the Lipschitz condition on the original objective, which is assumed in the theoretical analysis of homotopy methods in other studies ([8,9]). Therefore, our analysis applies to more situations.\n\nThe theory derived in this work is on the distance between the found solution and the optimal one, while the convergence analysis in other studies on homotopy is on the objective value of the found solution. Therefore, our theory has a wider range of applications (e.g., for problems that cares the distance between the found solution and the optimal like inverse problems and adversarial attack in image recognition).\n\nSECTION: 2Preliminaries\n\nWe rigorously prove the intuition that motivates PGS and EPGS: Given, for any, there exists a threshold such that wheneverexceeds this threshold, the global maximum point oflies within a-neighborhood of, where, and\n\nLetbe a continuous function that is possibly non-concave (and non-negative only for the case of PGS), whereis compact. Assume thathas a global maximumsuch thatfor any. For, define\n\nwhereis defined in (LABEL:fN) for either PGS or EPGS. Then, for anyandsuch that, there exists, such that whenever, we have for anythat:if, andif. Here,anddenote theentry ofand, respectively, where.\n\nSee the appendix for the proof for the EPGS setting. The proof for the PGS setting is similar.\n\u220e\n\nSECTION: 3Gaussian Smoothing with Power-Transformed Objective\n\nSECTION: 3.1The Solution Updating Rule\n\nFor the optimization problem (1), based on Theorem1, with the pre-selected hyper-parametersand, GSPTO follows a stochastic gradient ascent scheme to solve. Specifically, the rule for updating the solution candidate used is\n\nwhere,are independently sampled from the multivariate Gaussian distribution, andis defined in (LABEL:fN). Note thatis a sample estimate of the gradient:\n\nBased on GSPTO, PGS and EPGS are designed in Algorithm1. They normalize the gradient before updating the solution, which is a common practice to stabilize results.\n\nSECTION: 4Convergence Analysis\n\nWe perform convergence analysis for the updating rule (4) under the PGS and EPGS setting (LABEL:fN) on the optimization problem of (1), with, for some.\n\nWe show that, for anyand any, GSPTO converges to a-neighborhood ofwith the iteration complexity of. Specifically, withtimes of updating,, wherecan be arbitrarily close to 0. The result is summarized in Corollary1.\n\nSECTION: 4.1Notation\n\nIn this section, letandbe fixed, andbe such that for any:if, andif, for all. Such anexists because of Theorem1. Letbe as defined as in Theorem1. Unless needed, we omitandin this symbol as they remain fixed in this section, and writeinstead.refers to the gradient ofwith respect to. Letbe defined as in (LABEL:fN).\n\nSECTION: 4.2Assumptions and Lemmas\n\nAssume thatis a function satisfying the conditions specified in Theorem1.\n\nAssume that the learning ratesatisfies\n\nUnder Assumption1, any local or global maximum pointofbelongs to the set of, wheredenotes theentry.\n\nFor any point, we show that. If, thenand there is somesuch that, which impliesbecause of the definition ofin Section4.1.\n\nOn the other hand, if, there is at least onesuch that. Then,\n\nIn sum, for any point,, which further implies that any local or global maximum pointofbelongs tosince.\n\u220e\n\nUnder Assumption1, for any, the objective functionis Lipschitz Smooth. That is, for any,\n\nwherefor the case of PGS andfor the case of EPGS.\n\n, whereis as defined in (LABEL:fN)..\nThen,\n\nHence,, which isfor PGS andfor EPGS.\n\u220e\n\nUnder Assumption1, for any, letbe as defined in Algorithm1. Then,, where\n\nFor the case of EPGS,\n\nwhere the third line is by Cauchy-Schwarz Inequality. Replacingfor EPGS andfor PGS.\n\u220e\n\nSECTION: 4.3Convergence Rate\n\nLetbe produced by following the iteration rule of (4), with a pre-selected and deterministicand all the involved terms defined as in Section4.1. Then, under Assumption1and2, we have that\n\nwhereunder the PGS setting andunder the EPGS setting.\n\nIfwith. Then,\n\nThis inequality and Theorem2implies that aftertimes of updatingby GSPTO,. In sum, the GSPTO method (4) converges to a-neighborhood ofwith a rate of, wherecan be arbitrarily close to 0.\n\nBy the Gradient Mean Value Theorem, there existssuch that for each of theth entrylies betweenand, and\n\nHence, we have\n\nTaking the expectation of both sides gives\n\nwhere for the first line, note that\n\nTaking the sum fromtoon both sides of (5) gives\n\nRe-organizing the terms gives\n\n\u220e\n\nWe summarize the above results in the following corollary.\n\nSuppose Assumption1and2hold. Given anyand, there existssuch thathas all its local maximums in. For any, under either the PGS or EPGS setting, the updating rule (4) of GSPTO producesthat converges to a local maximum point of, which lies in a-neighborhood of, with the iteration complexity of. Specifically, aftertimes of-updating by (4),, whereis a parameter in the learning rateand can be arbitrarily close to 0.\n\nSECTION: 5Experiments\n\nSECTION: 5.1Effects of Increasing Powers\n\nWe illustrate the improvements made by increasingfor PGS/EPGS through an example problem of\n\nthe global maximum pointhas all its entries equal to, and the local maximum pointhas all its entries equal to. The graph of its 2D-version is plotted in Figure2.\n\nWith each value of, both the PGS and EGS are performed to solve this problem. The-candidate set isfor PGS andfor EGS. For eachvalue, we do 100 trials to stabilize the result. In each trial, the initial solution candidateis uniformly sampled from, whererepresents theentry of. We set the initial learning rate as 0.1, the scaling parameteris set as 0.5, and the total number of solution updates as 1000. The objective for Power-GS is modified to beto ensure that the PGS agent will not encounter negative fitness values during the 1000 steps.\n\nWe perform the experiments in two settings, one is two-dimensional () and the other is five-dimensional ().\nThe results, plotted in Figure3, show that, asincreases, the distance between the produced solutionand the global maximum pointapproaches zero (see the decreasing MSE curve in the plot), which is consistent with Theorem1and the idea that\u2019s maximumapproaches the global maximum pointofas we put more weight on\n\nSECTION: 5.2Performance on Benchmark Objective Functions\n\nIn this subsection, we test the performance of PGS and EGS on two popular benchmark objective functions, the Ackley and the Rosenbrock (max-version). The performances of other popular global algorithms (max-version) are also reported for comparison, including a standard homotopy method STD-Homotopy, ZO-SLGHd and ZO-SLGHr ([9]), the algorithms of ZO-SGD ([7]) and ZO-AdaMM([4]) for solving, as well as the evolutionary algorithm of particle swarm optimization (PSO, e.g.,[13, Section 3.1.5]and[14]). The hyper-parameters of these algorithms are selected by trials, and the optimal ones can be found in our codes athttp://github.com/chen-research/GS-PowerTransform.\n\nThe Ackley objective function features with a numerous number of local optimums and a single global optimum. We solve the max-version of the corresponding problem, which is\n\nThe graph of this function is plotted in Figure4(a). From both the functional form and the graph, it is not difficult to see thatattains its maximum at.\n\nThe solutions and their fitness values found by each of the compared algorithms are reported in Table1. From which we see that all of these algorithms are able to avoid the local maximum points and achieve the global maximum point.\n\nThe Rosenbrock objective is known to be difficult to optimize, since its global optimum pointis surrounded by a flat curved plane (see Figure). Specifically, the problem to be solved is, where\n\nWe use PGS, EPGS, and other algorithms to solve. Their performances are recorded in Table2, which shows that EPGS, STD-Homotopy, and PSO are superior than other algorithms on this task, since they are able to locate the true solution of (1,1).\n\nFor Ackley, all the algorithms are able to locate the true solution of (1,1), except PSO when the initial population is concentrated near the initial start of, which indicates that the performance of PSO depends more on the initial guess than other methods. For Rosenbrock, EPGS is one of the three algorithms that can locate the global optimum well.\n\nSECTION: 5.3Performance on the Black-box Targeted Adversarial Attack\n\nLetbe an black-box666A black-box classifier refers to a classification model whose parameters are not accessible.image classifier. The targeted adversarial attack onrefers to the task of modifying the pixels of a given imageso thatis equal to a pre-specified target label, wheredenotes the perterbation and. Another goal of this task is to minimize the modification. Hence, I set the loss as\n\nwheredenotes the predicted logit (i.e., log probability) for theclass,is a hyper-parameter that controls the certainty level of the attack,is a regularization coefficient, anddenotes thenorm of(i.e., the square root of the sum of squares of entries in). This loss function resembles the popular one in Carlini and Wagner (2017).\n\nWith EPGS and other compared algorithms, I perform adversarial attacks on 100 randomly selected images from each of two image datasets, the set of MNIST figures and the CIFAR-10 set. Specifically, the goal is to solve:\n\nThe hyper-parameters for ZO-SLGHd and ZO-SLGHr are set according to Iwakiri (2022), which performed the same task (but with a difference loss and more iterations for each trial). For ZO-AdaMM, the hyper-parameters are set according to Chen (2019) (Section 4.2). For others, the hyper-parameters are set by trials. We choose EPGS over PGS for this task since the fitness functioncan be negative and hence EPGS is more convenient. (But note that we can modifyby adding a large positive constant to facilitate PGS).\n\nThe image size in MNIST hand-written figures ([11]) ispixels and we down-sample it toto reduce the computational complexity. The classifierfor MNIST is a feed-forward neural net trained777We use TensorFlow ([1]) for training.on the training images, with a classification accuracy ofon the testing images.\n\nFor each imagethat is randomly drawn from the testing dataset, where, we randomly generate a target label. Then, for each algorithm, we perform an attack (i.e., experiment) ofiterations. Letdenote all the perturbations (solutions) produced in theseiterations. We say that a perturbationis successful if the predicted log probability of the target label is at leastgreater than that of other classes (i.e.,). We say that an attack is successful if the producedcontains at least one successful perturbation. If the attack is successful, letdenote the successful perturbation with the largest-value among, and letdenote the number of iterations taken by the algorithm to produce. Here, the-value ofrefers to thestatistic betweenand the perturbed image, which is computed as. In this formula,andranges over all the pixels (entries) ofand.\n\nWith the above notations, we construct three measures on the performances of an algorithm. One is the success rate, which refers to the ratio of successful image attacks out of the total number of attacks (100). The second measure is the average, which equals, wheredenotes the set of indices of the successful attacks. The last measure is the averageof.\n\nFor ZO-SLGHd, ZO-SLGHr, ZO-SGD, and ZO-AdaMM, the hyper-parameters are set the same as those experiments performed in[9](if available), since they also use these algorithms for MNIST and CIFAR-10 image attacks. For other algorithms, the hyper-parameters are selected by trials.\n\nTable3reports the results of each algorithm executed foriterations for each of the 100 images, from which we see that EPGS not only has asuccess rate, but also is the the fastest to produce a successful perturbed image that is close to the original one. Specifically, when, EPGS outperforms other algorithms in terms of accuracy (i.e.,) and time (i.e.,). This-score ofis not far from theproduced by other algorithms with 8,000 iterations.\n\nThe image size in CIFAR-10 dataset ([10]) ispixels and we down-sample it toto reduce the computational complexity. We train a convolutional neural neton the training images, which has a classification accuracy ofon the testing images. We perform per-image targeted adversarial attacks on 100 randomly drawn images from the testing set. The results are reported in Table4.\n\nThe experiments are performed in the same way as for figure-MNIST and their results are reported in Table4. Similar to the MNIST experiment, EPGS significantly outperforms other algorithms in terms of time complexity. Especially when, the quality (i.e.,) of its solution is much better, which is comparable to other algorithms performed foriterations.\n\nSECTION: 6Conclusion\n\nIn this paper, we propose the method of GSPTO for solving the global optimization problem of (1), which is featured with putting more weight on the objective\u2019s global optimum through power transformations. Both our theoretical analysis and numerical experiments show that GSPTO is significantly faster than other homotopy methods to produce high-quality solutions. This method provides a foundation for future studies to explore more efficient ways to increase the gap betweenand other values.\n\nSECTION: References\n\nSECTION: 7Appendix\n\nSECTION: 7.1Proof to Theorem1for EPGS\n\nRecall that for EPGS,For any given, defineand. Using this symbol, we re-writeas\n\nwhere\n\nwhere.\n\nWe derive an upper bound for. For any,\n\nwhere the third line is because, and the fifth line is by the separability of a multivariate integral.\n\nSinceis continuous, for(because), there existssuch that whenever,\n\nUsing this result, we derive a lower bound forwhenand.\n\nwhere the first equality is implied by the fact thatdoes not change sign astravels in(this fact is because of), and\n\nThe positive numberis constructed by solving the following inequality for, which involves the two bounds in (8) and (10).\n\nThe solution of this inequality is\n\nwhereand the numerator is negative for sufficiently large.\nTherefore, whenever\n\nwe have\n\nWhen,, and,\n\nwhere the third line is because the integrand of the first term is always negative in the integration region.\n\nOn the other hand, when,, and,\n\nThen, (12) and (13) imply the result in the theorem sinceandshare the same sign (see Eq. (7)).\n\u220e", "text_file": "data\\paper_texts\\2412.05204v1_content.txt"}, {"title": "Fast Laplace transforms on quantum computers", "authors": ["Julien Zylberman"], "published_date": "2024-12-06T16:44:00Z", "summary": "While many classical algorithms rely on Laplace transforms, it has remained\nan open question whether these operations could be implemented efficiently on\nquantum computers. In this work, we introduce the Quantum Laplace Transform\n(QLT), which enables the implementation of $N\\times N$ discrete Laplace\ntransforms on quantum states encoded in $\\lceil \\log_2(N)\\rceil$-qubits. In\nmany cases, the associated quantum circuits have a depth that scales with $N$\nas $O(\\log(\\log(N)))$ and a size that scales as $O(\\log(N))$, requiring\nexponentially fewer operations and double-exponentially less computational time\nthan their classical counterparts. These efficient scalings open the\npossibility of developing a new class of quantum algorithms based on Laplace\ntransforms, with potential applications in physics, engineering, chemistry,\nmachine learning, and finance.", "arxiv_id": "2412.05173v1", "html_link": "https://arxiv.org/html/2412.05173v1", "search_term": "ti:\"transformers\"", "html_error": "Error processing HTML: HTTP Error 404: Not Found"}, {"title": "Transformers Can Navigate Mazes With Multi-Step Prediction", "authors": ["Niklas Nolte", "Ouail Kitouni", "Adina Williams", "Mike Rabbat", "Mark Ibrahim"], "published_date": "2024-12-06T15:19:10Z", "summary": "Despite their remarkable success in language modeling, transformers trained\nto predict the next token in a sequence struggle with long-term planning. This\nlimitation is particularly evident in tasks requiring foresight to plan\nmultiple steps ahead such as maze navigation. The standard next single token\nprediction objective, however, offers no explicit mechanism to predict multiple\nsteps ahead - or revisit the path taken so far. Consequently, in this work we\nstudy whether explicitly predicting multiple steps ahead (and backwards) can\nimprove transformers' maze navigation. We train parameter-matched transformers\nfrom scratch, under identical settings, to navigate mazes of varying types and\nsizes with standard next token prediction and MLM-U, an objective explicitly\npredicting multiple steps ahead and backwards. We find that MLM-U considerably\nimproves transformers' ability to navigate mazes compared to standard next\ntoken prediction across maze types and complexities. We also find MLM-U\ntraining is 4x more sample efficient and converges 2x faster in terms of GPU\ntraining hours relative to next token training. Finally, for more complex mazes\nwe find MLM-U benefits from scaling to larger transformers. Remarkably, we find\ntransformers trained with MLM-U outperform larger transformers trained with\nnext token prediction using additional supervision from A* search traces. We\nhope these findings underscore the promise of learning objectives to advance\ntransformers' capacity for long-term planning.", "arxiv_id": "2412.05117v1", "html_link": "https://arxiv.org/html/2412.05117v1", "search_term": "ti:\"transformers\"", "html_content": "SECTION: Transformers Can Navigate Mazes With Multi-Step Prediction\n\nDespite their remarkable success in language modeling, transformers trained to predict the next token in a sequence struggle with long-term planning.\nThis limitation is particularly evident in tasks requiring foresight to plan multiple steps ahead such as maze navigation.\nThe standard nextsingletoken prediction objective, however,\noffers no explicit mechanism to predict multiple steps ahead\u2014or revisit the path taken so far.\nConsequently,\nin this work we study\nwhether\nexplicitly predicting multiple steps ahead (and backwards) can improve transformers\u2019 maze navigation.\nWe train parameter-matched transformers from scratch, under identical settings, to navigate mazes of varying types and sizes with standard next token prediction andMLM-, an objective explicitly predicting multiple steps ahead and backwards.\nWe find thatMLM-considerably improves transformers\u2019 ability to navigate mazes compared to standard next token prediction across maze types and complexities.\nWe also findMLM-training is 4more sample efficient and converges 2faster in terms of GPU training hours relative to next token training.\nFinally, for more complex mazes we findMLM-benefits from scaling to larger transformers.\nRemarkably, we find transformers trained withMLM-outperform larger transformers trained with next token prediction using additional supervision from A* search traces.\nWe hope these findings underscore the promise of learning objectives to advance transformers\u2019 capacity for long-term planning.\n\nDespite their remarkable success in language modeling, transformers trained to predict the next token in a sequence struggle with long-term planning.\nThis limitation is particularly evident in tasks requiring foresight to plan multiple steps ahead such as maze navigation.\nThe standard nextsingletoken prediction objective, however,\noffers no explicit mechanism to predict multiple steps ahead\u2014or revisit the path taken so far.\nConsequently,\nin this work we study\nwhether\nexplicitly predicting multiple steps ahead (and backwards) can improve transformers\u2019 maze navigation.\nWe train parameter-matched transformers from scratch, under identical settings, to navigate mazes of varying types and sizes with standard next token prediction andMLM-, an objective explicitly predicting multiple steps ahead and backwards.\nWe find thatMLM-considerably improves transformers\u2019 ability to navigate mazes compared to standard next token prediction across maze types and complexities.\nWe also findMLM-training is 4more sample efficient and converges 2faster in terms of GPU training hours relative to next token training.\nFinally, for more complex mazes we findMLM-benefits from scaling to larger transformers.\nRemarkably, we find transformers trained withMLM-outperform larger transformers trained with next token prediction using additional supervision from A* search traces.\nWe hope these findings underscore the promise of learning objectives to advance transformers\u2019 capacity for long-term planning.\n\nSECTION: 1Introduction\n\nTransformers trained to predict the next token in a sequence have become the de facto approach in today\u2019s best language models(Dubey et\u00a0al.,2024; Gemma,2024). Despite their remarkable success, such transformers encounter challenges when tasked with planning and decision-making over extended horizons. This limitation becomes particularly evident in tasks requiring foresight such as maze navigation.\n\nTo effectively navigate a maze, a model must have the foresight to plan ahead multiple steps.\nThe de facto next token prediction training approach, however, offers no explicit mechanism to predict multiple steps ahead or revisit the path taken so far.\nThe model is trained to only predict the next step in the input sequence given the previous steps. Prior work has shown next token prediction can fall prey to shortcuts in navigation tasks, particularly as path complexity increases(Bachmann & Nagarajan,2024).\nConsequently, we ask:Can explicitly learning to predict multiple steps ahead (and backwards) improve transformers\u2019 ability to navigate mazes?\n\nTo answer this question,\nwe isolate the effect of learning objectives by training transformers from scratch to navigate mazes.\nInspired by prior work to remedy shortcomings of next token prediction(Bachmann & Nagarajan,2024; Gloeckle et\u00a0al.,2024), we explore the\ntheMLM-objective fromKitouni et\u00a0al. (2024a)as an alternative to next token prediction.MLM-proposes\nmasking arbitrary subsets of the input sequence to explicitly predict a variable number of steps ahead and backward as shown inFigure1.\nWe then assess whetherMLM-by explicitly predicting multiple-steps during training can improve transformers\u2019 performance on maze navigation.\n\nWe operate with a collection of mazes with varying levels of grid-size complexities.\nTwo common types of mazes generation approaches are studied that differ in shortest path solution lengths as well as maze text representations.\nFor one setting, we train transformer models for both objectives, standard next token prediction andMLM-. In the other setting, we compareMLM-against published results on next token training fromLehnert et\u00a0al. (2024).\nFinally, we compare learning objectives across several transformer model sizes by measuring maze navigation, data sample efficiency, as well as training efficiency in terms of GPU hours to convergence.\n\nOur results indicateMLM-can improve maze navigation accuracy and training efficiency compared to standard next token prediction.\nRemarkably, we find a transformer trained withMLM-outperforms larger transformers trained with next token prediction using additional supervision from A* search traces(Lehnert et\u00a0al.,2024). Specifically, relative to standard next token prediction training, we find that:\n\nMLM-considerably improves transformers\u2019 ability to navigate mazes.\n\nMLM-outperforms comparable next token transformer models across every maze type and grid size complexity tested. For example, an 8M parameter transformer trained withMLM-can perfectly solve all mazes of grid sizes up to 20x20, whereas next token training peaks at 20.6% navigation accuracy on held-out 20x20 test mazes (shown inFigure1).\n\nMLM-outperforms next token transformers trained with additional A* search trace supervision on complex mazes. For example, on 30x30 mazes an 8M parameter transformer reaches 85.5% navigation accuracy withMLM-, improving on the 70.2% navigation accuracy of a 175M parameter transformer trained with next token prediction and additional A* search trace supervision.\n\nMLM-training is 4x more data-efficient in terms of training samples. For simpler mazes (5x5) solved by bothMLM-and next token prediction,MLM-is 2x more efficient in GPU hours needed for convergence.\n\nMLM-benefits from scaling to larger transformers for more complex mazes. For example scalingMLM-from a 3M to an 8M parameter transformer boosts performance from 85% to perfect navigation on 20x20 mazes.\n\nThese findings suggest that the learning objective is critical to transformer\u2019s maze navigation abilities, offering a promising direction for future research in long-horizon planning tasks.\n\nSECTION: 2Related Work\n\nIvanitskiy et\u00a0al. (2023b)show transformers trained on maze navigation tasks learn internal states that allow a decoding of the entire maze. Despite this emergent state however,Bachmann & Nagarajan (2024)shows the limits of next token prediction objectives for basic graph navigation tasks. In particular, the work identifies a Clever-Hans cheat based on shortcuts in teacher forced training similar to theoretical shortcomings identified inWang et\u00a0al. (2024b). This demonstrates that while transformers can represent world states for mazes, they may struggle in planning that requires significant foresight. A remedy found byBachmann & Nagarajaninvolves removing the teacher forced supervision. Their view inspired us to look further into the training objective to encourage more explicit planning.\n\nMany deep learning approaches for maze navigation use reinforcement-learning objectives(Akmandor et\u00a0al.,2022; Wang et\u00a0al.,2024a; Tamar et\u00a0al.,2016; Wang et\u00a0al.,2024c; Kong et\u00a0al.,2024).Liu & Borisyuk (2023)compares the navigation strategies learned by reinforcement learning to those observed in animals suggesting some similarities in learning dynamics.Janner et\u00a0al. (2022)study reinforcement learning reward modeling with a diffusion objective with applications to planning tasks including maze navigation. While reinforcement learning approaches excel at tasks involving interaction and games, reinforcement learning has played a relatively minor role in foundation model pretraining.Outside of reinforcement learning approaches,Lehnert et\u00a0al. (2024)successfully train transformers with the next token objective to perform maze navigation. Crucially, they can vastly improve performance via additional supervision. By exposing the model to a trace of an A* algorithm solving the maze, they gain significant performance and data efficiency. Interestingly, just like inBachmann & Nagarajan (2024), the remedy to failure on a navigation task seems to involve changing the supervision structure. We directly compare this approach with theMLM-objective trained without any supervision from A* search traces.\n\nKitouni et\u00a0al. (2024a)usedMLM-, which can be seen as a diffusion objective(Austin et\u00a0al.,2021; Kitouni et\u00a0al.,2024b), to mitigate the reversal curse in language modelling(Berglund et\u00a0al.,2024), where models trained to answer questions in one way can not generalize to an inverse, semantically equivalent formulation. They also show thatMLM-performs well in the graph navigation task fromBachmann & Nagarajan (2024).Sahoo et\u00a0al. (2024); Austin et\u00a0al. (2021); Li et\u00a0al. (2022)incorporate diffusion objectives in masked language modeling for general purpose language models.He et\u00a0al. (2022)adds a diffusion objective to further train a pretrained BERT model showing improvements over standard BERT training in terms of perplexity and BLEU score on language tasks.\n\nSECTION: 3The role of learning objectives in maze navigation\n\nWe examine how the standard next token learning objective manifests itself in maze navigation, a task requiring planning multiple steps head. We contrast next token prediction withMLM-, a training objective explicitly encouraging predicting multiple steps ahead and backward.\n\nSECTION: 3.1Predicting the next step with standard training\n\nThe de facto learning objective used to train language models is next token prediction. This objective, which is also referred to as an autoregressive (AR) or causal-masked prediction objective, when paired with the transformer architecture has shown great success in language tasks at scale.\nSpecifically, given a sequence of inputs, the next token learning objective minimizes\n\nwhereindicates the index of the input sequence. This simple objective maximizing the probability of the next token given the previous tokens in the sequence has led to remarkable fluency in language tasks(Dubey et\u00a0al.,2024; Gemma,2024). However,\ntransformers trained with next token prediction exhibit limits in terms of planning.\n\nIn maze navigation, as shown inFigure1, next token prediction amounts to predictingonly the next stepgiven the path so far. The learning objective inEquation1does not explicitly encourage predicting multiple steps ahead.Bachmann & Nagarajan (2024)suggests the lack of multi-step prediction in standard next token training limits transformers\u2019 ability to navigate even simple graphs. One pitfall highlighted byBachmann & Nagarajan (2024)is that models fall prey to short-sighted shortcuts such as the Clever-Hans cheat, show because the model does not plan far enough ahead.Dziri et\u00a0al. (2024)show similar limits for other multi-step problems, especially as problem complexity increases.\n\nSECTION: 3.2Predicting multiple steps ahead and back withMLM-\n\nOne remedy discovered byBachmann & Nagarajan (2024)avoids supervision through teacher-forcing by allowing the model to predict the entire path before applying a gradient. However, this approach is slow to train, since it requires the sequential generation steps.\n\nGloeckle et\u00a0al. (2024)provide an elegant way to reason multiple tokens into the future by having multiple prediction heads. They found this method to have beneficial effects on decoder models of size 13B and above when employing up to 8 prediction heads for the 8 next tokens.\nMotivated byGloeckle et\u00a0al. (2024)we consider an explicit objective predicting multiple tokens both ahead and backwards with a variable, rather than fixed context size. Specifically, we study theMLM-objective fromKitouni et\u00a0al. (2024a)which predicts any subset of tokens given any others as context, hoping to capture long-term context dependence and explicit multi-step prediction.\n\nMLM-proposes\nmasking arbitrary subsets of the input sequence to explicitly encourage the model to predict multiple steps ahead and backwards. The masking ratio, which determines the portion of the input that is masked, is drawn uniformly from [0, 1] thereby encouraging a variable prediction window. Specifically, for a uniformly sampled maskwith masking rateover the input sequence, theMLM-learning objective minimizes\n\nwhereis the context used for prediction, equivalent to the complement of the masked target elements. Incidentally, this method is reminiscent of BERT(Devlin et\u00a0al.,2019), but with a uniform masking rate and without token substitution.(Kitouni et\u00a0al.,2024a, see their Figure\u00a02)argue that since the uniform masking rate exposes the model to different length sequences to be completed and to draw information from, there is no distributional shift in a generative inference step.\n\nFor maze navigation, as shown inFigure1, theMLM-objective inEquation2amounts to predicting multiple steps at various points in the navigation path thereby explicitly planning ahead and back multiple steps.\n\nWe study the role of the learning objective in maze navigation by comparing standard next token prediction toMLM-. We ask:can modifying only the learning objective to predict multiple steps ahead and back enable transformers to navigate complex mazes?\n\nSECTION: 4Methods\n\nTo study the role of learning objectives for maze navigation, we train transformer models from scratch to generate the shortest navigation path for mazes of increasing complexity.\nWe design our experiments such that transformer models are parameter-matched and trained under identical regimes to isolate the effect of next token versusMLM-learning objectives.\nWe assess models\u2019 ability to accurately navigate previously unseen mazes as well as their efficiency in terms of training samples and GPU training hours.\n\nSECTION: 4.1Mazes and Their Representations\n\nWe consider two maze generation approaches across several levels of grid-size complexities\nto ensure our findings are not specific to a single type of maze or representation, but hold more generally.\n\nFirst, we utilize the maze generation method fromIvanitskiy et\u00a0al. (2023a)to generate 2 dimensional mazes via the randomized Depth First Search (DFS) method. This method works by constructing a path from a uniformly random start node in a depth-first manner.\nThis generation approach yields long paths (relative to A* mazes described below), but does not allow ambiguity: the shortest path is also the only path that does not backtrack and thus overlap with itself. An example 10x10 DFS maze in show on the right panel ofFigure2.\nThe mazes are serialized into strings that enumerate the edges of the maze connection graph as a set of tuples. The start node, goal node and solution path are appended to form the full text that the model trains with. We generate 500k mazes across five levels of complexity as measured by the grid size of the maze spanning 5x5, 10x10, 20x20, and 30x30.\n\nSecond, we use the deterministic A* maze dataset fromLehnert et\u00a0al. (2024). Start and goal cell were uniformly sampled in a 2-dimensional grid with walls randomly placed in 30\u201350% of cells (see middle panel ofFigure2). The shortest paths are discovered via the A* algorithm and added to the dataset if the shortest path is at least of length, whereindicates the maze grid size (for anxmaze).\nIn A* mazes, grid cells are tokenized with individual tokens for x and y coordinate, which increases the input sequence length relative to the graph tuple encoding used for DFS.\nIn both datasets, the solution path is the last part of the string.\nIn contrast to the DFS mazes, however, A* mazes have many possible solutions, out of more than one are possibly the shortest ones.Lehnert et\u00a0al. (2024)experiment with both randomly and deterministically (heuristically) choosing the shortest path that the model sees as ground truth. We choose 10x10, 20x20 and 30x30 mazes from the deterministic setting, seeSectionD.2for additional details.\n\nTogether these maze generation approaches allow us to study mazes of varying complexities (in terms of grid size), differing distributions of shortest path lengths, as well as different maze text encoding approaches. InFigure2we show the distribution differences between solution path lengths for DFS versus A* mazes across three levels of grid-size complexities. Additionally in the middle and right panels, we show sample generations for DFS and A* mazes.\n\nSECTION: 4.2Standard Next Token Prediction and A* Search Dynamic Supervision\n\nWe evaluate the standard next token prediction learning objective for maze navigation. To do so, we train transformers from scratch on text representations of maze solutions similar toIvanitskiy et\u00a0al. (2023b). Mirroring the objective of modern language models the transformer predicts the next token based on the previous tokens in the maze solution path (seeEquation1). We investigate various transformer model sizes to understand the effect of model scale. We also evaluate the standard decoder-only transformer architecture as well as the encoder-decoder architecture fromLehnert et\u00a0al. (2024). Finally, to better contextualize our findings we also report the next token model fromLehnert et\u00a0al. (2024)trained with additional A* search trace supervision for the A* maze setting.\n\nSECTION: 4.3MLM-\n\nWe contrast next token prediction with theMLM-objective, explicitly predicting multiple steps both ahead and backward.\nWe closely follow the training setup inKitouni et\u00a0al. (2024a), including the encoder-decoder transformer architecture with RoPE positional embeddings (seeSectionsD.1andD.3).\nIdentical to the next token baselines, theMLM-objective is trained on text representations of the maze solutions.\nGeneration during inference is done in the same way as for the standard next token baselines, generating one token at a time from left to right, with temperature 0 (argmax).\nSince the uniform masking rate inMLM-(seeEquation2) exposes the model to different sequence prediction and context lengths, there is no distributional shift in a generative inference step as shown in Figure 2 ofKitouni et\u00a0al. (2024a).\nForMLM-, we also train transformers of varying model scales ranging from 3M to 25M parameters to study the effect of model scale on maze navigation.\n\nSECTION: 4.4Experimental setup\n\nTo isolate the effect of training objectives,MLM-versus next token prediction, we train all models from scratch using an identical setup.\n\nWe train transformers for up to 3000 epochs on 100,000 mazes for each setup. The performance of each model is evaluated on a held-out test set of 2000 mazes with the same configuration as the training set.\nTo ensure the baseline comparisons for next token prediction are competitive,\nwe conduct a sweep over learning rate choices and weight decay values (shown inAppendixB). We select the best choice of hyperparameters based on held-out shortest path accuracy for 10x10 DFS mazes. The architecture used to trainMLM-is an encoder-decoder (as inKitouni et\u00a0al. (2024a), detailed inSectionD.3), but for next token training in DFS mazes we found a decoder-only architecture to be superior to theMLM-encoder-decoder, seeSectionA.2.\nFor A* mazes, we report the best available numbers fromLehnert et\u00a0al. (2024)for next token prediction.\n\nWe evaluate models in terms of maze navigation accuracy, data efficiency as measured by the number of training mazes, and training efficiency in terms of GPU training hours needed for convergence.\nTo assess the correctness of a generated path similar toLehnert et\u00a0al. (2024)we compare whether the full path matches the shortest path. We additionally compare the token-wise accuracy inSectionA.1to assess navigation paths that only slightly deviate from the shortest path. Finally, to complement the overall maze navigation accuracy, we assess training dynamics by comparing convergence curves on training and held-out tests mazes.\n\nSECTION: 5Results: Learning to Navigate Mazes withMLM-Training\n\nWe compare the next token andMLM-objectives via maze navigation accuracy across three dimensions: maze complexity, training data efficiency and computational efficiency. We also investigate scaling laws as well as analyze the training dynamics ofMLM-.\n\nSECTION: 5.1MLM-and standard next token training in DFS mazes\n\nFirst, we compare the objectives in the setting with DFS generated mazes described in the first part ofSection4.1.\nWe train 8M parameter transformer models across mazes with grid sizes ranging from 5x5 to 30x30. We findMLM-is able to perfectly navigate mazes of up to a grid size of 20x20 and achieve nearly 3x the performance of next token training on more complex 30x30 mazes as shown inTable1. For example, even on comparatively small mazes of size 10x10 we find next token performance saturates below 50% accuracy. In contrast, a model of the same size can navigate 30x30 mazes with over 90% accuracy when trained withMLM-.\n\nTo evaluate the data efficiency ofMLM-relative to that of next token, we train 8M parameter transformer models while varying the number of mazes seen during training. We operate on maze sizes of 5x5 and 10x10 and train both models for 2000 epochs.\nAs shown inFigure3, we findMLM-is able to navigate both 5x5 and 10x10 mazes with only 25k training samples, while next token requires all 100k mazes to reach full accuracy in 5x5 and reaches a peak performance of less than 50% with 75k training samples, suggestingMLM-is 4more data efficient.\n\nWe compare the convergence rates both on training and held-out 5x5 mazes forMLM-and next token prediction. We choose this small setting because this is solvable by both objectives.\nWe find as shown inFigure4\n\nMLM-converges 2.17x faster in terms of the number of training epochs.\nWe additionally control for computational overhead in terms of GPU training hours, we find training on the same data for 2k epochs using 8M parameter transformers on 8 Tesla V100 32GB GPUs takes 13.7 hours for next token versus 17.7 hours forMLM-. Accounting for this additional 7% overhead, we find as shown inFigure4MLM-ismore efficient than a comparable next token model on small DFS mazes. As a caveat, we note that on 10x10 mazes, next token training crosses the 40% performance threshold faster thanMLM-, indicating faster initial learning before saturating at peak of 46% accuracy on held-out test mazes.\n\nSECTION: 5.2MLM-and next token training with A* Mazes\n\nIn this section, we train models withMLM-on the deterministic A* maze dataset fromLehnert et\u00a0al. (2024)as described in the second part ofSection4.1.\nWe compare those models to the ones trained inLehnert et\u00a0al.with and without additional supervision from A* search traces.\nFor example, a nearly 2x larger 15M parameter transformer trained with next token prediction achieves 13.3% navigation accuracy on 30x30 mazes whereasMLM-reaches 85.5% navigation accuracy.\nThe results can be found inTable2. The 8M parameterMLM-trained transformer compares favorably with all models fromLehnert et\u00a0al.trained on 100k mazes. This holds true even when aiding the training with additional supervision provided by the A* search trace, which boosts next token training by a significant margin.\n\nSECTION: 5.3Understanding the training dynamics of MLM-U Compared to next token\n\nWe compare the convergence rates both on training and held-out 10x10 DFS mazes for MLM-U compared to next token parameter-matched 8M parameter models inFigure5. Although we observe faster training convergence for next token models as shown on the left, we see the next token model is not able to generalize from the training data, with performance saturating at around 50%, while MLM-U is able to perfectly solve 10x10 mazes.\nThis suggests while next token training is susceptible to overfitting, whereMLM-exhibits good generalization without overfitting. We attribute this to the increased difficulty of the objective.MLM-is tasked to predict any subset of path tokens from any other, while next token training only ever sees the same sequence of conditionals for each maze.\n\nHere, we investigate the effect of scaling transformer model size for 20x20 DFS mazes, one the more challenging settings where next token training yields 22% accuracy. As shown inFigure6MLM-training improves navigation accuracy from 85% to perfect navigation accuracy when transformer model size is scaled from 3M to 8M parameters.\nFor next token prediction, we also observe improvements with transformer model scale, but at a relatively slower rate.\nA more than 8x increase in model size, from 3M to 25M, for a model trained with the next token objective yields a 43% relative performance improvement.\n\nSECTION: 5.4Positional encodings need more floating point precision\n\nAs we scaledMLM-training to more complex mazes, we found the precision of the positional encodings to be particularly important for good maze navigation performance.\nUnlike the learnable ((Radford et\u00a0al.,2019)) and sinusoidal encodings in the\noriginal transformer paperVaswani et\u00a0al. (2023)which are added to the input,MLM-uses Rotational Positional Encodings (RoPE,(Su et\u00a0al.,2023)), which\nbias the query and key vectors in the attention mechanism as a function of their relative positions.\nTo better understand the role of these positional embedding precision we train an 8M parameter transformerMLM-on a small set of 100 DFS mazes with increasing grid size complexities.\nWe found with 16-bit precision positional encodings (float 16 via the automatic mixed precision, AMP, package in PyTorch)\nas shown inFigure7(right),MLM-generally predicted the correct paths, but failed get the exact positions right, skipping some and duplicating others, resulting in low navigation accuracy on more complex (25x25 and larger) training mazes.\n\nWith full 32-bit precision positional encodings however, we foundMLM-was able to reach perfect navigation accuracy even on these more complex mazes. For example, as shown inFigure7on 30x30 mazesMLM-only reached 50% navigation accuracy with 16-bit positional encoding precision whereas with 32-bit positional encodingsMLM-solved 30x30 mazes perfectly. This suggests for larger grid sizes, higher precision in the positional encoding allowed the model to properly map the learned paths to their proper positions on the maze. We observed a similar improvement in performance with larger training data (100k samples) on 30x30 DFS mazes. In particular, by increasing the precision from 16 to 32-bits for positional encodings,MLM-performance on 30x30 DFS mazes improved from 40% to 93.8% highlighting the importance of higher positional encoding precision.\n\nWhile positional encodings have been tailored to next token prediction objectives, less emphasis has been placed on the best positional encoding strategies for masking objectives such asMLM-.\nConsequently, the above observations lead us to question whether current approaches are optimal for objectives such asMLM-.\nA promising path for training on more complex mazes with larger grid sizes could stem from a better understanding of how best to encode positions for longer-term planning objectives.\nTherefore, we consider the detailed study of positional bias in masking objectives likeMLM-crucial for future work.\n\nSECTION: 6Discussion\n\nBy adjusting the learning objective from next token prediction to one that explicitly predicts multiple steps ahead and back (MLM-), we show transformers can learn to effectively navigate mazes.\nFortunately, training with an explicit multi-step objective is also more efficient both in terms of training samples as well as GPU training hours and offers nice model scaling benefits with maze complexity.\n\nWe hope these findings spur the research community to explore learning objectives as a lever to address one of the main limitations of today\u2019s best transformer models: multi-step planning. In future work we hope to explore the role of learning objectives in a broader range of multi-step planning tasks.\n\nOf course, such an approach also comes with the typical limitations of transformers, including a fixed context length, which can limit or degrade the training speed of transformers as maze size grows. We observed the importance of positional encodings inMLM-training, particularly for more complex mazes. We suggest that there is more understand about the role of positional encodings for planning and identify this as important future work.\nFurthermore, we acknowledge the increased hardness of theMLM-objective. Instead of predicting the same token always with the same context, the context is randomly sampled every time the same training data is observed. For a sufficiently long sequence, the model will never see the same problem twice due to the exponentially increasing number of possible contexts. We cannot say how this impacts generalization speed in general, although we saw some favorable evidence in this work.\nIn an effort to keep the comparison as straight forward as possible, we usedMLM-exactly as described inKitouni et\u00a0al. (2024a). However, multiple improvements are possible. At inference time, it might be beneficial to generate tokens according to some heuristic about model certainty as opposed to left-to-right. Additionally, the uniform masking rate applied the same way to each token is certainly the simplest, but unlikely the optimal method. A semantic heuristic could favorably impact performance. A possible intuition here is that for many mask realizations, the problem is too easy or too difficult for the model, and it wastes time in those batches. Instead, over-sampling masks that make the problem hard but solvable might yield vastly increased convergence speeds.\n\nIn all, these findings shine light on a promising path forward for research to improve long-horizon planning in transformers, with lots of potential for future work.\n\nSECTION: References\n\nSECTION: Appendix AAdditional Results\n\nSECTION: A.1Per Token Results\n\nTo evaluate the possibility of the generated paths deviating only slightly from the shortest paths, we also compute the token-wise accuracy of the generated paths compared to the shortest path.\nInTable3andTable4we present per-token accuracies for the experiments fromTable1andTable2.\n\nSECTION: A.2Comparing transformer models for Next Token training\n\nWe compare two choices of architecture for autoregressive training with transformers: 1) the standard decoder architecture commonly used in modern language models, 2) the encoder-decoder architecture used for MLM-U. We train two 8M parameter transformer models with each of these architectures on 100k DFS 10x10 mazes and evaluate performance on held-out mazes. As shown inFigure8, we find the common decoder-only architecture converges more quickly and generalizes better than the comparable encoder-decoder architecture. We use the stronger decoder-only baseline for our experiments.\n\nSECTION: Appendix BAblations for Hyperparameters\n\nWe conduct hyperparameter ablations for learning ratesFigure9and weight decay inTable5.\nWe train the next token model with 8M parameters for 500 epochs on 100k 10x10 training mazes and evaluate per-token held-out accuracy to select the best learning rate. Based on this sweep we select 0.001 as the learning rate we use for all our experiments. ForMLM-we found learning rates to have negligible effect beyond an upper bound to ensure training stability. We select 0.001 as well. We found large weight decay values to be detrimental for next token training, seeTable5. InMLM-, we generally don\u2019t see overfitting and therefore also don\u2019t need any weight decay. We choosefor next token and no weight decay forMLM-. We found training to be most stable with the AdamW optimizer with beta valuesandand batch sizes of 128 and above.\n\nWe evaluate models of two different sizes: 8M parameter models with a width of 128, a depth of 40 and 4 heads per attention layer. For 25M parameter models, the width is 256 with a depth of 32 and also 4 heads per attention layer. In the case of an encoder-decoder, both encoder and decoder have depth/2 layers.\nDuring development of the experiments, we found that deeper models generally do slightly better in the 8M parameter setting, both innext token training and inMLM-.\n\nSECTION: Appendix CMLM-and Next token Failure Modes\n\nInFigure10we give some visual examples ofMLM-failure modes on 30x30 DFS mazes using the 8M model fromSection5.1. Often, the general path taken is mostly correct, but it takes a wrong turn or two and then backtracks to follow the right track, possibly ending up only a few steps short of the goal node.Figure11shows example failure cases of the next token model. Often, there is a general tendency towards the right path, but we find frequent backtracks, traversals through walls and often completely wrong end points.\n\nFigure12shows failures for the 8M model trained on the A* mazes, fromSection5.2. Note that in two of those failure cases (bottom left and right), the paths predicted are equivalent shortest paths. However, since we are checking for exact match in the deterministic A* setting fromLehnert et\u00a0al. (2024), those count as faulty. In those instances, the model does not seem to have picked up the way in which symmetry between shortest paths is broken in the deterministic dataset.\nNote that there also exist other failures that cause parsing errors and can therefore not be depicted. Those make up about half of all failure cases in the validation dataset for this 8MMLM-model.\nThe failure cases inFigure13for the 30x30 A* maze case are conceptually similar. However, the model fails in some additional ways. For instance, it sometimes misses \u2013or malforms\u2013 a step, which ends up being displayed as a diagonal move (left top and bottom). Or it predicts traversal through a wall (top right).\nThe bottom right path is a proper shortest path, but the model does not predict the last move correctly.\n\nSECTION: Appendix DMore details on the Experimental Setup\n\nSECTION: D.1MLM-training\n\nTheMLM-models are exposed to the same maze representation, start and end cells and subsequent solution path.\nUnlike the next token baselines the loss is not a next token prediction loss, but a masking loss reminiscent of the BERT training objective. Tokens are masked with a specific probability and the objective judges the model predictions on the masked tokens via the cross-entropy. In BERT, the masking rate is fixed, butMLM-draws masking rates uniformly for each batch.Kitouni et\u00a0al. (2024a)give an intuition for why uniform masking rates are advantageous. Since the uniform masking rate exposes the model to different length sequences to be completed and to draw information from, there is no distributional shift in a generative inference step, see Figure 2 inKitouni et\u00a0al. (2024a).\n\nFor this specific case of maze navigation, the only tokens that can be masked are part of the solution path. The model is never tasked to predict the maze representation or start or goal cells.Kitouni et\u00a0al. (2024a)report that theMLM-objective is best trained with a specific encoder-decoder architecture. The encoder has blocks in the layout of GPT-2 with a RoPE positional bias. The decoder input is a sequence of multiple copies of the same learnable token such that the decoder only has information about the positional bias via RoPE. See implementation details inSectionD.3.\n\nSECTION: D.2Maze Generation Details\n\nWe study two different kinds of mazes in this work. They have different properties and are represented in different formats. With that, we aim to demonstrate that our findings are not specific to a single type of maze or representation, but hold more generally.\n\nFirst, we utilize the maze generation method fromIvanitskiy et\u00a0al. (2023a)to generate 2 dimensional mazes via the randomized Depth First Search (DFS) method. This method works by visiting all grid cells in a depth-first manner. From a uniformly random start node, it uniformly picks a neighbor cell and removes walls between both cells whenever the target cell was not previously visited. If a cell does not have unvisited neighbors, it is declared a dead end and the algorithm backtracks until a cell with unvisited neighbors is found, starting a new \"descent\", like in standard depth first tree search. A goal cell is uniformly sampled. This generation algorithm makes for long paths, but does not allow ambiguity. The shortest path is also the only path that does not backtrack from dead ends.\nThe mazes are serialized into strings that enumerate the edges of the maze connection graph as a set of tuples. The start node, goal node and solution path are appended to form the full text that the model trains with. We generate 100\u2019000 mazes for each maze dimension, spanning 5x5 to 30x30.\n\nSecond, we use the deterministic A* maze dataset fromLehnert et\u00a0al. (2024). Start and goal cell were uniformly sampled in a 2 dimensional grid. Mazes were generated by randomly selecting 30-50of the cells to be walls and A* was used to solve those mazes. For anxmaze, the sampled problem is added to the dataset if the solution path is at least of length. In contrast to the DFS mazes, these mazes have many possible solutions, out of more than one are possibly the shortest ones.Lehnert et\u00a0al. (2024)experiment with both randomly and deterministically (heuristically) choosing the shortest path that the model sees as ground truth. Also unlike the DFS mazes, the text representation describes the set of walls rather than connections and puts the goal and final cell before everything else. In both datasets, the solution path is the last part of the string. Following, the setup inLehnert et\u00a0al. (2024)we train on mazes of varying complexities with grid sizes\n10x10, 20x20 and 30x30. We train only 100k mazes and reserve 2k mazes each for validation.\n\nFor a direct comparison of the maze setups, refer toFigures15and15. They depict how the prompt and response are made from maze instantiations of the A* and DFS type.\n\nNotably, the tokenizers for A* and DFS mazes treat cell representations differently. In DFS mazes each grid cell is one distinct token. This is done to avoid making the sequences too long. In A* mazes, grid cells are tokenized with individual tokens for x and y coordinate. We believe this presents a better inductive bias than individual tokens for each grid cell, but also increases the sequence length significantly. Since the solution paths are generally much shorter in these mazes, the extra sequence length is affordable. SeeFigure2for a comparison of path lengths between A* and DFS mazes.\n\nSECTION: D.3Implementation of Encoder-Decoder\n\nHere we show the exact encoder-decoder algorithm used forMLM-training on mazes, as it differs slightly from traditional models. Specifically, the difference lies in the fact that the decoder only sees a sequence of equal embeddings and only gathers information about the mazes from the cross attention with the encoder. Positional information is brought in via RoPE on queries and keys.\n\nSECTION: Appendix EMiscellaneous experiments\n\nSECTION: E.1Ordered masks\n\nOne of our motivations for utilizing a training scheme likeMLM-is that such a scheme enables more explicit reasoning over tokens that are further in the future than the immediate next token, hopefully aiding longer-horizon planning.\nIn light of this view we evaluate the following ablation: InMLM-each token in the solution path is masked with some (uniformly drawn) probability, independently of other tokens. Instead, we uniformly pick a position in the solution path and mask all tokens to the right of this position. Then we predict all of those tokens as a function of the context to the left of the chosen position. This method relates closer to the method used to solve the Star-Graph problem inBachmann & Nagarajan (2024). However, we find that this method is far inferior toMLM-in the 10x10 A* maze setting tested. The maximum per-token accuracy observed is 73%, with less than 4% full path accuracy.\n\nSECTION: E.2Generalization to smaller mazes\n\nTo see whether and how MLM-U and next token trained models perform out of their immediate training distribution, we evaluate models trained on 20x20 DFS mazes on smaller (10x10) mazes. Limitations in length generalization prohibit non-zero accuracies on larger mazes, but experiments on smaller mazes yield interesting results, seeTable6. In all experiments, we tokenize the 10x10 mazes via the 20x20 tokenizer. This is important because the 10x10 and 20x20 tokenizers in our training methods assign different tokens to the grid cells. While next token trained decoders can achieve non-trivial accuracy on smaller mazes out of the box, changing only the tokenizer,MLM-can not.In order to recover good performance inMLM-, we embed the 10x10 maze into the upper left corner of a random 20x20 maze in an effort to bring the smaller maze closer to the training distribution.", "text_file": "data\\paper_texts\\2412.05117v1_content.txt"}, {"title": "LayerShuffle: Enhancing Robustness in Vision Transformers by Randomizing\n  Layer Execution Order", "authors": ["Matthias Freiberger", "Peter Kun", "Anders Sundnes L\u00f8vlie", "Sebastian Risi"], "published_date": "2024-07-05T13:54:15Z", "summary": "Due to their architecture and how they are trained, artificial neural\nnetworks are typically not robust toward pruning or shuffling layers at test\ntime. However, such properties would be desirable for different applications,\nsuch as distributed neural network architectures where the order of execution\ncannot be guaranteed or parts of the network can fail during inference. In this\nwork, we address these issues through a number of training approaches for\nvision transformers whose most important component is randomizing the execution\norder of attention modules at training time. With our proposed approaches,\nvision transformers are capable to adapt to arbitrary layer execution orders at\ntest time assuming one tolerates a reduction (about 20\\%) in accuracy at the\nsame model size. We analyse the feature representations of our trained models\nas well as how each layer contributes to the models prediction based on its\nposition during inference. Our analysis shows that layers learn to contribute\ndifferently based on their position in the network. Finally, we layer-prune our\nmodels at test time and find that their performance declines gracefully. Code\navailable at https://github.com/matfrei/layershuffle.", "arxiv_id": "2407.04513v2", "html_link": "https://arxiv.org/html/2407.04513v2", "search_term": "ti:\"transformers\"", "html_content": "SECTION: LayerShuffle:\u00a0Enhancing Robustness in Vision Transformers by Randomizing LayerExecution Order\n\nDue to their architecture and how they are trained, artificial neural networks are typically not robust toward pruning or shuffling layers at test time. However, such properties would be desirable for different applications, such as distributed neural network architectures where the order of execution cannot be guaranteed or parts of the network can fail during inference. In this work, we address these issues through a number of training approaches for vision transformers whose most important component is randomizing the execution order of attention modules at training time. With our proposed approaches, vision transformers are capable to adapt to arbitrary layer execution orders at test time assuming one tolerates a reduction (about 20%) in accuracy at the same model size. We analyse the feature representations of our trained models as well as how each layer contributes to the models prediction based on its position during inference. Our analysis shows that layers learn to contribute differently based on their position in the network.\nFinally, we layer-prune our models at test time and find that their performance declines gracefully. Code available athttps://github.com/matfrei/layershuffle.\n\nSECTION: 1Introduction\n\nWhile demonstrating impressive performance in many domains(Krizhevsky et\u00a0al.,2012; Vaswani et\u00a0al.,2017; Radford et\u00a0al.,2021; Rombach et\u00a0al.,2022), deep learning systems demand both extensive computational resources and tight integration of their parts. For applications at scale, they therefore increasingly require the construction of large data centers with thousands of dedicated hardware accelerators. A paradigm shift from central to decentral model inference, where loosely coupled neural networks are distributed over a number of edge devices that share the computational load of the model(Gacoin et\u00a0al.,2019)therefore seems ultimately desirable.\nUnfortunately, current deep learning models lack the robustness necessary for such a paradigm shift.\n\nIn general, artificial neural networks (ANNs) are not robust toward pruning or replacing network layers during deployment.\nSimilarly, changing the order of execution in-between layers without further training usually results in catastrophic losses in accuracy. Nevertheless, these properties would be desirable e.g.\u00a0in distributed setups as described above, where a model is executed on a number of shared nodes in a network.\nThis way, overloaded or malfunctioning nodes could simply be skipped in favor of other available nodes.\n\nAugmenting models with these properties has historically been challenging. Due to the structure of the most common types of ANNs and how they are trained through backpropagation(Linnainmaa,1970; Werbos,1982; Rumelhart et\u00a0al.,1986), each neuron can only function by adapting to both its connected input and output neurons as well as the overall desired output of the network at training time.\nFurthermore, the hierarchical organization of explanatory factors is usually considered a necessary prior in deep learning, i.e.\u00a0one assumes that subsequent layers extract increasingly high-level features(Bengio et\u00a0al.,2013). Therefore, switching the execution orders of layers implies that layers would need to adapt and extract either low-level or high-level features depending on their position in the network. Unfortunately, network layers adapting in such a way to a changed order of execution appears to be infeasible for most known network architectures. The above prior is therefore violated and the overall performance of the network suffers beyond the point where the network successfully executes the task it has been trained for.\n\nThe more recently discovered transformer architecture(Vaswani et\u00a0al.,2017)has been shown to be more flexible. Transformers, when trained accordingly, can be layer-pruned at test-time(Fan et\u00a0al.,2019), and recent work merges similar transformer-based language models(Akiba et\u00a0al.,2024), all with only moderate reduction or even an improvement in performance. We hypothesize that the reason for the high adaptability of transformers can be found in self-attention modules being able to adapt their output based on the received input. Thus it should be possible to train a transformer network to not only adapt to the variation of its input features based on the overall network input but also the variations caused by receiving input from different layers during test time.\n\nWe propose and evaluate three training approaches for vision transformers to address the robustness issues laid out above. The most important component common to all approaches is randomizing the execution order of the vision transformer\u2019s stacked self-attention-and-feed-forward modules at training time (Figure1(a)). More precisely, the main contributions in this paper are:\n\nWith LayerShuffle, the layers of a vision transformer(Dosovitskiy et\u00a0al.,2020)are capable of adapting to an arbitrary execution orderat test time, assuming one tolerates a moderate reduction in performance. Providing each layer additionally with its current position in the network improves performance only slightly compared to a model without it, suggesting that each attention layer is already capable of determining its role based on the incoming data alone.\n\nAn analysis reveals that layers of models trained with LayerShuffle adjust their output depending on which position they hold in the network.\n\nTrained models can be layer-pruned at test time similar to the models trained with the techniques proposed inFan et\u00a0al. (2019), where their performance declines gracefully, i.e.\u00a0models with reduced amounts of layers still remain functional.\n\nSECTION: 2Related work\n\nZhu et\u00a0al. (2020)find that for particular subsets of inputs, transformers perform better when changing the execution order of layers to an input-specific sequence. They optimize the execution order per sample in order to maximize the performance of the model for natural language processing tasks.\n\nWhile the goal in their work is to find a layer sequence of a pre-trained model that is optimal for a given input, our approach aims to make the model robust to any sequence of execution, where layers might even be missing.\n\nIn parallel to our work on vision transformers, two groups have conducted similar experiments with the aim to understand how language models (LLMs) process data.Lad et\u00a0al. (2024)found that LLMs are very robust to changing the positions of adjacent layers or ablating single layers from the model.Sun et\u00a0al. (2024)perform similar experiments, and find that transformers improve iteratively upon their predictive output by subsequently refining the internal representation of the presented input. The main difference to our work is that the authors of these works do not perform any refinement on the models and switch and ablate layers locally with the aim of better understanding the inner workings of LLMs. Here we focus on methods and training approaches to increase this innate robustness of the transformer architecture to a point where models at test time function regardless of their layer execution order, and respond gracefully to the ablation of several layers in any position of the network.\n\nAnother related work is LayerDrop(Fan et\u00a0al.,2019), where the authors focus on robust scalability for models on edge devices. They propose dropping whole transformer layers during training and show that this training approach allows models to still deliver acceptable (if somewhat reduced) performance upon pruning layers at test time (e.g.\u00a0for balancing computational load). The main difference to our approach is that we randomly change the execution order during training, and, contrary to LayerDrop, do not remove any layers. Also, LayerDrop focuses on entirely on load balancing in compute-limited production systems while our main focus is on arbitrary execution order and the possibility to replace defective nodes by others on top of these issues in case of overloaded or malfunctioning nodes in distributed systems.\n\nWork on introducing permutation invariance into neural networks has been conducted byLee et\u00a0al. (2019),Tang & Ha (2021)as well asPedersen & Risi (2022). The corresponding former two approaches exploit the permutation equivariance of attention, i.e.\u00a0the fact that the order in which a sequence of vectors gets presented to the attention module does not change its result, but merely shuffles the sequence of output vectors. This equivariance is achieved by using a fixed-seed query vector in order to obtain an permutation invariant latent code. This latent code stays the same no matter in which order input tokens/patches are presented to the module. The main contrast to our work here is that we exploit permutation invariance in the order of layer executions rather than the order of tokens and patch embeddings and can therefore not make use of permutation equivariance of the attention operation, as it does not apply to switching inputs and outputs.\n\nFinally, the work ofGacoin et\u00a0al. (2019), not unlike our own, is motivated by the observation that a paradigm of distributed model inference over a number of loosely coupled compute nodes, edge devices or swarm agents promises a positive impact on the ecological and economical footprint of deep learning solutions. The authors propose a graph-theory-based framework to optimize the distribution of model parts to individual devices and optimize the overall energy consumption of the network. While our work sets out from the same motivation, it complements the approach ofGacoin et\u00a0al. (2019)as the the authors do not address robustness to adverse conditions in such distributed setups while it is the entire focus of this paper. The exact distribution of our models on the other hand, is beyond the scope of our work but combining our models with the approaches in(Gacoin et\u00a0al.,2019)seems a promising direction of future research.\n\nSECTION: 3Methods\n\nWe investigate three approaches for arbitrary layer execution order in vision transformers(ViT; Dosovitskiy et\u00a0al.,2020): First, we simply permute the order of layers randomly during training, such that every training batch is presented to the network\u2019s layers in a different random order (Section3.1). Second, while randomly permuting the layer order as in the previous approach, we use an layer-depth encoding inspired by learned word embedding approaches (Section3.2) to test if this additional information would further improve performance. Third, while randomly permuting layer order as in the previous approaches, we try to predict from the output of every layer at which position the layer is currently located in the network using a small layer position prediction network for every layer (Section3.3). A detailed overview on ViTs can be found in the appendix.\n\nSECTION: 3.1Randomly permuting layer order during forward pass\n\nDuring each forward pass, i.e.\u00a0for each batch presented to the ViT, we randomly permute the execution order of layers during training. The intention here is to teach the layers to not only extract meaningful intermediate representations when receiving input from a particular layer, but to be able to process and encode information from and for all possible layers in the network. In terms of training, exchanging the order of layers does not require any changes in the basic error backpropagation algorithm. For the forward path, the order how weight matrices are multiplied and activation and attention functions applied changes for every batch and forward pass. This needs to be accounted in the backward pass by propagating the gradients in the precise reverse order that has been set in the forward pass, i.e.\u00a0 multiplying the computed per-layer gradient matrices in the correct order. As we use Pytorch(Paszke et\u00a0al.,2019)in all our experiments, this aspect is taken care of the framework\u2019s autogradient feature. We refer to this model asLayerShuffle. To further illustrate the approach, a pseudo-code listing is given in Algorithm1.\n\nSECTION: 3.2Layer position encoding\n\nIn the second approach,LayerShuffle-position, we provide each layer with its current position in the network. Through this variation we aim to test if each layer can already adapt sufficiently by itself to information coming from different layers during test time or if giving it the current position can help further. In more detail, jointly with permuting the layer execution order, each layer learns a vector embeddingfor each possible index positionof the layer during training, where L is the number of layers andis our chosen embedding dimension. The layer\u2019s current indexin the network is presented together with the input to the layer(Figure2). The layer fetches the embedding vectorassociated with the passed indexand concatenates it to the input vector:N is the number of patches extracted form the input image, the functionsconcatandrepeatrespectively concatenate and repeat tensors along their last (most varying) dimension.\nA projection network, which consists of a LayerNorm (LN)(Ba et\u00a0al.,2016)module, a single linear layer, a GELU(Hendrycks & Gimpel,2016)activation function as well as a Dropout(Srivastava et\u00a0al.,2014)module, is then used to combine input and embedding and reduce it again to the used latent dimensionof the transformer. To ensure gradient flow during training, a residual connection is added as well:\n\nThe resulting outputis passed on to a regular multi-head-attention-and-feed-forward structure as described in Equations1and2.\n\nSECTION: 3.3Predicting current layer position\n\nTo determine if the incoming information to each attention layer is indeed sufficient for it to figure out its role, we specifically test for this ability with theLayerShuffle-predictvariant. We equip each layer of the network with a simple position prediction module that takes the current layer output as an input and seeks to predict the current position of the layer in the network (Figure3). The module consists of a single linear layerreceiving layer-normalized (LN)input..\n\nEach of these layer order prediction modules optimizes a cross-entropy loss where then the overall network optimizes the lossHere,is the regular cross-entropy loss of the output layer, andis the layer position prediction loss of layer i, which is also a cross-entropy loss:\n\nwhereis the number of layers in the network,is the-dimensional output of the position prediction network of layer, anddenotes the-th dimension of the vector.is the output logit denoting the network\u2019s predicted confidence that the layer currently is deployed at its actual position with index.\n\nSECTION: 4Experiments\n\nWe conduct our experiments on the ILSVRC2012 dataset(Russakovsky et\u00a0al.,2015), more commonly termed ImageNet2012, as well as the CIFAR-100 datasetKrizhevsky et\u00a0al.. We use the originalViT-B/16(Dosovitskiy et\u00a0al.,2020)vision transformer, as well theDeiT-Bdistilled data-efficient image transformerTouvron et\u00a0al. (2021). Pre-trained weights for ImageNet2012 are publicly available for both models(Dosovitskiy et\u00a0al.,2020; Wu et\u00a0al.,2020; Touvron et\u00a0al.,2021).\n\nTheViT-B/16has been pre-trained on ImageNet21k(Deng et\u00a0al.,2009; Ridnik et\u00a0al.,2021)at an 224224 input image resolution and refined on ImageNet2012 at the same resolution.DeiT-Bhas the same architecture asViT-B/16, but uses an additional destillation token during training, which is used to distill the inductive bias of a large convolutional network into the transformer in order to require less training data. It is pre-trained exclusively on ImageNet2012.\n\nBoth models are again refined on both ImageNet2012 and CIFAR-100 at the same resolution, but using the training processes as described in Section3. That is, layer execution order is randomly permuted while refining the model. To establish a baseline, on ImageNet2012, we refine the original models for one more epoch on without changing the layer order. Any longer training was found unlikely to bring additional improvement in preliminary experiments since our networks are already pretrained on ImageNet. For CIFAR-100, we refine our baseline for 20 epochs. For each approach, including the baselines, we trainnetworks and compare their average validation accuracy.\n\nAll models are refined using Adam(Kingma & Ba,2014)(,,), where an initial learning rate ofwas empirically found to work best. In terms of batch size, we evaluate training batch sizes of 640 images, which is the maximum multiple of 8 that can fit in the video memory of our used GPU, as well as 128 images for models that benefit from a smaller batch size. Even smaller batch sizes do not yield any improvement in performance for our models. Inspecting training curves shows that for ImageNet2012 the performance of models plateaus at 20 epochs the latest, which is therefore set as the maximum number of training epochs. For CIFAR-100 we use 100 epochs since the models were pretrained on a different dataset, i.e.\u00a0ImageNet. We use a form of early stopping by evaluating the model achieving the lowest crossentropy loss on the validation set after the maximum amount of training epochs. All models have been trained on a single NVIDIA H100 Tensor Core GPU with 80GB of memory. Training a single model on ImageNet2012 for 20 epochs takes about 7 hours whereas CIFAR-100 training times are significantly shorter due to the smaller train set.\n\nSECTION: 4.1Sequential vs.\u00a0arbitrary execution order\n\nThe average accuracy for all approach\non all vision transformer architectures and datasets is shown in Table1.\nWe make the following observations across all models and datasets:\n\nOn both the CIFAR-100 and the ImageNet2012 dataset, our baselines refined from pre-trainedViT-B/16andDeiT-Bmodels perform very much as expected. For a classic sequential execution order of the model layers, on ImageNet2012 the trained models achieve an average validation accuracy very close to the performance of the respective original pretrained models(Dosovitskiy et\u00a0al.,2020; Touvron et\u00a0al.,2021). Our refined baselineViT-B/16obtains an average accuracy ofwith a standard deviation of. TheDeiT-Bmodel attains a slightly lower accuracy ofwith a standard deviation of. Baseline results CIFAR-100 look similar withViT-B/16andDeiT-Bachieving(standard deviation:) and(standard deviation:) respectively.\nNot surprisingly, for an arbitrary layer execution order, the average model accuracy declines catastrophically to belowfor all trained models on both datasets. Our original assertion that in general, ANNs are not robust to changing the execution order of their layers, is in line with these results.\n\nOur LayerShuffle approaches show lower performance than the baselines when executing layers in their original order. On ImageNet2012, our trainedViT-B/16models obtain average accuracies of,, andrespectively for ourLayerShuffle,LayerShuffle-positionandLayerShuffle-predictapproaches. Average accuracies forDeiT-Bmodels are in a similar range with,, andfor our respectiveLayerShuffle,LayerShuffle-positionandLayerShuffle-predictapproaches. On CIFAR-100 on the other hand the gap between baseline and LayerShuffle models is somewhat larger for the trainedViT-B/16models. These models merely achieve,andwith slightly higher standard deviations forLayerShuffle,LayerShuffle-positionandLayerShuffle-predictapproaches. ForDeiT-Bmodels on the other hand, these approaches perform similarly well to the models trained on ImageNet with scores of,andfor the above mentioned techniques. A possible explanation for these discrepancies could be found inViT-B/16models requiring more and more diverse training data compared toDeiT-Bmodels, which has been pre-trained with the aim to reduce the amount of required training data.\n\nDespite being outperformed by the baseline in a sequential execution order setting, all models improve dramatically over their corresponding baseline models in an arbitrary execution order setting. Taking a closer look at LayerShuffle model performance in that setting, we find that the simplest approach performs very well across both architectures and datasets. For both ImageNet2012 as well as CIFAR-100 validation setsDeiT-Btrained on LayerShuffle yields the best performance with average accuracies ofandrespectively, narrowly outperformingLayerShuffle-position, which receives information about the layer position.LayerShuffle-positionachieves scores ofandon these datasets. ForViT-B/16models on the other hand,LayerShuffle-positionoutperformsLayerShuffle. The former achieves scores ofandof ImageNet2012 and CIFAR-100, with the latter performing only slightly worse withand. The most likely explanation for models of theViT-B/16architecture achieving significantly lower accuracies on CIFAR-100 thanDeiT-Bmodels can again be found in the former requiring less training data than the latter.\n\nWe find that the position prediction approach,LayerShuffle-predictis outperformed by both our remaining approaches on all datasets and architectures. On ImageNet2012, refinedViT-B/16models achieve average accuracies ofwhereasDeiT-Bmodels attain. On CIFAR-100 the former score, the latter. A possible explanation might be that due to optimization of multiple objectives (fitting both the output labels as well as predicting the current position of the layer) this approach requires more careful hyperparameter tuning.\n\nA further interesting observation is to be made when comparing the performance for sequential and arbitrary execution order for each approach respectively. For all approaches, using the original layer order for sequential execution still performs better than an arbitrary order. This is most likely a consequence of fine-tuning from a sequentially trained model.\n\nFor the layer position prediction approach, we measure the average accuracy of layer position predictions over all five trainedLayerShuffle-predictmodels, and find that the layer position is predicted correctly inof all cases. These results demonstrate that each layer has enough information coming from its inputs alone to predict where it is in the network, providing the basis to adapt to its current position. We investigate this further when analyzing intermediate network representations in Section4.3.\nIn conclusion, refining a pre-trained model while randomly permuting the execution order of the network layers can make a model more robust towards such arbitrary execution orders at test time. On the other hand, Dropout and LayerNorm by themselves do not have the same effect and fail to produce networks robust against layer shuffling.\n\nSECTION: 4.2Removing layers during test time\n\nTo determine how neural networks trained with LayerShuffle would perform when several devices in a (distributed) model become unavailable, we further investigate the effect of pruning an increasing amount of layers during test time. We evaluate its average validation accuracy over 5 models when only using 3, 6, or 9 layers. In addition, we refine the originalViT-B/16transformer using LayerDrop(Fan et\u00a0al.,2019)with a drop probability of 0.2 (as recommended by the authors) and compare it as a baseline to our approach under identical conditions. Note that whenever we evaluate the accuracy of our proposed approaches as well as the baseline, we do so two times: Once, for the original \u201dsequential\u201d layer order as originally intended and trained for theViT-B/16transformer, and once with arbitrary layer execution order where we change the order randomly for every forward path.\n\nFor sequential execution (Figures1(b)), LayerDrop with a drop rate of 0.2 behaves similarly to LayerShuffle, with the exception that our approach performs better for a small number (3) of layers with an average accuracy of approximatelyvs.\u00a0close tofor LayerDrop.\nWhile for 6 layers, both approaches are roughly on par, for 9 layers LayerShuffle is slightly outperformed by LayerDrop as both approaches show an average accuracy in therange. At the full amount of 12 layers, this gap in average accuracy stays roughly the same as the LayerDrop-refined model closes in on the full accuracy of the original model, while our LayerShuffle approach achieves slightly lower accuracies (see also Table1). For comparison, we also visualize models where we refined a reduced number of 3, 6, and 9 layers: while delivering similar performance as LayerDrop for 9 and 12 layers, these models perform significantly better than the previously discussed approaches at lower numbers, i.e.\u00a03 and 6 layers. They do however, bear the drawback that for each specific amount of layers a new model must be refined from the original model, whereas for both LayerDrop and our LayerShuffle approach, only a single full-size model needs to be refined and the number of layers can be configured at will at test time.\n\nFor arbitrary execution (Figure1(c)), LayerShuffle is the only approach that succeeds, with the average accuracy improving as the number of layers is increased. LayerDrop does not perform well regardless of the number of layers in the model.\n\nA noteworthy detail is the comparable high average accuracy of the fully retrained baseline with 3 layers. Given the low performance of the refined models with 6 and 9 layers, as well as that there are only 6 possible permutations for 3 layers, the most likely explanation is that one of the 5 random permutations evaluated for the model was the original layer execution order the model has been trained for, i.e.\u00a0 [1,2,3] therefore skewing the achieved accuracy in this case. In conclusion, we find that our proposed approach has similar test-time scaling capabilities as LayerDrop, while still ensuring robustness towards arbitrary layer execution orders.\n\nSECTION: 4.3Analysis of intermediate network representations\n\nTo gain a deeper insight into how information is encoded in the models, we conduct two experiments. First, we compute Uniform Manifold Approximation and Projection (UMAP)(McInnes et\u00a0al.,2018)embeddings of the entire output of a particular attention module (i.e. combined self-attention and feed-forward layers), where we color-code all output vectors based on the position the module held in the network when producing this output. In more detail, we concatenate all patch tokens of a single image together with the class token as a single vector, and use this representation as a single state vector in our compression. To extract a sufficient number of these state vectors, we present 1,000 randomly sampled images from the ImageNet2012 validation set to a LayerShuffle-trained model. While we use an evaluation batch size of 1 image and record all outputs of a single, previously selected layer, we randomly permute the execution order of layers such that the selected layer changes position in the network during every forward path. After the layer\u2019s output vectors for all 1,000 images have been recorded, a UMAP reduction of the output space to 2D is performed.\n\nSecond, in order to investigate how much each layer contributes to the final classifier output when deployed in different positions within the model, we compute the L2-Norm of the class-token of each layer output. We correct for the contribution of previous layers by subtracting the class token of the previous layer before computing the token\u2019s norm. That way, we consider solely the additive contribution of the layer to the class prediction of the model. We collect these token norms for all network layers as we shuffle their position while presenting 1000 randomly sampled input images in an identical manner as in the previous experiment.\n\nFinally, to establish a baseline, we extract both representations for the originalViT-B/16weightsDosovitskiy et\u00a0al. (2020)as well.\nFigure4(b)shows the obtained visualizations for the originalViT-B/16model acting as a baseline as well as our model refined withLayerShuffle.\nIn more detail, Figures4(a)and4(b)show the UMAP embeddings of a single layer\u2019s output for both the baseline and our model. The current position of the layer in the network when producing a given output is color-coded from dark (position close to the input) to light (position close to the output). Note that this information about the layer position has not been presented to the UMAP algorithm. Apart from rough trends, no clear ordering of the space is visible for the baseline (Figure4(a)). For LayerShuffle, while there is no sharp separation between outputs generated at different positions in the network, the layer clearly adapts to its current position and extracts different features for different positions in the network (Figure4(b)).\n\nA further interesting observation is the very distinct collection of points for layer positions close to the input, which are detached from the remaining manifold of points. This results suggests that extracting low-level features, requires special treatment.\n\nFigures4(c)and4(d)show the distributions on the normalized L2-norm of additive contributions to class tokens for different layer positions in the transformer for both the baseline and our model. Each x-axis in the plot corresponds to a single layer of the network, where position of the layer in the network is color-coded again from close to the input (dark) to close to the output (light). x-axes are also ordered corresponding to the layer\u2019s original position in the pre-trained model, where the order of layers is top to bottom.\nWe can see that for the baseline model norms are basically spread out over the whole range. This implies that layers in the baseline model overall contribute evenly to the predictive output of the model, regardless of their current position in the network.\n\nOn the other hand, the ridge plot gathered from layer outputs of the model refined with our method paints a different picture. The norm of attention modules output and therefore it\u2019s contribution to the model\u2019s prediction varies based on the distance to it\u2019s original position in the networks. Modules which were originally closer to the input (x-axes on top of the plot) often show larger contributions to the predictive output of the model when on positioned closer to the input and vice versa. This indicates that our refinement of the model conditions its layers to contribute to the overall predictive output if the received input lies within the layers learned distributions of inputs (i.e.\u00a0the layer is close at a position assigned to it in the original pre-trained network), and withhold or reduce their output otherwise. This is also in line with recent work conducted in parallelSun et\u00a0al. (2024), which frames transformer layers as incrementally refining a rough sketch of the model\u2019s output, an iterative process which is enabled by the transformer\u2019s extensive utilization of skip-connections.\n\nIn conclusion, our analysis indicates that refining networks withLayerShufflemakes vision transformers robust to arbitrary execution orders as it trains the layers to solely add to the models contribution if the layer input is in-distribution and reduce their output otherwise, in which case the model\u2019s skip-connection forwards the out-of-distribution output to the subsequent layer.\n\nSECTION: 5Discussion and Future Work\n\nThis paper presented a new approach called LayerShuffle, which enabled vision transformers to be robust to arbitrary order execution as well as pruning at test time.\nFor sequential execution, LayerShuffle performs on average\nonly slightly worse than the LayerDrop approach but is the only method that works when the layer execution is arbitrary.\n\nOur analysis confirmed that layers of models trained with LayerShuffle adjust their output depending on which position they hold in the network. Furthermore, our results indictate that refining networks withLayerShuffletrains the layers to only contribute to the model\u2019s class prediction if the layer input is in-distribution and reduce their output otherwise, in which case the layer\u2019s skip-connection forwards the barely modified out-of-distribution embedding to the subsequent layer.\n\nIn the future, these properties could makeLayerShuffle-trained models ideal candidates to be distributed over a number of very loosely coupled compute nodes to share the computational load of model inference. Given the enormous engineering, financial and logistical effort as well as the environmental impact(Strubell et\u00a0al.,2020)of building and maintaining datacenters for state-of-the-art deep learning approaches on the one hand, as well as the large amount of available, but scattered compute through existing smartphones, laptop computers, smart appliances and other edge devices on the other hand, approaches that allow distributed neural networks to perform inference could be of great impact. We therefore consider the deployment and orchestration of our trained models onto an actual set of edge devices and the practical implementation of the inference process on a network of such devices, likely by combining our approach with previously proposed frameworks to address this issue(Gacoin et\u00a0al.,2019), a very promising direction of future research.\n\nSECTION: Acknowledgements\n\nWe would like to thank Prof.\u00a0Christian Igel for his insightful feedback on the manuscript. Furthermore we thank the members of the REAL and Creative AI lab for fruitful discussions. This work was supported by a research grant (40575) from VILLUM FONDEN.\n\nSECTION: References\n\nSECTION: Vision Transformers\n\nDosovitskiy et\u00a0al. (2020)have successfully adapted the transformer architecture to computer vision by introducing a preprocessing step that converts images to suitable sequences.\nThey do so by splitting an imageinto a sequence of N flattened patches, and then pass each patch through a linear embedding layer,andare here the height, width and number of channels of the image respectively andis the patch size.is the internal latent dimension of the transformer which remains constant throughout the network and can be set as a hyperparameter.\n\nAfter converting the image into a sequence that can be processed by a transformer encoder, inspired by BERT(Devlin et\u00a0al.,2018), the authors prepend aclasstoken toin which the class information of the input image can be aggregated by the transformer. To encode position information into the embedding, a positional embedding tensoris added. Both theclasstoken as well as the positional embeddings are learnable embeddings, which are trained jointly with the rest of the network. The resulting input sequence presented to the transformer network can be expressed as\n\nThis sequence is presented to a standard transformer architecture of stacked attention modules. Each attention module consists of a multi-head self-attention (MSA) layer and a feedforward layer or multilayer perceptron (MLP) layer. MSA layers utilize self-attention (SA)(Vaswani et\u00a0al.,2017), a powerful concept that allows transformers to relate and combine its feature embeddings with each other. Self-attention extracts features from the input sequence, which in turn preforms a transformation of the input vector sequence.\n\nSpecifically, self-attention extracts query, key and value sequences,andfrom the input sequence using a linear projection:Theandsequences are then used to compute a Softmax-normalized transformation matrixindicating how to incorporate information of the whole sequence (i.e.\u00a0in our case all image patches) for every single vector of the sequence:Scaling the dot-product product byhere ensures a balanced distribution of the Softmax output. After obtaining, the output of SA is computed as\n\nA multi-head self-attention (MSA) layer(Vaswani et\u00a0al.,2017)performs several attention operations in parallel, concatenates the result and projects it back to the internally used latent dimension of the transformer:\n\nIn an attention module the multi-head self-attention layer is followed by a multi-layer-perceptron (MLP) layer transforming the recently combined embeddings to extract new feature representations. Before presentingto each layer in the module, the embeddings are normalized using LayerNorm(Ba et\u00a0al.,2016). To ensure consistent gradient flow during training, residual connections(He et\u00a0al.,2016)are behind both the MSA and the MLP layers(Wang et\u00a0al.,2019). Furthermore, as a regularization measure, Dropout(Srivastava et\u00a0al.,2014)is applied after every MSA and MLP layer.\nIn summary, given the sequencefrom a previous attention module as input, we first compute the intermediate representation\n\nwhich is the presented to the MLP layer to compute the final output of the module\n\nFinally, after N attention modules, the first vector of the sequence (corresponding to theclass-token in the preprocessed input) is handed to a linear layerto predict the final class of the image:denotes the number of classes.", "text_file": "data\\paper_texts\\2407.04513v2_content.txt"}, {"title": "An Evolved Universal Transformer Memory", "authors": ["Edoardo Cetin", "Qi Sun", "Tianyu Zhao", "Yujin Tang"], "published_date": "2024-10-17T02:47:10Z", "summary": "Prior methods propose to offset the escalating costs of modern foundation\nmodels by dropping specific parts of their contexts with hand-designed rules,\nwhile attempting to preserve their original performance. We overcome this\ntrade-off with Neural Attention Memory Models (NAMMs), introducing a learned\nnetwork for memory management that improves both the performance and efficiency\nof transformers. We evolve NAMMs atop pre-trained transformers to provide\ndifferent latent contexts focusing on the most relevant information for\nindividual layers and attention heads. NAMMs are universally applicable to any\nmodel using self-attention as they condition exclusively on the values in the\nproduced attention matrices. Learning NAMMs on a small set of problems, we\nachieve substantial performance improvements across multiple long-context\nbenchmarks while cutting the model's input contexts up to a fraction of the\noriginal sizes. We show the generality of our conditioning enables zero-shot\ntransfer of NAMMs trained only on language to entirely new transformer\narchitectures even across input modalities, with their benefits carrying over\nto vision and reinforcement learning.", "arxiv_id": "2410.13166v3", "html_link": "https://arxiv.org/html/2410.13166v3", "search_term": "ti:\"transformers\"", "html_content": "SECTION: An Evolved Universal Transformer Memory\n\nPrior methods propose to offset the escalating costs of modern foundation models by dropping specific parts of their contexts with hand-designed rules, while attempting to preserve their original performance.\nWe overcome this trade-off with Neural Attention Memory Models (NAMMs), introducing a learned network for memory management that improvesboththe performance and efficiency of transformers.\nWeevolveNAMMs atop pre-trained transformers to provide different latent contexts focusing on the most relevant information for individual layers and attention heads.\nNAMMs are universally applicable to any model using self-attention as they condition exclusively on the values in the produced attention matrices.\nLearning NAMMs on a small set of problems, we achieve substantial performance improvements across multiple long-context benchmarks while cutting the model\u2019s input contexts up to a fraction of the original sizes. We show the generality of our conditioning enables zero-shot transfer of NAMMs trainedonlyon language to entirely new transformer architectures even across input modalities, with their benefits carrying over to vision and reinforcement learning.\nOur source code is available athttps://github.com/SakanaAI/evo-memory.\n\nSECTION: 1Introduction\n\nTransformer architectures have become the golden standard in deep learning, with ubiquitous applications in the design of modern foundation models, exhibiting exceptional performance and scalability(Achiam et\u00a0al.,2023; Das et\u00a0al.,2023; Team et\u00a0al.,2023; Dosovitskiy et\u00a0al.,2020; Chen et\u00a0al.,2021a; Brohan et\u00a0al.,2023; Gur et\u00a0al.,2023).\nThe outputs of a transformer are exclusively conditioned on a recent context of input tokens, which for language models (LMs) generally correspond to a window of preceding words.\nThus, addressing the challenge of extending this context window is critical to enable tackling long-range tasks and is currently a focal area of research(Huang et\u00a0al.,2023).\nHowever, long contexts also immediately impact training and inference costs, with modern foundation models being increasingly resource-hungry and expensive. Many recent methods proposed to partially offset these costs by studying how to heuristically quantify the importance of each token stored in the model\u2019slatent memory, i.e., stored in itsKey-Value (KV) cache. Then, by simplyevictingthe least important tokens with hand-designed strategies, they have shown early success at reducing memory size while limiting performance losses(Luohe et\u00a0al.,2024).\n\nOur research aims to go beyond these hand-designed strategies as we hypothesize that shaping the latent memory KV cache of transformers entails new opportunities toimprovetheir capabilities in downstream tasks.\nOne widely evidenced example in support of our hypothesis is the effectiveness of hand-crafted input context modifications through prompt engineering(Liu et\u00a0al.,2023), even allowing foundation models to learnin-contextentirely new skills at test time(Brown et\u00a0al.,2020).\nFurthermore, unlike prompt engineering, directly managing the memory of transformers enables the provisioning of distinct contexts to each latent level independently, such that individual layers and attention heads can focus on the most relevant information for their specific needs.\n\nMotivated by these considerations, we propose Neural Attention Memory Models (NAMMs), introducing a new class of networks trained with evolution to learn an efficient memory system that maximizes the downstream performance of pre-trained transformers.\nEvolution inherently overcomes the non-differentiability of memory management operations with binary outcomes (selecting tokens to preserve/discard) which renders gradient-based optimization incompatible.\nOur efforts are inspired by the key role that natural evolution played in shaping human memory, which analogously appears to selectively incorporate and actively prune information based on its lifelong usefulness(Sherry & Schacter,1987; Nairne & Pandeirada,2010; Frankland & Bontempi,2005).\n\nOur NAMMs are conditioned on features entirely constructed from the attention matrix, making them universally applicable to any transformer-based architecture.\nLearning NAMMs atop a pre-trained Llama 3 8B model(Dubey et\u00a0al.,2024), we not only obtain efficiency benefits, with substantial reductions in the number of retained tokens in the KV cache, but alsoexceedthe performance of the full-context model with notable margins. We validate these findings across 36 different tasks from LongBench(Bai et\u00a0al.,2023), InfiniteBench(Zhang et\u00a0al.,2024a), andChouBun111ChouBun is the pronunciation of \u201c{CJK}UTF8ipxm\u9577\u6587\u201d, literally translating to \u201clong text\u201d in Japanese., a new Japanese benchmark designed to assess long-context capabilities beyond the common English and Chinese. These results mark a clear contrast with the aforementioned hand-designed strategies that appear to inevitably trade off efficiency for performance, in line with their stated purpose.\n\nFurthermore, we show that the generality of our parameterization enableszero-shot transferof NAMMs trained on three natural language tasks to entirely new transformer models.\nIn particular, we obtain further performance and efficiency improvements not only when using the evolved NAMMs with other LMs of increased size, but also transformers with entirely different architectures concerned with new input modalities, for problems such as vision and reinforcement learning.\nIn a nutshell, our main technical contributions can be summarized as the following:\n\nWe introduce NAMMs, a novel memory evolution framework that adds a new dimension to optimizing transformer models without altering their powerful architectures.\n\nWe design and successfully train NAMMs on top of pre-trained transformer models, obtaining both performance and efficiency gains on several long context language tasks.\n\nWe show NAMMs, trained only on language tasks, can be transferred zero-shot to any other transformers, retaining benefits across different input modalities and task domains.\n\nSECTION: 2Background and preliminaries\n\nAttention and transformers.Transformers are neural network architectures designed specifically for efficiently processing input sequences.\nThese models take as input a stream of tokens (e.g., embeddings of words, image patches, robotic states, etc.) and, produce a set of latents with the same length within their layers.\nMulti-headed dot product attention(Vaswani et\u00a0al.,2017), or simplyself-attention, characterizes modern transformers, facilitating effective information sharing across token representations.\nThe attention layer conducts a set of parallel computations, each known as an attention head, mapping tokens to query, key, and value vectors. These vectors are organized along the sequence dimension in the matrices,, and, and the layer\u2019s output is computed as:\n\nHere,represents an optional mask multiplying theattention matrix, usually enforcing anauto-regressive conditioningsuch that each token cannot attend to its future. An interpretation of the attention layer comes from the elements of the attention matrix, i.e., the dot products between each keyand querynormalized along the column dimension.\nIntuitively, each of these values can be understood as the relativeimportanceof tokenin processing the input representation of token.\n\nFrequency-based feature extraction.An established canonical technique to pre-process one-dimensional non-stationary signals is the Short-Time Fourier Transform (STFT)(Allen & Rabiner,1977). This technique has seen plenty of applications for feature extraction concerning audio, biomedical, seismic, and many more kinds of modalities. The STFT performs a time-convolution of a signal, shifting each convolutional window to the frequency domain through a discrete Fourier transform, producing aspectrogramrepresentation of the original input. We useto denote the fixed-sized vector produced at each timestep, where thefrequencies span from zero up to the Nyquist frequency (half the original sampling rate).\nMathematically,the-thfrequency from an STFT for timeis extracted from an input vectoras:\n\nHere, the convolutional filter of the SFTF is defined by the product of a finite-lengthwindow functionwith each exponential term in the Fourier transform. A popular choice foris the Hann window(Oppenheim,1999), employing a smooth decay at its edges which helps minimize the overestimation of the magnitudes of the higher frequencies indue tospectral leakage(Harris,1978).\n\nSECTION: 3Neural Attention Memory Models\n\nAn immediate limitation of transformers is the quadratic costs associated with computing the attention matrix.\nTo partially address this issue, during auto-regressive generation, the latents for the keys and values of the tokens generated at the previous steps are usually stored in what is referred to as the KV cache.\nThis object can be regarded as being analogous to thememoryof the transformer, which now, at each step, only needs to compute the query, key, and value of the latest token and perform attention over a horizontal vector by exploiting causal ordering. In this section, we describe the feature extraction, architecture, and optimization of NAMMs, which have been designed to act on the KV cache to improve both the performance and practicality of this powerful class of models.\n\nSECTION: 3.1Attention spectrograms for model-agnostic feature extraction\n\nThe feature extraction framework of NAMMs is designed to be agnostic to the parameterization of the base transformer they are applied for.\nIn particular,we build a representation for each token in the current KV cache memory directly from its corresponding unmodified column vector in the attention matrix.To meaningfully compress this unbounded vector signal, we process it via an STFT with a fixed-sized Hann window (Figure2, left).\nThis operation produces a spectrogram representation of the attention columns, representing the frequencies with how the queries attend to each of the stored key tokens (indexed by) on a compressed time-axis (indexed by).\nThus, this representation exposes precisely the knowledge of how each token\u2019s relative importance varies across all past queries in a compact form factor, discarding all other information specific to the learned transformer weights.\n\nAs NAMMs rely only on the attention values for their input, they are universally applicable to any layer producing an attention matrix.\nThis property is crucial, enabling us to avoid learning individual memory models for the different layers of a transformer, thus, greatly limiting the number of total optimized parameters. Furthermore, it also allows efficient training on top of smaller foundation models for targeted problems, and later transferring the resulting models zero-shot at test-time to larger architectures and arbitrary applications.\n\nSECTION: 3.2Memory model design and cross-token communication\n\nNAMMs parameterize a small neural networkto output a scalarselection scorefor eachtoken in the KV cache.\nFirst, to obtain a consistent input dimension, we reduce the attention spectrogram into a smaller feature vectorby compressing the time-axis via an element-wise exponentially moving average (EMA:; Figure2, center).\nWe then append positional encodings and feed the vectorto the memory model\u2019s networkto produce the score.\nFinally, we evict from the KV cache memory all latent tokens with, effectively treating the problem as a binary classification task.\nWe repeat this process with a fixed interval, every set number of new input tokens,.\n\nBackward attention memory models (BAM).For the design of, we posit that sharing information from all tokens in memory could be key for assessing their importance.\nA particularly motivating scenario in LMs arises when considering the case of repeated words or sentences, where learning a diversity measure that compares different tokens would allow preventing redundancies in the KV cache.\nCorroborating this intuition, even from a biological perspective, memory formation and retention appear to adhere to models of neuronal competition(Han et\u00a0al.,2007).\n\nBased on these considerations, we design the backward attention memory architecture (BAM) for parameter-efficient sharing of information while making use of the powerful inductive biases enabled by the masked self-attention operation.\nIn particular, we implementvia an initial self-attention layer with acounter-causalmask, which we refer to asbackward(Figure3). This design serves to introduce a purposeful asymmetric relationship, allowing to distinguish between older and newer tokens. We then outputfrom a final linear operation:\n\nwhereare the key, value, and query matrices from all feature vectorsin memory.\nUsing BAM to tackle the previous motivating scenario, only the representation for older tokens would be potentially affected by the presence of newer duplicates.\nThus, just by learning a simple diversity metric within self-attention, backward masking would provide the memory model with the potential to preserve only the most informed occurrence of each token without risking discarding any information in its entirety (since the score for the latest instance of each repeated token would be independent of its past).\n\nInpractice, when applying NAMMs, we only affect the KV cache of the base model with a fixed frequency, once everysteps. When feeding longer prompts to our model, we simply split the tokens into-sized chunks. We summarize the full execution pipeline of NAMMs in Algorithm1. We refer to AppendixAand our shared code for additional implementation details and discussion.\n\nSECTION: 3.3Incremental evolution\n\nWe evolve the network weights of our NAMMs to directly optimize the performance on a subset of long-context language modeling tasks from LongBench(Bai et\u00a0al.,2023). As we share a singleacross all layers, even with our largest NAMM we only evolve about 4000 total parameters.\nWe use the seminal CMA-ES optimization algorithm(Hansen,2006)and apply NAMM atop a Llama 3 8B base model(Dubey et\u00a0al.,2024)with a context extended from 8192 to 32768 tokens via NTK-aware positional interpolation(bloc97,2023).\nDue to the inference costs of LMs with long inputs, we sample a subset of different prompts from each task in each generation and propose training in anincrementalfashion: starting from a single task, and adding additional tasks at later training stages.\nEmpirically, we found both these choices to provide effective regularization, improving generalization (see AppendixC).\nThe performance of modern LMs on LongBench varies considerably across tasks, and even across different task prompts.\nHence, instead of using the raw scores, we opt to maximize normalized performance relative to the vanilla base model\u2019s stored evaluation performance on each same subset of prompts, retaining all tokens in its KV cache memory.Using evolution, we note that our training loop simply corresponds to running inference NAMMs atop the base, requiring no expensive backpropagation or dedicated hardware.\n\nWe choose three tasks from different LongBench categories across both English and Chinese where the Llama 3 base model seems to particularly struggle: PassageRetrieval-en, DuReader, and NarrativeQA; optimizing the normalized exact match, ROUGE-L, and F1 metrics, respectively.\nWe evolve our NAMM for 300 generations in its first incremental phase, 250 in its second, and 120 in its third. We diminish the number of generations to counteract the increasing costs with each additional phase and make more efficient use of computational resources.\nAt the end of each phase, we resume from the best previous checkpoint. We provide training curves of our main backward-attention model in Figure4, showing the average and standard deviation of the normalized batch performance across the population (left), together with the normalized per-task and average performance on all samples of the optimized mean from CMA-ES (right).\nWe refer to AppendixAfor additional architectural and optimization details, together with the set of hyper-parameters. We also provide additional statistics and training curves for other memory model designs in AppendixC.\n\nSECTION: 4Experimental Results\n\nIn this section, we evaluate and analyze evolved NAMMs as compared to full-context transformers andthree recent hand-designed methods for KV cache management: H2O(Zhang et\u00a0al.,2024c)and L2(Devoto et\u00a0al.,2024), and FastGen(Ge et\u00a0al.,2024). We compare each method in terms of absolute and normalized performance and also provide the resulting average cache size recorded at the end of each prompt. We first consider three long-context language modeling benchmarks spanning 36 diverse tasks in three languages, using the same Llama 3 8B base transformer from training. Then, we evaluate the capabilities of zero-shot transferring NAMMs to otherunseentransformers and task domains. In particular, we not only consider transfer to larger LMs, but also transformers with tokens constructed from modalities other than language. Across all these settings, we also compare BAM with a simpler 2-layer MLP architecture and provide summarized results after every stage of incremental evolutions. We refer to AppendixCadditional evaluations (e.g., transferring NAMMs to a Mistral LM), ablation studies (e.g., comparing different architectures and input features), and all learning curves. Lastly, we conclude the Section with a targeted qualitative analysis, aimed at understanding the behavior of our new memory framework.\n\nSECTION: 4.1Long-context language understanding\n\nLongbench.In Table2, we provide results across all LongBench tasks(Bai et\u00a0al.,2023)and in Figure5we provide a summarized comparison varying the maximum cache size of H2O and L2 (we provide a similar analysis for FastGen in Figure9).\nOur NAMM yields concrete improvements to the Llama 3 8B transformer both when considering the full set or exclusively the held-out set oftesttasks that were not used for evolution, with improvements of 11% and 7% respectively. At the same time, our NAMM also yields efficiency side benefits, notably reducing the context-extended KV cache size.Instead, H2O, L2, and Fastgen all come with performance costs which notably grow the smaller their cache sizes - in line with their stated objective ofretainingrather thanimprovingthe original full-context performance. These results emphasize the inevitable tradeoff induced by prior hand-designed methods, able to obtain efficiency gains but at increasing performance costs due to their lossy heuristics. On the other hand, we find NAMMs successfully provide a paradigm shift, yielding consistent improvements from the base model across both\nperformance and efficiency axes by learning to discard unhelpful information, highlighting how end-to-end evolutionary optimization can open new orthogonal directions beyond what is feasible with manually-designed heuristics.\n\nInfiniteBench.In Table3, we provide results across the InfiniteBench tasks(Zhang et\u00a0al.,2024a). In this benchmark, the average prompt length is close to 200K tokens making it extremely challenging, especially for LMs that were not expensively finetuned for very long context understanding. In fact, as reported byZhang et\u00a0al. (2024a), even GPT4(Achiam et\u00a0al.,2023)cannot exceed a performance of 1% on some of its problems. In line with these results, the full-context Llama 3 together with H2O and L2 obtain near-zero performance on most tasks. Instead, our NAMM provides outstanding improvements, bringing overall benchmark performance from 1.05% to 11%.\nWe also observe that while our NAMM\u2019s memory size is larger than for LongBench, it is considerably lower in relation to the base model\u2019s (now only 40%). This result suggests that NAMMs emergently learned a scalable memory strategy, forgetting redundant and detrimental information at an increasing rate with longer contexts without requiring the hand-designed hard cache limits enforced by L2 and H2O.\n\nChouBun.Our new benchmark focuses on tasks designed exclusively in Japanese, a novel language unseen during NAMMs training. We hope this benchmark might itself be a valuable contribution to the research community, allowing the assessment of long-context capabilities in multilingual LLMs beyond the already-ubiquitous English and Chinese. We provide further benchmark statistics, details about task composition, together with evaluation metrics for a wider range of popular LLMs in Appendix10. In Table4, we report our results evaluating NAMMs. Once again, we observe a clear contrast with prior hand-designed methods. While integrating either H2O or L2 leads to notable performance drops, NAMMs provides substantial improvements, with overall performance up by 15% from the full-context Llama 3 8B base model.\n\nSECTION: 4.2Zero-shot transfer across architectures and modalities\n\nCross-scale adaptation.In Table5, we provide results zero-shot transferring our NAMM from the Llama 3 8B to the Llama 3 70B model on LongBench. Across all tasks, we find performance to be very close to the full-context baseline with an overall gap of less than 1% even for the test subset. While NAMMs are not able to improve the overall full-context performance in this first transfer setting outside specific task categories (e.g., coding and few-shot learning), they still outperform both H2O and L2 baselines and retain a similar efficiency as with their original training transformer.\n\nVision Language Understanding.In Table6, we provide results zero-shot transferring to the computer vision domain, evaluating NAMMs with a Llava Next Video 7B model(Zhang et\u00a0al.,2024b)on LongVideoBench(Wu et\u00a0al.,2024)and Multi-Task Long Video Understanding (MLVU)(Zhou et\u00a0al.,2024). As when evaluated atop Llama 8B, our NAMM is the only method recording gains over the full-context base transformer in both benchmarks. Furthermore, we find that NAMMs learns to forget almost exclusively parts of redundant video frames rather than the language tokens describing the final prompt, even though they were never faced with such modality during training. This result validates that our NAMM recovered a domain-agnostic memory management strategy, further highlighting their flexibility.\n\nReinforcement learning.In Table7, we provide our zero-shot transfer results for the offline reinforcement learning setting, where we apply NAMMs atop a decision transformer(Chen et\u00a0al.,2021b)using the open-sourced models fromBeeching & Simonini (2022)pre-trained on the canonical the continuous-control tasks from D4RL(Fu et\u00a0al.,2020). We find our NAMM improves the base transformer quite considerably in this domain across eight out of nine offline tasks with over 9% overall gains, opposing the performance loss of the other efficient baselines. We posit that since the nature of the decision transformer optimization is closely tied to behavior cloning, the ability to discard part of the context is likely to allow NAMMs toforgetand avoid imitating previous mistakes autoregressively. In support of this hypothesis, we observed slightly higher average rewards in the transitions for the retained tokens (by 1.4%, 0.8%, and 12.3% for the Hopper, Walker2d, and HalfCheetah environments, respectively).\n\nNAMMs comparison.In Table8, we provide summarized results comparing NAMMs with either BAM or the simpler MLP architecture at the end of each stage of incremental evolution. First, we note that even the MLP NAMM after stage 1 impressively improves performance across all language benchmarks. Additionally, performance sees near-monotonic improvements with each additional stage of incremental evolution in both language and zero-shot transfer settings. Comparing our implementations, the performance benefits from the memory models with BAM appear consistently superior to the MLP. Moreover, on ChouBun. we observe that the performance with BAM sees a notable upswing after the second stage of incremental training, which might be associated with the introduction of another ideogram-based language in the training set.222The DuReader task, used in the second stage of incremental training, uses the Chinese language.The same improvement not occurring with the MLP-based NAMMs might be further evidence of architectural performance saturation, highlighting the effectiveness of our main implementation.\n\nSECTION: 4.3Understanding Neural Attention Memory Models\n\nInfluence of layer depth.We begin analyzing NAMMs by focusing on the final amount of retained tokens and their oldness333We defineoldnessof a retained token as the number of new queries since its introduction in the KV cache..\nAt the top of Figure6, we provide these normalized metrics as a function of layer depth.\nInterestingly, our learned NAMM does not appear to affect the KV cache uniformly, retaining visibly more and older tokens for some of the early-middle layers of the base transformer.\nOne possible interpretation of our results, complementing recent analysis(Wendler et\u00a0al.,2024), is that these layers might be particularly important for processing and aggregating information over longer contexts, thus requiring larger memories than the rest.\n\nInfluence of task structure.At the bottom of Figure6, we instead provide these metrics while varying the source task, this time normalized by the average prompt lengths shown in green. Our results illustrate an inverse correlation between normalized memory size and prompt length (with a Pearson coefficient of -0.84), further confirming our earlier observations of sub-linear memory growth and favorable scaling to longer contexts. Additionally, we observe that in the code completion tasks (with task id 6-1 and 6-2) NAMMs learn to preserve visibly more tokens relative to their average prompt lengths. This result appears intuitively consistent with the higher information density in code, leaving room for less redundancy as opposed to natural language.\n\nSelected qualitative examples.We qualitatively find these analyzed trends by inspecting the text corresponding to the forgotten tokens for a few selected prompts. In particular, we consider the layers with the highest and lowest average retained tokens (15 and 24), for tokens from either a natural language or coding task (PassageRetrieval-en, id 5-1, and RepoBench-P, id 6-2). As shown in Figure7, for early-middle layers, NAMMs tend to focus on retaining global information such as the task preamble and key words throughout the text. Instead, for later layers, NAMMs seem to forget many of these tokens, whose information has likely been already incorporated in the previous layers, allowing the transformer to focus more on tokens with more detailed local information. Furthermore, in coding tasks, we find that the pruned tokens are mostly contiguous, corresponding to whitespace, comments, and whole segments of boilerplate code. This is in contrast to natural language tasks, where NAMMs appear trying to exploit some of the grammatical redundancies of the English syntax often dropping specific tokens mid-sentences.\n\nAdditional analysis.We provide additional analytic results in AppendixD. For instance,we analyze how the presence of each token in memory affects the scores of the other tokens, we compare the generated responses before and after the introduction of NAMMs in a very long context task, and show the sensitivities of the token scores for each input feature.These results show that NAMMs learn mechanisms for \u2018cross-token\u2019 competition relying on high-frequency components of the attention matrices and illustrate how they learn to overcome different failure modes of long context LMs,further evidencing the need to go beyond simple strategies and the potential of end-to-end learning for token-level memory systems.\n\nSECTION: 5Related works\n\nDevoto et\u00a0al. (2024)andYao et\u00a0al. (2024)try to identify the least important tokens to evict using heuristics such as L2 magnitude and entropy to improve efficiency. Alternative strategies include considering simple statistics from the attention matrix(Liu et\u00a0al.,2024b; Oren et\u00a0al.,2024; Zhang et\u00a0al.,2024c).Ge et\u00a0al. (2024)andLi et\u00a0al. (2024b)build on these ideas by applying multiple strategies based on matching specific attention patterns. Motivated by similar considerations,Nawrot et\u00a0al. (2024)proposed directly fine-tuning the original base transformer to compress the KV cache while minimizing a regularization loss to preserve the original base model\u2019s behavior and limit performance degradation.Complementary to our method, MQA(Shazeer,2019)and GQA(Ainslie et\u00a0al.,2023)propose merging attention heads during training to improve inference throughput. Similarly, also KV cache quantization is another orthogonal area where different hand-designed strategies have been proposed(Hooper et\u00a0al.,2024; Dong et\u00a0al.,2024a;b), with even recent work empirically showing their direct compatibility with token eviction methods(Liu et\u00a0al.,2024a).\nWe note that, unlike this prior work, our approach uniquely learns a black-box model tomaximize performancethrough token-level memory management and shows potential for providing improvements to both the effectiveness and efficiency of transformers.We refer to App.Efor references and to the wider literature, including efficient architectures, memory, and evolution.\n\nSECTION: 6Discussion and future work\n\nThis work introduced Neural Attention Memory Models, providing a new framework to enhance the performance of transformers while significantly reducing memory footprint.\nBy evolving NAMMs on top of pre-trained LMs, we demonstrated their effectiveness across diverse long-context tasks in three languages, significantly surpassing previous hand-designed KV cache eviction frequently hindering performance, and the original model relying on costly full-context conditioning.\nOur carefully designed approach also enabled NAMMs, trained solely on language tasks, to achieve zero-shot transferability across architectures, input modalities, and task domains.While NAMMs do appear to provide benefits beyond what achieved with hand-designed strategies, we believe there is much room for improvement (e.g., see LimitationsF).This work has only begun to explore the design space of our memory models, which we anticipate might offer many new opportunities to advance future generations of transformers.\nIn this regard, we believe NAMMs should not be viewed as a replacement for gradient-based optimization, but rather an orthogonal framework that could be combined and alternated with parameter fine-tuning. Such an extension has the potential to unlock efficient long-context training, drawing parallels to the iterative process of learning and evolution that shaped human memory.\n\nSECTION: 7Author contributions\n\nEdoardo Cetin initiated the project, led the design and implementation of NAMMs, and provided major contributions to writing. Qi Sun designed and implemented the zero-shot transfer experiments with Llama 3 70B and Llava Next Video 7B, and provided contributions and feedback to writing. Tianyu Zhao devised and implemented ChouBun, and provided contributions and feedback to writing.\nYujin Tang coordinated the project, gave key advice for the design of NAMMs, and provided major contributions to writing.\n\nSECTION: Acknowledgements\n\nThe authors would like to thank David Ha and Llion Jones for providing valuable discussions during the early stages and feedback while drafting the text. This paper is based on results obtained from a project, JPNP20017, subsidized by the New Energy and Industrial Technology Development Organization (NEDO).\n\nSECTION: References\n\nSECTION: Appendix AImplementation details\n\nSECTION: A.1Model specifics and NAMMs execution\n\nWe evolve our Neural Attention Memory Models on top of a context-extended Llama 3 8B(Dubey et\u00a0al.,2024)base model. In particular, we employ the NTK-aware positional interpolation strategy(bloc97,2023)to extend the context by four times from 8192 to 32768. Unlike prior strategies that require further gradient fine-tuning to avoid performance collapse(Chen et\u00a0al.,2023), NTK-aware positional interpolation has been shown to produce sensible results even when applied zero-shot. In case the length of a task prompt still exceeds 32768 we perform mid-sentence cropping(Xiao et\u00a0al.,2023; Jin et\u00a0al.,2024), as standard in long-context LM evaluation(Bai et\u00a0al.,2023; Zhang et\u00a0al.,2024a).\n\nWhen applying NAMMs, we only affect the execution of the base model with a fixed frequency, once everysteps. When feeding longer prompts to our model, we simply split the tokens into-sized chunks. We note that due to modern frameworks being bound primarily by memory constraints, input-splitting in itself has minimal effects on running time, with similar approaches being already performed under the hood by established kernel procedures(Dao et\u00a0al.,2022).\n\nSECTION: A.2Feature extraction and architecture details\n\nOur new feature extraction framework is a key component for enabling the transfer properties of NAMMs. In practice, we extract the attention spectrogram from the real-valued attention matrix using a Hann window of size, resulting in just seventeen complex-values frequencies that we convert to real numbers by simply taking their magnitude, yielding each. We use a stride of half the window size, producingfrequency representations over the time axis of the attention matrix from the latest chunk ofqueries,. Thus, we reduce these frequency representations over the time axis via an element-wise exponentially moving average operation. We note that our EMA does not only consider therepresentations computed for the frequency of each token in the-sized chunk of the latest queries, but also the discounted EMA at theprevious execution stepor our memory for each retained token, denoted. Thus, each of our reduced spectrogram representations reflects the full history of previous attention values:\n\nwhere we useto denote the EMA\u2019s discount factor. To expedite learning the weights of our architecture, we ensure all spectrogram features have unit variance at initialization across our training data, using the statistics of the base Llama 3 model computed on the first task employed in incremental learning (PassageRetrieval). Finally, we also concatenate a small eight-dimensional sinusoidal positional embedding using theoldnessof each token, i.e., the amounts of new queries observed since its introduction in the KV cache.We provide an extended summarized pseudocode description of the execution pipeline in Algortihm2(to complement Algorithm1in the main text).\n\nOur backward-attention memory network processes these representations by directly first applying the self-attention layer employing the counter-autoregressive backward masking introduced in Section3, designed to facilitate asymmetric interactions between tokens in memory. The output of self-attention is then fed to a single final linear layer to obtain the final score. We employed a few important additional design choices following some preliminary testing. First, motivated by efficiency considerations, we use a single head within our attention mechanism and no layer normalization. Second, our attention layer produces outputs that are twice the dimensionality of the spectrogram features. These outputs are integrated back into the main network before the final linear layer via both residual and multiplicative interactions. We provide a schematic depiction of our minimal architecture in Figure8. Through our minimalist design choices, our full network comprises only just over four thousand learnable parameters, a negligible amount, orders of magnitudes lower than even a single layer in modern transformers.\n\nSECTION: A.3Zero-shot transfer\n\nFor our zero-shot transfer experiments, we consider a Llama 3 transformer with 70B parameters(Dubey et\u00a0al.,2024), a Llava Next Video transformer with 7B parameters(Zhang et\u00a0al.,2024b), and a decision transformer(Chen et\u00a0al.,2021b)with about 1M parameters. For our 70B experiments, we follow the exact same setup as when evaluating our 7B Llama model used in training. For our video-language model, we extractimage tokens from 48 uniformly sampled frames, 6912 in total. We also slightly shift the selection score threshold by 5, to counteract the lower number of total tokens and get a comparable average cache size to the L2 and H2O baselines. We adapt the code and follow the standardized experimental setup fromLi et\u00a0al. (2024a). For the reinforcement learning experiments, we encode each state, action, and return-to-go into separate tokens and do not apply any restrictions or modifications to our standard NAMM LM setup. We average the performance collected over 20 random seeds to account for the stochasticity of the initial state in the Gym Mujoco environments(Brockman et\u00a0al.,2016).Rather than re-training a decision transformer from scratch, our RL experiments adapt the open-sourced checkpoints and implementation provided byBeeching & Simonini (2022). We would like that note that on some task-dataset combinations of D4RL, these checkpoints appear to yield lower performance than what was reported in the original decision transformer paper (e.g., Walker pre-trained on medium-expert data)Chen et\u00a0al. (2021b). However, we do not believe these differences should affect our conclusions as we used the same base model for all our memory management baselines.\n\nSECTION: A.4Evolutionary optimization\n\nAs described in Section3, we optimize NAMMs with the Covariance Matrix Adaptation Evolution Strategy (CMA-ES)(Hansen,2006). Being an evolutionary algorithm, CMA-ES does not require any gradient information and can directly optimize black-box undifferentiable metrics. This property allows us to both optimize for the non-differentiable token selection task of our NAMMs and also maximize non-differentiable task performance metrics directly. In the case of the LongBench(Bai et\u00a0al.,2023)tasks considered for training, these metrics correspond to exact match accuracy (PassageRetrieval-en), ROUGE-L score (DuReader), and F1 score (NarrativeQA).\n\nOn a high level, given a neural network withparameters, CMA-ES maintains a mean vectorand a covariance matrix. Then, it repeats the following steps:\n\nSampling.CMA-ES generates a population of neural networks, sampling their parameters from the multivariate normaldistribution.\n\nEvaluation.Each population candidate is evaluated to the objective function for the objective function used.\n\nUpdating.By both selecting a subset of the population candidates and also weighting them based on their overall ranking the mean and covariance are updated towards higher-performing regions of the search space.\n\nWe provide the main hyper-parameters in Table9and refer to either the work byHansen (2006)or our shared code for the full implementation details.\n\nSECTION: A.5FastGen implementation and tuning\n\nWe re-implemented the recent FastGen method proposed byGe et\u00a0al. (2024), which proposes to adopt a hand-designed combination of different strategies targeted to retain tokens with high attention values, belonging to recent words, or encoded from particular grammatical features (i.e., punctuation, \u2018special tokens\u2019). In particular, after observing the input prompt, FastGen performs a \u2018profiling step\u2019 where the strategy able to evict the most amount of tokens is selected such that:\n\nHere,is the full-cache attention matrix,is the \u2018reconstructed\u2019 attention matrix re-calculated after performing a softmax between each layer\u2019s queries and keys with masked-out entries for the keys evicted by the individual strategies. Furthermore, T is the main threshold hyper-parameter, determining how aggressively FastGen is allowed to prune tokens even if resulting in degradation to the attention-reconstruction heuristic.\n\nWe note that, unlike our other baselines, FastGen is only directly compatible with language modeling tasks. This is because one of the main ways it differs from H2O is by preserving particular grammar-based tokens in some of its strategies (e.g., punctuation, special words, etc.). Thus, as this baseline was specifically designed for LMs rather than arbitrary transformers, we did not consider applying it in the 0-shot transfer settings, and only focused on Llama 3 8B.\n\nWenote that as we are dealing with much longer prompts (sometimes far beyond tens/hundreds of thousand tokens), for efficiency consideration, we performed the profiling steps in our re-implementation after the first 4096 tokens any prompt exceeds this length. We also found to avoid losing too much performance over the base model on longer context tasks we had to retune its main \u2018threshold.\u2019 We selected T=0.999, as this choice allowed FastGen to retain over 95% normalized performance while still discarding a non-trivial portion of tokens on all LongBench, as shown in Figure9.\nOther than the main threshold for attention reconstruction, FastGen has two other main hyperparameters: the \u2018recency ratio,\u2019 the \u2018attention ratio\u2019 determining the portion of most recent tokens or with the highest attention values to retain in its individual strategies. We set these hyper-parameters to 0.3, following the paper\u2019s recommendation.\n\nSECTION: Appendix BBenchmark descriptions\n\nSECTION: B.1ChouBun details\n\nThe ChouBun benchmark is created to assess the generalization ability of NAMMs to a new language (Japanese), but we hope it will also serve as a standard benchmark for Japanese LLMs. The benchmark is composed of two task categories \u2014 extractive QA and abstractive summarization \u2014 and four tasks as follows.\n\nJA.WikiQAis an extractive QA task about 20 randomly sampled articles from the 20240429 dump of Japanese Wikipedia444https://dumps.wikimedia.org/other/cirrussearch/. Each article corresponds to 10 QA pairs, and there are 200 QA pairs in total.\n\nJA.EdinetQAis an extractive QA task based on 20 security reports from EDINET555https://disclosure2.edinet-fsa.go.jp/. The EDINET security reports are in CSV format, which makes them less human-readable. Nevertheless, we choose not to convert the format because the conversion process per se is non-trivial, and using a CSV-style text input helps us evaluate a model\u2019s capability of understanding structured data. The total number of QA pairs inJA.EdinetQAis 390.\n\nJA.CorpSecQAis another extractive QA task based on 30 security reports downloaded from three corporation websites (MUFG666https://www.mufg.jp/ir/report/security_report/, NTT777https://group.ntt/jp/ir/library/results/, and Toyota888https://global.toyota/jp/ir/library/securities-report/). We extract texts from original file in PDF format. There are 150 QA pairs in total.\n\nJA.CorpSecSumis an abstractive summarization task based on the same data ofJA.CorpSecQA. Each document corresponds to one data point, and we collect 5 reference summaries for each data point.\n\nCollecting human annotations for long-text tasks is challenging, therefore we use synthetic QA pairs and summaries. In particular, we prompt various LLMs999gpt-4o-2024-05-13,gpt-4o-mini-2024-07-18,gpt-4-turbo-2024-04-09, andclaude-3-5-sonnet-20240620to generate multiple question-answer pairs or summaries for each document. Different instructions are designed for the two tasks and they are shown in Figure10. To improve the reliability of the synthetic data, we ensure that every answer in extractive QA tasks is a text span presented in its corresponding source document. In Table10, we provide the statistics of the benchmark.\n\nWe use F1 score and ROUGE score for evaluation in the extractive QA tasks and summarization task, respectively. Reference text and hypothesis text are pre-tokenized by the MeCab tokenizer101010https://github.com/polm/fugashi. A wider range of LLMs\u2019 performance on the ChouBun benchmark is presented in Table11.\n\nSECTION: B.2Benchmarks summary\n\nWe provide a summary of the types of tasks and domains of the other benchmarks we considered for our experiments. We refer interested readers to the relative referenced papers for full details.\n\nLongBench(Bai et\u00a0al.,2023).This benchmark comprises 21 different tasks targeted to evaluate the long-context capabilities of LMs. These tasks include both English and Chinese and come from either modified/subsampled versions of existing datasets or synthetic generation. The authors divided them in 6 categories, numbered with the prefixes 1 to 6: single-document QA, multi-document QA, summarization, few-shot learning, synthetic, and code. The tasks have a reported average length of 6711 English words and 13386 Chinese characters.\n\nInfiniteBench(Zhang et\u00a0al.,2024a).This benchmark comprises 12 different tasks designed to go beyond the existing benchmarks and push the limits in long-context LMs. In fact, while popular prior long context benchmarks, including LongBench, focus on prompts of around 10K tokens InfiniteBench considers tasks with contexts beyond 100K tokens. These tasks again include both English and Chinese and come from either modified/subsampled versions of existing datasets or synthetic generation. The authors divided them into 5 categories: retrieval, dialogue, novel, math, and code. We note some of these tasks are considered extremely difficult, with even powerful proprietary LMs such as GPT4 not able to get above a performance of 1%.\n\nLongVideoBench(Wu et\u00a0al.,2024).This benchmark comprises 3763 curated long videos with subtitles. These videos are coupled with 6678 human-annotated questions focusing on 17 different categories. The benchmark is focused on what the authors refer to as frame-specific \u2018reasoning\u2019 style questions. In particular, for these kinds of questions, video language models are tasked to respond to \u2018referred queries\u2019 targeting particular parts of the whole video context.\n\nMulti-task Long Video Understanding Benchmark(Zhou et\u00a0al.,2024).This benchmark focuses on evaluating long-video understanding performance. It includes videos averaging 12 minutes in length up to 2 hours. The videos span different genres such as movies, documentaries, surveillance videos, ego-centric videos, games, and cartoons. In total, this benchmark comprises 2593 evaluation problems divided into 9 categories: topic reasoning, anomaly recognition, video summarization, needle question\nanswering, ego reasoning, plot question answering, sub-scene captioning, action\ncount, and action order. These problems are quite diverse including both multi-choice and generation-style questions for video language models.\n\nD4RL(Fu et\u00a0al.,2020).This benchmark focuses on evaluating offline reinforcement learning agents(Lange et\u00a0al.,2012). In particular, it provides pre-training datasets for different reinforcement learning tasks simulated through Mujoco based on OpenAI gym(Brockman et\u00a0al.,2016). The datasets are named based on the displayed agent skills (e.g., expert medium), and based on their inclusion of \u2018replay data\u2019 from the demonstrator agent\u2019s own prior learning experiences. Evaluation is then performed after pre-training by running the learned agents online in the respective environments. We focus on the most popular subset of this benchmark, involving continuous-control tasks with three different agents: Hopper, HalfCheetah, and Walker-2d, evaluating the agent after pre-training on Expert, Medium, and Medium Replay data. Rather than re-training from scratch, we use the open-sourced checkpoints fromChen et\u00a0al. (2021b)and focus on the evaluation aspect of the benchmark.\n\nSECTION: Appendix CAdditional results\n\nSECTION: C.1Performance across incremental stages and architectures\n\nWe provide additional results and analysis to the summarized one, complementing Section4, with the detailed performance across different NAMMs, evaluating the best checkpoints after each stage of incremental training stage, and ablating the BAM architecture with an MLP.\n\nExtended language modeling results.We report our results for LongBench, InfiniteBench, and ChouBun in Tables12,13,14. First, we note that even training on a single task with our simple MLP architecture impressively improves performance across all benchmarks. Additionally, performance across benchmarks sees near-monotonic further improvements with each stage of our incremental evolution recipe. Comparing our implementations, we note that the performance benefits from the memory models with backward attention are consistently superior to the fully connected variant in both initial stages of incremental training, empirically validating our hypothesis about the importance of global KV cache information for determining the importance of each token. Lastly, on ChouBun. we observe that the performance with BAM sees a notable upswing after the second stage of incremental training, which might be associated with the introduction of another ideogram-based language in the training set.111111The DuReader task, used in the second stage of incremental training, uses the Chinese language.The same improvement not occurring with the MLP-based NAMMs might be further evidence of architectural performance saturation, highlighting once again the effectiveness of our main implementation design.\n\nExtended zero-shot transfer results.We report our extended zero-shot transfer results for the 70B model and the offline RL setting in Tables15,16, and17. We see the benefits from NAMMs again increase as we incorporate backward attention, and with each stage of incremental training to a similar extent as with the language modeling tasks. These results further highlight the potential benefits of scaling up the architecture of our memory model and increasing the number of incremental stages. To this end, given the generality of our parameterization, an interesting unexplored approach could be to incorporate different base models and input modalities during evolutionary training, something that would substantially increase problem diversity to obtain an even more robust transfer behavior.\n\nSECTION: C.2Training curves with fully-connected NAMMs\n\nIn Figure11, we provide training curves of our Neural Attention Memory Model using a simple MLP architecture rather than backward attention, evaluated in Section4. In the left sub-plot, we show the average and standard deviation of the normalized batch performance across the population, while in the right sub-plot, we show the normalized per-task and average performance on all samples of the optimized mean from CMA-ES. When compared with the BAM training curve from Figure4, we note a few interesting differences, although its evaluation performance on the full LongBench benchmark is lower across both incremental phases (see Table2), both its population batch performance and the CMA-ES full-task performance on the training sets are either comparable or slightly higher than BAM\u2019s. This dichotomy appears to indicate that cross-token interactions might provide a better inductive bias, mitigating the overfitting potential of NAMMs.\n\nSECTION: C.3Evolution of memory size during training\n\nIn Figure12, we provide training curves for the evolution of the memory size collected at the end of each task prompt of our NAMMs. On the left and right subplots, we provide results for the BAM and MLP implementations, respectively. For both architectures, we find that the memory size generally increases with training. This result suggests that NAMMs might learn to recognize additional valuable tokens as training progresses, enabling the corresponding performance improvements on the training tasks. Hence, they might indicate that there is some degree of a trade-off between the efficiency and performance of NAMMs. However, we note that both models are trained only for performance maximization, without any incentive to be more conservative. To this end, exploring regularization strategies to make NAMMs aware of deployment costs is an interesting direction for future work to obtain tailored sweet spots to cater to instance-specific resource constraints.\n\nSECTION: C.4Incremental training ablation\n\nWe provide a full set of ablations results for our incremental training strategy, training a Neural Attention Memory Model with the BAM architecture from scratch on both the PassageRetrieval-en and DuReader tasks, as employed during the second stage of incremental learning. We evolve this Neural Attention Memory Model for 360 consecutive generations and provide training curves in Figure13. In the left sub-plot, we show the average and standard deviation of the normalized batch performance across the population, in the center sub-plot, we show the normalized per-task and average performance on all samples of the optimized mean from CMA-ES, and on the right subplot we show the corresponding memory size. Furthermore, in Table18, we provide the full LongBench evaluation results for this baseline, also showing our original incremental model\u2019s performance for ease of comparison. Interestingly, the non-incremental NAMM obtained a notably higher score on the training tasks with a normalized performance of 1.57, in contrast to the normalized performance of 1.41 achieved by the best checkpoint from the second incremental training stage. Yet, outside the PassageRetrieval-en and DuReader tasks, its performance is notably inferior and very close to the original performance of the base model. These results appear to indicate that the usefulness of incremental training goes beyond the faster evolution provided by reducing the number of evaluation prompts to assess performance and that this strategy plays an important role in regularizing evolution and making Neural Attention Memory Models effectively generalize to new tasks.\n\nSECTION: C.5Running times and memory savings\n\nWe provide details about the efficiency and costs of NAMMs on top of the Llama 3 8B base model used for training. For our main experimental setup, we used rented cloud instances with Nvidia H100 GPUs, Intel Xeon Platinum 8481C CPUs, and 1932GB of RAM. We performed model inference for each prompt on a single GPU, with batch size 1. During training, we used a single node with 8 GPUs, distributing the evaluation of our population across 8 processes. However, we like to remark that since training NAMMs does not require any gradient computation, we were not restricted by any kind of hardware during training. In this regard, using inference-specialized resources beyond GPUs might provide considerable speedups and lower costs to ones employed in this work.\n\nTraining.We collected the training time for each generation of NAMMs. As detailed in Section3and AppendixA, with the employed hyper-parameters, each generation consisted of running the base model forprompts for each task. Thus, each incremental phase got linearly more expensive, with up toNAMM evaluation in the final phase. These prompts were distributed across our 8 processes balancing the number of tokens evaluated in each. We also note that the average prompt length and the nature of each task (e.g., exact match, summarization, etc.) varied quite significantly in LongBench, making their evaluation costs non-uniform.\n\nInference.We collected running times of NAMMs and our baselines in different settings. In particular, these include both: 1. Using samples from the full LongBench benchmark with an average length of 12099 2. Using only samples from LongBench selected to exceed the base transformer maximum length with an average length of 32641. Finally, we also record the running time of an ablated version of our NAMM run on top of the base transformer that does not modify its KV cache, in order to disentangle the gains from the reduced memory and analyze the pure overheads from our model\u2019s execution.\n\nAs shown in Table19, the running time overhead of our NAMM ablation that does not evict tokens is small when compared to the base model. Instead, the running time of NAMMs and the baselines while evicting tokens is always inferior to the base model in all settings, and scales positively with longer prompts.\n\nMemory.Furthermore, in Table21, we also reported estimated effects in peak GPU memory consumption, which were calculated from the peak KV cache sizes, together with the sizes of additional information (e.g., attention matrix) and models used by each method (again recorded on LongBench). We would like to note, however, that as the main objective of our work was to provide performance benefits we did not particularly optimize our code for memory efficiency or speed. Thus, actual empirical savings with our shared implementation might differ from these calculated estimates. For instance, both our NAMMs and H2O baseline do not employ specialized kernels to replace FlashAttention(Dao et\u00a0al.,2022).\n\nSECTION: C.6Mistral base model and finetuning NAMMs\n\nWe also analyzed an additional 0-shot transfer setting, this time applying NAMMs on top of the Mistral 7B base model(Jiang et\u00a0al.,2023). We considered 2 different setups: 1. Zero-shot application, taking our best NAMM model trained with the Llama 8B context-extended model. 2. Post cross-model fine-tuning, running a small amount of additional evolutionary optimization comprising 20 generations using CMA-ES and the same 3 training tasks used for Llama. We provide our full results and analysis in Table22.\n\nAnalogously to our other zero-shot transfer results provided in Section4, we find that NAMMs yield considerable benefits also when transferred to the Mistral model, overcoming the efficiency-performance tradeoff of hand-designed baselines. Furthermore, this analysis also shows that performance could be further improved by a few finetuning generations after transferring to a different base models. While we did not investigate finetuning with our other transformers (e.g., Llama 70B), we believe these results highlight the potential of cheaply improving NAMMs \u2019 already-remarkable zero-shot benefits, which we hope will be further explored in future work.\n\nSECTION: C.7Attention spectrogram features ablation study\n\nWe examined ablating the attention spectrogram features produced by the STFT procedure and re-training our NAMMs with two different alternatives:\n\nThenaiveapproach of using the raw attention values directly (cropped to a fixed length) as input to NAMMs.\n\nSubstituting the STFT features by constructing a \u2018handcrafted\u2019 feature representation that simply includes three values: i. The sum of the attention values of each token. ii) The recency of each token. iii. The diversity of each token (computed by concatenating the keys and values to represent each token and averaging the L2 distance to all other tokens). We refer to this baseline asRAD.\n\nWe trained these baselines only for two incremental phases on the PassageRetrieval-en and Dureader tasks (thus, we also compared them with our original NAMM model after phase 2). Please refer to Table23for our results. Overall, we find our baselines yield quite different behaviors, both underperforming our original NAMM design.\n\nFirst, we find our naive baseline, taking as input the cropped attention value, is not able to improve over the full cache model when evaluated on the whole of LongBench. However, we note that its performance on the training task is significantly beyond the base model. Thus, we find this is strongly suggestive of the occurrence of overfitting, which we believe is to be expected as our memory model now only conditions on very high-frequency information that only considers the latest attention values.\n\nSecond, we find that our \u2018handcrafted\u2019 Recency-Attention-Diversity baseline is instead able to improve over the original model, but its improvements are only marginal. We find these results consistent with section D.2 of the extended analysis, which suggests that the behavior of NAMMs is considerably influenced by a combination of different frequencies in the attention spectrogram which are lost by this approach.\n\nSECTION: Appendix DAdditional analysis\n\nSECTION: D.1Backward attention cross-token interactions\n\nWe analyze the cross-token interactions learned through our BAM architecture by recording the gradients of each token scorewith respect to all input featuresforall tokens in memoryafter storing 1024 tokens, i.e., for. We denote these quantities as:\n\nWe provide a qualitative visualization of our results on the PassageRetrieval-en task for a randomly selected layer and prompt in Figure14. On the left subplot, we provide a visualization of the squared magnitudesfor each combination of tokens (either scored or attended upon in BAM, i.e., indexed byor). Here, the effects of the backward mask are clearly visible, allowing tokens to exclusively attend to later ones, where. Predictably, these magnitudes mostly peak on the subplot\u2019s diagonal, indicating the self-influence that each token\u2019s features have on its corresponding output score. However, there are also notable exceptions, as shown in the center subplot, where we overlap three slices from our left surface plot corresponding to the gradients of the first, together with the highest and lowest-scored tokens in memory (respectively indexed by0, 292, and 800). We provide additional directional information of each gradient vector from these slices in the right subplot, where we take its dot product with the scored token\u2019s own feature vector. After the first notable spike, at, most other dot-product spikes with the largest magnitudes consistently have negative values. Hence we can logically deduce that the scores of these tokens would benefit from pushing the representations of future tokens away from their own. This result appears to validate the hypothesis that BAM learns a mechanism for cross-token competition, incentivizing diversity and promoting tokens covering unique frequencies in the attention spectrogram.\n\nSECTION: D.2Sensitivity to attention frequencies and positional encodings\n\nWe analyze the magnitudes of the gradients of the token scoreswith respect to each dimension in the token feature vectors. This procedure quantifies how varying each dimension in our attention spectrogram representation locally affects the output score of NAMMs, thus, providing a heuristic measure of its relevance (since scores determine which tokens get discarded). In Figure15, we plot the distribution of magnitudes for all the seventeen features up to the Nyquist frequency (to) in the attention spectrogram. All frequency distributions seem to cover a wide range of values, with each mean being close to the global mean, seemingly indicating NAMMs learn to make use of all available spectrogram information for at least some of the tokens. Additionally, we note that many of the higher frequencies have distributions with higher means and larger tails than the \u2018ground frequency\u2019 at dimension 0. Furthermore, as shown in the rightmost-lower subplot, NAMMs appear visibly less sensitive to recency information provided by the concatenated positional embeddings, with a lower total influence than frequency information on token scores. Overall, these observations seem to further validate the importance of going beyond simple hand-designed methods solely based on token recency and the sum of the attention values, which has so far been considered a strong established recipe for KV cache management(Oren et\u00a0al.,2024; Zhang et\u00a0al.,2024c; Ge et\u00a0al.,2024; Devoto et\u00a0al.,2024).\n\nSECTION: D.3InfiniteBench results comparison\n\nOn the InfiniteBench tasks, our NAMM achieve particularly outstanding improvements over the base model and other baselines, with an over ten-fold score increase (from 1.05% to 11%). However, we note that even with NAMMs, the performance of Llama 3 8B still lags considerably behind the performance of powerful LMs designed specifically for long-context problems, as reported inZhang et\u00a0al. (2024a). Nonetheless, on the En.Sum task, concerned with the summarization of fictitious novels, we find our main NAMM brings the performance of the context-extended Llama 3 from 7.73 to 14.91 even slightly beyond GPT4\u2019s (14.73). While this performance is still low in absolute terms121212InfiniteBench tasks are scored in a range between 0 and 100., such a result appears quite notable and suggests that improvements from NAMMs are orthogonal in nature to the ones brought by architectural improvements and scaling, which, by themselves, might be insufficient to address the challenges brought by long and noisy contexts.\n\nWe qualitatively inspect the effects of NAMMs on En.Sum by comparing example answers generated by Llama 3 with and without our memory models, together with examples generated by GPT4. As illustrated in Figure16, we find both the Llama and GPT models to incur several failure modes, producing answers that entirely miss the objective of the original task. For instance, the context-extended Llama 3 often gets stuck in generation loops continuously repeating part of sentences without coherent structure. Instead, the GPT answers appear to forego summarizing the text and rather attempt to continue the provided passage, by generating end-of-text tokens or even roleplaying some of the characters. However, while introducing NAMMs appears to avoid many instances of these failure modes, we find the summarization of the memory-augmented Llama 3 still displays many imperfections such as misspelling character names (left) or lacking much depth by being extremely concise (right).\n\nSECTION: Appendix EExtended related works\n\nSimilar to our NAMMs implementation, memory management through token eviction has been explored mostly to reduce memory constraints and enable querying LMs with longer contexts(Luohe et\u00a0al.,2024).\nCommonly, strategies entail simply cropping input prompts to a shorter length, often more effective when done from the middle rather than the ends(Xiao et\u00a0al.,2023; Jin et\u00a0al.,2024).\nMore advanced, several heuristic strategies have been proposed to identify and evict the least important tokens in the KV cache, selectively pruning it to a fixed size for each layer.\nThese strategies assess token relevance using metrics like L2 magnitude(Devoto et\u00a0al.,2024)or entropy(Yao et\u00a0al.,2024), or analyze statistics from the attention matrix, such as value magnitude or cumulative sums(Liu et\u00a0al.,2024b; Oren et\u00a0al.,2024; Zhang et\u00a0al.,2024c).\nBuilding on these ideas,Ge et\u00a0al. (2024)andLi et\u00a0al. (2024b)apply multiple strategies simultaneously, choosing the best fit for each layer by matching them with specific attention patterns. Similar ideas where also explored in older work targeting encoder-decoder models, for instance,Huang et\u00a0al. (2022)proposed a more complex strategy for token selection based on solving thecore-setproblem with a parallelized greedy approach.\nHowever, unlike previous work, our approach uniquely employs a black-box model tolearnKV cache management in order to boost the base model\u2019s performance with improved efficiency coming as a free side benefit.\n\nMany other methods to reduce memory consumption, affecting the KV cache, are mostly orthogonal and likely complementary to our approach.\nFor instance, MQA(Shazeer,2019)and GQA(Ainslie et\u00a0al.,2023)propose merging different attention heads during the training of LLMs, either fully or partially, to improve deployment-time throughput.Brandon et\u00a0al. (2024), pushed these strategies further, attempting to merge heads even across different layers.\nGQA is commonly employed in many modern LMs, including the LLama 3 family of models which we use to train and evaluate NAMMs on language tasks(Dubey et\u00a0al.,2024).\nFurthermore, several methods have looked at KV cache compression through either quantization of the keys and values(Hooper et\u00a0al.,2024; Dong et\u00a0al.,2024a;b)or even the whole hidden states(DeepSeek-AI et\u00a0al.,2024).\nSimilarly to the aforementioned prior work concerning KV cache pruning, these methods considered mainly hand-designed strategies, such as employing different quantization rates based on heuristically recognizing important tokens.\nWe note that using evolution to optimize for which channels to merge or compress could also yield new interesting unexplored approaches, combining these orthogonal directions with some of the principles introduced by NAMMs.\n\nThere has also been much research interest in exploring new architectures to explicitly model components of a memory system or to address key challenges of reasoning over longer contexts.\nFor instance, past work has looked at incorporating neural models of memory within neural networks by implementing different reading and writing operations - either directly replacing their layers(Weston et\u00a0al.,2014; Sukhbaatar et\u00a0al.,2015), or introducing new auxiliary components(Rae et\u00a0al.,2016; Lample et\u00a0al.,2019).\nIn relation to transformers, more recent works have been proposed rethinking the ingredients of the self-attention operation, mostly in the context of LMs.\nThese works looked at either efficient linear approximation to self-attention to overcome quadratic costs(Beltagy et\u00a0al.,2020; Katharopoulos et\u00a0al.,2020; Wang et\u00a0al.,2020; Peng et\u00a0al.,2021), or introducing new kinds of persistent tokens and storage to extend information propagation(Dai et\u00a0al.,2019; Munkhdalai et\u00a0al.,2024; Hwang et\u00a0al.,2024).\nHowever, as also noted byDao et\u00a0al. (2022), none of these methods and approximations have managed to replace standard approaches so far.\nWe take a different approach that can be integrated in a zero-shot manner even without any fine-tuning.\n\nLastly, methodologically related to NAMMs, there have been other prior methods making use of evolution for or with transformer models.\nFor example,Tang & Ha (2021)also trained a small attention-based model through evolution, exploiting the inherent parameter efficiency behind these operations.\nFurthermore,So et\u00a0al. (2019)proposed using evolution to meta-optimize the basic building of transformers via neural architecture search, whileAkiba et\u00a0al. (2024)focused on evolving different merging strategies across layers belonging to LMs with different capabilities.\nAs for these works, we note that evolution plays a critical role for NAMMs, allowing us to directly optimize for target performance and overcome the inherent non-differentiability underlying our new framework.\n\nSECTION: Appendix FLimitations and future extensions\n\nSECTION: F.1Exploring the design space of Neural Attention Memory Models\n\nIn this work, we introduced Neural Attention Memory Models and showed their efficacy and potential to improve the performance and efficiency of transformers, even when evaluated zero-shot for unseen architectures and domains. However, given the novelty of our framework, we note that our design choices were mostly motivated by simplicity and practicality rather than quantitative empirical evidence. Thus, there is an extremely large design space in terms of the implementation, training, and deployment of these models that should be explored beyond this work, which is likely to yield further improvements.\n\nFor instance, while our current feature extraction, based on computing the spectrogram of the attention matrix, enables capturing global frequency information about the attention values of each token, it might fall short of modeling local information with enough granularity. This hypothesized limitation inherently comes from a few design choices we made with the purpose of limiting the input size and corresponding parameter count of our memory models. In particular, our spectrogram features only consider the real components of a short-time Fourier transform with a small Hann window of size thirty-two. Thus, we only provide NAMMs information about a relatively limited number of thirty-two frequencies, losing any notion of the phase of the attention matrix that would be captured by the full complex-valued Fourier coefficients. Consequently, the representations of tokens with high attention values for entirely non-overlapping queries occurring with the same frequency would be indistinguishable to our models. Moreover, our exponentially moving average reduction over the time dimension of the spectrograms provides an additional layer of heavy compression inevitably trading off expressivity for simplicity.\n\nTo partially address these concerns, an alternative design we explored entailed delaying the initial element-wise exponentially moving average reduction. Concretely, this involved computingdifferent scores, feedingall feature vectorsfor, across the attention spectrogram\u2019s compressed time axis, only then reducing the resulting scoresvia EMA. While, in principle, this alternative ordering would allow for additional expressivity without adding to the parameter count, in practice, when evaluated with an initial version of the simple 2-layer MLP model, we found no significant performance difference and opted for the former lighter option. However, introducing cross-token interactions with the improved BAM design and further scaling is likely to introduce a need of re-evaluating this choice.\n\nOne further limitation comes from the current reliance on the exact values of the attention matrix. This reliance precludes NAMMs training from making use of fast kernel algorithms developed to accelerate inference by foregoing materializing attention values(Dao et\u00a0al.,2022). While the main focus of this work has been to introduce NAMMs and display its potential to improve transformers across different domains, more scalable parameterizations and efficient backend integrations remain exciting open challenges for future research.\n\nSECTION: F.2Improving long-context sparse retrievals\n\nOne notable example exemplifying some of the aforementioned limitations, comes from the canonicalNeedle In A Haystacktask(Kamradt,2024), which has been used to qualitatively evaluate LLMs for their ability to remember sparse information over longnoisyhorizons. We provide results on this task using the best-performing NAMM after three stages of incremental training with the BAM architecture, averaging evaluation scores provided by a GPT-4 model(Achiam et\u00a0al.,2023)across different prompt ranges, consistently withBai et\u00a0al. (2024). As shown in Table24, while NAMMs do not manage to exceed the overall performance of the base model, they still provide some notable efficiency gains. However, looking more closely at the score distribution across different prompt length ranges we observe an unexpected trend that is in contrast with the rest of our results on other benchmarks. In particular, while our NAMM obtains slightly higher than the base model for prompts with a size less than 10000, it seems to increasingly struggle with longer prompts.\n\nAfter comparing the spectrogram features extracted for the different prompts, our explanation for these results highlights one current failure mode of the current implementation. In particular, the Needle In a Haystack task is constructed such that the model is tasked to remember some important information introduced at the beginning of the prompt, and later followed by completely unrelated \u2018filler\u2019 text. Hence, the attention scores and the corresponding spectrogram features for the tokens containing the relevant information are forcibly sparse, being high only at the very beginning of the prompt. Yet, since the evaluated NAMM reduces these features over the time axis of the spectrogram with an EMA coefficient of, all the frequency information regarding these tokens will be inevitably overwritten. To empirically validate our theory we provide results simply raising the EMA coefficient fromto. Since our NAMMs was never actually trained with this higher coefficient, we note that this change effectively brings the input features out-of-distribution. Nonetheless, as shown in the final row of Table24, the larger coefficient still manages to improve performance on the longer prompts by enabling the preservation of the frequency components from the target \u2018needle\u2019 over a longer horizon. These findings suggest that future NAMM designs should consider higher EMA reduction coefficients or, potentially, even directlylearningthis parameter with evolution in addition to the NAMM\u2019s network weights.", "text_file": "data\\paper_texts\\2410.13166v3_content.txt"}, {"title": "Power Plant Detection for Energy Estimation using GIS with Remote\n  Sensing, CNN & Vision Transformers", "authors": ["Blessing Austin-Gabriel", "Cristian Noriega Monsalve", "Aparna S. Varde"], "published_date": "2024-12-06T12:15:11Z", "summary": "In this research, we propose a hybrid model for power plant detection to\nassist energy estimation applications, by pipelining GIS (Geographical\nInformation Systems) having Remote Sensing capabilities with CNN (Convolutional\nNeural Networks) and ViT (Vision Transformers). Our proposed approach enables\nreal-time analysis with multiple data types on a common map via the GIS,\nentails feature-extraction abilities due to the CNN, and captures long-range\ndependencies through the ViT. This hybrid approach is found to enhance\nclassification, thus helping in the monitoring and operational management of\npower plants; hence assisting energy estimation and sustainable energy planning\nin the future. It exemplifies adequate deployment of machine learning methods\nin conjunction with domain-specific approaches to enhance performance.", "arxiv_id": "2412.04986v1", "html_link": "https://arxiv.org/html/2412.04986v1", "search_term": "ti:\"transformers\"", "html_error": "Error processing HTML: HTTP Error 404: Not Found"}, {"title": "Factorisability of the Harer-Zagier Transform of the HOMFLY-PT\n  polynomial", "authors": ["Andreani Petrou", "Shinobu Hikami"], "published_date": "2024-12-06T10:41:36Z", "summary": "The Harer-Zagier (HZ) transform maps the HOMFLY-PT polynomial into a rational\nfunction. For some special knots and links, the latter has a simple factorised\nform, both in the numerator and denominator. This property seems to be\npreserved under full twists and concatenation with the Jucys--Murphy's braid,\nwhich are hence used to generate infinite families with HZ factorisability. For\nsuch families, the HOMFLY-PT polynomial can be fully encoded in two sets of\nintegers, corresponding to the numerator and denominator exponents. These\nexponents turn out to be related to the Khovanov homology and its Euler\ncharacteristics. A criterion for when factorisability occurs is found via a\nconjectural relation between the HOMFLY-PT and Kauffman polynomials, which is\nproven in several special cases. The latter is equivalent to the vanishing of\nthe two-crosscap BPS invariant of topological strings.", "arxiv_id": "2412.04933v1", "html_link": "https://arxiv.org/html/2412.04933v1", "search_term": "ti:\"transformers\"", "html_content": "SECTION: 1Introduction\n\nThe relation between knot polynomial invariants and the 3-dimensional Chern\u2013Simons (CS) gauge theory[1,2]is well known and has led to an increasingly fruitful interchange between pure mathematics and theoretical physics.\n\nThe HOMFLY\u2013PT polynomial[3,4]of a knot or link, is a Laurent polynomial in two variables that can be defined via the skein relation\n\nalong with normalisation condition(\u20dd)=1.\nIt corresponds to the-invariant in the context of CS theory, within which one can derive the quantityby taking averages of Wilson loops around the knot. This can be related to the HOMFLY\u2013PT polynomial as defined in (1) by\n\nThis-invariant is referred to as the unnormalised HOMFLY\u2013PT since(\u20dd)=. Hereis a quantum group parameter[2]and can be determined by the levelof CS theory and the rankof the gauge group as.\nWhen(i.e.) and(),\nthe HOMFLY\u2013PT reduces to the Jones and Alexander polynomials, respectively. The-invariant corresponds to the Kauffman polynomial.\n\nThe Harer\u2013Zagier (HZ) transform was first introduced in[5]where it was used as a generating function for the computation of Euler characteristics of the moduli space of Riemann surfaces, via random matrix theory[6,7]. This matrix model approach\nis useful, not only for the study of Euler characteristics, but also for computing\nthe intersection numbers of the orbifold, which are the Gromov-Witten invariants for a point[8,9,10,11].\nThe HZ transform in relation to Gaussian correlators (Gaussian means) has been considered in[12].\n\nThe HZ transform has recently been applied to knot polynomials[13,14].\nIn this context, it amounts to a discrete version of the Laplace transform applied to the parameterin the unnormalised HOMFLY\u2013PT polynomial,\nwhich can be expressed as\n\nAt each monomial() in, the sum overcan be evaluated via geometric series (assuming)\n\nThus the evaluation of HZ amounts to the substitutioninand, hence, it transforms the HOMFLY\u2013PT polynomial into a rational function, involving the ratio of polynomials in the parametersand.\nThe HZ transform is said to befactorisable, if both the numerator and denominator can be expressed as the product of monomials inof the form.\n\nAs an explicit example, consider the right handed trefoil knot, which has HOMFLY\u2013PT polynomial\n\nIts unnormalised version is obtained by multiplying with an overall factorand becomes. The HZ transform is obtained by applying (3)\nand has the factorised form\n\nIn fact,belongs to the family of torus knots, whose all members are known to have factorised HZ transform[13]. Beyond the torus family, a hyperbolic family of pretzel knots that also enjoys this property was recently found in[14].\n\nIn the present article, we extend the results of[14]by finding further infinite families of hyperbolic knots which admit a factorisable HZ transform. As explained in Sec.2.1, such knots are related to each other by full twistsor concatenations of their braid representatives with Jucys\u2013Murphy braid, examples of which are shown in Fig.1. The factorisability property seems to be preserved under these operations applied an indefinite amount of times and, hence, they are used to generate the infinite families. Furthermore, in Sec.2.2we consider the HZ factorisability in the case of links with more than one components.\nSuch knots and links are special since their HZ function can be fully determined by two sets of integers corresponding to the numerator and denominator exponents of; and a parameter, which gives a lower bound to the braid index, according to the Morton\u2013Franks\u2013Williams inequality (c.f. Remark2.2).\nWe describe how the HZ transform may give an explanation for the non-sharpness of the inequality for the exceptional cases.\n\nIn Sec.3, Theorems3.1and3.2suggest a way to recover the HOMFLY\u2013PT and Alexander polynomials of a knot or link, given its HZ function. This is achieved by applying the inverse HZ transform, which is computed via contour integrals. As an application, we provide the compact formula (38) for the HOMFLY\u2013PT polynomial of the infinite families of knots that admit HZ factorisability, which is somewhat reminiscent of the Rosso-Jones formula for torus knots[16]. In (48) we also give a closed expression for the Alexander polynomial of the factorised cases.\n\nFurthermore, in Sec.4.1we conjecture that HZ factorisation for knots occurs if and only if their HOMFLY\u2013PT and Kauffman polynomials are related by(is defined in (59)),\nhence providing a necessary and sufficient condition for factorisability.\nThis can also be reduced to a relation (76) between the Alexander and Kauffman polynomials. In fact, such relations were long known to exist for torus knots[15]. We find that this holds for an exhaustive list with up-to-12-crossings knots that admit HZ factorisability and, in Theorem4.1\u2013which is among the central results of this article\u2013, we prove it for the families,,and(defined in Sec.2). A similar HOMFLY\u2013PT-Kauffman relation (82) is found in the case of links, but it is no longer in 1-1 correspondence with HZ factorisability. In Sec.4.2we further suggest that (59) is equivalent to the vanishing of the two-crosscap BPS invariantof topological string theory, which is related to link invariants via the gauge/string duality[34]. The BPS invariants for several HZ-factorisable two-component links are included in the Appendix.\n\nIn Sec.5, we investigate the homological interpretation of the factorised HZ transform. This is based on the observation that the HZ exponents for a given knot, coincide with the numberappearing in the rows of its Khovanov homology (Kh) table[17]and relate to the corresponding Euler characteristics[18,19,20,21].\nIn fact, according to Prop.5.1, in cases of factorisable HZ transform, the HOMFLY\u2013PT and Jones polynomial (the latter being the graded Euler characteristic of Khovanov homology) are exactly equivalent.\n\nA summary and discussions can be found in Sec. 6, while in a follow up paper[22], more general knots, whose HOMFLY\u2013PT polynomial does not admit the HZ-factorisability property, shall be considered.\n\nSECTION: 2Knots and links with factorised HZ transform\n\nSECTION: 2.1Knots\n\nWe consider the unnormalised HOMFLY\u2013PT polynomialof a knot as the two-parameter polynomial invariant defined in (2).\nIts HZ transform (3) is said to be factorisable when it can be written in the form\n\nwhereare constants that depend onand satisfy. In fact, the denominator exponentsare fully determined by, which is the lowest power ofinand the parameter. As we shall explain below, the latter in most cases coincides with the braid index of the knot.\n\nIn the previous article[14]we verified that the factorisability property holds for the-stranded torus knots, whose exponents areand; and we also found that the pretzel family111The overline notationis used interchangeably with the more standardto indicate negative tangles., which is hyperbolic, also admits (7) with,,and.\n\nAn exhaustive list for hyperbolic knots with up to 13 crossings satisfying (7) together with their HZ parameters is the following222We used the website KnotInfo[23]for the data of the HOMFLY\u2013PT polynomial of knots with up to 13 crossings and its HZ transform was computed using the Mathematica software. Note that, although we often use Rolfsen notation to describe a knot, we sometimes refer to the mirror imageof the one originally listed in the Rolfsen table.\nThe HOMFLY\u2013PT polynomials and HZ transforms of mirror knots are related byandrespectively, and hence conclusions about factorisation are not affected by this choice. For clarity, sometimes a superscriptwill be added to the Rolfsen notation indicating weather we refer to the positive or negative one (as indicated by the sign of their writhe), respectively, which are the mirror image of each other. Note that the sign of the writhe is also reflected in the sign of the exponents in the HZ transform..;;,;;;,;;;,;;;,,;;;;;;,,;;,;.;;,;.;;,;.;;,,;.;;;.\n\nThese knots\ncan be related to one another by two operations. The first, is by introducing a full twiston an-stranded braid representative of a knotthat satisfies (7). The resulting knot, which will be denoted asor, does not depend on the choice of minimal braid representative forbecause full twists are equivalent to Dehn twists. The second operation, is by concatenation with the Jucys\u2013Murphy braidor equivalently (rotated by)[25,26], on some special-stranded braid representative of. Note that, unlike with the full twists, the resulting knot and its factorisability will now strongly depend on the choice of braid representative and, hence, the latter should be specified to obtain an unambiguous definition for.\nExamples of such twists forare shown in Fig.1.\n\nThe relations for the knots listed above under these operations are summarised in the tables (a) and (b) of Fig.2.\n\nThe factorisability of the HZ transform seems to further be preserved after applying these operations an arbitrary number of times.\nTherefore, infinite families of knots with a factorised HZ transform can be generated (in a non-unique way) by concatenating withandfor,\nas we shall see in several examples below.\n\nThe family, which is\ngenerated byfull twists onthat has braid index, is depicted in Fig.3. This family has been previously considered in[27].\n\nFor instance, atit includes the 10-crossing knot. In turn, as suggested in the table (a) of Fig.2, the knotitself can be obtained by a full twist on\na 3-stranded version of the left handed trefoilwith braid word, which can be schematically depicted as\n\nHere vertical lines represent the strands, while solid or dashed horizontal ones represent a positive or negative crossing, respectively. Hence,and.\nThe HOMFLY\u2013PT polynomial for this family can be computed recursively forby\n\nand its HZ transform can be written in a symmetric way in terms of the number of crossingsas\n\ni.e., in terms of,,and.\n\nNote that for each subsequent member of the family the value ofincreases by, which is equal to the number of crossings in the full twists.\nMoreover, the values of the numerator exponentsalso change in an additive way with each additional full twist.\n\nThe whole of the pretzel familycan be further generalised into,\nwith HZ transform333In general, it is often difficult to find simple recursive or explicit formulas for the HOMFLY\u2013PT polynomial for families generated by full twists. However, for the first few members of the family (with no more than 45 crossings), it can be computed using the Mathematica package \u201dKnot Theory\u201d. Although the HOMFLY\u2013PT polynomial becomes increasingly complicated, the exponents in the HZ transform, which remains factorisable, show a clear pattern that follows the observation in Remark2.1. In the sequel, the HZ formulas for such families are determined in this way, by extrapolating on the pattern exhibited by their first few members. It will be an interesting future task to prove these formulas more rigorously.\n\nAtit reduces to the familymentioned above, while atit contains the subfamily,\nwhich includes the knotat. Further such relations can be traced in table (a) of Fig.2.\n\nA different family can be generated byfull twistson the mirror of the pretzel family. It is denoted asand has\nHZ transform\n\nAt, these are simply torus knotsand.\nNote that this expression is very similar to Eq.\u00a0(10) with the exponents ofrelated to(amount of full twists) being the same, while the remaining ones switch sign (c.f. footnote2).\nSimilarly, by applying (positive) full twists on the mirror of any of the knots satisfying (7),\ndifferent families can be generated,\nbut their HZ transforms will be related in a similar way as in this case.\n\nThe familycontains the torus knotsat,at,atand the hyperbolic knotat. The braid of the latter, for example,\nis schematically depicted as\n\n.\n\nThis representation is not unique, however, sincecan equivalently be obtained by a full twist on a 4-stranded trefoil, i.e. as, or alternatively as(c.f. Fig.2(a)).\nThe HOMFLY\u2013PT polynomial of this family can be obtained recursively via the formula\n\nand its HZ transform, in terms of the crossing number, reads\n\nBy increasing the number of full twists, this can be generalised further towith\nHZ transform\n\nAtmembers of this family can be expressed as the closure of the braid, or alternatively, of.\n\nThe knot, which has braid index, can be obtained by a full twist on a 4-stranded version ofin the form depicted below\n\n.\n\nIt generates the familywith HZ transform\n\nThe knot, which has braid indexbut has the same HOMFLY\u2013PT polynomial as, can be obtained by a full twist on a 4-stranded version ofin the form.\nIt generates the familywith\nHZ transform\n\nNext, we turn to families obtained by concatenation with the Jucys\u2013Murphy\u2019s braid. As we mentioned earlier, this will be braid word depended and hence for the resulting knotto be well defined, the former\nshall be specified in each case.\n\nThe familyhas HZ transform\n\nNote that atthis is just the pretzel family.\nAtthis is a family that includesat,at,atandat(c.f. Fig.2(a)). For instance, the familyhas HZ exponents,,.\nMoreover, at, the knot corresponding tocan also be obtained asand toas. The familycan alternatively be obtained via a full twiston a 4-strandedin the form\n\n\u2026\n\n.\n\nIts HOMFLY\u2013PT polynomial has the recursive formula\n\nAt, the recursive formula becomes\n\nbut it gets much more complicated for.\nFor general, atthe family can also be expressed as,\nwithin which(at)\nhas the same HOMFLY\u2013PT polynomial as.\nMoreover, atthis corresponds to,\nwithin whichat(c.f. Fig.2(a)).\n\nThe family, which is generated byhas HZ transform\n\nThe familywhenhas HZ transform\n\nWhen, i.e.(), the HZ forcan still be obtained from the above formula since.\nThe knotsandalso belong to the pretzel family(c.f. eq.\u00a0(13) and Fig.2).\nThe knotcorresponds to an-crossing knot, which can equivalently be expressed in terms of torus knots with additional strands of the formortimes a full twist.\n\nThe familywhenhas HZ transform\n\nWhen, i.e.(), we getsince.\n\nInterestingly, the HZ transform foris not factorisable.\n\nThe familywhich has arbitrary braid indexhas HZ transform444We were only able to confirm the validity of (2.1) for.\n\nMany more infinite families of knots satisfying (7) can be generated in a similar fashion.\n\nSome members in the family oftwisted torus knots, which are obtained from a torus knotbyfull twists on its laststrands, also have a factorised HZ transform. Depending on the relative values ofthese contain many different kinds of knots (see e.g.[28]). For instance, they are torus knots when(or), cables of torus knots whenis a multiple of(or) and hyperbolic whenis not a multiple of(to guarantee hyperbolicity, it is also required that the number of full twists is).\nAn example of twisted torus knots which are hyperbolic is the family,\nfor which\n\nNote that, while the particular cases() and().\nAnother example of twisted torus knots with factorised HZ transform is the family, a member of which is depicted in Fig.4\n\nand whose HZ transform for oddis\n\nFor even, the numerator exponents are changed to,and. At(the factors with exponentcancel), these are just the torus knots. They can be hyperbolic only when. Note that the twisted torus knotsandhave HZ transform that only partially factorises, or in other words, admits quasi-factorisation, meaning that the numerator can be expressed as an overall factortimes a more general ()-polynomial.\n\nSECTION: 2.2Links\n\nTo remain consistent with our conventions in (1), the normalisation factor of the HOMFLY\u2013PT polynomial for linkswith even number of components should be multiplied by an overall minus sign555This ambiguity in the overall sign is an artifact of a discrepancy between conventions used in the literature. The fact that it is needed for links with even number of components is because of the odd parity in the powers ofandthat appear in.. That is, for a linkconsisting ofcomponents the unnormalised HOMFLY-PT polynomial refers to the quantity\n\nThe 2-stranded torus linkswith parallel relative orientation666Throughout this paper, when we refer to torus linkswe mean the links obtained as the closure of the braid, which naturally induces the parallel orientation on all components.on the 2 components have a factorisable HZ transform. A simple example\natis shown in Fig.5. This corresponds to the Hopf link, which is tabulated asin[23], where the notationdenotes the choice of orientation. For, the torus links with parallel orientation are tabulated asand.\n\nFor these torus links the HZ transform reads\n\nThe difference with the HZ function corresponding to 2-stranded torus knotsis the plus instead of minus sign that appears in the numerator. Note that in the case of the Hopf link, the opposite orientation, i.e.results in the same HOMFLY\u2013PT polynomial and hence its HZ transform still factorises, but this is not true for.\nThe HZ transform is also not factorisable for torus links with higher number of strands, no matter the choice of relative orientation. For example, the HZ transform of the 3-stranded torus linkbecomes\n\nBelow, we give an exhaustive list of non-torus links with up to 11 crossings (using the notation777As for knots (c.f. footnote2), the links referred to here might, in fact, correspond to the mirror image, as compared to the one listed in[23]. Their positivity/negativity is again denoted by a superscriptand it is reflected in the signs of the HZ exponents.of[23]) that admit a factorised HZ transform in the form\n\n;,;;;;,;;;,;;,;;,;;,;;,;;,;.\nFor all of the above linksand the braid index is 3, with the exception ofwhich has braid index 4. They all consist of two components. For the links,andthese are,\nwhile for the remaining ones they are. They are all hyperbolic links, with the exception of the twisted torus links, which have vanishing hyperbolic volume (i.e. their complement does not admit a hyperbolic structure).\n\nThe family of twisted torus linkswith,\nwhich contains,and, has HZ transform when\n\nwhile the numerator exponents becomewhenand,, when. Additional full twistsjust yield. Furthermore, by concatenating withwe obtain:\u2013;,,.\u2013;,,.\u2013:,,, whileis not factorisable.\u2013.\u2013, while for;,,.\n\nIn general, a more thorough consideration seems to be necessary in order to better understand the conditions under which concatenation with the Jucys\u2013Murphy braid preserves factorisability or not.\n\nThe family, whilehas HZ transform\n\nThe familyhas\n\nand with full twists the HZ becomes\n\nThe 2-component twisted torus linkswith braid index 4, has HZ transform888We were only able to confirm (34) for.\n\nNote that the HZ transform forandforare only quasi-factorisable, while foris completely not factorasibale. Moreover,is again quasi-factorisable, whilecontains two overall factorsin the numerator.\n\nNote that, at, all the above formulas become(c.f. proposition5.3).\n\nThe constantthat appears in the HZ transform (29), is related to the-span in the HOMFLY\u2013PT polynomial of a knot or link.\nAccording to theMorton-Franks-Williams (MFW) inequality[29,30], the latter\nprovides a lower bound of its braid index999The braid index is equal to the least number of Seifert circles in any of the knot\u2019s projections.. Explicitly,, whereandare the lowest and highest exponents ofin the normalised HOMFLY\u2013PT polynomial. In terms of the HZ parameter, the MFW invequality can be simply rephrased as\n\nAlthough the inequality is sharp for most knots, there exist 5 exceptional cases with up tocrossings, namelyand, which all have braid index(see the braid index table 15.9 of[2]and the comments therein). The reason this occurs for the knotsand, is that the HOMFLY\u2013PT polynomial can not distinguish them from other knots that have a lower braid index and for which the inequality is sharp. In particular,and, which hence determinesand, which are smaller than their actual braid index. Similarly, for the two-component linkwe haveand hence.\n\nSuch coincidences, may be interpreted through the HZ transform to be\na result of a cancellation between several factors of the form, which occur both in the numerator and denominator. In the case offor which the HZ is factorised, this can be clearly seen fromin (16) at, in which the factorsandcancel. Prior to this cancellationis equal to the braid index, and hence the MFW inequality is sharp.\nBeyond 10-crossings, the same holds, for example, for the knotsand,\nas their HOMFLY\u2013PT polynomials are equal to the ones for101010Classifying hyperbolic knots which have the same HOMFLY\u2013PT polynomial as a torus knot, which hence are also cases of non-exact MFW inequality, seems to be an interesting task that we aim to address in the future.and, respectively, and hence they both have. Their braid indexcoincides within (17), in which, indeed, a factor cancels at the valuesand.\nSimilarly, for the remaining cases which do not admit HZ factorisability, it can be understood that the HZ transform is really quasi-factorisable, but the overall factor cancels out. By considering the corresponding familiesobtained by full twists, we can deduce what those \u201dmissing factors\u201d are. In particular, we findfor,for,for,forandfor111111Note thatis only quasi-factorisable atand non-factorisable for.. Notwithstanding, the HOMFLY\u2013PT polynomial itself is, in fact, oblivious to this information, and recovering it in this way through the HZ transform requires prior knowledge of the braid indexin each case.\n\nSECTION: 3Inverse HZ transform\n\nAlthough the HOMFLY\u2013PT is a complicated polynomial for knots and links with high number of crossings, we have seen that in the several cases the HZ transform takes a remarkably simple, factorised form that can be easily extrapolated to include members of infinite families. Hence, it is important to be able to recover the HOMFLY\u2013PT polynomial from the HZ transform and this can be achieved via the following theorem.\n\nThe HOMFLY\u2013PT polynomialcan be obtained fromvia a contour integral around the pole atas\n\nAlternatively, choosing a contour enclosing all the simple poles at, it can be expressed as\n\nThe way to recover the HOMFLY\u2013PT polynomial from the HZ function is by applying the inverse HZ transform, i.e. by inverse Laplace transform. First, notice that the HZ function in (3) is a Maclaurin series with coefficients the unnormalised HOMFLY\u2013PT polynomial, () with. The radius of convergenceof this series is given by, which givesfor a fixeds.t.. Henceis analytic in the disk.\nTo understand the equivalence ofwith a discrete Laplace transform we setfor some complex parameter(), with which it becomes periodicand it is analytic for. Then, from the inverse Laplace (or Mellin) transform we get. After the change of variablesthis gives the contour integral in (36) evaluated on a counterclockwise contour including only the pole of orderat, which is the only one lying in the domain of analyticity. Finally, dividing by the normalisation factorgives the normalised HOMFLY\u2013PT polynomial. The alternative contour integral in (37) utilises the fact that the residue of a pole inside a given contour is equal to minus the sum of residues of the poles lying outside of it.\n\u220e\n\nAs an application of the above theorem, the HOMFLY\u2013PT polynomial of knots that admit HZ factorisability, i.e. satisfy (7), can be written in terms of the HZ parametersandandas ()\n\nSome explicit examples follow.The two-stranded torus knotshave HZ paramers,. From the above theorem, their HOMFLY\u2013PT polynomial can be derived as, in agreement with the Rosso\u2013Jones formula. This can be written as a polynomial inandafter the substitutionsand.The Pretzel knotshave HZ transform (13) with parameters,,, where.\nUsing (38), we obtain\n\nFor the familywith, the HOMFLY\u2013PT polynomial can be derived from (21) via the above formula\nto be\n\nThe HOMFLY\u2013PT reduces to the Jones polynomialwhen.\nTherefore we deduce the following\n\nThe Jones polynomialcan be expressed as the following contour integral ofaround the double pole at,\n\nExample:The Jones polynomial for the familycan be computed from (10) atto be\n\nTheorem3.1holds for all, but it requires a specification for(), corresponding to the Alexander polynomial. This is because dividing bybecomes problematic in the limit.\n\nThe Alexander polynomial can be obtained fromby\n\nin which\nthe integration contour at eachshould include the pole\nat.\n\nUnder partial fraction decompositioncan be decomposed as\n\nin which the functionsare polynomial inand are independent of.\nBy residue calculus,can be obtained fromvia contour integrals around the pole atas\n\nThese clearly are the coefficients of(which is replaced byunder HZ transform)\nin the unnormalised HOMFLY\u2013PT polynomial and hence the latter can be expressed as. The Alexander polynomialcan be obtained as the limit, which evaluates to. Using de l\u2019H\u00f4pital\u2019s rule\n\n\u220e\n\nNote that the above theorems apply to knots and links with both factorised and non-factorised HZ transform.Example: The figure-8 knot with(non-factorisable),\nyields via (43) and the residue theorem\n\nThis yields the usual Alexander polynomialafter replacing.\n\nFor torus knots there is a well known formula for their Alexander polynomial, which reads\n\nwhich has alternating coefficients. This can be seen, for example, inandApplying theorem3.2togiven by (7), with the help of the residue formula we obtain the following closed expression for the Alexander polynomial for knots with factorised HZ transform121212Alternatively, the Alexander polynomial for factorised cases could be determined by first computing the normalised HOMFLY\u2013PT polynomial using (38), in which the factoralways cancels, and then set.\n\nThis formula is applied to several examples below.\n\nThe Alexander polynomial for the pretzel familycan be determined from its HZ parameters,,andvia (48) to be\n\nFor the family(includingfull twists) from (10) we get\n\nwhich reduces to (49) at, and atgives, with.\nAtit can also be recursively determined\n(for) by\n\nAnother example is, for which (14) yields\n\nForthe Alexander polynomial can be derived explicitly from (15) as\n\nFor the family(shown in Fig.3), from (9) we obtain\n\nor recursively by().\n\nIt may be interesting to note that the Alexander-Conway polynomial, with, for several knotsfor which HZ factorisability holds,\ncan be written in terms ofandas follows\n\nNote also that.\n\nFurthermore, noting thatis a root of, at which the the Alexander polynomials for,,are vanishing, it may be interesting to consider evaluation of the Alexander polynomials of the remaining factorised cases at this value.\nWe findFor the pretzel familyfor, while it becomes equal toforand, respectively.For thefamilywhenis even, otherwise it is.The Alexander polynomialtakes the valueswhen, respectively.The Alexander polynomial for(which can be derived from\n(16)) shows the same mod 3 behavior, since, at, respectively.\n\nThe Alexader polynomial of knots with factorised HZ\ntake the valuesor,\nwhen evaluated at.\n\nSECTION: 4Kauffman polynomial and HZ factorisability\n\nThe Kauffman polynomial[31]of a knot or linkcan be defined as131313The definition here differs from the original in[31]by. Our convention is such that for positive knots, the Kauffman polynomial has positive powers of, as it is the case for the HOMFLY\u2013PT polynomial defined in (1) and hence it will allow their comparison later in this section., whereis the writhe of the diagram, whilesatisfies the skein relation\n\nand the normalisation condition. For 2 disconnected knots holds that.\nNote that although the skein relation forapplies to unoriented links, a choice of orientation is necessary to compute the writheand hence the Kauffman polynomial is orientation depended.\n\nAs it is the case for the HOMFLY\u2013PT polynomial, it also admits a definition via Chern\u2013Simons theory with the gauge group141414Note that the Euler characteristics for the moduli space\nof Riemann surfaces[5]can be extended to Lie algebras of type(i.e.) containing symmetric and anti-symmetric terms[32].or. Such a definition is in agreement with (55) after an overall multiplication withand the substitutionsand. The resulting polynomial is referred to as the unnormalised (or unreduced) Kauffman polynomial.\n\nSECTION: 4.1A Relation between the HOMFLY\u2013PT and Kauffman polynomials of HZ-factorisable knots and links\n\nThere is a well known relation between the Kauffman polynomialand the Jones polynomial(with) given by[17]\n\nAlthough, in general, the Kauffman polynomial contains many more terms as compared to the HOMFLY\u2013PT polynomial, in the case of torus knots, withco-prime, there exist a relation between them. Denoting the Kauffman\nand HOMFLY\u2013PT polynomials for torus knots byand, respectively, their relation reads[2,15]\n\nHererefers to the Dubrovnik version of the the Kauffman polynomial,\nwhich is obtained by substitutingand, i.e.[15].\nThe first and second parts, which shall be labeled as\n\ncontain the terms with even and odd powers ofand, respectively.\nIt can be easily verified for simple torus knots, such as, for instance, for which. The odd part is, which is divisible by. With the even part being, the r.h.s of (57) indeed yields the HOMFLY\u2013PT polynomial of the trefoil.\nNote, however that (57) does not hold for torus links, i.e. whenare not co-prime.\n\nThe above relation also does not hold for general, non-torus knots.\nFor instance, forthe r.h.s. of (57) becomes, which differs from its HOMFLY\u2013PT polynomial. Similarly, the invariants of the knots,anddo not satisfy (57).\n\nIt is remarkable, however, that for, which is a hyperbolic knot but has a factorised HZ transform, the formula (57) is true. Explicitly,, while, hence yielding the correct HOMFLY\u2013PT polynomial.\n\nThe HOMFLY\u2013PT and Kauffman polynomials for the following families of knots(i),(ii)forand(iii), which admit a factorisable HZ transform,\nsatisfy\n\n(i) For the first pretzel family(with), using (55) we find the following recursive relation for (the Dubrovnik version of) its Kauffman polynomial\n\nHereandwith. After some simple algebraic manipulations it can be shown that\n\nThis is the same expression as for the recursive formula of its HOMFLY\u2013PT polynomial found in151515Note that in[14]we considered the mirror familyand hence the HOMFLY\u2013PT polynomial ofis related to that via.[14]. That is, all the extra factors arising from the last term in the Kauffman skein relation (55), which is not present in the HOMFLY\u2013PT skein relation (1), cancel.\nUsing the fact that (59) holds for the torus knotsand, the proof can be easily completed by induction.\n\nThe proof is very similar for the second pretzel family, which has Kauffman polynomial\n\nyielding\n\nAgain, this agrees with the recursive formula for its HOMFLY\u2013PT polynomial161616Note that this is an alternative but equivalent version of the recursive formula (12) for(after), which was expressed in terms of torus knots only.\n\n(ii) The Kauffman polynomial of the familywith, which is the closure of the braid, can be obtained recursively by\n\nThis yields\n\nUsing,and, one can easily show that the terms on the last two lines exactly cancel and hence (66) has the same form as the recursive formula for the HOMFLY\u2013PT polynomial in (18). Since the Theorem is valid for the knots,and, the proof can again be completed by induction.\n\nSimilarly, the family() has the recursive formula for its Kauffman polynomial\n\nwhich yields\n\nUsing that,and,and after some simple algebra, one can show that the terms in the last 3 lines exactly cancel and hence (4.1) has the same form as the recursive formula for the HOMFLY\u2013PT polynomial in (19), which only involves knots that satisfy theorem4.1. For the last 2 families the proof can be extended to include the cases with, whose recursive formulas will differ slightly from (66) and (4.1) in a similar way as they do for the two pretzel families.\n\n(iii) For the family, obtained byfull twistson, we find\n\nwith which we get\n\nThis recursive formula is almost the same as (8) for the HOMFLY\u2013PT polynomial of, apart from an extra term in the last line. This is present due to the fact thatfor 3-stranded torus links, which are included in the summation of the first line whenever.\nTo redeem this, we compute\n\nand\n\nThe latter yields\n\nwhere. By further using (83) one can show that\n\nIt is then straight forward to compute.\nTaking this into account and using thatfor torus knots, the recursive formula (4.1) becomes exactly equal to (8) for the HOMFLY\u2013PT polynomial of.\n\u220e\n\nUsing the data in[23], one can further easily confirm that (59) is also satisfied by the knots,,and, which, together with the ones included in the above families, complete the list with up to 12 crossings for which the HZ transform of their HOMFLY\u2013PT polynomial factorises.\nWe have also checked that some other factorised cases, such as the twisted torus knots also satisfy (59) and, in general, no exception has been found so far.\nIn fact, we have further confirmed that, for knots with up to 12 crossings, (59) is valid if and only if the HZ transform of their HOMFLY\u2013PT polynomial factorises.\nThis motivates the following.\n\nThe HOMFLY\u2013PT and Kauffman polynomials for a knotsatisfy (59) if and only if the HZ transform of its HOMFLY\u2013PT polynomial admits a factorised form (7), i.e.\n\nFor torus knots, there is also a relation between the Kauffman polynomialand the Alexander-Conway polynomial, which can be derived from (57)[15].\nAs a consequence of Theorem4.1, since the Alexander-Conway polynomial corresponds to thelimit of the HOMFLY\u2013PT polynomial, we have\n\nThe relation between the Alexander-Conway and Kauffman polynomials\n\nwhich is true for torus knots[15], also holds for the hyperbolic families,,and.\n\nNote that the first term in (76), which comes from, is always unity at, for all knots.\nA general hyperbolic knot, such as, does not satisfy the relation (76), since the r.h.s. is, while its Alexander-Conway polynomial is.\n\nWe have further tested that this relation holds for all the remaining up-to-12-crossings knots with factorised HZ transform.\nAs a natural consequence of thepart of conjecture4.1, (76) should remain valid for all the hyperbolic families with factorised HZ transform. However, () does not hold when restricting to the Alexander case (), since the hyperbolic knots,andwhose HZ transform does not factorise, do satisfy (76).\n\nTo get an idea of some of the implications of conjecture4.1we consider the following.\nFirst, note that when the HOMFLY\u2013PT polynomial of a knot has a factorised HZ transform, it means that it is expressible in terms of HZ parameters as in (38), which is an expansion in powers ofwith-polynomial coefficients.\nAlternatively, the unnormalised HOMFLY\u2013PT polynomial of a knot171717More generally, for a link withcomponents the lowest power ofinis.can be written as an expansion in powers ofas\n\nin which the coefficientsare polynomial in. Similarly, we can expandand. For all knots it holds that[33], which in the factorised cases can be expressed in terms of the HZ parameters as\n\nwhereis the smallest power ofsuch that.\nFor (59) to hold it is required thats.t., whiles.t.. For the latter to be true, anecessary (but not sufficient) conditionis that\n\nwhich can happen only ifthe Kauffman polynomial has even highest power of. For alternating knots the highest power ofis equal to, whereis the number of crossings[31]. This implies that no alternating knot with even number of crossings can satisfy (59). In the light of conjecture4.1, this also becomesa necessary condition for factorisability of the HZ transform.\nIndeed, the list of up-to-13-crossings hyperbolic knots with factorised HZ transform (see Sec.2) contains only non-alternating knots with even number of crossings181818Note that for odd number of crossings the only knots with factorised HZ are alternating and includeand the torus knots, which are excluded from that list. Up tothe latter are listed asin the Rolfsen table, and extend toandin[23].. The highest power ofin the Kauffman polynomial for non-alternating knots is not known in general, but for up to 10-crossing-knots it is equal to(with the only exception beingfor which it is) and hence satisfy the necessary condition (79).\n\nThe above discussion applies only to knots. However, it is also of interest to examine the relation between the HOMFLY\u2013PT and Kauffman polynomials in the case of links, with multiple components. It is noteworthy that links may be interesting from a physics viewpoint, due to the fact that their Khovanov homology is more clearly related to the space of BPS states in M-theory[44].\n\nThe HOMFLY-PT polynomialandfor knots or links with odd number of components are always even Laurent polynomials in.\nHowever, for links with even number of components,involves only odd powers of, and hence (59)\ncannot hold.\nAs a consequence, while for links with odd number of components (including knots) we comparewith\n\nwhen the number of components is even, we should instead comparewith\n\nAs discussed in Sec.2, all the links admitting a factorisable HZ transform known to us, have 2 components.2-component links\n\nThe HOMFLY\u2013PT polynomial for the majority of the HZ-factorisable 2-component linkswith up to 11 crossings,\nsatisfy the following relation to their Kauffman polynomial191919As a reminder, compared to the data of[23], the variableof the Kauffman polynomial should be replaced by, in order to match our conventions.\n\nin whichis the linking number of, which is obtained as a sum ofover the crossings between.\n\nWe have explicitly verified that this holds for the following links from Sec.2with,,,,,,. It also valid for the ones included in the following families.\n\nThe 2-stranded torus links with parallel orientationwith. These include,,,,. For example, for() the r.h.s. of (82) becomes.\nThe opposite choice of orientation, results in a HOMFLY-PT polynomialthat differs toby, but still has a factorised HZ transform. In this case\nthe r.h.s becomes, in agreement with.\n\nThe twisted torus linkssatisfy\n\nwhere,and. For instance, for:, for:and for:.\n\nFrom (82) at, we obtain thatin the case of a factorised links with two components, sinceand, which are multiplied byvanish.\nFor, the HOMFLY\u2013PT polynomialand the factorbecome 1. Hence. For instance, for, which satisfies (82) with, we haveand.\n\nHowever, the link, despite its factorisability, is an exception, which does not satisfy (82) since evaluation of the l.h.s. yields\n\nThis can be rewritten as, which yieldsat(). It is noteworthy that, although the HOMFLY\u2013PT polynomial can not distinguish this link from, their Kauffman polynomialsare very different.\nThe reason for this link being exceptional remains mysterious and deserves further investigation. The breaking of (82) in this case, implies that a relation between the HOMFLY\u2013PT and Kauffman polynomials for links can not serve as a criterion for factorisability.\n\n3-component linksThe 3-stranded torus links,\nalthough their HZ transform is not factorisable, satisfy the simple relation\n\nas proven in (iii) of Theorem4.1.\nFor instance, the HZ transform of the HOMFLY\u2013PT polynomial of, which is given in (28)\nis simple but not factorisable.\nDifferent choices of orientation\nare labelled asand their HZ becomes. They have the same HOMFLY\u2013PT and Kauffman polynomials, which relate as. This can be a bit simplified by considering.\n\n4-component linksFor the torus link, which is the closure ofand its HZ transform does not factorse, we find\n\nwhile forwe get\n\nThus we observe that, from 4 components, not only the HZ transform for torus links is not factorisable, but also they no longer have a simple relation between the HOMFLY\u2013PT and Kauffman polynomials. For both of the above links it holds that, for,.\n\nFor an-component link.\n\nSECTION: 4.2BPS invariants and HZ factorisability\n\nThere are intriguing relations between knot and link polynomials and BPS invariants of topological string theory, known as the LMOV conjecture[35,36,37]. In this context, as we shall explain below, the factorisation criterion of the HZ transform due to conjecture4.1seems to be related to the vanishing of the two-crosscap BPS invariantof a non-orientable surface.BPS invariants for knots\n\nThe unnormalised HOMFLY-PT polynomial of knots, corresponding to the-invariant in CS theory, can be expanded as\n\nin which the coefficientsare constants. Such an expansion of both the fundamental HOMFLY\u2013PT polynomial and its colored versions202020More generally, the integer coefficients are denoted by, where the subscriptdenotes the representation (color), usually indicated by the corresponding Young tableaux.\nIn the present paper, however, since we restrict only to the fundamental representation, we shall omit the additional subscript on the coefficientsfor simplicity.have been previously studied in[34,37], in relation to topological string theory.\nIn particular,\nthe coefficientsin (88)\ncorrespond to the spectrum of BPS states of topological strings[37]. The topological string interpretations of the indicesandare the spin and thebrane charge, respectively[36].\n\nSome properties ofcan be derived from (88). First, note that at, corresponding to the unnormalised Alexander polynomial,is always vanishing\ndue to the overall factorthat is included in it212121Sincecorresponds to, this is also the reason that there is no constant-term in the HZ transform[13]., and hence (88) yields\n\nSinceis apriori an arbitrary parameter, this implies that for each exponent\n\nNoting that the derivativeand sinceby taking the-derivative of (88) and using (89) we find that in thelimit\n\nTables 1-3 below include the values offor,and.\n\nIt can be easily checked that these examples satisfy\n(90) and (91).\nThe BPS invariantsin the case ofwere also computed in[34]and their values (up to\u2013due to mirroring\u2013 and an overall minus sign) agree with Table 3.\n\nThe Kauffman polynomial, corresponding to the gauge group, may admit a similar expansion with a topological string interpretation,\nby splitting its partition function into an orientable and an un-orientable part according to222222As conventional, the symbolis used here to denote the partition function, but it should not be confused with the HZ transform.[34].\nThe orientable part is described by the crosscap number, and the corresponding coefficients in the expansion, denoted as, satisfy\n\ni.e. they have twice the value of thecoefficientsin (88).\nHence, the orientable part is solely described by the HOMFLY\u2013PT polynomial.\nThe un-orientable part\ncorresponds to crosscap numbers. A surface withis, while with two crosscapsis the Klein bottle, which is obtained from two projective planesas[39]. The expantion for the un-orientable part can be expressed in terms of the BPS invariantsandas follows[33,38,40]\n\nWe suggest thatmay be given explicitly in terms of a combination of the HOMFLY\u2013PT polynomialand the (Dubrovnik version of the) unnormalised Kauffman polynomial. The latter includes the overall normalisation factor(which is equal to the Kauffman polynomial of the unlink).\nExplicitly,\n\nConsidering the parity of the powers ofandin the knot polynomials, this can be split into the crosscapandparts, respectively, as\n\nand\n\nThese formulas are consistent (up to an overall minus signs)\nwith the values of the Kauffman LMOV invariants computed via the methods of[34],\nfor all the knots for which the data are available in[24](up to 8 crossings).\nFor instance, in the example of, the values ofobtained via (95) which are listed in Table 4, match the result in[34]\n\nand satisfy the relationfor.\nFrom (96),\nthe value ofis found to be zero for,\nagain in agreement with[34].\nFor the trefoil, the r.h.s. of (95) forbecomes, which is consistent with the result reported in[40].\n\nAccording to (96), the relation (59) between the Kauffman and HOMFLY\u2013PT polynomial of a knot is equivalent to the vanishing of the two-crosscaps BPS invariants, i.e.\n\nIn agreement with Conjecture4.1,was found to be vanishing for torus knots[41]and for the up-to-10-crossings hyperbolic knots,,,,,and[34], which are (up to 10 crossings) precisely the ones with factorised HZ transform.\nIn other words, from a topological point of view, there is no contribution from Klein bottles () in the cases of\nfactorised HZ.\n\nFor the figure eight knot, the BPS invariant, defined by (95), becomes zero forsince the l.h.s. of (95) becomes just. For, the l.h.s. of (95) becomes,\nsois\nnon vanishing.The vanishing of the one-crosscap BPS invariantin (95), seems to also be an interesting phenomenon.\nBeyond, this occurs also for the knot, which, like, has HZ denominator exponents.\nAlso we find for the knot, the l.h.s. ofis one,\nwhich means, except at. The common feature of the knotsis that, in their Khovanov homology, the Euler characteristicis 1 for(for),(for) and 0 for other, as can be seen in their Khovanov tables in[17]. The Jones polynomial ofand(which have the same homology tables) is, while. These are alternating with coefficientsaccompanied withsymmetry (the latter is due to their amphichirality).\n\nThe BPS invariant for 1 crosscapis vanishing if, except for a maximum and minimum non-vanishingin the Khovanov homology tables, in which. Up to 11 crossing knots, this occurs for.\n\nBPS invariants for two-component linksFor a 2-component link,\nassuming the fundamental representation on each component232323As before, we omit the subscriptsused in[34]to label representations on each component., the BPS invariants corresponding to the orientable part, given as the coefficients in the expansion,\ncan be expressed in terms of the HOMFLY\u2013PT polynomial as\n\nwhereis the linking number of, whiledenotes the link obtained fromby reversing the orientation of one of its components242424The linking numbers ofandare related by. Note that for a fixed orientation, the linking number of a linkand its mirror imageare also related by.. The expression (98) is based on[34], but we suggest that the additional factorsshould be included.\nFor the unorientable partwe propose\n\nin whichcorresponds to the (Dubrovnik version of the) unnormalised\nKauffman polynomial and hence it\nshould be replaced by. By splitting the terms with odd or even parity,\nwriting these in terms of\n\nrespectively, we find\n\nNote that the identity\n\nholds252525The same identity applies also to some 3-component links, such as e.g., when(i.e. at) and for some choice of relative orientations., and therefore BPS invariants are independent of the orientation of link.\n\nIn the example of, which does not admit a factorisable HZ transform,\nthe BPS invariants can be evaluated from the above definition\nand the obtained results\nagree with[34]. Namely, using thatand henceand that\u20dd, while noting that for 2-component linksis given by (26) (i.e. it is multiplied by an overall minus sign),\nfrom (98) we find\n\nNote that the subtraction by the termcompletely cancels the term of order, which is always present infor two-component links.\nThe BPS invariant of two crosscap number, i.e., is given by (101) using thatand\n\nThe r.h.s. becomes, which is same one as in262626Note that in[34]the values forandwere accidentally swaped, as we confirmed with the authors in a private communication.[34]. Note that the terms of orderandin the first and last three terms are canceled with each other.\nTheBPS invariant can be computed via (100) as\n\nagreeing with[34], up to an overall minus sign.\n\nAnother example is the linkwith, for which. ItsBPS invariants can be computed from (98) to be\n\nThe BPS invariants for one crosscapis obtained from (100)\n\nin which the last term cancels the term of order.\n\nFor the linkand, the BPS invariants are similarly evaluated to be\n\nForwithwe find\n\nThe BPS invariants of two crosscapsfor,andare computed via (101) to beThese evaluations agree (up to some overall minus signs) with the\nresult provided to us via a personal communication with the authors of[34].\nSome further evaluations for links with factorised HZ transform are included in the Appendix.\nWe have confirmed thatis valid for all links that admit HZ factorisability, with the exception of, which does also not satisfy the HOMFLY\u2013PT-Kauffman relation (82). Moreover, it is interesting that although the HZ factorisability depends on the choice of orientation (unless the linking number is), the vanishing of the two-crosscap BPS invariant is orientation independent.\n\nSECTION: 5Homological relation between Kh and HZ\n\nKhovanov homology (Kh) is the categorification of the Jones polynomial. It is a doubly graded theory, which is constructed in a way such that the Jones polynomial is interpreted as the\ngraded Euler characteristic[18,17].\nIn this section, we establish a connection between the factorised HZ transform of the HOMFLY\u2013PT polynomial and Khovanov homology.\nThis is based on the observation that the exponents ofin the numerator and denominator of the HZ for a given knot or link, coincide with the values ofappearing in the rows of its Khovanov homology table[17,20].\nIn the follow-up paper[22], a possible generalisation of this connection applicable to non-factorised cases will be discussed.\n\nA Khovanov table contains the dimensions of the non-trivial doubly graded homology groupsand can be derived\nfrom a graphical\nanalysis, using the Khovanov cube, as explained in[20]. Using this information, one can build the Khovanov polynomial (the notation here\nfollows[20])\n\nThe graded Euler characteristic is given by. It can be thought of as a generating function since, for each, the Euler characteristicis the coefficient of the term. In other words, for a fixed row, the value ofis equal to the alternating sum overof the dimensions ofcontained in it.\nThe graded Euler characteristic is in fact the same as the unnormalised Jones polynomialand its normalised version can be obtained after deviding withas\n\nKnotsBelow we present several examples of Khovanov tables for knots with factorised HZ transform, as given in272727As before (c.f. footnote2), in some cases we consider the mirror image of knots as compared to the ones presented in[17]. The difference of their Khovanov tables is a change of sign inandand hence, also, in the signature.[17]. In these tables, we add an extra column labelled as \u201dHZ exponents\u201d (or just \u201dHZ\u201d), indicating which numbers appear as exponents in the denominator and which in the numerator.\nAs a first example, we consider the right handed trefoil knot.:\n\nTable 5: ()\nfor[17]\n\nThe-values(corresponding to the circled entries in table 5) and(boxed entry), coincide with the exponents in the denominator and numerator of, respectively. The entry indicated bycorresponds to the appearance ofin the integral homology table (labelled asin[17]), as we shall explain below.\nWhenevercoincides with an exponent in the numerator of HZ, their contribution tois equal to, while in the denominator it is equal to. This is due to the fact that those entries appear at columns of odd\nor even, respectively.\n\nMoreover, the table can be split into two parts corresponding to the exponents in the denominator and numerator, since there are no overlapping values among them.\nWe say that, the Kh table in the factorised cases can be separated282828However, separability of the Kh table does not necessarily imply the factorisability of HZ transform. An example is the\nknot, whose HZ can be expressed aswhich is clearly not factorisable, but its Kh table can be separated..\nMoreover, for simple knots with factorisable HZ transform, most entries in the Khovanov homology tables are 1.\n\nThe Khovanov polynomial of the trefoil\nbecomes\n\nSubstituting, we obtain the generating function of the Euler characteristics\n\nwhich reflects the above observations about the exponents ofand the signs of the coefficients.\nDividing withand further substituting, this results in the familiar Jones polynomial of the trefoil knot\n\nWhen a knotis HZ-factorisable,\ni.e.is of the form (7), then its graded Euler characteristic, or unnormalised Jones polynomial, can be written as\n\nSince,\nthis implies that the Euler characteristicin the Khovanov homology ofis equal toorwhencoincides with an HZ exponent in the denominator or numerator, respectively.\n\nThis can be easily proven by applying Cor.3.1(excluding the normalisation factorin (41)) to the general expression of the factorised HZ transform (7). Explicitly,.\n\u220e\n\nNote that after dividing withto obtain the normalised Jones polynomial, the coefficients are often again just(at least for simple knots), but there are exceptions such asand).\n\nThe sum of Euler characteristicsover allis always equal to. This is due to the fact that the Jones polynomial for all knots satisfies, while the valuecomes form the normalisation factor.\n\nIn cases when it is restricted to two main diagonals, the Khovanov homology for\nthe factorised knots can be determined from the Jones polynomial and the signature.\nThesignatureof a knot, which is an invariant quantity defined as the difference between\nthe number of positive and negative eigenvalues of its Seifert matrix, can also be obtained from the Khovanov table[17]by\n\nIn the example of, from Table 5 we observe that for,, the signaturesatisfies, while for,,, and hence.\n\nThe Jones polynomial of the trefoil can alternatively be obtained\nfrom its Khovanov table according to\n\nwhich gives (112) for. As we shall see in several examples below, Eq.115is valid whenever the Kh table contains only 2 diagonal lines.\n\nThis simple Kh table structure is shared for all 2-stranded torus knots. In general, it consists of a region with the rowscorresponding to the three HZ denominator exponents, and a region withcorresponding to the single numerator exponent, with Euler characteristicsand, respectively. Between them there is a row with, at which () is inserted, that\nconnects the two regions via a \u201d-lego piece\u201d.\nThis-lego structure may be related (due to parity) to\nthe criterion of HZ factorisation via the Kauffman polynomial in (75) and (82), and hence we now turn to describing it in more detail.\n\nBy the fundamental theory of Abelian groups, an integral homologycan be decomposed into a direct sum\n\nwhere the last finite cyclic groupis the torsion part. The asteriskinserted in the Kh table 5 for the trefoil indicates the position of thetorsion for the integral homology[17], which can be evaluated from the cycles in the Kauffman state[42].\nIts positionin the table is at the box located\none step down from the entry at, and a right diagonal upward step up from the entry at. These two positions are related by a knight move[43].\nFor factorised knots,\nthe torsionalmost always appears within the\nknight move, at the place which shall be indicated by the asterisk in the examples below.\nThe entries related by the knight move, together with the asterisk, may be grouped together as, which is what we refer to as a-lego piece.\n\nThe Khovanov table of the factorised knotcan be obtained from the one for the torus knotby shifting alltoand then adding\na lego piece with thepositioned at.\nSimilarly, the table ofis obtained from thetable by again shifting alland adding a lego piece at the position; and the one ofby adding a lego piece atto thetable. The table ofis obtained fromwith the shift ofand the addition of two lego pieces atand.\nThe table ofis obtained from the table of the torus knotby inserting one lego piece at (-11,-3) and one lego piece at the position\n(-15,-5).\nThe integral homology ofis also obtained similarly fromby adding two lego pieces.\nThis implies that there are successive sequences for factorised knotsobtained by adding lego pieces.\nSince these knots are, in fact, members of the pretzel familyfor, and hence they are related by an addition of a (2 stranded) twist, the insertion of such lego pieces may correspond to this operation. Moreover, the sequence,andis obtained by addition of two lego pieces. The table of(braid index=3) is obtained fromby the shift of a-lego, which also adds the required additional factors in HZ.\n\nSome further examples follow.:\n\nTable 6:for\n\nIn this case there is an overlap of exponents at. Note, however, that due to the factorisability, the corresponding factors in the denominator and numerator cancel each other and the Euler characteristicin this rowvanishes. If we ignore such cancellations, we can still say that the Khovanov table is separable.\nThe Jones polynomial (109) becomes. This can also be obtained by Remark5.2with signature(or from the HZ formula via corollary3.1).\n\n(Table 7):\n\nTable 7:of: type (5,3,1,-1)\n\nIts Jones polynomialcan also be obtained from Remark5.2.\n\nTorus knot;\n\nTable 8: () for\n\nThe signature. The Jones polynomial (109) is, which is obtained fromdivided byand with the replacement of. Remark5.2does not hold in this case, since there more thandiagonal lines in the Kh table. The entries belonging to the third diagonal are underlined (these correspond to the entries colored by red in[17]).\n\nIt is remarkable that the entries in the homological tables of torus knotshave a (roughly) reflection symmetric structure about the main diagonal. This excludes the entry of the formappearing in theorcolumn, whenor, respectively. In the latter case, this can be seen in Table 8.\n\nTorus knot:\n\nTable 9:of\n\nHere the signs ofandcorrespond toand, respectively. The Jones polynomial is, which is obtained by (109) (replacing). The signature ofis. Remark5.2does not hold in this case because there are five diagonal lines in the Kh table (there are 10 underlined entries in Table 9, which lie off the main diagonals).\n\nThe configuration of entries in the last 5 rows in the Kh table of, for all odd, follows the same pattern,\nwhich includes\none, indicated asand the entries, corresponding to denominator exponents, which are located at even values of.\n\nProposition5.1implies that in the factorised cases,essentially contains the same information asand hence it is equivalent to the HOMFLY\u2013PT polynomial. Furthermore, if the Khovanov homology is restricted to the 2 main diagonals, the Khovanov table can be reconstructed byplus the signature.\nSince the Alexander polynomial can be obtained fromvia Theorem3.2, this implies that for such knots the Alexander polynomial can be related to the Jones polynomial and its categorification.\n\nNote that neither Khovanov homology nor the HOMFLY\u2013PT polynomial are stronger to each other since the knotsandhave the same Khovanov homology but are distinguished by the HOMFLY\u2013PT polynomial. On the other hand,while, but their signatures differ asand.\n\nLinksIn Sec.2, we found that several links also admit a factorisable HZ and hence it is natural to also consider their relations to Khovanov homology.\nAn explicit example belonging to the family of 2-stranded torus links, is, for which\n\nTable 10:of\n\nThe Euler characteristicsat eachin the Kh homology table can again be determined by the exponents of the HZ transform, but now it is equal tofor\u2019s corresponding to both denominator and numerator exponents. This is related to the positive sign in the numerator of (117).\n\nWhen a 2-component linkhas a factorised HZ transform of its HOMFLY\u2013PT polynomial with, i.e.is of the form (27) or (29), then the graded Euler characteristic, or unnormalised Jones polynomial, can be written as\n\nThe proof is very similar to that of Proposition5.1.\n\u220e\n\nThe Jones polynomial ofcan also be obtained by the alternating sum of(with signature)\n\nThe sum of all Euler characteristicsfor a link\nwithcomponents satisfies. This is the same as the value of their Jones polynomial at, which is, multiplied by a factor.\nThis is consistent with the sum of Euler characteristics for knots (), which, as mentioned in Remark5.1, it is always equal to.\n\nThe coefficient ofin a power series expansion of the HZ transform is the Euler characteristics corresponding to. For the 3-component link, likewhose HZ transform is given in (28), the HZ athas a power series expansion as\n\nFor the 3-component link(with braid index 5) for instance, HZ becomes\n\nwhich is same as (120).\n\nThe HZ transform atfor a linkwithcomponents (), is expressed as\n\nThe coefficient is equal to the HOMFLY\u2013PT polynomialand it has the same value as the sum of Euler characteristicsin anhomology (corresponds to the Khovanov homology).\n\nSECTION: 6Summary and discussions\n\nExtending the previous study[14], we found new infinite families of hyperbolic knots and links whose HOMFLY\u2013PT polynomial has a factorised HZ transform. We prescribed methods for generating them involving full twists and the Jucys-Murphy braid. Moreover, we have derived a closed formula (38) for the HOMFLY\u2013PT polynomial for such families, obtained via inverse HZ transform.\n\nFurthermore, we discovered a\nrelation between the HZ exponents and Khovanov homology, as the former coincide with the values ofin the rows of the Khovanov table.\nIn fact, in the factorised cases, the HZ transform and hence the HOMFLY\u2013PT polynomial, is equivalent to just the Jones polynomial.\nIn an upcoming paper[22]we shall discuss the relation between the HZ transform and Khovanov homology for more general knots that do not admit HZ factorisability.\n\nWe have proven a relation (59) between the Kauffman and HOMFLY\u2013PT polynomials for several factorised families of knots, which also reduces to a relation (76) between the Alexander and Kauffman polynomials. The HOMFLY\u2013PT - Kauffman relation can conjecturally be extended into a condition for factorisability of the HZ transform. We showed that it is equivalent to the vanishing of the number of two-crosscap BPS states in topological string theory.\nIn the case of links, a similar relation (82) to the to Kauffman polynomial\nhas been found which, again, seems to correspond to the vanishing of the two-crosscap BPS invariant.\n\nThe connection between the factorised HZ transform and\nthe vanishing of the two-crosscap BPS degeneracy in a-brane of topological strings, may be interpreted as\nsupersymmetry manifesting in the HZ transform, viewed as a Hilbert (Poincar\u00e9, Molien) series. The property of BPS states which we have discussed, shows a relation between knots, links and supersymmetric gauge theory, similar to the relation between the HOMPLY-PT polynomial and Wilson loops. The suggestion of the new interpretation of Khovanov homology by a time-dependent gauge theory[44]is attractive. Further investigation may provide the connection to supermatrices, which is realised in the intersection numbers of a moduli space[39,45,46,47].\n\nThe authors have no competing interest to disclose.\n\nAcknowledgements:We are grateful to Andrew Lobb for several useful discussions\nand we thank Michael Willis for pointing out the Jucys\u2013Murphy braid to us. We also thank Alexei and Andrey Morozov for providing us with their knot data tables.\nWe acknowledge Dror Bar-Natan for his several suggestive lectures, including the one at the University of Tokyo-OIST Symposium in September 2023. We also thank Stavros Garoufalidis for his suggestion of several computer softwares for knot invariants. We are grateful to Reiko Toriumi and her unit members, especially Cihan Pazarba\u015f\u0131, for constructive feedback and for assisting with coding.\nThis work is supported by the funding of OIST, and by JSPS Kakenhi-19H01813.\n\nAppendix: BPS invariants for links\n\nThe BPS invariants were introduced in Sec.4.2. Here we list the results obtained via (98) and (100)-(101) for the links that admit a factorisable HZ transform. In all cases, with the only exception being, we find.;\n\n;\n\nFrom the above two cases and also from the results forandgiven in (106) and below, we can observe that the torus linksfollow the pattern shown below, in which the sign of non-zero entries is indicated.\n\n;\n\n;\n\n;\n\n;\n\n;\n\n:g \\Q024605-1515-515-2020-521- 88- 130-110\n\n;\n\n;\n\n;\n\nObserve that the BPS invariants for the family(c.f.above andin Sec.4.2) again follow a certain pattern. This involves an alternating sign in each column and the fixed entries oforat the bottom left corner forand, respectively.;\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.04933v1_content.txt"}, {"title": "Dynamics of Aggregation Processes and Electrophysical Properties of\n  Transformer Oil-Based Magnetic Fluids", "authors": ["Alexander D. Kurilov", "Anastasia V. Gubareva", "Sergei A. Zubkov", "Yulia A. Alekhina", "Alexander V. Simakin", "Denis N. Chausov"], "published_date": "2024-12-06T10:06:58Z", "summary": "Magnetic fluids exhibit tunable structures and electrophysical properties,\nmaking them promising for adaptive optical systems, biomedical sensors, and\nmicroelectromechanical devices. However, the dynamic evolution of their\nmicrostructure under varying magnetic fields remains insufficiently explored.\n  This study investigates the structural and dielectric properties of\ntransformer oil-based magnetic fluids containing 0.2-10 vol% magnetite\nnanoparticles, across a frequency range of 20 Hz to 10 MHz. Particular\nattention is given to the dynamics of aggregate reorientation in response to\nalternating magnetic fields. Experimental results demonstrate that low\nnanoparticle concentrations lead to a linear increase in dielectric\npermittivity and conductivity, consistent with the Maxwell-Wagner model. In\ncontrast, higher concentrations exhibit conductivity saturation and dispersion\neffects due to the formation of elongated aggregates.\n  An analysis based on the Boyle polarization model describes the relaxation\nand structural changes associated with aggregation dynamics. Changes in the\nmagnetic field orientation induce aggregate reconfiguration and significant\nstructural transformations. At early stages, elongated chains form,\nsubsequently thickening until an equilibrium state is reached. Elevated\ntemperatures accelerate these processes by reducing medium viscosity and\naggregate order.\n  The findings highlight the critical role of reorientation dynamics in\ndesigning high-speed magnetic sensors, vibration isolation systems, and\nadaptive devices operating in dynamic magnetic environments.", "arxiv_id": "2412.04911v1", "html_link": "https://arxiv.org/html/2412.04911v1", "search_term": "ti:\"transformers\"", "html_content": "SECTION: Dynamics of Aggregation Processes and Electrophysical Properties of Transformer Oil-Based Magnetic Fluids\n\nMagnetic fluids exhibit tunable structures and electrophysical properties, making them promising for adaptive optical systems, biomedical sensors, and microelectromechanical devices. However, the dynamic evolution of their microstructure under varying magnetic fields remains insufficiently explored.\n\nThis study investigates the structural and dielectric properties of transformer oil-based magnetic fluids containing 0.2\u201310 vol% magnetite nanoparticles, across a frequency range of 20 Hz to 10 MHz. Particular attention is given to the dynamics of aggregate reorientation in response to alternating magnetic fields. Experimental results demonstrate that low nanoparticle concentrations lead to a linear increase in dielectric permittivity and conductivity, consistent with the Maxwell-Wagner model. In contrast, higher concentrations exhibit conductivity saturation and dispersion effects due to the formation of elongated aggregates.\n\nAn analysis based on the Boyle polarization model describes the relaxation and structural changes associated with aggregation dynamics. Changes in the magnetic field orientation induce aggregate reconfiguration and significant structural transformations. At early stages, elongated chains form, subsequently thickening until an equilibrium state is reached. Elevated temperatures accelerate these processes by reducing medium viscosity and aggregate order.\n\nThe findings highlight the critical role of reorientation dynamics in designing high-speed magnetic sensors, vibration isolation systems, and adaptive devices operating in dynamic magnetic environments.\n\nKeywordsMagnetic fluidsDielectric spectroscopyAggregation dynamicsInterfacial polarizationRelaxation processes\n\nSECTION: 1Introduction\n\nMagnetic fluids are colloidal suspensions of magnetic nanoparticles stabilized by surfactants in a carrier liquid. These systems possess unique properties, including the ability to dynamically alter their structure and macroscopic characteristics in response to external magnetic fields. As a result, magnetic fluids find applications across a wide range of fields, from magnetorheological devices and sensors to electronics and biomedical technologies[22,45,39,34].\n\nAmong these applications, particular attention has been paid to the dielectric properties of magnetic fluids in variable electric and magnetic fields, as these investigations offer critical insights into the mechanisms of polarization, relaxation, and energy dissipation in such systems[28,5,29]. Dielectric spectroscopy is a powerful tool for exploring the frequency-dependent behavior of magnetic fluids. This technique is highly sensitive to polarization and relaxation processes arising from interactions among particles, their aggregates, and the carrier medium[4,40,41].\n\nMagnetic fluids exhibit a complex interplay of factors influencing their dielectric response, including nanoparticle concentration, aggregation processes, and the anisometric nature of their internal microstructure. For instance, the formation of chains or clusters of magnetic nanoparticles under a magnetic field induces dielectric anisotropy, which depends on the field\u2019s strength and orientation. As the field intensity increases, the system transitions from isolated particles to microclusters, and ultimately to elongated structures. This induced dielectric anisotropy is driven by differences in polarization mechanisms along and perpendicular to the field direction, as well as charge transfer between particles within clusters[29,10].\n\nThe formation of anisotropic structures in magnetic fluids and their impact on dielectric properties have been well described. Previous studies have highlighted the role of dipole-dipole interactions in forming elongated aggregates under magnetic fields, significantly altering the effective dielectric parameters of magnetic fluids[13,42]. These effects are particularly pronounced in systems with high particle concentrations, where interactions between aggregates amplify the anisotropy of dielectric properties.\n\nInduced dielectric anisotropy is strongly frequency-dependent and is associated with two main mechanisms: interfacial polarization (Maxwell-Wagner polarization[27,15]) and hopping ionic conductivity[11,36]. Both mechanisms dominate at low frequencies and are enhanced in systems with high conductivity contrast between the particles and the carrier medium. At higher frequencies (above 1 GHz), interfacial polarization occurs at the level of individual nanoparticles. Therefore, broadband dielectric spectroscopy is the most accurate method to capture structural changes within the system as a function of concentration and magnetic field direction, consistent with effective medium approximations such as the Bruggeman model[36,49,26,20].\n\nSubsequent studies expanded these findings by incorporating models that relate cluster geometry to the frequency-dependent anisotropy of dielectric properties (e.g., Hanai\u2019s model)[1,30]. These results underscore the importance of considering cluster anisometry and orientation for accurately describing the dielectric properties of magnetic fluids. Cluster stability is determined not only by dipole-dipole interactions but also by van der Waals forces at small particle separations (approximately 1 nm)[36].\n\nTheoretical approaches to describing these systems have evolved over time, incorporating factors such as particle shape distribution, material properties, and the presence of nanoparticle coatings. For instance, an approach developed for studying biological systems using dielectric spectroscopy has been applied to analyze membranes, liposomes, cells, and microcapsules[2].\n\nA crucial factor in low-frequency relaxation for dispersed systems is the electric double layer (EDL), which forms around charged nanoparticles[12]. The EDL comprises a compact layer of counterions and a diffuse outer layer, both of which contribute to the overall dielectric response. Interfacial polarization mechanisms within clusters must be accounted for, especially at low frequencies, where EDL polarization becomes prominent[42,21,51,43].\n\nWhile many studies have focused on static or equilibrium structures, the dynamics of reorientation and microstructural evolution under changing magnetic field directions have received less attention. This study addresses this gap by examining the dielectric properties of transformer oil-based magnetic fluids over a wide concentration range (0.2\u201310 vol%) with a focus on reorientation dynamics and microstructural formation immediately following changes in magnetic field direction. Measurements were conducted across a frequency range of 20 Hz to 10 MHz, capturing both low-frequency interfacial polarization effects and EDL contributions. High-frequency parasitic effects were carefully mitigated to ensure measurement accuracy[9].\n\nTo interpret the observed phenomena, this study employs the Boyle polarization model[8], which provides deeper insights into microstructural formation, relaxation processes, and dielectric anisotropy in high-concentration magnetic fluids.\n\nSECTION: 2Materials and Methods\n\nSECTION: 2.1Magnetic Fluid Samples\n\nTransformer oil-based magnetic fluid (MFTO) samples with magnetite (\\ceFe3O4) nanoparticles were synthesized by the Problem Research Laboratory of Applied Ferrohydrodynamics at Ivanovo State Power Engineering University. The synthesis employed chemical precipitation from a liquid phase in the presence of a surfactant, with oleic acid used as a stabilizer. The magnetite concentration in the samples ranged fromto.\n\nSECTION: 2.2Magnetic Properties\n\nThe magnetic properties of the fluids were studied using a Lake Shore 7410 vibrating sample magnetometer. Measurements were conducted at room temperature across a field range ofto. The resulting magnetization curves were analyzed to determine magnetic susceptibility, saturation magnetization, and to perform magnetogranulometric analysis.\n\nSECTION: 2.3Density Measurements\n\nSample density was measured using the pycnometric method over a temperature range ofto. Aglass pycnometer was immersed in a liquid cryothermostat (LOIP FT-316-25). Following temperature stabilization to within, the pycnometer was weighed on an analytical balance (Gosmetr VL-220M) with a precision of.\n\nSECTION: 2.4Transmission Electron Microscopy (TEM)\n\nMorphological analysis was performed using a LIBRA 200 FE HR transmission electron microscope (Carl Zeiss AG) at an accelerating voltage ofand a resolution of. Sample preparation involved diluting the magnetic fluid with pure transformer oil, precipitating the particles, and repeatedly washing with dimethylformamide (DMF) before reprecipitation. A drop of the resulting solution was placed on a TEM grid and air-dried. The average particle size calculated from TEM images was.\n\nSECTION: 2.5Dielectric Spectroscopy\n\nThe dielectric properties of the magnetic fluids were investigated using a precision impedance analyzer (WK65120P) across a frequency range of\u2013. To avoid nonlinear and orientational effects, the test signal voltage was limited to. The measurement cell consisted of a parallel-plate capacitor filled with the sample under study.\n\nDuring experiments, the capacitance and conductivity values were recorded to calculate the real and imaginary components of the dielectric permittivity[3]:\n\nwhereis the capacitance of the empty cell,is the capacitance of the filled cell,is the parasitic capacitance,is the conductivity, andis the angular frequency of the test signal.\n\nThe filled cell was sealed and placed in a thermostatic circuit with temperature controlled by a flow thermostat (LOIP FT-316-25) over a range ofto. Each sample was thermostated for at least 10 minutes before measurements, ensuring temperature stabilization to.\n\nTo minimize high-frequency distortions, a compensation method for parasitic inductance and resistance effects was applied, as described in previous studies[37,38]. Dielectric spectra of magnetized magnetic fluid samples were recorded after preconditioning in a magnetic field with a strength offor at least 48 hours, ensuring equilibrium aggregation states.\n\nMeasurements were automated using custom Python-based software, providing high accuracy and reproducibility. For complete spectral analysis, 100 points were evenly distributed on a logarithmic scale across the frequency range ofto. For dynamic measurements of time-dependent characteristics, fixed frequencies of,,,,, andwere used[24].\n\nSECTION: 3Results and discussion\n\nSECTION: 3.1General Characterization of Samples\n\nFigure1presents the temperature-dependent (a) and concentration-dependent (b) variations in the density of magnetic fluids. The temperature dependencies (Figure1a) exhibit a linear decrease in density with rising temperature, which is attributed to the thermal expansion of the carrier medium. The concentration dependencies (Figure1b) show that density increases with the volume fraction of magnetite nanoparticles, reflecting the higher density of magnetite compared to the carrier fluid. Solid lines in Figure 1b represent calculated values based on a linear density mixing model. The excellent agreement between experimental and calculated data confirms the accuracy of the reported magnetite concentrations and their consistency with expected values.\n\nFigure2shows the magnetization curves of the magnetic fluid samples measured at room temperature. The curves exhibit typical superparamagnetic behavior with negligible hysteresis and an almost zero coercive force, indicating that the magnetic core size of the nanoparticles does not exceed 25\u00a0nm[19]. Magnetization curve analysis was used to determine the initial magnetic susceptibility (). For dilute samples,increases linearly with the nanoparticle volume fraction (), indicating minimal interparticle interactions. However, in concentrated systems, dipole-dipole interactions become significant and are described by a second-order modified mean-field theory[48,17]. This model provides the best fit to the experimental data and allows estimation of the interparticle interaction parameter (), which was determined to be(SI units).\n\nFigure3illustrates the concentration-dependent initial magnetic susceptibility () (a) and saturation magnetization () (b). The concentration dependence of(Figure3a) confirms a linear increase up to 2\u00a0vol%, indicating weak interparticle interactions. Beyond this concentration,exhibits an additional increase consistent with the modified mean-field theory. Figure3b reveals that the saturation magnetization of the nanoparticles is approximately 20% lower than that of bulk magnetite (). This reduction is attributed to surface effects, such as spin frustration and the presence of defects on the nanoparticle surfaces[25,44].\n\nFigure4displays the particle size distribution analysis. Magnetic granulometric analysis (Figure4a) determined the probability density function of magnetic core sizes, with an average size of 5\u00a0nm. Comparisons with transmission electron microscopy (TEM) images (Figure4b) indicated larger nanoparticle sizes (approximately 16\u00a0nm). This discrepancy arises because TEM measures the total particle diameter, including any surfactant or aggregated layers, whereas magnetic granulometry reflects only the magnetic core size. Additionally, sample preparation steps for TEM, such as washing and sedimentation, may have contributed to the observed particle size increase due to aggregate formation.\n\nSECTION: 3.2Dielectric Spectroscopy of Non-Magnetized Magnetic Fluids\n\nDielectric spectroscopy is a powerful method for studying the electrical properties of materials, allowing the determination of parameters such as dielectric permittivity and conductivity, which play a critical role in understanding a system\u2019s behavior under an electric field.\n\nFigure5shows the temperature dependencies of dielectric permittivity (a) and ionic conductivity (b) for magnetic fluids with varying volume fractions of magnetite. Across all samples, the temperature dependence of dielectric permittivity is negligible. The carrier medium (transformer oil, TO) is characterized by minimal dielectric permittivity values (), which increase significantly upon the addition of magnetite nanoparticles. At 10\u00a0vol% magnetite, the permittivity reaches a maximum value (), driven by nanoparticle-induced polarization and additional mechanisms such as interfacial polarization (Maxwell-Wagner mechanism[1,30]) and the contribution of the electric double layer (EDL) surrounding the nanoparticles[12,21,51].\n\nThe ionic conductivity of all investigated systems increases with temperature due to the enhanced kinetic energy of ions, which facilitates diffusion. The pure carrier medium exhibits minimal ionic conductivity (), which increases gradually with temperature. The introduction of magnetite nanoparticles causes a sharp rise in conductivity; at 1\u00a0vol%, conductivity increases 10\u201360 times, depending on temperature. This behavior is attributed to the formation of additional charge transfer pathways via ions adsorbed on nanoparticle surfaces. However, further increases in magnetite concentration lead to saturation or even a decline in conductivity. For samples containing 1\u201310\u00a0vol% magnetite, conductivity stabilizes atat room temperature. This decrease may result from ion trapping by nanoparticles, whose surfactant-coated surfaces participate in ion exchange or create localized ion traps[50]. Concentration-dependent dielectric permittivity and ionic conductivity at room temperature are shown in Figure6for clarity.\n\nFigure7depicts the spectra of the real (a) and imaginary (b) components of dielectric permittivity for non-magnetized magnetic fluids at. Two characteristic dispersion regions are observed: a low-frequency region (below) and a high-frequency region ().\n\nIn the low-frequency region, the primary mechanism is the polarization of the electric double layer. This effect manifests as a characteristic increase in dielectric permittivity and losses with increasing magnetite concentration, caused by charge accumulation at phase boundaries facilitated by the EDL around nanoparticles. Consequently, EDL polarization intensifies with the number of nanoparticles.\n\nIn the intermediate and high-frequency regions (), pronounced dielectric dispersion is accompanied by significant increases in dielectric losses. This behavior is attributed to relaxation processes associated with interfacial polarization of nanoparticle aggregates. At high nanoparticle concentrations, the dispersion spectrum becomes broader, indicating a wide range of relaxation times. This phenomenon is well described by the Havriliak-Negami model[16], which accounts for the distribution of characteristic relaxation times in complex multicomponent systems.\n\nThe broadening of the spectrum arises from aggregation processes, where complex structures with varied relaxation times are formed. Interactions among particles within aggregates lead to significant variability in relaxation times and a widening of the dispersion spectrum. The observed increase in dielectric losses at high magnetite concentrations confirms that the primary contribution to dispersion arises from charge relaxation at aggregate-matrix interfaces.\n\nFor individual magnetite nanoparticles in transformer oil, Maxwell-Wagner relaxation occurs in the GHz range[14,31], excluding its contribution to the observeddispersion. However, in aggregates, the characteristic relaxation time increases due to collective charge accumulation effects and weakened interaction with the surrounding medium. This shifts the relaxation frequency to the range corresponding to the observed dispersion. Thus, at high magnetite concentrations, aggregates dominate the dielectric properties of the magnetic fluid.\n\nSimilar polarization mechanisms are observed in magnetized magnetic fluids. These include EDL polarization, interfacial polarization of anisometric aggregates, and single nanoparticle polarization. These processes underlie the dispersive behavior and unique properties of magnetic fluids in variable magnetic fields. A schematic representation of the polarization mechanisms for magnetic fluids containing ellipsoidal aggregates is shown in Figure8.\n\nSECTION: 3.3Dielectric Spectroscopy of Magnetized Magnetic Fluids\n\nThe temperature dependencies of dielectric permittivity and ionic conductivity for magnetized magnetic fluids under parallel (HT) and perpendicular (HG) magnetic field orientations are shown in Figures9a and9b, respectively. Similar to non-magnetized fluids, the dielectric permittivity exhibits negligible temperature dependence. However, its values vary significantly with the orientation of the magnetic field. In the parallel orientation (HT), the dielectric permittivity is substantially higher than in the perpendicular orientation (HG). This difference arises from the formation of elongated nanoparticle aggregates aligned along the magnetic field, enhancing the contribution of interfacial polarization in the parallel configuration[46,23].\n\nIonic conductivity, on the other hand, increases with temperature, showing relatively small differences between the parallel and perpendicular orientations. This behavior can be attributed to the limited impact of aggregate orientation on ion migration, unlike its more pronounced influence on the system\u2019s polarization properties.\n\nDielectric spectra of magnetized magnetic fluids with varying nanoparticle concentrations and magnetic field orientations, measured at, are shown in Figure10. The primary differences between non-magnetized and magnetized fluids lie in the appearance of dielectric anisotropy and variations in the strength of dielectric relaxation.\n\nDielectric anisotropy, observed as the difference in permittivity between parallel and perpendicular orientations, exhibits a clear frequency dependence: it decreases with increasing frequency. This behavior is attributed to the diminishing contribution of interfacial polarization from elongated aggregates at higher frequencies.\n\nDielectric losses, shown in Figure10b, also depend on the magnetic field orientation. In the frequency range of, losses in the parallel orientation significantly exceed those in the perpendicular orientation, reflecting stronger relaxation processes under parallel alignment. This is due to the higher effective polarization of aggregates when the magnetic field is parallel to the electric field.\n\nTemporal changes in dielectric permittivity (10\u00a0kHz) and dielectric losses (20\u00a0Hz) of a magnetic fluid with a 10\u00a0vol% magnetite concentration under abrupt switching of the magnetic field orientation (from parallel to perpendicular and vice versa) are presented in Figure11. Data are shown for two temperatures:(Figures11a and11b) and(Figures11c and11d). These results illustrate the dynamics of aggregate reorientation and structural evolution under the influence of the magnetic field. The formation of the internal structure of magnetic fluids is governed by diffusion processes[46], which can require significant time, often up to tens of hours.\n\nThe time-dependent permittivity curves are well-approximated by a two-phase exponential decay function with a temporal offset, as indicated by solid lines on the graphs. At, the characteristic relaxation times areand, reflecting the relatively slow restructuring of aggregates due to the high viscosity of the fluid. At, the formation of a stable structure is significantly accelerated, with relaxation times ofand. This acceleration is attributed to the reduced viscosity of the carrier medium at elevated temperatures, facilitating nanoparticle aggregate reconfiguration in the magnetic field.\n\nSECTION: 3.4Discussion of Results\n\nFigure12presents the dielectric spectrum of a magnetic fluid with 10 vol% magnetite concentration at, approximated using the generalized Havriliak-Negami model. The solid red line represents the approximation results, accounting for relaxation time distributions and asymmetry in the dispersion curve[16]. According to this model, the dielectric spectrum is described by the equation\n\nwhereis the high-frequency limit of dielectric permittivity,is the relaxation strength,is the static (low-frequency) permittivity,is the relaxation time,specifies the relaxation time distribution\u2019s width, whileaccounts for the dispersion curve\u2019s asymmetry.\n\nTo accurately describe the experimental data, additional mechanisms that significantly influence the dielectric spectrum were considered. At low frequencies (), electrode polarization contributes substantially to the real part of the permittivity. This effect, proportional to, whereandare empirical parameters, reflects charge accumulation at the electrode surfaces or ion migration processes. This empirical correction accounts for electrode polarization phenomenologically.\n\nAdditionally, ionic conductivity significantly affects dielectric loss dispersion at low frequencies. This behavior is described using the model\n\nwhereis the static conductivity,is the ionic hopping frequency,is the vacuum permittivity, andcharacterizes the frequency dependence. In the absence of electrode polarization, (, corresponding to ohmic conductivity.\n\nAt high frequencies (), parasitic contributions from resonances in the measurement cell emerge, related to the cell\u2019s inductance () and resistance (). These effects were accounted for following methods outlined in previous studies[37,38].\n\nCombining these factors yielded excellent agreement between experimental data and the theoretical model, shown by the solid blue line in Figure12.\n\nTo further analyze the dielectric properties, Boyle\u2019s model for polarizable systems with ellipsoidal particles was employed[8]. The effective permittivityof a suspension of oriented ellipsoids is given by[2]\n\nwhereandare the complex permittivities of the medium and aggregates, respectively,is the aggregate volume fraction, andis the depolarization factor dependent on aggregate shape[32].\n\nIn numerical modeling, the aggregate volume fraction was assumed equal to the magnetite concentration, while the effective permittivities of the aggregates and medium, along with the aspect ratio of ellipsoids, were treated as variables. The results validated the model\u2019s suitability, enabling detailed descriptions of relaxation processes in magnetic fluids.\n\nFigure13shows the temperature dependencies of dielectric anisotropy (a) and the aspect ratio of magnetically induced aggregates (b) obtained from the dielectric spectra of magnetic fluids with 10\u00a0vol% magnetite. Dielectric anisotropy, representing the difference in the system\u2019s response to an external electric field along different directions, indicates the internal structural order. The analysis reveals a decrease in anisotropy with increasing temperature, attributed to the reduction of the interparticle interaction parameter (), which leads to a loss of structural order in magnetically induced aggregates. This trend is supported by the decreasing aspect ratio of aggregates, indicating a transition from elongated structures to more compact forms.\n\nFigure14a presents time-dependent dielectric permittivity (10\u00a0kHz) analyzed using the Boyle model. The results, represented by solid lines, demonstrate the synchronous approximation for parallel and perpendicular orientations. The data indicate significant structural changes in the magnetic fluid upon abrupt changes in magnetic field orientation.\n\nAt the initial stages, elongated chain-like aggregates form rapidly (Figure14b), subsequently thickening and reaching equilibrium over several hours. This process aligns with experimental observations and theoretical models describing chain-like structure formation and phase rearrangements in moderate magnetic fields[18,47,35].\n\nIt is important to note that aggregate aspect ratio data are qualitative due to limitations in dynamic measurements, including the restricted frequency range, potentially affecting the accuracy of time-dependent approximations. Furthermore, aggregate size heterogeneity, confirmed by optical experiments on magnetorheological fluids, adds complexity to data interpretation[7,6,33].\n\nThese findings confirm that changes in external magnetic fields significantly alter the dielectric and structural properties of magnetic fluids. This underscores the critical role of aggregation processes in accurately predicting the performance of magnetic fluid devices, particularly under dynamic field conditions.\n\nSECTION: 4Conclusion\n\nThis study provides a comprehensive investigation of the structural and dielectric properties of transformer oil-based magnetic fluids over a wide concentration range. Particular attention was given to the dynamic aggregation processes of nanoparticles under magnetic fields and their influence on the system\u2019s electrophysical characteristics.\n\nDielectric spectroscopy revealed that increasing the concentration of magnetite nanoparticles leads to nonlinear changes in the effective dielectric permittivity. At low concentrations, both dielectric permittivity and ionic conductivity exhibit an almost linear increase, consistent with the Maxwell-Wagner model. At higher concentrations, effects such as conductivity saturation and dispersion in dielectric spectra emerge, driven by the formation of elongated aggregates. Analysis of the dielectric spectra demonstrated that the observed relaxation processes are caused by interfacial polarization of aggregates and charge accumulation in the double electrical layer.\n\nDielectric spectroscopy of magnetized samples showed that the orientation of the magnetic field significantly influences the internal structure and properties of magnetic fluids. The external magnetic field induces the formation of elongated structures, leading to frequency-dependent dielectric anisotropy. At higher frequencies, the contribution of interfacial polarization from elongated aggregates diminishes, reducing the dielectric anisotropy.\n\nThe results were analyzed using Boyle\u2019s polarization model, which provided deeper insights into microstructure formation, relaxation processes, and dielectric anisotropy in high-concentration magnetic fluids. It was established that increasing temperature reduces aggregate ordering due to weakened interparticle interactions. This effect is corroborated by the decreasing aspect ratio of aggregates, indicating a transition from elongated aggregates to more compact structures.\n\nDynamic dielectric spectroscopy illustrated the complex reorganization dynamics of magnetically induced aggregates and their growth evolution under the influence of magnetic fields. Data analysis, based on Boyle\u2019s model, shows that abrupt changes in magnetic field orientation induce significant alterations in the internal structure of magnetic fluids. At initial stages, elongated chain-like aggregates form rapidly and then thicken, reaching equilibrium over tens of hours.\n\nThis study advances the understanding of relaxation mechanisms, interfacial polarization, and aggregation dynamics in magnetic fluids. The findings have important implications for the design of high-performance devices operating in dynamic magnetic and electric fields, including sensors, magnetorheological devices, and electronic components.\n\nSECTION: Acknowledgments\n\nThis research was supported by the Russian Science Foundation grant No. 24-29-00178 (https://rscf.ru/project/24-29-00178/).\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.04911v1_content.txt"}, {"title": "TPRF: A Transformer-based Pseudo-Relevance Feedback Model for Efficient\n  and Effective Retrieval", "authors": ["Hang Li", "Chuting Yu", "Ahmed Mourad", "Bevan Koopman", "Guido Zuccon"], "published_date": "2024-01-24T15:06:44Z", "summary": "This paper considers Pseudo-Relevance Feedback (PRF) methods for dense\nretrievers in a resource constrained environment such as that of cheap cloud\ninstances or embedded systems (e.g., smartphones and smartwatches), where\nmemory and CPU are limited and GPUs are not present. For this, we propose a\ntransformer-based PRF method (TPRF), which has a much smaller memory footprint\nand faster inference time compared to other deep language models that employ\nPRF mechanisms, with a marginal effectiveness loss. TPRF learns how to\neffectively combine the relevance feedback signals from dense passage\nrepresentations. Specifically, TPRF provides a mechanism for modelling\nrelationships and weights between the query and the relevance feedback signals.\nThe method is agnostic to the specific dense representation used and thus can\nbe generally applied to any dense retriever.", "arxiv_id": "2401.13509v2", "html_link": "https://arxiv.org/html/2401.13509v2", "search_term": "ti:\"transformers\"", "html_content": "", "text_file": "data\\paper_texts\\2401.13509v2_content.txt"}, {"title": "DrIFT: Autonomous Drone Dataset with Integrated Real and Synthetic Data,\n  Flexible Views, and Transformed Domains", "authors": ["Fardad Dadboud", "Hamid Azad", "Varun Mehta", "Miodrag Bolic", "Iraj Mntegh"], "published_date": "2024-12-06T05:47:55Z", "summary": "Dependable visual drone detection is crucial for the secure integration of\ndrones into the airspace. However, drone detection accuracy is significantly\naffected by domain shifts due to environmental changes, varied points of view,\nand background shifts. To address these challenges, we present the DrIFT\ndataset, specifically developed for visual drone detection under domain shifts.\nDrIFT includes fourteen distinct domains, each characterized by shifts in point\nof view, synthetic-to-real data, season, and adverse weather. DrIFT uniquely\nemphasizes background shift by providing background segmentation maps to enable\nbackground-wise metrics and evaluation. Our new uncertainty estimation metric,\nMCDO-map, features lower postprocessing complexity, surpassing traditional\nmethods. We use the MCDO-map in our uncertainty-aware unsupervised domain\nadaptation method, demonstrating superior performance to SOTA unsupervised\ndomain adaptation techniques. The dataset is available at:\nhttps://github.com/CARG-uOttawa/DrIFT.git.", "arxiv_id": "2412.04789v1", "html_link": "https://arxiv.org/html/2412.04789v1", "search_term": "ti:\"transformers\"", "html_content": "SECTION: DrIFT: AutonomousDrone Dataset withIntegrated Real and Synthetic Data,Flexible Views, andTransformed Domains\n\nDependable visual drone detection is crucial for the secure integration of drones into the airspace. However, drone detection accuracy is significantly affected by domain shifts due to environmental changes, varied points of view, and background shifts. To address these challenges, we present the DrIFT dataset, specifically developed for visual drone detection under domain shifts. DrIFT includes fourteen distinct domains, each characterized by shifts in point of view, synthetic-to-real data, season, and adverse weather. DrIFT uniquely emphasizes background shift by providing background segmentation maps to enable background-wise metrics and evaluation. Our new uncertainty estimation metric, MCDO-map, features lower postprocessing complexity, surpassing traditional methods. We use the MCDO-map in our uncertainty-aware unsupervised domain adaptation method, demonstrating superior performance to SOTA unsupervised domain adaptation techniques. The dataset is available at:https://github.com/CARG-uOttawa/DrIFT.git.\n\nSECTION: 1Introduction\n\nUncrewed Aerial Vehicles (UAVs), also known as drones, have gained popularity in recent years due to their versatility and cost-effectiveness for various operations[19,67], including healthcare[46], surveillance[17,10], delivery[40], agriculture[5,3], construction and mining[58,9], infrastructure inspection[2], and search-and-rescue[39]. However, their ubiquitous use has raised safety concerns, such as the possibility of their use for malicious activities and collisions with other objects in the airspace[30,63]. Achieving autonomous flight capabilities in challenging environments, for individual and swarm drones, is vital for various applications[26].\nEnsuring the safety of such operations depends on the accurate and efficient processing of drone-related data.\n\nIn particular, vision-based drone detection plays a crucial role, as it faces challenges such as distant small objects, handling complex backgrounds (BGs), and distinguishing drones from other visually similar flying objects. Deep Neural Networks (DNNs) have demonstrated exceptional capabilities in multiple applications, including drone detection[7,12,16,28]. However, distribution shifts from the training to the test set, caused by environmental variations, various points of view (PoVs), and background changes, pose intrinsic challenges in drone detection and affect the DNNs capabilities. Specifically, BG shift,e.g.training with data mostly captured with sky background while sky, tree, and ground backgrounds appear in the validation set, is also called unseen BG[68]. Gathering supervised data for all domains to ensure DNN generalization is impractical and often costly for data collection and annotation[36,50,41], especially, it is worsened in adverse weather conditions or under regulatory constraints for drone-based applications.\n\nUnsupervised Domain Adaptation (UDA)[41,6,8,22,43]is a principal approach to addressing domain shift (DS) in object detection (OD). Domain shift refers to shifts in the input image due to environmental factors that affect the performance of drone detection due to their impact on the drone\u2019s appearance in the scene. UDA aims to transfer knowledge from the source to the target domain, despite the lack of supervision in the target domain. This approach has gained popularity in applications such as autonomous vehicles[61,20,55,54]and other edge-AI, where DSs are common, and supervised data is not guaranteed[42,14]. UDA methods have also been employed extensively to address DSs in drone detection[52,53]. However, unforeseen situations that cause DSs, such as drones with novel shapes, can still occur. Despite the trend toward using UDA in the field, there is a lack of comprehensive exploration of specialized domain shift and UDA methodologies in drone detection[52,60,28,71,51,7,65,62,49,69]. This gap has catalyzed our work to design a new dataset that addresses these specific challenges.\n\nCombining existing datasets often results in multiple uncontrolled DSs co-occurring, making it difficult to isolate and examine the impact of specific shifts. Moreover, existing datasets lack systematic background segmentation and comprehensive coverage of DS types, making manual annotations costly and infeasible. To overcome this, the DrIFT dataset was designed to provide a controlled environment in which individual DS types, PoV, season, weather, and background can be studied independently. DrIFT ensures a balanced distribution across its fourteen distinct domains, addressing limitations in previous datasets and enabling the systematic study of multiple DS types simultaneously. Driven by the need to address these shortages, we present the DrIFT dataset with the following pivotal contributions:\n\nThe DrIFT dataset introduces a vision-based drone detection dataset. Uniquely, DrIFT comprises fourteen distinct domains constructed by combinations of four major domain shifts: PoV, synthetic-to-real, season, and weather. In most domains, there are sky, trees, and ground backgrounds.\n\nWe employ BG segmentation maps to introduce the concept of BG shift as a distinct challenge. This novel approach allows us to report BG-wise metrics (e.g.,:of sky background detections), providing a focused study on how\nBG shift\ninfluence the object detection.\n\nWe introduce a novel uncertainty evaluation method for OD, surpassing existing methods (Tab.3). Our method, utilizing a score map, offers significant advantages such as lower complexity of postprocessing and superior capability in capturing DS.\n\nOur uncertainty-aware UDA method\noutperforms state-of-the-art (SOTA) UDA methods for drone detection (Tab.4).\n\nSECTION: 2Related Work\n\nSECTION: 2.1Drone Datasets\n\nDrone datasets have recently become publicly available to address the increasing interest in drone detection[29]. Many datasets have significant limitations, as highlighted inTab.1. For instance, the dataset in[12]lacks certain weather conditions and uses a stationary camera. The datasets in[60]and[28]are limited to a single PoV. The dataset in[71]is restricted to partly cloudy and clear weather. Other datasets[51,7,65]offer limited diversity in weather and PoV. UAV-200[52]uses supervised domain adaptation with a fraction of the target domain during training and only examines the synthetic-to-real DS, while DrIFT studies four types of domain shift in a UDA manner. The number of DS types studied is indicated inTab.1. The datasets in[62]and[69]feature multiple drone models but lack comprehensive DSs. The datasets in[1]and[16]focus primarily on ground PoV videos with limited weather conditions.[4]lacks real-world domain.\n\nTheDrIFT datasetintroduces fourteen distinct domains constructed by combinations of four major DS elements: PoV, synthetic-to-real, season, and weather, with sky, trees, and ground backgrounds (Fig.1). DrIFT uniquely emphasizes BG shift as a separate challenge and employs BG segmentation maps to create BG-wise metrics. This comprehensive approach addresses the lack of datasets that study various DSs in drone detection, making DrIFT the first dataset to comprehensively study all four DSs.\n\nSECTION: 2.2Land Vehicle Datasets\n\nLand vehicle datasets constitute another topic similar to those of drones within the realm of autonomous vehicles. As inspiration for DrIFT, the SHIFT[61]autonomous driving dataset offers DSs across a spectrum of parameters, such as weather conditions, time of day, and density of vehicle and pedestrian, but does not investigate BG shifts.\n\nSECTION: 2.3Uncertainty Estimation\n\nUncertainty estimation is crucial for assessing the safety level of autonomous vehicles, especially drones, by effectively dealing with DS. Conventional methods categorize uncertainty in deep learning into aleatoric and epistemic uncertainties. Aleatoric uncertainty arises from data noise, while epistemic uncertainty is due to limited data or domain coverage, which is more relevant to DS[18].\n\nHistorically, uncertainty estimation involves sampling-based techniques like Monte Carlo dropout (MCDO)[21], which, although effective in capturing epistemic uncertainty, are computationally intensive due to their iterative nature and postprocessing complexity[23].\n\nTo address computational constraints and accurately capture the epistemic uncertainty arising from data gaps, recent studies[48,47]explore using gradient self-information directly to assess uncertainty. Nevertheless, they do not inherently encompass the true essence of uncertainty.\n\nTo address these computational constraints and the lack of a comprehensive sense of uncertainty, we leverage an efficient approach that combines the strengths of MCDO with a simplified postprocessing mechanism. Our method utilizes MCDO to generate uncertainty maps for each detection, performing multiple inference passes and aggregating these uncertainties into an overall score map[44], which reduces the postprocessing complexity (Sec.4.1).\n\nSECTION: 2.4Detection Calibration Error Estimation (D-ECE)\n\nD-ECE is critical for providing accurate confidence assessments in neural networks, especially for safety-critical applications. The calibration error measures the alignment between the predicted confidence and the actual results, helping to assess the reliability of a model[35,24]. D-ECE extends from classification-based calibration error estimation but applies specifically to detection tasks, focusing on the regression outputs of object detectors. The concept, introduced by[35], addresses unique detection confidence calibration errors. Further details on its calculation are provided inSec.4.1.\n\nSECTION: 2.5Unsupervised Domain Adaptation (UDA)\n\nUDA addresses domain shift by transferring knowledge from a labeled source domain to an unlabeled target domain. UDA for object detection was first introduced by[8].\n\nMany approaches in UDA have been introduced that come with notable limitations. Pseudo-labeling and self-training methods, such as[32]and[31], generate target pseudo-labels, but incorrect labels can propagate errors, especially in complex backgrounds like our application. Image-to-image translation techniques[25,56]reduce the domain gap by converting source images into the target style, but these often introduce artifacts and require extensive training data to perform well, which is not feasible in our application.\n\nAmong the more recent advancements, uncertainty-aware methods have gained attention for their ability to improve domain adaptation by estimating and incorporating prediction uncertainties. These methods, such as[41,22,43,6], leverage uncertainty metrics to focus on areas where domain shifts are most pronounced. Adversarial training, introduced by[8], complements this by aligning feature distributions between domains. Together, these approaches provide a robust mechanism for handling domain shift, focusing on confident regions and learning domain-invariant features to reduce errors and enhance model robustness. The details of our approach are discussed further inSec.4.1.\n\nSECTION: 3DrIFT Dataset\n\nWe have developed a vision-based drone detection dataset consisting of image frames, ground truth bounding boxes, and BG segmentation maps (Fig.1(d)). InSec.3.1, an overview of the DrIFT dataset\u2019s sensor, experimental setup, annotation, and dataset design has been presented. In the following, precise information regarding DrIFT\u2019s various domains has been compiled to represent the dataset\u2019s purpose for the DS. For more detailed statistics of DrIFT, the reader can go through the supplementary materials.\n\nSECTION: 3.1The DrIFT Story\n\nReal Ground PoV\u2019svideo recordings for the DrIFT dataset were captured with a Bosch pan-tilt-zoom (PTZ) camera. The DJI Phantom 2/3, Phantom 4/Pro, Inspire, and Mavic (Fig.1(d)) were captured between 0.1 and 1.5 kilometers away in the recordings. A drone is predominantly present in the frames. The semi-automatic annotation has been done using the CVAT[15]. We have generated multiple other domains of data in our dataset to represent the DS.\n\nReal Aerial PoVhas been added to the DrIFT dataset to achieve the PoV shift concept. For the aerial PoV, a custom-built drone model was utilized (Fig.1(d)).\nIn this experiment, mobile electro-optical cameras, the Infiniti STR-8MP-3X and GoPro were used to record multiple drone footage between 20 and 100 meters in the line of sight. The frames were recorded in different seasons, resulting in various BGs, such as the sky, trees in various seasons, and the ground with different colors.\n\nSynthetic Datais recorded in the AirSim[57]simulator for simulating real-world data counterparts in a simulated environment for all domains for considering synthetic-to-real domain shift, and due to the impossibility of flying in adverse weather conditions.\n\nIt is a common practice for domain-adaptive network training to have the same number of samples in the source and target domains[13].\nTherefore, we designed the DrIFT dataset to maintain a balanced number of samples across domains within both the training and validation sets as long as we had sufficient real data for the domains.\n\nBackground segmentation,as one of the contributions of the DrIFT dataset, is important for its innovative exploration of BG shift. All validation frames\u2019 backgrounds have been segmented into sky, tree, and ground segments (Fig.1(d)) using the Track Anything platform[66,33]. By utilizing segmentation maps, it becomes feasible to utilize different metrics corresponding to different backgrounds (Tab.2, details inSec.4.1).\n\nAll annotations were then double-checked and refined by human annotators to ensure accuracy.\n\nSECTION: 3.2Dataset Design\n\nTo address a deficiency in drone detection datasets, we designed DrIFT with a concentration on studying common domain shifts in the wild.\n\nSynthetic-to-Real:In practical scenarios, capturing every conceivable real-world situation can be infeasible due to logistical challenges, resource limitations, and the prohibitive costs of annotation. To this end, we brought up synthetic data in order to initiate research on synthetic-to-real DS.\nIn DrIFT, all real-world data domains have simulated counterparts except for adverse weather conditions that do not exist in our real-world part of the dataset.\n\nPoV Shift:The camera\u2019s PoV change (ground and aerial) contains different BGs and orientations of the target objects. This shift can significantly impact detection performance, making it a distinct type of DS.\n\nWeather Shift:drones cannot be easily deployed in adverse weather. On the other hand, because this is a common DS in the wild, the system must be robust. Therefore, synthetic data is collected to study weather DS.\n\nBackground Shift:The unseen background problem[68], also called BG shift in DrIFT, is present in various drone detection or autonomous driving datasets regarding the aforementioned DS. Nevertheless, no study has explicitly looked into the BG shift in object detection using BG segmentation maps. DrIFT investigates the BG shift from the sky to the tree and ground.\n\nSECTION: 4DrIFT Benchmark\n\nThis section first provides a comprehensive overview of the methodology used for the benchmark.\nThe following subsection provides a comprehensive overview of the different benchmark scenarios. Subsequently, the results of the benchmark are reported.\nThis section concludes with a comprehensive analysis of the benchmark outcomes and the dominant challenges of the DrIFT dataset. The supporting statements will be presented in the supplementary materials.\n\nSECTION: 4.1Methodology\n\nThe primary goal of the DrIFT benchmark is to evaluate the performance of OD models under various shifts and the capabilities of UDA methods to address this issue.\nPerformance metrics include average precision (AP), uncertainty metrics, and D-ECE which are reported BG-wise.\n\nLetbe the dataset, whereare the input images andare the set of ground truth annotations containing bounding box coordinates and class labels for objects within each input image. The OD model predicts a set of detections, where each detectionconsists of bounding box coordinates, class label, and confidence score.andare the number of samples in the dataset and the number of detections for the i-th input image, respectively.\n\nIn OD, after initial detections, the Non-Maximum Suppression (NMS) process[45]filters out redundant or suboptimal detections. NMS first generates a set of candidates for each detection. These candidates are defined as all other predictions sharing the same class label and having an Intersection-over-Union (IoU), a measure of the overlap between bounding boxes, above a threshold. Detections below a confidence thresholdare then discarded. The candidate set for a given detectionis defined as:\n\nAfter filtering, the remaining detection with the highest confidence score is retained as the final prediction. AP[45]has been employed to quantify OD performance.\n\nD-ECE[35]is just used in our benchmark to study domain shift impacts on the calibration error. D-ECE[35]was calculated by binning the confidence space as well as box coordination parameters space in which there areequally distributed bins corresponding to the k-th dimension,. The goal of binning is to account for variations in calibration error across different confidence levels and spatial dimensions, ensuring that errors are captured in an unbiased manner. Therefore, D-ECE could be formalized\n\nWithinEq.2,is used to describe the cardinality of the bin, whereasrepresents the total number of detections.denotes the mean confidence score of the detections within the bin, whereasis a statistical metric that quantifies the proportion of true positives among the detections in the bin.\n\nWe utilize MCDO-based and gradient self-information metrics to estimate uncertainty in the presence of DS and compare their capabilities with our proposed method,MCDO-map, to take advantage of them in our UDA method. The utilized methods are referred to asMCDO-NMSandGrad-loss, respectively. TheGrad-losscaptures the degree of epistemic uncertainty for each detection.Grad-loss-localizationandGrad-loss-classificationrefer to the localization and classification terms, respectively.\n\nThe MCDO-based method involves running multiple inference passes with dropout activated. Detections are matched to a candidate list based on the highest IoU threshold. The standard deviation of the localization parameters and the entropy of the mean classification probabilities are calculated for each list. This technique includesMCDO-NMS-localizationandMCDO-NMS-classification. For details on these methods, please refer to the supplementary materials.\n\nAs opposed to utilizing NMS-based or data association techniques in an MCDO scheme, a score map is constructed in a pixel-wise manner. Given the predictions, we convert the detection outputs to a 3D map. Letbe the set of detections for an input image. The score mapis a tensor of shape, whereandare the height and width of the input image, andis the number of classes. For each detection, the scoreis assigned to each pixel inside the bounding box,\n\nTheis zero initiated, resulting in all-zero vectors for pixels that are not contained within any bounding box. For these pixels, we replace the all-zero vectors with a vector with a 1 for the background element and zeros for all other elements.\nAfter populating the score map, we normalize it using the softmax function.\nNext, we calculate the mean and standard deviation of the score map over multiple iterations of our object detector forward path that are\n\nFinally, we compute the entropy of the mean score map,, and concatenate the standard deviations to create the uncertainty map,.\nFrom an intuitive standpoint, it can be observed that increasing changes in localization parameters of the predictions are associated with a corresponding increase in the standard deviation of the boundaries surrounding pixels. For example, inFig.2, the left magnified detection in the target std map shows higher deviation (with colors closer to red) compared to the source std map, where the corresponding detection is mostly blue, indicating lower deviation across pixels.\nSimilarly, a higher frequency of change in prediction scores is shown to be linked to an elevated level of entropy. The same behavior in the entropy maps can be observed inFig.2.\nIn contrast to traditional MCDO-based approaches, instead of handling individual bounding boxes from each iteration and suffering postprocessing complexity[18], our method generates a pixel-wise score map during each iteration and avoids complex postprocessing.\n\nDS occurs when the training (source) domainand the testing (target) domaindiffer, leading to a performance drop in machine learning models. Letand. We denote the source and target distributions asand, respectively. Distribution shift is defined as. If we considerand, the DS happens when\n\nOur UDA method focuses on leveraging uncertainty information to enhance the robustness of the object detector in the presence of DS. We got inspired by ADVENT[64]while modifying it by changing the representation of input data to the discriminator and introducing a novel uncertainty estimation method.\nThe intuition behind this method is that DS introduces uncertainty in predictions, especially in regions where the model is less confident. Our uncertainty maps highlight areas where the domain shift has the most impact, guiding the adaptation process to focus on these challenging regions.\nFollowing the concatenation process, theMCDO-mapis subsequently forwarded to a domain discriminator to fool it, initiating adversarial training (Fig.2). The calculation of the overall loss iswhere the detection lossis a combination of cross-entropy classification and smoothregression loss,. The adversarial lossis\n\nwhen theis the discriminator network. The detection base network is updated to minimize the total loss,, while the discriminator network is updated to maximize the adversarial loss,.\n\nSECTION: 4.2Benchmark Scenarios\n\nWe will begin our benchmark withTab.2, illustrating the impact of domain shift on object detection using the AP, uncertainty, and D-ECE metrics.Tab.3presents a comparison between our proposed MCDO-map method and other uncertainty metrics.\nFinally, our novel uncertainty-aware UDA object detector is compared with SOTA UDA methods inTab.4. Supplementary materials have been provided to support our discussions.\n\nBackground-wise Metrics:To assess OD performance under BG shifts, we introduce BG-wise metrics, which calculate metrics separately for different BGs (e.g., sky, tree, ground). Given detectionsand ground truth, we classify each into BG categories using segmentation maps to identify the background category to which most of the pixels within the bounding box belong. The metricfor each BG category can be expressed asThese metrics provide a detailed analysis of how different BGs affect object detection performance.\n\nSECTION: 4.3Experiments and Results\n\nTab.2shows different DS scenarios and their impact on object detection models. It is our contribution that metrics are reported background-wise, highlighting the influence of background shifts. The AP under the Sky column in row one is the reference. Significant AP decreases are evident, such as in row two when shifting from synthetic to real. PoV and weather shifts in rows 9 to 11 also show notable changes. Comparing the reference AP and the sky APs in other rows demonstrates decreased APs for tree and ground backgrounds (see Fig. 7 in supplementary material). The domain I inTab.2is the source domain all over the text unless other domains are mentioned.\n\nWe calculated the Kullback-Leibler (KL) divergence between source and target domain feature map distributions (see supplementary material)\nto analyze the relationship between metrics and different shifts fromTab.2.Fig.3shows a heatmap of Pearson correlations among AP, D-ECE, MCDO-map, and KL divergence. The high positive correlation between our MCDO-map and KL divergence indicates the MCDO-map\u2019s effectiveness in capturing DS.\n\nThe negative correlation of AP with MCDO-map and KL divergence suggests that higher AP corresponds to lower uncertainty and smaller feature map distribution distances. The positive correlation between AP and D-ECE indicates model miscalibration under DS. Additionally, the positive relation between D-ECE, MCDO-map, and KL divergence highlights their significant association with DS.\n\nThe goal ofTab.3is to compare our MCDO-map method with the MCDO-NMS and Grad-loss. Our method consistently shows increased uncertainty with DS, highlighting its effectiveness in capturing DSs. The wider violin plots for the MCDO-map in Fig. 4 of supplementary material demonstrate its superior capability to separate different DS levels compared to other metrics. MCDO-NMS-Classification for TPs shows some capability in separating different DSs, highlighted inTab.3and supplementary material, but requires supervision and often decreases with DS. Grad-loss-localization is consistently capturing the DS (Tab.3) but lacks the potential to separate DSs effectively.\n\nOur results inFig.3and supplementary material further support the MCDO-map\u2019s effectiveness in capturing DS. Thus, we conclude that the MCDO-map is the best method for our UDA approach, offering significant improvements over traditional uncertainty estimation techniques.\n\nInTab.4, the results of some SOTA UDA object detectors on the DrIFT dataset are reported alongside our results. One significant DS is from sky to tree, where AP dropped from 67.1 to 0.2 (Tab.2, first row). Our UDA method outperforms others with an AP of 10.7 for the tree background and 46.3 in total, demonstrating its effectiveness in adapting to different BGs. Similarly, for the ground background domain, our method achieves an AP of 44.8 in total and 1.2 for the tree background, showcasing robustness.\n\nFor the aerial-synthetic-winter-normal domain, multiple BGs in each frame could take place, so we did not specify any BG for it. Our UDA method achieves the highest AP in total (17.8), sky (41.3), and tree (0.5), indicating its capability to adapt to different PoVs and complex scenes. In the ground-real-winter-normal-sky domain, our method achieves the highest AP in total (5.7) and tree (0.8), proving its effectiveness with real-world data and different seasons. The results are consistent for tree and ground backgrounds as well, demonstrating our method\u2019s adaptability for two types of DSs occurring simultaneously.\n\nOur method aims to deceive a domain discriminator by making the uncertainty maps for both source and target domains nearly identical. This dual focus on source and target domain alignment is crucial for robust performance across various DSs. The adaptation process involves a trade-off, accepting some degradation in the source domain to achieve significant improvements in the target domain.\n\nSECTION: 5Conclusion\n\nThe DrIFT dataset addresses the need for a vast study of domain shift in drone detection by introducing fourteen distinct domains and emphasizing background shift utilizing background segmentation maps. Our findings show a positive correlation between MCDO-map uncertainty, domain shift, and D-ECE, and a negative correlation with AP. The MCDO-map outperformed other uncertainty metrics in capturing domain shift in the DrIFT dataset. Our uncertainty-aware UDA on object detection also surpassed SOTA methods in the DrIFT dataset. In the future, we aim to explore more nuanced domain adaptation techniques that minimize the source domain performance degradation, which is a drawback for our UDA method in some cases.\n\nSECTION: References\n\nSECTION: S1Introduction\n\nThis supplementary material contains important information that could not be included in the main paper due to space constraints and aims to support the discussions in the main paper. The structure of the main paper is followed.\n\nSECTION: S2DrIFT Dataset\n\nSECTION: S2.1Dataset Characteristics and Statistics\n\nFig. 1 in the main paper displays a variety of backgrounds from our dataset, including the sky, trees, and ground during three distinct seasons (fall, winter, and summer) or adverse weather conditions (foggy, snowy, and rainy).Fig.S1demonstrates that the DrIFT possessesimage frames.\nAs discussed in Subsec. 3.1 \"The DrIFT Story\" in the main paper, we attempted to keep the balance between training and validation sets for almost all domains:frames for training and 300 frames for validation.\nThis standard practice facilitates a proper platform for evaluating the UDA algorithms.Fig.S2shows the number of existing background samples in each domain. It is important to note that, as shown in the last three rows ofFig.S2, the dataset includes only a validation set for the aerial-real domains, without a corresponding training set.\nAdditionally, it is noteworthy that our adverse weather domains only contain a sky background. Hence we have avoided reporting metrics for tree and ground backgrounds within these domains inTab.S1.\n\nFig.S3depicts drones\u2019 relative size and location distribution in real and synthetic domains. The center point, width, and height are normalized to the image width and height. For the aerial-real data inFig.3(c), the means of the relative width and height are approximately 0.015, whereas inFig.3(d), representing aerial-synthetic data, these values are around 0.02 and 0.015, respectively. The width and height of the ground-real, illustrated inFig.3(g), are about 0.03, although for the ground-synthetic shown inFig.3(h), these are approximately 0.02. These numbers indicate that we deal with extremely small objects in comparison to other applications,e.g., autonomous land vehicles[13,61]. It makes DrIFT more challenging in terms of training the detector models.\n\nSECTION: S3DrIFT Benchmark\n\nSECTION: S3.1Methodology\n\nIn[47,48], the researchers employed a gradient function and introduced the concept of self-learning gradient as a metric to evaluate the uncertainty of each detection. If we consider the supervised learning scenario the gradient of the loss function for each detection is. Theis the network\u2019s weight vector. If the ground truth,, is replaced with detection,, and the detection is replaced with its candidates,, the self-gradient metric would be\n\nThe self-gradient metric,, referred to asGrad-loss, operates as a characteristic that signifies the degree of epistemic uncertainty, which is the focal point for investigating DS.Grad-loss-localizationis called the corresponding localization term of the loss, althoughGrad-loss-classificationpoints to the classification term in the loss. Nevertheless, it does not inherently encompass the true essence of uncertainty. To consider other methods, we employ a technique based on MC-dropout to capture the inherent uncertainty associated with each detection. In this approach, we activate dropout at inference time and run our model fortimes. Let us consider the output of the model at each iterationwhere. Initially, we create an-length list corresponding to all output detections in the first iteration. Subsequently, we perform some NMS like the one in Eq. 1 of the main paper to have a candidate list and assign the best candidate with the highestto each list. A detection in each iteration is only allowed to be a member of one list, and a new list is created if there is no option with a higherthreshold.\nUltimately, we calculate the standard deviation of localization parametersand the entropy of the mean of the classification probability vector,, respectively. If we assume we havelists of outputs, we can compute the uncertainty for each list as follows:\n\nHere,is the number of members in each list,is the index of each list, anddenotes the mean of the underlying variable. This technique is referred to asMCDO-NMSwhich is divided intoMCDO-NMS-localization, referred to as, andMCDO-NMS-classification, referred to asinEq.S2. Inspired by[44], which suggests averaging individual uncertainties as one possible aggregation solution, we take a weighted average of classification entropy. Similarly, we sum the square residuals of localization parameters, take a weighted average, and calculate the square root at the end.\n\nSECTION: S3.2Benchmark Scenarios\n\nNormalization has been done for each metric by subtracting a reference value, which is the value of the metric for the source domain, and then dividing by the same reference value. The normalization is mathematically expressed by\n\nFor a set of values of a metricand corresponding reference value, the normalized set is.\nThe subtraction of the reference valueensures that the data is centered around zero, and the subsequent division byscales the data, making it comparable or suitable for further analysis.Fig.S4is illustrated using normalized values of different metrics. All metrics are normalized to their values for the source domain. Positive values indicate increases.\n\nViolin box plot[27]is a graphical representation that combines aspects of both box plots and kernel density plots. It provides a concise and informative way to visualize the distribution, central tendency, and spread of a variable.\nIn the violin box plot:\n\nThe central box represents the interquartile range (IQR) of the data, with the line inside indicating the median.\n\nThe \"violin\" shape surrounding the box displays the probability density function of the data, providing insights into the distribution\u2019s shape.\n\nWider sections of the violin indicate higher data density, while narrower sections represent lower density.\n\nOutliers, if any, are often displayed as individual points.\n\nPearson correlation coefficient[11], denoted by, is a measure of the linear relationship between two variablesand. It is defined as the ratio of the covariance ofandto the product of their standard deviations,\n\nThe Pearson correlation coefficient ranges from -1 to 1. A value of 1 indicates a perfect positive linear relationship, 0 indicates no linear relationship, and -1 indicates a perfect negative linear relationship. Positive values indicate that as one variable increases, the other variable tends to increase as well. Negative values indicate that as one variable increases, the other variable tends to decrease. This coefficient has been used in Fig. 3 of the main paper to analyze the relationships between different metrics.\n\nKL divergence[34]is a measure of how one probability distribution diverges from a second, expected probability distribution. In the DrIFT benchmark, it serves as a metric to quantify the distance between feature map distributions of different domains with the source domain, which is ground-synthetic-winter-normal-sky. The KL divergence is defined\n\nin which N is the cardinality of the feature map distributions,. We assume the two distributions have the same size,.is the feature map distribution of each domain that is taken as the target domain, andis the source domain\u2019s feature map distribution. i is the index of existing elements in each domain\u2019s feature map distribution.\n\nSECTION: S3.3Experiments and Results\n\nThe ground-synthetic-winter-normal-sky is taken as the source domain all over the paper and supplementary material unless we specify other domains.\n\nFor the object detector in this work, the Faster R-CNN[45]architecture with a VGG16[59]in the mmdetection platform[38]has been utilized. For generalization and MC-dropout uncertainty evaluation implementation, the dropout has been activated within the VGG. The experiments were run on a Desktop with a Geforce RTX 3090 and a High-performance computing cluster providing 4 x NVidia A100 (40 GB memory). For the vanilla network training that was started from scratch, we used a stochastic gradient descent optimizer for 73 epochs, for which the learning rate was 0.24 for a batch size of 6 on each GPU. For adaptation training, the vanilla network is used as the pre-trained weights. The learning rate has been decreased to, and the discriminator, which is a simple convolutional neural network, has been trained by an Adam optimizer with a learning rate of. The codes and details will be available on an online platform.\n\nThe objective ofTab.S1andTab.S2is to compare our uncertainty estimation method,MCDO-map, with various uncertainty estimation metrics mentioned in the main paper. InTab.S1, the source domain was ground-synthetic-winter-normal-sky, while ground-real-winter-normal-sky served as the source domain inTab.S2. To provide a comprehensive explanation, we utilizedFig.S6to discover a meaningful relation between different uncertainty evaluation metrics, AP, D-ECE, and KL divergence metric (which measures the distance between feature map distributions of different domains relative to the source domain) using the Pearson correlation coefficient. The findings inFig.S6could be summarized as follows:\n\nMCDO-map exhibits the highest positive correlation (0.81) with KL divergence, indicating its superior capability to capture DSs. A greater level of shift, reflected by increased distance or KL divergence, correlates with higher values of MCDO-map.\n\nAs an uncertainty evaluation metric, a negative correlation with AP is expected, implying that higher AP values correspond to lower uncertainty levels. In this context, MCDO-NMS-Loc-Total, MCDO-NMS-Loc-FP, and MCDO-map yield the best results.\n\nPositive correlations between D-ECE and most uncertainty evaluation metrics suggest that increased uncertainty tends to coincide with calibration errors.\n\nA positive correlation between D-ECE and AP indicates that even with higher AP values, the model may exhibit over or under-confidence, compromising its reliability.\n\nPositive correlations between D-ECE and most uncertainty evaluation metrics, such as 0.36 for MCDO-map, suggest that higher levels of uncertainty are associated with calibration errors.\n\nConsequently, MCDO-map emerges as a wise choice for our UDA algorithm to capture DSs effectively.\n\nTo enhance the understanding of our results, we present three examples of the outputs generated by the trained Faster R-CNN model on the ground-synthetic-winter-normal-sky domain, depicted inFig.S7. InFig.7(a), the drone with a sky background exhibits low uncertainty, as indicated by the blue bounding box on the entropy map. We observe non-zero std values only at the edge of the bounding box in the std map (inside blue, red at the edge). However, a few false detections occur during the MCDO iterations, resulting in non-zero values in both maps around the intersection of the tree and ground. Moving toFig.7(b), the drone with a tree background demonstrates higher uncertainty. The bounding box exhibits some red areas in the entropy map, accompanied by non-zero standard deviation values inside the bounding box. Once again, false detections contribute to non-zero values in the maps. Finally, inFig.7(c), the drone with a ground background is detected with the highest level of uncertainty among these cases, corresponding to way too red color for the bounding box in the entropy map and nonzero values within the bounding box in the std map. However, a significant number of false detections around trees contribute to a considerable level of uncertainty in the maps, reflecting the low AP for trees and, consequently, higher uncertainty in this domain. Detailed AP and uncertainty values for trees are provided in Tab. 2 of the main paper.", "text_file": "data\\paper_texts\\2412.04789v1_content.txt"}, {"title": "Slicing Vision Transformer for Flexible Inference", "authors": ["Yitian Zhang", "Huseyin Coskun", "Xu Ma", "Huan Wang", "Ke Ma", " Xi", " Chen", "Derek Hao Hu", "Yun Fu"], "published_date": "2024-12-06T05:31:42Z", "summary": "Vision Transformers (ViT) is known for its scalability. In this work, we\ntarget to scale down a ViT to fit in an environment with dynamic-changing\nresource constraints. We observe that smaller ViTs are intrinsically the\nsub-networks of a larger ViT with different widths. Thus, we propose a general\nframework, named Scala, to enable a single network to represent multiple\nsmaller ViTs with flexible inference capability, which aligns with the inherent\ndesign of ViT to vary from widths. Concretely, Scala activates several subnets\nduring training, introduces Isolated Activation to disentangle the smallest\nsub-network from other subnets, and leverages Scale Coordination to ensure each\nsub-network receives simplified, steady, and accurate learning objectives.\nComprehensive empirical validations on different tasks demonstrate that with\nonly one-shot training, Scala learns slimmable representation without modifying\nthe original ViT structure and matches the performance of Separate Training.\nCompared with the prior art, Scala achieves an average improvement of 1.6% on\nImageNet-1K with fewer parameters.", "arxiv_id": "2412.04786v1", "html_link": "https://arxiv.org/html/2412.04786v1", "search_term": "ti:\"transformers\"", "html_content": "SECTION: Slicing Vision Transformer for Flexible Inference\n\nVision Transformers (ViT) is known for its scalability.\nIn this work, we target to scale down a ViT to fit in an environment with dynamic-changing resource constraints.\nWe observe that smaller ViTs are intrinsically the sub-networks of a larger ViT with different widths.\nThus, we propose a general framework, namedScala, to enable a single network to represent multiple smaller ViTs with flexible inference capability, which aligns with the inherent design of ViT to vary from widths.\nConcretely, Scala activates several subnets during training, introduces Isolated Activation to disentangle the smallest sub-network from other subnets, and leverages Scale Coordination to ensure each sub-network receives simplified, steady, and accurate learning objectives.\nComprehensive empirical validations on different tasks demonstrate that with only one-shot training, Scala learns slimmable representation without modifying the original ViT structure and matches the performance of Separate Training.\nCompared with the prior art, Scala achieves\nan average improvement of 1.6% on ImageNet-1K with fewer parameters.\nCode is available athere.\n\nSECTION: 1Introduction\n\nVision Transformers (ViTs)[9]are renowned for its scalability and various avenues[41,5,7]have been explored to scale up ViT models.\nTo tailor ViTs to run on devices with limited resources,\nsome recent progress[36,35]utilize knowledge distillation[13]to scale down ViT.\nParticularly,\nDeiT[29]introduces two smaller variants of DeiT-B: DeiT-Ti and DeiT-S\nwhich have been widely used in resource-limited applications.\nAlthough these small ViTs exhibit enhanced efficiency,\nthey lack the flexibility to implement customized adjustments that accommodate dynamically changing resource constraints in real-world scenarios,\ne.g., the computation budget of mobile phones depends on the energy level (low-power mode) and number of running apps.\nConsequently, the standard Separate Training (ST) protocol trains models with different sizes separately to provide a spectrum of options with diversified performance and computation.\nST requires repetitive training procedures to produce multiple model choices,\nand the challenge is amplified for\nfoundation models[26,24,17].\nFrom users\u2019 perspective, they are only offered limited model choices,\nthat might not cater to all scenarios.\n\nAnalyzing the architectures of ViT-Ti/S/B,\nwe observe that these ViTs\nare the same architecture with the only difference in the number of embedding dimensions\n(we ignore the difference in the number of heads as it does not impact the overall model size),\nindicating smaller ViTs are intrinsically the sub-networks of larger model with different widths (see Fig.2).\nThis suggests that a large ViT can be transformed to represent small models by uniformly slicing the weight matrix at each layer.\nGiven a width ratio, we adjust the size of the network by this single hyperparameter,\nallowing a single ViT to represent multiple small variants with the weights of those sub-networks shared in a nested nature,\ne.g., ViT-B (=0.25) equals ViT-Ti and ViT-B (=0.5) corresponds to ViT-S.\nIn this manner, we empower ViTs for flexible inference capability,\nand we aim to slice a ViT within a broad slicing bound and fine-grained slicing granularity so that the diversity and number of sub-networks can be ensured for higher flexibility.\nThis problem is non-trivial as fully training all the sub-networks within a constrained budget is nearly infeasible. Consequently, it is quite challenging for these subnets to match the performance of separate training.\n\nAlthough various approaches have delved into slicing deep networks for flexible inference, the problem we target to resolve,\ni.e., uniformly slicing ViTs within a large slicing bound and fine-grained slicing granularity,\nis intrinsically different from others in three perspectives:\n(1) slicing strategy: as shown in Fig.2(i), the supernet training techniques[2,38,4]in NAS usually slice through multiple dimensions with a small slicing bound, resulting in irregularities in model architectures\nand a minor computational adjustment space.\n(2) slicing granularity: recent width slicing approaches[14,18]either slice specific portions of the network or utilize a considerably large slicing granularity, leading to a limited number of models produced.\n(3) network architecture: US-Net[37]shares a similar vision with us but it has only demonstrated success in the CNN architecture.\n\nIt is crucial to note the fundamental differences between slicing CNN and ViT:\n(1) vanilla small ViTs such as ViT-Ti/S/B, are inherently designed to vary based on widths, aligning with our approach.\nConversely, many CNNs are structured to vary from depths, like ResNet-18/34[12], and slicing them by width brings unconventional architectures.\n(2) slimmable CNN necessitates calibration[39,37]for each sub-network pre-inference due to Batch Normalization[15],\nunlike slimmable ViTs that can be directly utilized for evaluation.\n(3) the transformer architecture[33]has wider applications than CNN in this era, e.g., MAE[11], CLIP[26], DINOv2[24], LLMs[31,32,1].\nNevertheless, ViTs have much less image-specific inductive bias than CNN and\ntheir slimmable ability remains unclear.\nAs shown in Fig.2, we empirically implement US-Net on ViT-S and observe substantial performance gaps at most width ratios compared to ST, indicating that the available solution of uniform slicing does not work well on the transformer architecture.\n\nTo investigate the underlying causes of this phenomenon, we conduct analyses in Sec.3which are briefly summarized in two folds:\n(1) ViTs display minimal interpolation ability, indicating that the optimization of intermediate subnets falls notably short compared to Separate Training (ST);\n(2) sustained activation of the smallest sub-network poses a negative effect on other subnets, which affects the overall performance as their weights are shared in a nested nature.\nTo resolve these issues, we propose a general framework, namedScala, to enforce ViTs to learn slimmable representation.\nSpecifically,\nwe propose Isolated Activation to disentangle the representation of the smallest sub-network from other subnets while still preserving the lower bound performance.\nBesides, we present Scale Coordination to ensure each subnet receives simplified, steady, and accurate learning objectives.\nIn this manner, the slimmable ViT can be transformed into multiple smaller variants during inference and match the performance of ST.\n\nCompared to ST which trains all the subnets individually, Scala reduces the storage and training costs remarkably since the weights of smaller ViTs are shared with the full model and we only need one-shot training without extending the training duration.\nFurther, Scala has a very large slicing bound and fine-grained slicing granularity, enabling diverse sub-network choices during evaluation.\nIn this way, the delivered system can make tailored adjustments that accommodate dynamically changing resource constraints in real-world scenarios, promising the application on edge devices.\nCompared with the prior art SN-Net[25]which supports flexible inference on ViTs, Scala clearly outperforms it under different computation with fewer parameters.\nMoreover, Scala matches the performance of ST on various tasks without modifying the network architecture,\ndemonstrating its generalizability and potential to replace ST as a new training paradigm.\nThe contributions are summarized as follows:\n\nAlthough slicing ViTs exhibits multiple advantages, we provide detailed analysis and practical insights into the slimmable ability between different architectures (Sec.3and Tab.2) and find slicing the ViT architecture to be the most challenging problem.\n\nWe propose a general frameworkScalato enable ViTs to learn slimmable representation for flexible inference.\nWe present Isolated Activation to disentangle the representation of the smallest subnet and Scale Coordination to ensure each subnet receives simplified, steady, and accurate signals.\n\nComprehensive experiments on different tasks demonstrate that Scala, requiring only one-shot training, outperforms prior art and matches the performance of ST, substantially reduces the memory requirements of storing multiple models.\n\nSECTION: 2Related Work\n\nScaling Up ViTs.Like Transfromer[33]in NLP, scalability and performance improvements in ViTs[9]have been a central focus of recent research.\nSpecifically,\nstrategies have been explored to scale the depth of ViT[46,30]and it is scaled to even larger sizes with almost 2 billion parameters and reaches new state-of-the-art results[41].\nAfterward, ViTs have been scaled up to 4 billion[5]and 22 billion[7]parameters with extraordinary performance and enormous costs.\n\nScaling Down ViTs.The advent of ViTs has also sparked interest in scaling down these models.\nTechniques such as knowledge distillation[13]have been explored to reduce the ViT model size[36,35].\nFor example, DeiT[29]presents smaller ViTs with 5M parameters.\nAdditionally, researchers have explored quantization methods[22,21]to further compress ViTs for deployment on edge devices.\nUnfortunately, these static models cannot make customized adjustments for resource-changing environments in real scenarios.\n\nSlimmable Neural Network.The derivation of multiple smaller models from a single network has been previously explored but most works focus on the CNN structure.\nSlimmable Networks[39,37]and its variants[34,3,10]train a shared network which adapts the width to accommodate the resource constraints during inference.\nLater, this idea is adapted into two-stage NAS methods[2,38,4]for supernet training.\nThe supernet is scaled at multiple dimensions with a small computation change, in contrast to our work where we only scale the width dimension with a large slicing bound.\nSN-Net[25]is a recently proposed method that constructs a supernet with several pre-trained models and inserts linear layers to build dynamic routes for flexible inference.\nRecently, several of these techniques have been extended to Transformer architecture[14,18], while they either scale part of the network or the slicing granularity is large which means they could only deliver very few models in the end.\nDiffering from the previous works, our method is the first work to scale the ViT structure with large slicing bound and small slicing granularity which is intrinsically a more challenging problem.\n\nSECTION: 3Revisiting Slicing in Vision Transformer\n\nDue to the excessive costs of\nconstantly activating all the sub-networks during training,\nthe sandwich rule is proposed in US-Net[37]to train the slimmable network at the smallest width, largest width, and 2 random intermediate widths in each iteration so that the performance of the lower bound and upper bound are guaranteed.\nAlthough the intermediate sub-networks are optimized less frequently compared to Separate Training (ST), US-Net manages to achieve comparable performance with ST on the CNN architecture.\nTo have a better understanding of the distinction between CNN and ViT,\nwe apply US-Net to MobileNetV2[28], a CNN, and DeiT-S[29], a ViT, but constantly activate four sub-networks with the width ratio ofat each iteration.\nSubsequently, we evaluate the pre-trained models at both inboundand outboundunseen width ratios to evaluate their interpolation and extrapolation abilities, respectively.\nShown in Fig.4,\nCNN exhibits moderate interpolation and extrapolation capabilities by achieving acceptable performance at previously unobserved widths during training.\nIn stark contrast, ViT fails entirely at unseen widths, suggesting that optimizing larger sub-networks does not directly benefit the performance of smaller ViTs, even though their weights are shared in a nested nature.\n\nWe analyze the results from the expected training epochs for each sub-network.\nLetrepresent the total number of networks to be delivered,\nwhereinintermediate sub-networks are included, and according to the sandwich rule, two of these are randomly sampled during each iteration.\nFormally, the expected training epochs for the intermediate networkscan be expressed as:\n\nwhereis the number of training epochs for the full model\nand it suggests that the optimization of most sub-networks falls notably short compared to ST.\nAs ViT has demonstrated minimal interpolation ability at unseen widths compared to CNN,\neach sub-network within the slimmable ViT requires optimal utilization of every training iteration to achieve satisfactory performance.\nNevertheless, the smallest subnet is constantly activated during training according to the sandwich rule and we hypothesize that the over-emphasis of the smallest sub-network, often exhibits the worse performance, may increase the training difficulty of other subnets as their weights are shared in a nested nature.\nTo validate it, we implement US-Net[37]on DeiT-S without constantly activating the smallest sub-network.\nFig.4verifies our hypothesis showing an accuracy drop at the smallest subnet but a significant performance improvement at other width ratios.\n\nSECTION: 4Scala\n\nWe first introduce the training and inference paradigms of Scala.\nThen, we describe Isolated Activation which disentangles the smallest subnet from other sub-networks while maintaining the lower bound performance.\nFurther, we present Scale Coordination to ensure each subnet receives simplified, accurate, and steady learning objectives.\nWithout any modification to the architecture, we deliver a general framework Scala which could be easily built on existing methods.\n\nSECTION: 4.1Framework\n\nOur goal is to build a general framework that makes a ViTslimmable, i.e., the delivered network can be transformed into different small variants for flexible inference.\nFirst, we introduce a hyperparameterto denote the width ratio of the sub-network.\nBased on our analysis in Fig.4, ViTs have minimal interpolation ability, which suggests that all subnets have to be individually optimized to achieve decent performance.\nFollowing the sandwich rule[37], we sample the smallest, largest(), and 2 random intermediate width ratios,at each iteration during training.\nThe corresponding sub-networks are:,,and.\nand we accumulate the gradients of those subnets at each iteration.\nAt the inference stage, the networkis evaluated at an arbitrary width ratio that has been optimized during training by adjusting.\n\nSECTION: 4.2Isolated Activation\n\nIllustrated in Sec.3, constant activation of the smallest sub-networkensures its own accuracy at the cost of other subnets\u2019 performance.\nThis is a dilemma as there is a significant accuracy drop ofif we do not constantly activate it (see Fig.4), otherwise, the performance of other sub-networks are severely limited.\nTo alleviate this issue, we propose Isolated Activation to disentangle the representation offrom other sub-networks while still constantly activating it.\nIt not only ensures the performance of the lower bound but facilitates the optimization of other subnets as well.\n\nFormally, given the learnable weightof a random layer in ViT (stands for the output, input channel number,represents the height and width of the convolution kernel,for fully connected layers), the weight ofwhereis selected as:\n\nIn contrast, the smallest sub-networkis activated as:\n\nwhere we slice the weights in a reverse direction so that we disentangle the representation offrom other sub-networks.\nWith this simple but critical design, we not only ensure the performance ofwith constant activation, but also alleviate the negative effects it brings.\n\nSECTION: 4.3Scale Coordination\n\nWe present the training strategy of Scala in this section.\nWe follow the setting of DeiT[29]to train the full modelwith knowledge distillation and introduce a distillation token for knowledge transfer between sub-networks.\nAs our goal is to scale down a given networkto multiple smaller variants, we simply choose the pre-trained model itselfas the external teacher for the full networkto facilitate training.\nTo optimize the sub-networks at different scales, we present the Scale Coordination training strategy,\nwhich is composed of three techniques: Progressive Knowledge Transfer, Stable Sampling, and Noisy Calibration, to ensure that each subnet receives simplified, accurate, and steady learning objectives.\n\nProgressive Knowledge Transfer.Given an input image, the activated sub-networkproduces two predictions:\n\nwhereanddenote the prediction generated by the classification and distillation head, respectively.\nAs we activate multiple sub-networks:,,andat each iteration during training, our idea is to utilize the predictions of the larger network to facilitate the optimization of smaller subnets.\n\nGiven the sorted width ratio list, we utilize the KL divergence[19]loss to progressively distill the knowledge of the larger network into the smaller one:\n\nwhererepresents the number of classes,anddenotes the index ofin.\nInstead of usingas the optimization target for all smaller networks, we ensure each subnet receives simplified learning objective as small subnet have large capacity gap compared to(e.g.,is almost 16 times larger thanif) and minimizing their KL loss complicates the optimization process and leads to inferior performance.\nWith Progressive Knowledge Transfer, we simplify the optimization objective for small sub-networks by utilizingandas the teacher assistants to fill the gap betweenandand train them in a one-shot manner.\n\nStable Sampling.As the knowledge is gradually transferred from the larger network to the smaller one, the two intermediate networks serve as the bridge to connectand, asis the student ofandis the teacher of.\nTherefore, we need to carefully control the width ratiosandto prevent the obvious model capacity variation.\n\nConcretely, we introduce the slicing granularityand\nthe number of networkswe can deliver (including the full model) with a single ViT is denoted as:\n\nThen, we divide the slicing boundinto two smaller ones:\n\nwhereand,will be the random integer sampled from the uniform distribution,, respectively. Thus,andare defined as:\n\nand we ensure the model capacity gap between the four networks is stable and secure the learning objective for each subnet is steady.\n\nNoise Calibration.Although all the subnets receive guidance from larger networks, a notable issue is that the predictions from the teacher are not always accurate, sometimes even noisy, especially at the early training stage.\nTo avoid the noisy signal dominating the optimization direction, we first calculate the Cross-Entropy loss by:\n\nwhererepresents the one-hot label for class. Then, we calibrate the noise by combining the KL divergence loss and Cross-Entropy loss:\n\nwhereis a hyperparameter used to balance the two losses and we empirically letin our implementations.\nBy doing so, we mitigate the negative effects brought by the noisy predictions of the teacher model and ensure each subnet is guided by the accurate learning objectives.\n\nSECTION: 5Experiments\n\nWe validate Scala with the plain ViT structure DeiT[29].\nWe first analyze of the main property of Scala\nand compare our method with the state-of-the-art method SN-Net[25]and Separate Training (ST) at a larger scale.\nMoreover, we examine the transferability of Scala and its application on Semantic Segmentation.\nFinally, we provide ablations to validate the efficacy of our designs.\n\nSECTION: 5.1Experiment Settings\n\nAll the object recognition experiments are carried out on ImageNet-1K[8].\nWe follow the training recipe of DeiT[29]and conduct the experiments on 4 V100 GPUs.\nFor Scala, we set,, andso that we could enable a single ViT to represent 13 different networks () with a large slicing bound (i.e.,is almost 16 times larger than).\n\nSECTION: 5.2Proof-of-Concept\n\nIn this part, we conduct experiments over DeiT-S[29]for 100-epoch training to prove the concept.\n\nComparison with scaling baselines.We compare Scala with multiple scaling baselines, including:\n(1) AutoFormer[4]: we apply this ViT-based supernet training method into our setting to scale through width;\n(2) US-Net[37]: the prior work that obtains similar performance with ST over the CNN structure;\n(3) Separate Training (ST): we repetitively train the model with different widths from scratch and evaluate them individually.\nTab.1shows that AutoFormer lags behind Scala remarkably as we target to scale in a wider range.\nUS-Net shows significantly worse performance compared to ST which indicates that scaling down ViT is a more challenging problem compared to the CNN architecture.\nNevertheless, Scala achieves better performance compared to ST at all width ratios with one-shot training, reducing the storage costs of saving multiple models observably.\n\nSlicing Granularity and Bound.Fig.6shows the results of various slicing granularity.\nFirst, Scala outperforms ST with differentand the advantage at small width ratios is more obvious, which promises its application on edge devices.\nMoreover, it is shown that less fine-grained granularityresults in better overall performance with the same slicing bound as the expected training epochsfor intermediate subnets increase correspondingly.\nWe further conduct experiments with differentwhile fixingandto study the effect of the slicing bound.\nFig.6shows that smaller bounds lead to markedly better performance and it further verifies that slicing through a large bound is intrinsically more difficult, which distinguishes Scala from the supernet training methods[2,38,4]in NAS.\n\nApplication on Hybrid Structures.We experiment Scala on the CNN-ViT hybrid architecture Uniformer-S[20].\nAs Uniformer contains Batch Normalization (BN)[15]which cannot be directly evaluated after slicing because of normalization shifting[39], we calibrate the statistics of BN before inference following[37].\nShown in Fig.8, Uniformer-S is scaled down to 13 different variants with better performance compared to ST, demonstrating the generalization ability of Scala.\nHowever, performing BN calibration at each width ratio requires considerable extra effort.\nThis highlights the benefit of ViT, as Layer Normalization (LN) allows direct evaluation without additional operations.\n\nApplication on Lightweight Structures.We further validate Scala on lightweight structure Uniformer-XS[20]which integrates the design of token pruning and train these methods for 150 epochs. Shown in Fig.8, Scala still matches the performance of ST and exhibits a significant advantage at small width ratios, which promises its application on edge devices with a limited budget.\n\nFast Interpolation of Slimmable Representation.Training models with different slicing granularityfrom scratch is time-consuming and here we show that the slimmable representation of certain granularity can be scaled to others with a small amount of training epochs.\nSpecifically, we train the model with the originalfor 70 epochs and decrease the value ofin the last 30 epochs to deliver more sub-networks for higher inference flexibility.\nFig.10shows the results of fast interpolation are similar to those trained from scratch and the newly appeared sub-networks are quickly interpolated to achieve decent performance.\nWe further increasefor sub-networks with higher performance and the phenomenon shown in Fig.10is similar to down interpolation.\nBesides, we observe that the accuracy of abandoned sub-networks gradually decreases but they maintain the performance to a great extent.\n\nSlimmable Ability across Architectures.We examine the slimmable ability of different architectures in Tab.2by applying Scala on different architectures and evaluating these networks at unseen width ratios to explore the interpolation ability.\nCNN exhibits very strong interpolation ability as the performance at unseen widths lies in the range of trained width ratios.\nIn contrast, CNN-ViT and ViT suffer from remarkable performance decreases to different extents\nand ViT achieves almost zero accuracy which further validates that the problem we target to solve, i.e., slicing ViT, is the most challenging one.\n\nSECTION: 5.3Comparisons over Extended Training\n\nIn this section, we perform training over DeiT-B[29]for 300-epoch training following the standard protocol on ImageNet-1K[8]to compare with the state-of-the-art.\n\nComparisons with state-of-the-art.SN-Net[25]is state-of-the-art work that supports flexible inference on ViT.\nSpecifically, it utilizes several pre-trained models (e.g., DeiT-Ti/S/B) to construct a supernet and inserts additional layers to build dynamic routes for flexible inference.\nShown in Tab.5.2, we empirically compare Scala with SN-Net over DeiT-B following the standard 300-epoch training protocol[29].\nScala obtains similar performance with SN-Net at large width ratios and clearly outperforms it at small computational budgets.\nBesides, SN-Net has to preserve the parameters of multiple models and additional layers, while Scala only needs to keep the weights of the full network.\nWhen adopting the stronger teacher network[27]as SN-Net does, Scala outperforms SN-Net with an average improvement of 1.6across all width ratios.\n\nComparisons with Separate Training.In Tab.5.2, we compare with ST on DeiT-B[29]with longer training process, i.e., 300-epoch training, wherecorresponds to DeiT-Ti, DeiT-S and DeiT-B, respectively.\nScala exhibits a clear advantage atand matches the performance of ST exceptdue to significantly less training time.\nWhen, we can achieve similar performance atwith 40training epochs of ST.\nFurther reducingto 4, resulting in the constant activation of the two intermediate networks, allows us to consistently outperform ST at all width ratios. This substantiates the effectiveness of Scala and the slimmable representation.\n\nSECTION: 5.4Transferability\n\nTo assess the transferability of Scala, we employ DeiT-B[29]as the backbone for a 300-epoch pre-training on ImageNet-1K and leverage the foundation model DINOv2-B[24]as the teacher network to inherit good behaviors.\nOur study aims to address two key questions:\n\nWhether the slimmable representation can be transferred to downstream tasks?As depicted in Fig.11(a), Scala consistently outperforms Separate Training (ST) across all width ratios, despite the intermediate sub-networks being trained for approximately 55 epochs.\nAfter that, we conduct linear probing on video recognition dataset UCF101 with 8 evenly sampled frames and average their features for the final prediction.\nFor the classification head added on Scala, we make it slimmable to fit the features with various dimensions and follow the same training protocol as in object recognition. In Fig.11(b), two notable observations emerge:\n(1)\nScala consistently outperforms ST across different width ratios on the UCF101 dataset, implying the great transferability of the slimmable representation;\n(2) Scala retains its slimmable ability when applied to a new task and exhibits promising performance across a wide slicing range (10141 GFLOPs), promising its application on other downstream tasks.\n\nWhether the generalization ability can be maintained in the slimmable representation?Inspired by the work[43]which replicates the success of vision foundation models on ImageNet-1K, we remove all the Cross-Entropy losses during training to alleviate the dataset bias issue and inherit the strong generalization ability of the teacher network DINOv2. Then we conduct linear probing on 12 fine-grained classification datasets following the setup in DINOv2. Tab.5shows that Scala significantly outperforms DeiT variants on the average performance of fine-grained classification which suggests that Scala indeed inherits the fruitful knowledge from DINOv2 with remarkable improvement in its generalization ability. Moreover, the improvement over DeiT does not decrease when we scale down the width ratios during inference and it indicates that Scala maintains the flexible inference capability very well even though it contains more knowledge than before.\n\nAircraft\n\nCal101\n\nCars\n\nC10\n\nC100\n\nDTD\n\nFlowers\n\nFood\n\nPets\n\nSUN\n\nVOC\n\nCUB\n\nAverage\n\nSECTION: 5.5Dense Prediction\n\nIn previous sections, we have validated the effectiveness of Scala on classification tasks, we further examine whether the slimmable representation could be transferred for dense prediction task like semantic segmentation. We utilize the pre-trained model Uniformer-S[20]drawn from Fig.8, which has a hierarchical design and is obtained by 100-epoch training (our results lag behind official results where the backbone is trained for 300 epochs), and equip it with Semantic FPN[16].\nTo compare with Separate Training (ST), we extract four subnets from Scala (Uniformer-S) and train them separately.\nShown in Tab.5.6, Scala outperforms ST at all widths which verifies the slimmable representation benefits the downstream tasks.\nNote that we do not scale the decoder as it involves extra designs and is out of the scope of this work.\nHowever, we show that the slimmable representation can be generalized to semantic segmentation as feature extractors because the feature maps are spatially intact,\npromising its application as an end-to-end slimmable framework on dense prediction tasks.\n\nSECTION: 5.6Ablation Study\n\nWe conduct ablation to examine the effectiveness of our designs in Tab.5.6.\nFirst, we build Scala without Isolated Activation so that the smallest sub-network will entangle with others and it shows an obvious performance drop at all width ratios.\nThen, we remove Progressive Knowledge Transfer (PKT) and pass the knowledge fromto smaller subnets through classification token following US-Net[37].\nIt shows much worse performance, especially at small ratios, which proves the strength of PKT as it implicitly introduces some teacher assistants to simplify the optimization objective for small sub-networks.\nFurther, we random sample the width ratios ofandbetweenand compare it with Stable Sampling (SS).\nThe results are slightly inferior to SS which suggests SS is helpful in securing the steady learning objective for each sub-network.\nFinally, we remove Noise Calibration (NC) from Scala and only use the predictions from larger networks to guide the small subnets.\nIt shows remarkable performance drops at small width ratios, where the noise from the teacher network is most obvious, demonstrating the effectiveness of NC in calibrating the noise and providing accurate signals for sub-networks.\n\nSECTION: 6Conclusion and Limitations\n\nIn this paper, we observed that smaller ViTs are intrinsically the sub-networks of a large ViT with different width ratios.\nHowever, slicing ViT is very challenging due to its poor interpolation ability.\nTo address this issue, we proposed Scala to enable a single network to represent multiple smaller variants with flexible inference capability.\nSpecifically, we proposed Isolated Activation to disentangle the representation of the smallest subnet from others and presented Scale Coordination to ensure the sub-network receives simplified, steady, and accurate learning objectives.\nExtensive experiments on different tasks prove that Scala, requiring only one-shot training,\noutperforms the state-of-the-art method under different computations\nand matches the performance of Separate Training with significantly fewer parameters,\npromising the potential as a new training paradigm.\n\nOne limitation of Scala is the longer training time compared to\nconventional supervised learning of a single model, attributable to the activation of multiple subnets during training.\nNevertheless, our training time is obviously less than separately training all the sub-networks.\nIn the future, we aim to enhance the training efficiency of Scala.\n\nSECTION: References\n\nSECTION: Appendix AAppendix\n\nSECTION: A.1Implementation Details\n\nFor Separate Training (ST), we follow the exact training strategy of the official DeiT[29]and Uniformer[20]setting.\nWe use random horizontal flipping, random erasing[44], Mixup[42], CutMix[40], and RandAugment[6]for data augmentation. AdamW[23]is utilized as the optimizer with a momentum of 0.9 and\na weight decay of 0.05. We set the learning rate to 1e-3 and decay with a cosine shape.\nThe models are trained on 4 V100 and 8 A100 GPUs with a total batch size of 1024.\nWe adopt Exponential Moving Average (EMA) following the official setting.\n\nWhile we utilize the pre-trained ST model () as the teacher forto facilitate training as mentioned in the Scale Coordination section in the main paper, we adopt a larger learning rate (2e-3) and mild data augmentation (reduce the magnitude for RandAugment[6]to 1 and turn off repeated augmentation) because Scale Coordination already regularizes the network training strongly.\nAt every training iteration, we activate four sub-networks separately based on Stable Sampling and accumulate their gradients for backpropagation.\nThe rest hyperparameters are set as the same as those in Separate Training.\n\nSECTION: A.2Slimmable Ability of Vanilla Representation\n\nTab.1shows that ViT is not slimmable if we directly evaluate the vanilla pre-trained model at other widths.\nHere, we further explore the slimmable ability of vanilla representation and fine-tune the vanilla pre-trained model with Scala.\nTab.8shows that fine-tuning obtains obviously worse performance at small width ratios compared to training from scratch, which denotes that the vanilla representation is not slimmable and is essentially different from the slimmable representation.\n\nSECTION: A.3Larger Slicing Bound\n\nAs discussed in the main text, the slicing bound has a huge impact on the performance of Scala.\nHere we further expand the slicing bound fromto.\nAs shown in Fig.12, Scala suffers from an obvious performance drop atas it is not constantly activated in the new setting.\nNevertheless, our method still manages to outperform ST at all width ratios and shows a significant advantage at the smallest ratio.\n\nSECTION: A.4Longer Training Process\n\nPrevious experiments validate that Scala achieves achieves performance comparable to that of Separate Training (ST) on DeiT-B[29]in the 300-epoch training setting, even though the intermediate sub-networks training time is much less.\nWe further extend the training process to 400 epochs in this section and the results are shown in Tab.9.\nThe overall performance at various width ratios is improved with longer training and Scala () outperforms ST at all widths even though the expected training epochs for intermediate sub-networks are still much less than ST.\n\nSECTION: A.5More Ablation Studies on Activation Method\n\nWe validated the effectiveness of Isolated Activation by removing this component and we further conduct more ablation studies on the activation methods where the designs are illustrated in Fig.13.\nAs shown in Tab.10, choice (b) leads to slightly better performance at, but the performance drops atsignificantly as it is entangled withand the over-emphasize ofadversely affect its performance.\nOn the other hand, choice (c) results in a similar performance at, but the accuracy decreases significantly atwhich further verifies our hypothesis thatshould be isolated to reduce its negative impact on other sub-networks.\n\nSECTION: A.6Verification of Slimmable Ability\n\nIn the main text, we found that ViT has the minimal interpolation ability compared to the CNN structure.\nThis suggests that optimizing larger sub-networks does not directly contribute to the performance improvement of smaller variants, even though their weights are shared in a nested nature.\nA further question is, whether ViT can still maintain the slimmable ability for the unseen width ratios during training.\n\nTo verify this point, we respectively fix the width ratios ofto 0.8125 and 0.4375 during training, so that only one sub-network is optimized at each range.\nFig.14shows that the accuracy of unseen sub-networks is very low due to the lack of interpolation ability.\nNevertheless, the performance at other width ratios remains similar to the default setting even though their weights are shared with each other.\nThis indicates that the correlation between sub-networks in ViT is weak and further highlights how challenging this problem is.\n\nSECTION: A.7Comparisons with Distillation Baselines\n\nWhile we have shown that Scala outperforms baseline methods US-Net[37]and Separate Training (ST), we further compare Scala with much stronger baselines, by adding an external teacher to their full network during training.\nSpecifically, we adopt the pre-trained full model from ST as the teacher and conduct knowledge distillation for models with different widths separately.\nTab.11shows that ST+KD exhibits similar performance at larger width ratios with Scala, despite that Scala clearly outperforms ST+KD at smaller widths, promising its application on edge devices.\nAlthough obtaining a better full model, US-Net+KD exhibits worse performance at smaller width ratios because it utilizes the full network as the teacher for all subnets and this phenomenon verifies our motivation of proposing Progressive Knowledge Transfer.\n\nSECTION: A.8Comparisons in Training Time\n\nAssuming to deliver 13 models in the end, we compare the training time (100 Epoch) of Scala with US-Net[37], Separate Training on 8 A100 GPUs. The difference between US-Net and Scala is not large as the transformer architecture has been well-optimized on GPU and we do observe a significant time gap between Scala and Separate Training as they have to train 13 models iteratively. Moreover, Scala can be configured to deliver 25 models without an increase in training time as we sample 4 networks at each iteration in all scenarios which further highlights our strengths.\n\nSECTION: A.9Comparisons with MatFormer\n\nMatFormer[18]only slices the FFN block in the transformer architecture so it offers a minor computational adjustment space and we adapt their method on DeiT-S to compare with Scala. Fig.15shows that Scala achieves comparable performance with it (better in most cases) whenwith a larger adjustment scope.\n\nSECTION: NeurIPS Paper Checklist\n\nClaims\n\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope?\n\nAnswer:[Yes]\n\nJustification: Our contribution is to enable ViT to become slimmable during inference which is described in the abstract and introduction.\n\nGuidelines:\n\nThe answer NA means that the abstract and introduction do not include the claims made in the paper.\n\nThe abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.\n\nThe claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.\n\nIt is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.\n\nLimitations\n\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\n\nAnswer:[Yes]\n\nJustification: See conclusion.\n\nGuidelines:\n\nThe answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.\n\nThe authors are encouraged to create a separate \"Limitations\" section in their paper.\n\nThe paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.\n\nThe authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.\n\nThe authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.\n\nThe authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.\n\nIf applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.\n\nWhile the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren\u2019t acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.\n\nTheory Assumptions and Proofs\n\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?\n\nAnswer:[N/A]\n\nJustification: We do not have theory included.\n\nGuidelines:\n\nThe answer NA means that the paper does not include theoretical results.\n\nAll the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.\n\nAll assumptions should be clearly stated or referenced in the statement of any theorems.\n\nThe proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.\n\nInversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.\n\nTheorems and Lemmas that the proof relies upon should be properly referenced.\n\nExperimental Result Reproducibility\n\nQuestion: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?\n\nAnswer:[Yes]\n\nJustification: We have included the implementation details in the main text and appendix.\n\nGuidelines:\n\nThe answer NA means that the paper does not include experiments.\n\nIf the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.\n\nIf the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.\n\nDepending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.\n\nWhile NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example\n\nIf the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.\n\nIf the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.\n\nIf the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).\n\nWe recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.\n\nOpen access to data and code\n\nQuestion: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?\n\nAnswer:[Yes]\n\nJustification: We have included the details and will release the code soon.\n\nGuidelines:\n\nThe answer NA means that paper does not include experiments requiring code.\n\nPlease see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n\nWhile we encourage the release of code and data, we understand that this might not be possible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).\n\nThe instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n\nThe authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.\n\nThe authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.\n\nAt submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).\n\nProviding as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.\n\nExperimental Setting/Details\n\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?\n\nAnswer:[Yes]\n\nJustification: We conduct experiments following the standard protocol and include the details.\n\nGuidelines:\n\nThe answer NA means that the paper does not include experiments.\n\nThe experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.\n\nThe full details can be provided either with the code, in appendix, or as supplemental material.\n\nExperiment Statistical Significance\n\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?\n\nAnswer:[No]\n\nJustification: Repeating experiments on ImageNet requires lots of resources.\n\nGuidelines:\n\nThe answer NA means that the paper does not include experiments.\n\nThe authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.\n\nThe factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).\n\nThe method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)\n\nThe assumptions made should be given (e.g., Normally distributed errors).\n\nIt should be clear whether the error bar is the standard deviation or the standard error of the mean.\n\nIt is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.\n\nFor asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).\n\nIf error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.\n\nExperiments Compute Resources\n\nQuestion: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?\n\nAnswer:[Yes]\n\nJustification: We have provided details of our computing resources in the appendix.\n\nGuidelines:\n\nThe answer NA means that the paper does not include experiments.\n\nThe paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.\n\nThe paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.\n\nThe paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn\u2019t make it into the paper).\n\nCode Of Ethics\n\nQuestion: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethicshttps://neurips.cc/public/EthicsGuidelines?\n\nAnswer:[Yes]\n\nJustification: We have read and follow the rules.\n\nGuidelines:\n\nThe answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n\nIf the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.\n\nThe authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).\n\nBroader Impacts\n\nQuestion: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?\n\nAnswer:[N/A]\n\nJustification: There is no societal impact.\n\nGuidelines:\n\nThe answer NA means that there is no societal impact of the work performed.\n\nIf the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.\n\nExamples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.\n\nThe conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.\n\nThe authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.\n\nIf there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).\n\nSafeguards\n\nQuestion: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?\n\nAnswer:[N/A]\n\nJustification: Our work has no such risks.\n\nGuidelines:\n\nThe answer NA means that the paper poses no such risks.\n\nReleased models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.\n\nDatasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.\n\nWe recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.\n\nLicenses for existing assets\n\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?\n\nAnswer:[Yes]\n\nJustification: We have cited the papers that created the datasets.\n\nGuidelines:\n\nThe answer NA means that the paper does not use existing assets.\n\nThe authors should cite the original paper that produced the code package or dataset.\n\nThe authors should state which version of the asset is used and, if possible, include a URL.\n\nThe name of the license (e.g., CC-BY 4.0) should be included for each asset.\n\nFor scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.\n\nIf assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets,paperswithcode.com/datasetshas curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.\n\nFor existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.\n\nIf this information is not available online, the authors are encouraged to reach out to the asset\u2019s creators.\n\nNew Assets\n\nQuestion: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?\n\nAnswer:[N/A]\n\nJustification: We do not release new assets.\n\nGuidelines:\n\nThe answer NA means that the paper does not release new assets.\n\nResearchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.\n\nThe paper should discuss whether and how consent was obtained from people whose asset is used.\n\nAt submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.\n\nCrowdsourcing and Research with Human Subjects\n\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?\n\nAnswer:[N/A]\n\nJustification: Our paper does not involve crowdsourcing nor research with human subjects.\n\nGuidelines:\n\nThe answer NA means that the paper does not involve crowdsourcing nor research with human subjects.\n\nIncluding this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.\n\nAccording to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.\n\nInstitutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects\n\nQuestion: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?\n\nAnswer:[N/A]\n\nJustification: Our paper does not involve crowdsourcing nor research with human subjects.\n\nGuidelines:\n\nThe answer NA means that the paper does not involve crowdsourcing nor research with human subjects.\n\nDepending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.\n\nWe recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.\n\nFor initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.", "text_file": "data\\paper_texts\\2412.04786v1_content.txt"}, {"title": "MaterialPicker: Multi-Modal Material Generation with Diffusion\n  Transformers", "authors": ["Xiaohe Ma", "Valentin Deschaintre", "Milo\u0161 Ha\u0161an", "Fujun Luan", "Kun Zhou", "Hongzhi Wu", "Yiwei Hu"], "published_date": "2024-12-04T11:23:15Z", "summary": "High-quality material generation is key for virtual environment authoring and\ninverse rendering. We propose MaterialPicker, a multi-modal material generator\nleveraging a Diffusion Transformer (DiT) architecture, improving and\nsimplifying the creation of high-quality materials from text prompts and/or\nphotographs. Our method can generate a material based on an image crop of a\nmaterial sample, even if the captured surface is distorted, viewed at an angle\nor partially occluded, as is often the case in photographs of natural scenes.\nWe further allow the user to specify a text prompt to provide additional\nguidance for the generation. We finetune a pre-trained DiT-based video\ngenerator into a material generator, where each material map is treated as a\nframe in a video sequence. We evaluate our approach both quantitatively and\nqualitatively and show that it enables more diverse material generation and\nbetter distortion correction than previous work.", "arxiv_id": "2412.03225v2", "html_link": "https://arxiv.org/html/2412.03225v2", "search_term": "ti:\"transformers\"", "html_content": "SECTION: MaterialPicker: Multi-Modal Material Generation with Diffusion Transformers\n\nHigh-quality material generation is key for virtual environment authoring and inverse rendering. We propose MaterialPicker, a multi-modal material generator leveraging a Diffusion Transformer (DiT) architecture, improving and simplifying the creation of high-quality materials from text prompts and/or photographs. Our method can generate a material based on an image crop of a material sample, even if the captured surface is distorted, viewed at an angle or partially occluded, as is often the case in photographs of natural scenes. We further allow the user to specify a text prompt to provide additional guidance for the generation. We finetune a pre-trained DiT-based video generator into a material generator, where each material map is treated as a frame in a video sequence. We evaluate our approach both quantitatively and qualitatively and show that it enables more diverse material generation and better distortion correction than previous work.\n\nSECTION: 1Introduction\n\nHigh-quality materials are a core requirement for photorealistic image synthesis. We present a multi-modal material generator, conditioned on a text prompt and/or an image. The image can be a photograph containing a material sample captured at any angle, potentially distorted or partially occluded. Our model lets users \u201cpick\u201d materials from any photograph just by outlining a rough crop square around the material sample of interest.\n\nTraditional material acquisition often requires tens or hundreds of photo samples under known light conditions and camera poses. Even with recent advances in material acquisition allowing single or few image(s) capture[8,9,30,42,17,55,47], strong restrictions on the capture conditions are imposed. These methods typically require a flash light co-located with the camera as the only light source, and/or a fronto-parallel view of a flat sample. Even methods designed for capture using non-planar photographs[28]cannot handle significant texture distortion in the input photographs. Many recent material generation methods are trained from scratch on synthetic materials, limiting the generation diversity due to limited datasets[47,1], as compared to general-purpose text-to-image diffusion models[31,38,39].\n\nWe propose to tackle these challenges with two new ideas. First, we create a dataset which contains 800K crops of synthetic scene renderings, textured with randomly assigned materials, with each crop paired with its ground truth material. Using this data, we can train our model for the \u201cmaterial picking task\u201d, with various observation angles and distortion. We additionally use a text-to-material dataset[47]containing 800K pairs of text descriptions and associated ground truth material maps, encouraging material generation diversity and resulting in a multi-modal generator that can accept images, text or both.\n\nSecond, we re-purpose a text-to-video generation model to generate material maps instead. We use a Diffusion-Transformer (DiT) based architecture, which has been shown to be effective for high-quality video synthesis[6]. However, our target domain is materials, which we represent as a set of 2D maps (albedo, normal, height, roughness, metallicity). To adapt our base DiT model, trained on videos, to materials, we finetune it by considering each material map as a \u201cframe\u201d in a video sequence. This approach preserves the strong prior information in the video model, improving our method\u2019s generalization and robustness.\n\nWe evaluate our model on both real and synthetic input images and compare it against the state-of-the-art methods for texture rectification[19], material picking[28]and text-to-material generation[47,48]. We show that our approach generates materials that follow the input text prompt and/or match the appearance of the material sample in the input image, while correcting its distortions.\nIn summary, we make the following contributions:\n\nWe propose a material generation model which uses text and/or image prompts as inputs, while being robust to distortion, occlusion and perspective in the input image.\n\nWe design a large-scale dataset of crops of material samples paired with the corresponding ground truth material maps, enabling our model to handle a range of viewing angles and distortion.\n\nWe adapt a Diffusion Transformer text-to-video model for material generation by treating material maps as video frames, preserving the original prior knowledge embedded in the model to generate diverse materials.\n\nSECTION: 2Related Work\n\nSECTION: 2.1Material Acquisition and Generation\n\nWhile material acquisition has been a long standing challenge[15], lightweight material acquisition and generation have seen significant progress using machine learning. Various methods were proposed to infer PBR material maps from only a single or few photographs[8,9,10,42,17,55]. However, these methods rely on specific capture condition using a flash light co-located with the camera location.Martin et\u00a0al.[30]propose to use natural illumination but doesn\u2019t support direct metallic and roughness map estimations. Further, these methods rely on the camera being fronto-parallel or very close to it. This kind of photographs require specific captures, making the use of in the wild photos for material creation challenging.\n\nAs an alternative to create materials, generative model for materials were proposed. GAN-based approaches[18,56]show that unconditional generation of materials is possible and can be used for material acquisition via optimization of their noise and latent spaces. Recent progress in generative model, and more specifically diffusion models[39], enabled more stable, diffusion based, material generators[48,50]. Such diffusion models can also be used to support material acquisition tasks[47], for example when paired with ControlNet[54].\nAll these diffusion-based approaches either attempt to train the model from scratch, using solely synthetic material data[48,47]or significantly alter the architecture of the original text-to-image model[50], preventing the use of the pre-existing priors in large scale image generation models[39], limiting their generalization and diversity. Further, image prompts are limited to fronto-parallel photographs, which requires a specific capture.\n\nOther methods leveraged transformers as a model for material generation[16,25]but focused on procedural material, which relies on generating functional graph generation, a very different modality. These procedural representations have resolution and editability benefits, but cannot easily model materials with complex texture patterns in the wild. In contrast, our model supports generating materials from any image or text prompt and produces varied, high-quality material samples.\n\nSECTION: 2.2Material Extraction and Rectification\n\nDifferent methods were proposed to rectify textures or generally enable non-fronto-parallel textures as input. Some approaches[52,51]aim to evaluate the materials in an image through a retrieval and optimization method. Given an image, they retrieve the geometries and procedural materials in databases to optimize their position and appearance via differentiable rendering[43,53]. Closest to our work is Material palette[28], targeting material extraction from a single photo, not restricted to fronto parallel images. The method leverages Dreambooth[40]optimized through a LoRA[24]on Stable Diffusion[39]to learn a \u201cconcept\u201d for each material. This lets them generate a texture with a similar appearance to the target material and use a separate material estimation network to decompose the texture into material maps. However, this LoRA optimization step takes up to 3 minutes for each image, and we find that our approach reproduces better the target appearance.\n\nA related field is that of texture synthesis from real-world images. Wu et al.[49]present an automatic texture exemplar extraction based on Trimmed Texture CNN. VQGAN[12]achieves high resolution image-to-image synthesis with a transformer-based architecture. These methods however do not support the common occlusions and deformations that occur in natural images. To tackle this limitation,Hao et\u00a0al.[19]propose to rectify occlusions and distortions in texture images via a conditional denoising UNet with an occlusion-aware latent transformer. We show that our approach yields better texture rectification and simultaneously generates material parameters.\n\nSECTION: 2.3Diffusion Models and Diffusion Transformers\n\nDiffusion models[23,44,45,46]are state-of-the-art generative models, showing great results across various visual applications such as image synthesis and video generation. The core architecture of diffusion models progressed from simple UNets, incorporating self-attention and enhanced upscaling layers[11], prior-based text-to-image model[31,38], a VAE[26]for latent diffusion models (LDM)[39]and temporal attention layers for video generations[3,2]. These image generation methods all rely on a U-net backbone, a convolutional-based encoder-decoder architecture.\n\nRecently, transformer-based diffusion models, Diffusion Transformers (DiT) were proposed[35], benefiting from the scalability of Transformer models, removing the convolutions inductive bias. PixArt-presents a DiT-based text-to-image that can synthesize high resolution images with low training cost. Stable Diffusion 3[13]demonstrates that a multi-modal DiT model trained with Rectified Flow can achieve superior image synthesis quality. Compared to the U-net architecture, the DiT shows greater flexibility in the representation on the visual data, which is particularly important to video synthesis tasks. Sora[6], a DiT-based video diffusion model, encodes video sequences as tokens and uses transformers to denoise these visual tokens, demonstrating the ability to generate minute-long, high-resolution high-quality videos. We adapt a DiT-based video generation model for our purpose and show that it can be flexibly transformed into a multi-channel material generator.\n\nSECTION: 3Method\n\nSECTION: 3.1Diffusion Transformer\n\nDiffusion models are generative models that iteratively transform an initial noise distribution (e.g. Gaussian noise) into a complex real-world data distribution (e.g., images, or their encodings).\nThe diffusion process relies on aforwardprocess that progressively transforms the original data distribution into a noise distribution. For example, this can be achieved by iteratively adding Gaussian noise to the data sample. Given data samples, corrupted dataare constructed indiffusion steps.\n\nTo sample the original data distributionfrom the noise distribution, areversemappingneeds to be modeled whereis the noise predicted at each step by a neural network. The neural networkis conditioned on the denoising stepto predict the noise, which is then used to reconstructfromin each reverse step[23]:\n\nwhereis conditional inputs (e.g., text prompts or images).\n\nWe use a Diffusion Transformer[35]architecture as a backbone to model. The visual datais tokenized patch-wise, resulting in visual tokenswhereare the spatial and temporal dimensions of the video,is the number of tokens andis the feature dimension. Positional encoding is also added toto specify spatial and temporal order. Any conditionis also embedded as tokenswhereis the number of the tokens for conditional inputs. For example, whenis a text, it is encoded by an pre-trained encoder[37]with additional embedding layers to map it into the same feature dimension. The transformeris trained to denoise each patch at timestep. The final denoised patchesare reassembled as visual dataafter decoding through linear layers. Since the number of tokens grows quickly with resolution, we use a variational autoencoder (VAE) model[35,39]before the tokenizing process, producing a latent representation ofof the original datafor the transformer to process.\n\nSECTION: 3.2Datasets\n\nTo train our material generative model, we propose two datasets,ScenesandMaterials. Together, these datasets enable joint training for both surface rectification and high quality material generation.\n\nFor theScenesdataset, we build a set of synthetic indoor scenes with planar floors, walls, and randomly placed 3D objects, such as cubes, spheres, cylinders, cones, and toruses, similar to random Cornell boxes[34]. Each object is randomly assigned a unique material from around 3,000 stationary (i.e., approximately shift invariant) materials. Using this approach we create a dataset of 100,000 high-resolution rendered images, with different kinds of light sources, including point lights and area lights, to simulate complex real-world illumination (see Fig.2). We randomly place cameras to capture a wide variety of view points and maximize coverage.\n\nWe further crop the rendered images to construct training data, including input images, corresponding material maps, binary material mask, and the material name as an optional text prompt. During cropping, we ensure that the dominant material occupies at least 70% of the region. Importantly, we rescale the material maps based on UV coordinates to ensure that the rendered crops and target material maps share a matching texture scale. After cropping, this dataset contains 800,000 text-image-mask-material tuples.\n\nAs ourScenesdataset only contains stationary materials, it may fail to represent the full diversity of textures in the wild. To enhance the generalization capability, we use an additionalMaterialsdataset[30], which we augment to 800,000 cropped material maps. We use the name of the materials as the text prompts for text-to-material generation. These data items can be thought of as text-material pairs. This additional data diversity leads to significant improvement for non-stationary textures in input photographs as discussed in Sec.4.4.2.\n\nSECTION: 3.3Generative Material Model\n\nWe employ a pre-trained DiT-based text-to-video generative model, with an architecture similar to the one described inBrooks et\u00a0al.[6], as a base model. We retarget it into a multi-channel material generator.\n\nTo retarget this model while preserving its learned prior knowledge, we stack the material maps(albedo map, normal map, height map, roughness map and metallicity map) into a \u201cvideo\u201d of 5 frames , and compute the temporal positional embedding assuming their time stamp interval is 1 e.g., fps=1. Since DiT flexibly generates tokenized data, as opposed to a U-net architecture[2], the number of frames it is able to produce is not fixed, allowing us to adapt the original video generator to generate the right number of \u201cframes\u201d to meet our requirement. Our proposed use of a video model is in contrast to image diffusion models which typically generate 3 channels (RGB) and need to be non-trivially adjusted to generate more channels[27].\n\nTo enable material generation from an image input, we consider the input imageas the first frame, with the model generating the stacked material mapsas the subsequent frames, similar to a video extension model. Recall that the input image can be captured with arbitrary camera pose, and may include perspective and distortions. Using this approach, the self-attention mechanism of the transformer ensures that the generated material parameters are aligned with each other, and allows for non-aligned pixels between the input image condition and the generated material maps. The model simply learns that all frames except the first need to be aligned. This property is key for texture rectification, which is challenging for convolution-based architectures, in which (approximate) pixel alignment between input and output is built in due to the convolution inductive bias.\n\nWe additionally train our material generator to produce a segmentation mask for the dominant material in the crop. Typically, the user-provided crop is not entirely covered by a single material (see Fig.4). Performing conservative cropping on an image may reduce the number of usable pixels, while using an additional segmentation mask requires additional user input or a separate segmentation model[41]. Instead, our model automatically identifies the dominant material[29]in the image. We add a maskto be inferred from the input image as the second frame. Our training datacan thus be represented as, where; we have 7 RGB frames: input, mask, and five material maps. Noiseis applied only to the last six frames occupied byand, resulting in, with the first frame (input image) remaining free of noise. Our objective from Eq.1is\n\nwheredenotes the text input (material description). This process can be seen as a completion of the frames (mask and material channels) given the input image and text condition. The notationrefers to the last 6 frames generated by the Transformer.\nWhen the input consists solely ofwithout,whereis a uniformly white RGB image. The computation of the loss remains unchanged.\n\nSECTION: 3.4Training and Inference\n\nWe finetune the pre-trained DiT model using the AdamW optimizer on 8 Nvidia A100 GPUs. The learning rate is set atwith an effective batch size of 64. The model is finetuned onresolution for about 70K steps, which takes 90 hours. During training, we feed data from our two training datasetsScenesandMaterialsin a 5:3 ratio, prioritizing the task of image-conditioned material generation. We also randomly drop the conditions to retain the capacity to use classifier-free guidance (CFG)[22]. For text-only or unconditional generation, the mask is replaced by a completely white image placeholder.\n\nOur model completes a generation in 12 seconds using 50 diffusion steps. The model natively outputs a resolution of 256 due to limited computational resources. We apply an upsampler[32]to increase the resolution of each material map to 512 \u00d7 512.\n\nSECTION: 4Results\n\nWe evaluate the performance of our MaterialPicker across multiple dimensions. First, we perform qualitative and quantitative comparisons with Material Palette[28]on material extraction using both synthetic and real-world images (Sec.4.2). Next, we compare with a texture rectification method on real-world images (Sec.4.2) and with MatGen[47]and MatFuse[48]on text-to-material generation (Sec.4.3). Finally, we conduct ablation studies on multi-modality, dataset design and evaluate the impact of the input image scale(Sec.4.4). We also ablate the usage of a mask and evaluate robustness to distortion and lighting/shadowing in the supplemental materials.\n\nSECTION: 4.1Evaluation dataset and metrics\n\nSynthetic Evaluation dataset.For systematic evaluation, we build a synthetic evaluation dataset by gathering a diverse set of 531 materials from PolyHaven[36], applied to three interior scenes from the Archinteriors collection[14](which are completely independent from our training set). For each scene, we sequentially apply the 531 collected materials to a designated object inside the scene, and render 2D images using Blender Cycles[4]with the scene\u2019s default illumination setup. We generate a total of 1,593 synthetic renderings, and crop a square around the location of the object with replaced materials.\n\nReal Photographs Evaluation Dataset.To validate the generalization of our models, we curate an evaluation dataset containing real photographs captured by smartphones. This dataset covers a comprehensive set of real-world materials observed under both natural outdoor lighting and complex indoor illumination. We crop the photographs with a primary focus on our target material, without strictly limiting the cropping boundaries.\n\nEvaluation Metrics.Since we do not target pixel-aligned material capture, per-pixel metrics cannot be used for our results. Instead, we focus on theappearance similarityof the materials extracted from the photo inputs. Following related work on high-fidelity image synthesis such as DreamBooth[40], we leverage CLIP-I, which is the average pairwise cosine similarity between ViT-L-14 CLIP[37]embeddings of two sets of images. We also use the DINO metric[40]to measure the average pairwise cosine similarity between ViT-L-16 DINO embeddings. Additionally, we report the FID score[21]to measure the statistical visual similarity between two image distributions. We compute the FID score for each of the 531 output material map sets against the corresponding ground truth material map sets, and average the FID scores over the three scenes in our synthetic evaluation dataset.\n\nSECTION: 4.2Image Conditioned Generation.\n\nWe evaluate the performance of our model on both synthetic images and real photographs. We first show a visual comparison with the state-of-the-art method Material Palette[28]on our synthetic evaluation dataset (Sec.4.1). Since Material Palette generates only three material maps (albedo, normal, and roughness), we present both qualitative and quantitative results for these channels, along with the re-rendered images using these generated material maps. Our method takes 12 seconds to generate a material while Material Palette takes 3 minutes, on the same Nvidia A100 GPU, a 15 times speedup. Furthermore, our model can generate materials in batches. In Fig.3we show that our model produces material maps with a closer texture appearance and better matching the ground-truth material maps. In contrast, Material Palette struggles to reconstruct structured textures often resulting in distorted lines. We also observe that in the rendered images, our generated materials better matches the original input images.\n\nWe include a quantitative comparison with Material Palette on the entire synthetic dataset in Tab.1. We find that our proposed model performs better on all three metrics for the vast majority of the generated material channels and the corresponding re-rendered images.\n\nOurs\n\nMaterialPalette\n\nOurs\n\nMaterialPalette\n\nOurs\n\nMaterialPalette\n\nOurs\n\nMaterialPalette\n\nOurs\n\nMaterialPalette\n\nOurs\n\nMaterialPalette\n\nWe show qualitative evaluation on real photographs in Fig.4where we can see that our model generalizes well to photographs of materials from various angles. We render the generated materials on a planar surface under environment lighting, showing strong visual similarity to the original input images. Unlike Material Palette, which requires input masks from a separate segmentation step[41], our model operates out-of-box with an input image only, showcasing its potential as a lightweightMaterialPicker.\n\nSince our model automatically performs perspective rectification on the generated materials, we further compare against another state-of-the-art texture rectification and synthesis method[19]. In Fig.5, we evaluate both methods using real photographs. Since our model directly outputs material maps, instead of textures, we present our results by rendering them under different environment maps. We find that the compared method doesn\u2019t generalize well to real-world photographs, taken from non-frontal and/or non-parallel setups and fails to correct distortion in these cases. In contrast, our approach synthesizes a fronto-parallel view and remains robust across various real-world lighting conditions and viewing angles. Finally, as previously, our model does not require detailed masks as input, directly rectifying the dominant texture in the input image.\n\nSECTION: 4.3Text Conditioned Generation.\n\nAlthough the primary focus of our method is the generation of materials from photos, our multi-modal model also supports text-conditioned generation without image inputs. We evaluate its performance on the text-to-material task, comparing it with two state-of-the-art diffusion-based generative models for material synthesis: MatFuse[48]and MatGen[47]. As shown in Fig.6, our model demonstrates strong text-to-material synthesis capability, producing high-quality material samples, comparable to other state-of-the-art approaches. Leveraging a pretrained text-to-video model as a prior, our model can interpret complex semantics beyond the material-only training set, such as \u201dwood rings\u201d and \u201dfloral\u201d patterns.\n\nOurs\n\nMatGen\n\nMatFuse\n\nOurs\n\nMatGen\n\nMatFuse\n\nOurs\n\nMatGen\n\nMatFuse\n\nSECTION: 4.4Ablation Study\n\nOur generative material model takes advantages of its multi-modality. Though it is designed to create material maps from input photographs, it can benefit from additional signal to reduce the ambiguity of a single in-the-wild photograph. We present different combinations of input conditions in Fig.7including 1) text condition only; 2) image condition only and 3) text+image dual conditions. We found that text conditioning provides high level guidance for material generation. On the other hand, image conditioning contains ambiguities, as lighting and camera poses are uncontrolled. Combining both options enables text prompts to guide the model in identifying the reflective properties of a material. For instance, by prompting the model with appropriate text, it can better differentiate between metallic and non-metallic materials, as shown in the third example in Fig.7.\n\nIn Sec.3.2, we introduce two datasets used to train our model. To confirm that using both datasets help, we train a variant using only theScenedataset. Since this dataset primarily contains stationary materials, training exclusively on it reduces our model\u2019s generalization for complex texture patterns commonly found in real-world scenarios as shown in Fig.8. By mixing additional training data, our model synthesizes more diverse texture patterns and features such as woven pattern or the texture of a manhole cover.\n\n\u201cWovenratten\u201dMixed dataset\n\n\u201cWovenratten\u201dSingle dataset\n\n\u201cManholecover\u201dMixed dataset\n\n\u201cManholecover\u201dSingle dataset\n\nReproducing the texture scale in the input photos is critical for material generation. As we process our training data to align the scales of input images and output material maps (Sec.3.2), our model generates scale-matched materials, as shown in Fig.9. We see that our result follow the scale of the input as it increases from top to bottom.\n\n\u201dMarble tiles\u201d\n\n\u201dMarble tiles\u201d\n\n\u201dMarble tiles\u201d\n\nSECTION: 5Limitations\n\nDespite strong generation capacity, our model may still encounter challenging inputs, as shown in Fig.10. In the first row we show an example where our model confuses shading and albedo variation. Our model may also have difficulty handling materials with cutouts or holes, since it does not produce opacity maps as outputs. Finally, preserving semantically meaningful patterns, such as text, is a remaining challenge in our approach.\n\n\u201cConcretepavement\u201d\n\n\u201cPerforatedmetal\u201d\n\n\u201cTowel\u201d\n\nSECTION: 6Conclusion\n\nWe present a generative model for high-quality material synthesis from text prompts and/or crops of natural images by finetuning a pretrained text-to-video generative model, which provides strong prior knowledge. The flexible video DiT architecture lets us adjust the model for multi-channel material generation. We show extensive evaluation on both synthetic and real examples and conduct systematic ablation studies and test on robustness. We believe that our re-purposing of a video model for multi-channel generation opens an interesting avenue for other domain which require the generation of additional channels, such as intrinsic decomposition[47].\n\nSECTION: References\n\nSupplementary Material\n\nSECTION: Appendix AMore Results\n\nSECTION: A.1Image Conditioned Generation\n\nWe show qualitative and quantitative comparisons with Material Palette[28]on material extraction in Sec.4.2of the main paper.\nWe provide more visual comparisons in Fig.15and Fig.16. Since Material Palette generates only three material maps (albedo, normal, and roughness), we present results for these channels, along with the re-rendered images. We report the average CLIP-I metricand DINO metricbetween the output material maps and ground truth in Tab.1in the main paper. We further report the 95% confidence interval in Tab.2of the Supplemental Material. Our method achieves higher 95% confidence intervals for the vast majority of the generated material channels with the exception of the Albedo for which the intervals overlap. Our re-rendered images also show consistently higher alignment with the ground truth.\n\nSECTION: A.2Ablation Study\n\nAs opposed to existing material generation models, our model doesn\u2019t require the target material to cover the entire input image[47]or manually-created masks[28]to identify the sample of interest. Our model instead outputs a mask along with the generated materials. To assess the impact of generating this mask, we train an alternative model using our two datasets, with a slight modification to the model configuration. We add noiseto the material mapsonly, with, leaving the image and mask as non-noised inputs (orwithout), using our adaptation of a video model (as described in Sec.3.3).\nThe loss is then computed on the material mapsonly. As shown in Fig.11, we find that our proposed model, which automatically predicts a mask, performs comparably well to this variant requiring the mask as input.\n\n\u201dCeramictiles\u201d\n\nInput\n\n\u201dCeramictiles\u201d\n\nOutput\n\n\u201dStone wall\u201d\n\nInput\n\n\u201dStone wall\u201d\n\nOutput\n\nSECTION: A.3Evaluations on the Robustness.\n\nWe further test our model\u2019s robustness under various factors, including different texture scales (in the main paper), varying levels of distortion, and diverse lighting conditions.\n\nTo examine the robustness of our model to strong, real-world, distortions, we generate a synthetic test set that use textures from the texture datasets TexSD[28]and follow the texture processing steps outlined byHao et\u00a0al.[19]. We apply homography transformations[20]and thin plate spline transformations[5]to the textures.\nOur results in Fig.12show that the model is robust to severe distortions, stretching, and the blurring effects introduced by these transformations. More examples of real photos with distortion or surface geometry diversity can be found in Fig.17, Fig.18, Fig.19, Fig.20, Fig.21and Fig.22.\n\n\u201dGingham\u201d\n\n\u201dMarbelmosaic\u201d\n\n\u201dIce\u201d\n\n\u201dLeather\u201d\n\nWe further evaluate the model\u2019s performance when the input image contains specular highlights and shadows in Fig.13. We see that these highlights and shadows in real photos do not \u201cleak\u201d into material maps, highlighting the model\u2019s robustness to various lighting conditions.\n\n\u201dWood\u201d\n\n\u201dLeather\u201d\n\n\u201dMarble\u201d\n\nSECTION: Appendix BAdditional Real Examples\n\nIn Fig.14, we provide the material maps of the five examples used for texture synthesis in Fig.5of the main paper. In Fig.17,\nFig.18, Fig.19, Fig.20, Fig.21and Fig.22, we show additional examples of material extraction using our model, along with the uncropped original images from our real photographs evaluation dataset (Sec.3.2). These examples include various indoor and outdoor materials captured under complex real-world lighting conditions. Our model generalizes well to real photos, producing renderings that are visually similar to the photographs and providing accurate masks, demonstrating our model\u2019s generalization capabilities.", "text_file": "data\\paper_texts\\2412.03225v2_content.txt"}, {"title": "Transformation and symmetries for the Andrews-Garvan crank function", "authors": ["Rishabh Sarma"], "published_date": "2023-01-26T08:43:35Z", "summary": "Let $R(z,q)$ be the two-variable generating function of Dyson's rank\nfunction. In a recent joint work with Frank Garvan, we investigated the\ntransformation of the elements of the $p$-dissection of $R(\\zeta_p,q)$, where\n$\\zeta_p$ is a primitive $p$-th root of unity, under special congruence\nsubgroups of $SL_2(\\mathbb{Z})$, leading us to interesting symmetry\nobservations. In this work, we derive analogous transformation results for the\ntwo-variable crank generating function $C(z,q)$ in terms of generalized eta\nproducts. We consider the action of the group $\\Gamma_0(p)$ on the elements of\nthe $p$-dissection of $C(z,q)$, leading us to new symmetries for the crank\nfunction. As an application, we give a new proof of the crank theorem predicted\nby Dyson in 1944 and resolved by Andrews and Garvan in 1988. Furthermore, we\npresent identities expressing the elements of the crank dissection in terms of\ngeneralized eta products for primes $p=11,13,17$ and $19$.", "arxiv_id": "2301.10991v3", "html_link": "https://arxiv.org/html/2301.10991v3", "search_term": "ti:\"transformers\"", "html_content": "SECTION: Transformation and symmetries for the Andrews-Garvan crank function\n\nLetbe the two-variable generating function of Dyson\u2019s rank function. In a recent joint work with Frank Garvan, we investigated the transformation of the elements of the-dissection of, whereis a primitive-th root of unity, under special congruence subgroups of, leading us to interesting symmetry observations. In this work, we derive analogous transformation results for the two-variable crank generating functionin terms of generalized eta products. We consider the action of the groupon the elements of the-dissection of, leading us to new symmetries for the crank function. As an application, we give a new proof of the crank theorem predicted by Dyson in 1944 and resolved by Andrews and Garvan in 1988. Furthermore, we present identities expressing the elements of the crank dissection in terms of generalized eta products for primesand.\n\nSECTION: 1.Introduction\n\nThe rank function is an important statistic in the theory of partitions. It is defined as the largest part minus the number of parts in a partition. In 1944, Dyson gave this definition of the rank statistic and conjectured that it decomposes the partitions ofandinto five and seven equinumerous classes respectively, a conjecture that was proven 10 years later by Atkin and Swinnerton-Dyer. He also prophesied the existence of a \u201ccrank\u201d statistic for a similar explanation of the modcongruence of Ramanujan viz.\n\nand it wasn\u2019t until 1988 that the elusive crank was found by Andrews and Garvan[1]. The crank of a partition is defined as the largest part if there are no ones in the partition and otherwise the number of parts larger than the number of ones minus the number of ones. It is worth mentioning here that after the discovery of the partition crank, Garvan, Kim and Stanton[13]came up with new statistics on partitions (also called cranks) which combinatorially prove Ramanujan\u2019s congruences for the partition function modulo,,and, giving explicit bijections between equinumerous crank classes. However, the crank statistic that we work on in this paper is the ordinary partition crank defined by Andrews and Garvan which we simply refer to as the crank. Letdenote the number of partitions ofwith crank congruent tomod. Then they show that\n\nthus providing an almost combinatorial explanation of all three partition congruences of Ramanujan. Let, where. Letdenote the number of partitions ofwith crank. Then the two variable generating function for the crank function, due to Andrews and Garvan (see[1]) is given by\n\nEquivalently we define\n\nIn this paper, we look at the transformation and symmetry of this function, whenis a primitive-th root of unity. We find that the crank function satisfies the same type of symmetries as the rank generating function, which was our subject of interest in[14].Throughout this paper we use the standard-notation:\n\nWe also define a class of generalized eta products and functions that we will use frequently in expressions for our crank identities and to study their transformations in the subsequent sections.\n\nFollowing Biagioli (see[5]), define\n\nwherewithand.Then, for a vector, define\n\nWe also define the weightslash operator on the set of meromorphic functions onby\n\nfor alland. We remind the reader that the automorphy factorappears as a prefactor in the transformation property whenis weakly modular of weight.Letdenote the number of partitions ofwith rankand letdenote\nthe two-variable generating function for the Dyson rank function so that\n\nThe modularity of the rank generating function, whereis replaced by a primitive-th root of unity for a general primewas first studied by Bringmann and Ono[7]. Building on the groundbreaking work of Zwegers[22]and[23]around the turn of the century, who realised how Ramanujan\u2019s mock theta functions occur naturally as the holomorphic parts of certain real analytic modular forms, Bringmann and Ono showed thatis the holomorphic part a weak Maass form of weighton, where the non-holomorphic part is a Mordell integral of a theta function. Garvan[12], in 2019, observed that the introduction of a simple multiplier and a generalized correction series factor reduces the modularity of the rank function to larger and more natural congruence subgroups of.In a subsequent joint work with Garvan[14], we generalized and improved these results of his paper[12]on transformations for Dyson\u2019s rank function leading us to exciting observations of symmetry among the elements of dissection of the rank generating function. In this paper, we try to implement our ideas in[14]and those of Garvan[12]to obtain analogous results for the crank generating function.Since its discovery, it has been well established that the crank statistic plays a central role in the theory of partitions. Perhaps the most important breakthrough came in 2005 when Mahlburg showed that the crank itself satisfies Ramanujan-type congruences using Hecke\u2019s theory of modular forms. In his seminal paper, Mahlburg establishes congruences for infinitely many non-nested arithmetic progressions modulo powers of primes by tying the crank generating function to the Dedekind eta function and Klein forms. Rewriting his functions in the framework of generalized eta products, we have the following result.\n\nLetbe prime, suppose, and define\n\nwhere, following Yang[21], we consider the generalized eta product\n\nThenis a weakly holomorphic modular form of weight 1 on the congruence subgroup.\n\nMahlburg further goes on to establish a rescaled form of the crank generating function as the sum of a weakly holomorphic function onand a modular form onof integral weights ([17], Section 4). We show that after multiplying with an appropriate eta product and a power of, we can deduce the modularity of the crank generating function to a simpler congruence subgroup of, resembling the result for the rank generating function[12, Theorem 1.2].\n\nLetbe prime. Then the function\n\nis a weakly holomorphic modular form of weighton the group, where.\n\nThis result is analogous to the modularity of the rank generating function that was studied in[12]. There the author also considered the modularity and transformation of elements of the-dissection of the rank generating function. We note that in the definition for these rank dissection elements, there are two cases pertaining to whetheris a quadratic residue or non-residue modulo[14, Definition 1.4]. In case of the crank generating function, we have the following analogous expression for elements of the dissection and subsequent modularity results. These dissection elements do not feature the correction factor involving the seriesas in the definition of the rank dissection elementswhere the seriesappears in the case whenis a quadratic residue modulo[14, Definition 1.4 (ii)].\n\nForprime,,and, define\n\nLetbe prime and suppose. Then\n\nis a weakly holomorphic modular form of weighton.\n\nIfthenis a weakly holomorphic modular form of weighton.\nIn particular,\n\nfor.\n\nAnalogous to the rank, the action of the congruence subgrouponleads to observation of symmetry among the elements of the crank dissection.\n\nLetbe prime,, and.\nThen\n\nassumingand\n\nNote : Whenis a quadratic residue/non-residue modulo, the same is correspondingly true for.Theelement of the dissection viz.defined above is also of further special interest. A beautiful symmetry among the zeta-coefficients in the identity forelement of the rank dissection expressed in terms of generalized eta functionswas given in[14]. Using our main symmetry result in Theorem1.6above and the conditions for modularity of the generalized eta functions[14, Theorem 3.1], we can determine the transformation ofand find that the exact same symmetry result holds for this crank counterpart of theelement. We do not pursue this here, but the interested reader could investigate further (see[14], Theorem 5.1 for details).The paper is organized as follows. In Section2we build up the framework of our results and introduce a class of eta functions arising in the definition of our completed crank function and determine their transformation under special congruence subgroups of. In Section3, we prove our main results on the modularity, transformation and symmetry of the crank function. Section4is devoted to calculating lower bounds for the orders ofat the cusps ofwhich we utilize to prove identities in the subsequent section. Finally, in Section5, we give a new proof of the crank theorem and also state identities forwhenandin terms of generalized eta-functions, proving two such identities modusing the Valence formula, the modularity conditions and the orders at cusps determined in the previous section.\n\nSECTION: 2.Preliminary definitions and Results\n\nIn Section1, we definedto be the number of partitions ofwith crank\ncongruent tomod, and let. Then we have\n\nWe introduce a class of generalized eta products and its transformation under matrices indue to Yifan Yang. We later show how it relates to the crank function.\n\nLetbe prime andbe arbitrary real numbers not simultaneously congruent tomodulo. For, we define the generalized Dedekind eta functionsby\n\nwhere. Then the following transformations hold :\n\n.\n\n.Moreover for, we have\n\nfor.\n\nforwhere\n\nWe now use Yang\u2019s transformation results to find the transformation of our functiondefined below under.\n\nLetbe prime, suppose, and define\n\nwhere, following Yang[21], we consider the generalized eta product\n\nThen\n\nwhere\n\nHereis the least nonnegative residue of.\n\nUsing Theorems2.1and[16, Theorem 2, p.51], we have\n\nNow, using Theorem2.1we have.Then,\n\nand we have our result.\n\u220e\n\nThe following corollary now follows easily and gives a simpler transformation.\n\nLetbe prime and suppose. Then\n\nwhere\n\nLetThenand. So\n\nTherefore,\n\nand we have our result.\n\u220e\n\nSECTION: 3.Proofs of main results\n\nSECTION: 3.1.Modularity and transformation results\n\nLet. The generating function of the crank due to Andrews and Garvan is given by\n\nReplacingbyin the above expression and multiplying both sides by, we can express the crank generating function in terms of our eta functions as\n\nThis leads to our first main result stated in the introduction section. We restate it here.\n\nLetbe prime. Then the function\n\nis a weakly holomorphic modular form of weighton the group.\n\nIn the light of Equation (3.1), the proof of the theorem is equivalent to showing thatis a weakly holomorphic modular form of weighton the group. The transformation condition holds using Corollary2.3and the well-known result thatis a modular function onwhenis prime. That is\n\nfor.We next check the cusp conditions and show thatwhen expanded as a series inhas only finitely many terms with negative exponents for. By Theorems2.1and[16, Theorem 2, p.51], we have\n\nwhereis a root of unity. Using the fact thatis a modular function onand the definition ofand, we find that the series has only finitely many terms with negative exponents.\u220e\n\nNext, we define the (weight) Atkinoperator by\n\nwhere\n\nand the more generalis defined as\n\nWe note that. In addition, if\n\nthen\n\nCombining Equation (3.1) and Definition1.4, we have\n\nForbe a prime andwe have\n\nThe modularity result of the crank function stated in the introduction section follows easily now. We restate the theorem here and note that the proof utilizes the same technique as it did for the rank dissection elements[12, Theorem 6.3, p.234]. The functions involved in the transformation are however different.\n\nLetbe prime and suppose. Then\n\nis a weakly holomorphic modular form of weighton.\n\nIfthenis a weakly holomorphic modular form of weighton.\nIn particular,\n\nfor.\n\nWe let\n\nand undergo the same matrix transformations as in the proof of[12, Theorem 6.3, p.234], with our functionreplacing, and apply Corollary2.3(since) to arrive at\n\nIt is easy to see that eachis holomorphic on. The cusp conditions follow by a standard argument. We examine orders at each cusp in more detail in a later section.\n\u220e\n\nSECTION: 3.2.Symmetry result analogous to the rank\n\nSupposeprime,, and.\nThen\n\nassumingand\n\nOnce again, we undergo the same matrix transformations as we did in the case of[14, Proposition 4.7], with our functionreplacing, and apply Proposition2.2. Since, we have\n\nTherefore\n\nusing the facts that\n\nand asruns through a complete residue system modso does.\nThe result follows.\n\u220e\n\nWe end this section with an illustration of the validity and advantage of our theorem. The following are the elements of the-dissection offorin terms of generalized eta-functions. A similar form of this identity can also be found in the works of Garvan[11]and Hirschhorn[15].\n\nWe consider theelement of the dissection\n\nand findfor\n\nUsing the transformations forand the theta function([12, Theorem 6.12 & 6.14, p.243]) and the fact that, we get\n\nwhich agrees with our symmetry result (3.4). We also make note of the fact thatandare both quadratic non-residues modulo. We can thus conclude that in the light of our symmetry theorem, the identities forin terms of generalized eta functions can be fully determined if we know one identity each for the cases when,is a quadratic residue modandis a quadratic non-residue mod.\n\nSECTION: 4.Lower bounds for orders ofat cusps\n\nIn this section, we calculate lower bounds for the orders ofat the cusps of, which we use in proving theidentities in the subsequent section.We first establish a few other useful transformation results.Letbe prime,, and define, where.We list the transformations ofunder matricesand, which are the generators of, and that ofunder.\n\n.\n\n.\n\n.\n\nProofs of (1) and (3) follow easily using the definition of. (2) follows from (1) and the transformation ofunderi.e.([16, Theorem 2, p.51]).\n\u220e\n\nLetThe action ofon each element can be given explicitly using Theorems[16, Theorem 2, p.51]and4.1. Also eachhas a-expansion\n\nwhere. We define\n\nFor any cuspwithwe define\n\nwhereand. We note that. We also note that this definition coincides with the definition of invariant order at a cusp[8, p.2319],[5, p.275].\nThe order of each functionis defined in the natural way i.e.\n\nwhereis the usual invariant order ofat the cusp[18, p.34].We determinefor each. After some calculation we find\n\nLetbe prime. Then\n\nWe also need[18, Corollary 2.2].\n\nLetand let\n\nwhere each. Then for,\n\nFrom[9, Corollary 4, p.930]and[9, Lemma 3, p.929]we have\n\nLetbe prime. Then we have the following set of inequivalent cusps forand their corresponding fan widths.\n\nWe next calculate lower bounds of the invariant order ofat each cusp of.\n\nLetbe prime, and suppose. Then\n\nand\n\nWe derive lower bounds forfor each cuspofnot equivalent to.We have\n\nwhere\n\nWe calculate\n\nfor eachand each.\n\n.\nChoosesuch that\n\nThen\n\nwhere\n\nFrom Proposition2.2we have\n\nwhereimplies thatis reduced modulo, and\n\n. In this case we find that\n\nwhere\n\nand\n\nFrom Proposition2.2we have\n\nBy Theorem4.1(2) we have\n\nand\n\nso that\n\nNow we are ready to exam each cuspof.\nWe choose\n\nHere,and we assume.\nIfthen applying (4.2) we have\n\nby Propositions4.3and4.2. Now applying (4.4)\nwithwe have\n\nagain by Propositions4.3and4.2. The result (i) follows since\n\nLetso that.\nIfwe apply (4.2) with, and find that\n\nNow we assumeand we will apply (4.4). We have\n\nand the result (ii) follows.\n\nChooseandso thatand. Sincewe may apply (4.2) for each. We find that, and\n\nThe result (iii) follows.\n\u220e\n\nSECTION: 5.Crank theorem and crank modidentities\n\nSECTION: 5.1.Dyson\u2019s rank conjectures and the crank theorem\n\n10 years after they were proposed by Dyson[10], Atkin and Swinnerton Dyer[4]came up with a proof of the Dyson\u2019s rank conjectures stated below.\n\nFor all nonnegative integers,\n\nGarvan, in[12]notes that the proof in[4]involved finding and proving identities for basic hypergeometric functions, theta-functions and Lerch-type series using the theory of elliptic functions. It also involved identifying the generating functions for rank differencesfor,for eachand each,,\u2026. Subsequently in his paper[12, Section 6.3], Garvan gives a new proof of these conjectures using the theory of modular forms, by calculating the orders of the rank dissection elementat the cusps offorandand making use of the Valence formula. In this section, we employ Garvan\u2019s idea in[12]to give a new proof of the crank theorem modulo(Equation (1.3)) using the Valence formula and the orders ofat the cusps ofcalculated in the previous section. It is easy to observe that in our setup, the crank theorem is equivalent to showing\n\nLetbe a modular form of weightwith respect to a subgroupof finite index\nin. Then\n\nwhereis indexinfor,\n\nis a fundamental region for,\nand\n\nfor a cuspanddenotes the fan width of the cusp.\n\nFor,is defined in terms of the invariant order, which is interpreted in the usual sense. See[20, p.91]for details of this and the notation used.\n\nIn the following table, we give the order ofat the cusps using Theorem4.5. Here.\n\nAn easy calculation by hand shows that. Expandingwe see that this implies the coefficient ofisand thus we have that. HenceBut. The Valence Formula implies thatis identically zero which proves the crank theorem.\n\nThe same method could be applied to prove the modandand cases of the crank theorem i.e. Equations (1.1) and (1.2).\n\nSECTION: 5.2.Crank modidentities\n\nIn this section, we give identities forwhenandin terms of generalized eta-functions given in Definition1.1. Crank modidentities were first studied by Bilgici in[6]similar to the work on rank modidentities by Atkin and Hussain[3]and rank modidentities considered by O\u2019Brien in his thesis[19]. In[14], we also gave such identities for elements of the the rank generating function. In general, these identities are of the form\n\nwhereare eta-quotients comprising of generalized eta-functions, andare cyclotomic coefficients. From Theorem3.3, we know thatis a weakly holomorphic modular form of weighton. The proof of the identities primarily involve establishing the equality using the Valence formula and showing that the RHS is also a weakly holomorphic modular form of weighton.In[14], we derived conditions for an eta-quotientto be a weakly holomorphic modular form of weighton.\n\nLetandbe as in Definition1.1.\nThenis a weakly holomorphic modular form of weight 1 onsatisfying the modularity conditionforprovided the following conditions are met :\n\nIn Section 7 of[14], we developed an algorithm for proving rank modidentities that utilizes the Valence Formula. Here, we follow the steps of the algorithm to prove crank identities modulo.For the casesand, we simply give the form of the identities omitting the values of the coefficients, for each of the casesand whenis a quadratic residue and a non-residue modulo. Their proofs follow the same technique using the algorithm as it does for.To help calculate the order of the generalized eta functions at the cusps, we also need the following result from[14, Prop.7.4].\n\nLetbe prime and supposeis one of the cusps listed\nin Proposition4.4withrepresented by.\nThen\n\nIfthen\n\nIfthen\n\nSECTION: 5.3.Crank mod 13 identities\n\nThe following is an identity forin terms of generalized eta-functions :\n\nwhere\n\nand the coefficients are :\n\nWe follow the steps of the aforementioned algorithm in the process of proving the identity.\n\nWe check the conditions for modularity as in Theorem5.3forinvolved in (5.7). Here,. In[14], Theorem 3.6 tells us that it suffices to check modularity for. With, we easily see that\n\nas required.Since, we skip Step 2 in accordance with our algorithm.\n\nUsing Proposition5.4, we calculate the orders of each of the six functionsat each cusp of.\n\nwhere.\n\nConsidering the LHS of equation (5.7), we calculate lower boundsof ordersat cuspsof of.\n\nwhereusing Theorem4.5. We note that each value is an integer.\n\nWe summarize the calculations in Steps 4 and 5 of the algorithm (see[14], Section 7) in a Table.\nThe gives lower bounds for the LHS and RHS of equation (5.7),\nat the cusps.\n\nwhere.The constantis the sum of the lower bounds in the last column, so that.\n\nThe LHS and RHS are weakly holomorphic modular forms of weighton. So in the Valence Formula,. The result follows provided we can show that. This is easily verified using MAPLE (seehttps://github.com/rishabh-sarma/crank-identities-modulo-13-17-19).\n\nThe following is an identity forin terms of generalized eta-functions :\n\nwhere\n\nand the coefficients are :\n\nWe follow the steps of the algorithm in the process of proving the above identity.\n\nWe check the conditions for modularity as in Theorem5.3forandinvolved in (5.8). We first make note of two aspects of our functions. First, from Apostol\u2019s[2, Theorem 4.9, p.87], we observe that the eta quotientis modular under the congruence subgroup. Also, writing the theta quotientin its equivalent vector notationin accordance with our Definition1.1, we observe that it doesn\u2019t contribute to the weight and\n\nThus it suffices to check the first step for the functions, which was achieved in Step 1 of the proof for the previous identity.\n\nNext, we calculate the orders of the generalized eta-functions atand considering the identity with zero coefficients removed, we find that. Thus we divide each generalized eta-function by, which has the lowest order at.\n\nUsing Proposition5.4, we calculate the orders of each of the six functionsat each cusp of.\n\nwhere.\n\nConsidering the LHS of equation (5.8) after division by,\nwe now calculate lower boundsof ordersat cuspsof.\n\nwhereusing Theorem4.5. We note that each value is an integer.\n\nWe summarize the calculations in Steps 4 and 5 in a Table.\nThe gives lower bounds for the LHS and RHS of equation (5.8) divided by,\nat the cusps.\n\nwhere.The constantis the sum of the lower bounds in the last column, so that.\n\nThe LHS and RHS are weakly holomorphic modular forms of weighton. So in the Valence Formula,. The result follows provided we can show that. This is easily verified using MAPLE (seehttps://github.com/rishabh-sarma/crank-identities-modulo-13-17-19).\n\nThe following is an identity forin terms of generalized eta-functions :\n\nwhere\n\nand the coefficients,are linear combinations of cyclotomic integers like the identities found previously. We do not include them here, but the exact identity along with the maple code can be found here :https://github.com/rishabh-sarma/crank-identities-modulo-13-17-19.\nUtilizing the same algorithm, the proof follows similar to the identity for the quadratic residue casedone previously. We omit the details here.Below, we present one identity for each of the quadratic residue, quadratic non-residue andcases forand. These are new and do not seem to appear in the literature elsewhere. We however make note of the fact that the same set of theta functionsand vectorsare involved in the corresponding identity for the rank ([14], Subsections 7.3 and 7.4). However, there are much fewer functions in the corresponding identity for the crank here. We do not include the cyclotomic coefficients here, but the exact identities alongwith the maple code can be found here :https://github.com/rishabh-sarma/crank-identities-modulo-13-17-19. Several of these coefficients are zero.\n\nSECTION: 5.4.Crank mod 17 identities\n\nThe following is an identity forin terms of generalized eta-functions :\n\nwhere\n\nand the coefficients,are linear combinations of cyclotomic integers like the modandidentities found previously.\n\nThe following is an identity forin terms of generalized eta-functions :\n\nwhere\n\nand the coefficients,are linear combinations of cyclotomic integers like the identities found previously.\n\nThe following is an identity forin terms of generalized eta-functions :\n\nwhere\n\nand the coefficients,are linear combinations of cyclotomic integers like the identities found previously.\n\nSECTION: 5.5.Crank mod 19 identities\n\nThe following is an identity forin terms of generalized eta-functions :\n\nwhere\n\nand the coefficients, andforare linear combinations of cyclotomic integers like the modandidentities found previously.\n\nThe following is an identity forin terms of generalized eta-functions :\n\nwhere\n\nand the coefficients,are linear combinations of cyclotomic integers like the identities found previously.\n\nThe following is an identity forin terms of generalized eta-functions :\n\nwhere\n\nand the coefficients,are linear combinations of cyclotomic integers like the identities found previously.\n\nSECTION: 6.Concluding remarks\n\nBuilding on our work on symmetries of the rank generating function[14], we have found analogous results for the crank generating function in this paper. The transformation and modularity properties as well as the-dissection identities are similar for the rank and crank functions. It would be interesting to examine the underlying theory behind this resemblance more closely. Two other natural problems to consider are finding crank dissection identities for higher primesand also exploring such symmetries for other partition statistics and also their overpartition analogues.\n\nSECTION: 7.Acknowledgments\n\nThe author would like to thank his advisor Frank Garvan for suggesting this problem and his support and guidance throughout the course of this project.\n\nSECTION: References", "text_file": "data\\paper_texts\\2301.10991v3_content.txt"}, {"title": "A Variational Approach to the Yamabe Problem: Conformal Transformations\n  and Scalar Curvature on Compact Riemannian Manifolds", "authors": ["Aoran Chen"], "published_date": "2023-09-05T17:11:23Z", "summary": "We start by taking the analytical approach to discuss how the minimizer of\nYamabe functional provides constant scalar curvature and its relationship with\nthe Sobolev Space $W^{1,2}.$ Then, after demonstrating the importance of the\nsphere $S^n$, with stereographic projection and dilation, we show that the\nminimizer of Yamabe functional on standard sphere is obtained from a standard\nround metric $\\bar{g}$ by a conformal diffeomorphism, thus giving us the\nconstraint $\\lambda (M) < \\lambda(S^n),$ which leads us to the final theorem\nthat the Yamabe problem is solvable when $\\lambda(M) < \\lambda(S^n).$ For the\nproof of this theorem, we adopt the approach of Concentration-Compactness.", "arxiv_id": "2309.02397v4", "html_link": "https://arxiv.org/html/2309.02397v4", "search_term": "ti:\"transformers\"", "html_content": "SECTION: A Variational Approach to the Yamabe Problem: Conformal Transformations and Scalar Curvature on Compact Riemannian Manifolds\n\nSECTION: 1Introduction and preliminaries\n\nThe Yamabe problem asks if any Riemannian metricon a compact smooth manifoldof dimensionis conformal to a metric with constant scalar curvature.\nThis problem was born in 1960, from Hidehiko Yamabe\u2019s attempt to solve the then not yet proved Poincar\u00e9 conjecture in his paper[yamabe].\n\nEvery simply connected, closed 3-manifold is homeomorphic to the 3-sphere. Where closed means compact without boundary.\n\nIn 2003 John Milnor wrote a nice survey[milnor]right after Perelman proved the Poincar\u00e9 conjecture. The Poincar\u00e9 conjecture is a purely topological statement, but turned out that it was proved by using the Riemannian metric structure on smooth manifolds.\n\nThe classical result by Killing and Hopf shows how the metric structure can give topological information, and it provides a road to the Poincar\u00e9 conjecture.\n\nThe universal cover of a manifold of constant sectional curvature is one of the model spaces:\n\nsphere (positive sectional curvature)\n\nplane (zero sectional curvature)\n\nhyperbolic manifold (negative sectional curvature)\n\n(SeeA.2for the definition of sectional curvature.)\n\nFrom the above theorem we understand that it suffices to prove the Poincar\u00e9 conjecture if, over on any simply connected closed 3-manifold, we can build a Riemannian metric with constant sectional curvature. It is a fundamental result that any smooth manifold has a Riemannian metric (see Definition 2.1 in the book[CarmoR]), but the question is whether such a Riemannian metric can have constant sectional curvature? Or, for an arbitrary Riemannian metric, are we able to deform this metric such that it has constant sectional curvature?\n\nAn approach to finding such a constant curvature metric is to identify the critical point of the Hilbert-Einstein action.\n\nFor a closed Riemannian manifold, the action is given as\n\nwhereandstand, respectively, for the scalar curvature (seeA.3for the definition of scalar curvature) ofand the volume form determined by the metric and orientation.\n\nThe survey[logunov2004were]provided a historical review on the finding of the Hilbert-Einstein equations. Following the idea of the least action principle (Hilbert\u2019s Axiom I), Hilbert found the critical point of the action, which gives us the well-known gravitational field equation.\n\nIfis the manifold constrained to metrics of volume one, the critical points of Hilbert-Einstein action must satisfy:\n\nwhereis the metric tensor,is the Ricci curvature (seeA.3for the definition of the Ricci curvature) ofandis the constant of proportionality.\n\nFor any Riemannian manifold whose Ricci tensor is proportional to the metric, it is an Einstein manifold; the metrics that satisfy (1) are Einstein metrics.\n\nIn Chapter 1.1 of the book[besse2007einstein], Besse introduced an important topological property of the Einstein Manifold:\n\nA closed 3-dimensional (pseudo) Riemannian manifold is Einstein iff it has constant sectional curvature.\n\nFrom the above proposition and Killing-Hopf (see Theorem1.1), we find that a 3-dimensional closed manifold that has constant sectional curvature is equivalent to a sphereSo it suffices to prove the Poincar\u00e9 conjecture if, for a 3-manifold, we can find critical points of the Hilbert-Einstein action that gives us constant sectional curvature.\n\nThis approach falls short because it is difficult to prove the existence of Einstein metrics that give us constant sectional curvature on any closed manifold. However, it is much easier to have a scalar curvature than having a constant sectional curvature. See the following example of a 3-manifold that does not have Einstein metric but has constant scalar curvature.\n\nThe topological property of Einstein metrics suggest that, if a 3-manifold has an Einstein metric, then it has constant scalar curvature, its universal cover is diffeomorphic toor(both Euclidean space and hyperbolic space are diffeomorphic to). Now considerwith the universal cover. Since it is not homeomorphic toor, hencedoes not have an Einstein metric.\nHowever, the scalar curvature of the product manifold is the sum of the scalar curvature of these two manifolds (see PropositionA.0.1), sohas constant scalar curvature ifandare equipped with the standard unit sphere metric.\n\nTherefore, it makes sense for Yamabe to consider another related problem in a more restricted area:\n\nGiven a compact Riemannian manifoldof\ndimension, is there a metricconformal tothat has a constant scalar curvature?\n\nThis survey serves as the capstone paper of my senior year under the guidance of Professor Engelstein. It follows that given in the survey[Neumayer], and the survey[Lee]; for the Riemannian geometry part it mostly refers to the book[CarmoR]; for the analysis part it mostly refers to the book[evans], and for the part of concentration compactness we follow the approach of Lions[lions1984concentration1,lions1984concentration2].\n\nWe will start by taking the analytical approach to discuss how the minimizer of Yamabe functional provides constant scalar curvature, and its relationship with the Sobolev SpaceThen with stereographic projection and dilation, we will show the importance of the sphere, and the fact that the minimizer of Yamabe functional on standard sphere is the standard metric and its conformal diffeomorphisms. This will give us the constraintwhich leads us to the final theorem that the Yamabe problem is solvable whenFor the proof of this theorem we follow the approach of concentration compactness.\n\nSECTION: 2The Analytical approach of the Yamabe Problem\n\nGiven two conformal metricsand(for choice of index, see AppendixB). Letanddenote the scalar curvatures ofand, respectively. These quantities are related by the identity\n\nRecall the Hilbert-Einstein action (see Definition1.1), that is,. Here we consider a normalized version\n\nin whichis chosen such thatfor any.\n\nNotice thatis the exponent in the strict Sobolev embedding (see AppendixCfor the fundamental knowledge of the Sobolev space), which will play a decisive role in the latter part.Therefore, we define the Yamabe constant ofby\n\nNow take the variation. For any variation, we have\n\nNotice thatandare numbers, letand we have. If u is a minimizer, from (2) we see thatis constant.\n\nTo summarize thus far, to solve the Yamabe problem, it suffices to show the existence of a smooth positive minimizer of. However, we will see in the later part that establishing such a minimizer is difficult because the problem lacks compactness.\n\nSECTION: 2.1Why do we consider Sobolev space\n\nTo find a minimizer of Yamabe functional, for a minimizing sequencethat satisfies, the goal is to show thatconverges to a smooth metric, which is the minimizer of the Yamabe functional.\n\nThen naturally 2 questions arise:\n\n1. How do we show the convergence?\n\n2. What are the properties of the limit metric?Whenever considering convergence of functions, we first need to choose a topology of convergence. In this case, we choose a norm on the function space. As we solve differential equations, it is natural to consider the Sobolev space. (seeC.1about the definition of Sobolev space.) The question is why are we looking at space?\n\nBasically, this is asking that, given the basic condition of the problem, what is the largest space thatcould live in andmust be square integrable for us to start talking about.\n\nConsider the minimizer of the normalized Hilbert-Einstein action. Its numerator is, and its denominator is. It is natural to let the normalizing term, and sinceis compact, we havefor some constant. Therefore,is bounded. It follows thatis bounded. Also, sinceis bounded, we see thatis bounded.\n\nIn summary, we have,,all bounded. Sois bounded in bothand. The Sobolev inequality (seeC.2about the Sobolev inequality) tells us thatis a more restrictive condition, hence it is natural to consider.\n\nSECTION: 2.2Lack of compactness of the embeddinginto\n\nThe Yamabe functional consists two parts: the numerator, which is close tonorm (seeC.2about Sobolev norm), and the denominator. So when the Yamabe functional is bounded and approaching a value, we can expect our function to weakly converge in thespace, and by the Sobolev embedding theorem (seeC.2.1), this also means that it will weakly converge in thespace.\n\nTherefore we cannot just apply the direct method of the calculus of variations, since the embedding ofintois not compact (for compactness theorem seeC.4), and we could have lower semicontinuity of the energy only if there is strong convergence of the minimizing sequence in\n\nThe following example shows that the direct method can be used to establish the existence of minimizers in a particular case of.\n\nSince, we have that, andiffis minimizer of the.Sinceis compact, we have the Sobolev embedding (seeC.2.1):\n\nThe first embedding is not compact. Hence ifis a sequence withconverge to, then by the Sobolev inequality, we see thatbounded impliesbounded (we can constrain on). So there is a subsequence such thatweakly converge inand, and converge strongly in. Since, we have that\n\nNote that ifthen the above inequality may not hold, since the lower semicontinuity of thenorm goes in the wrong direction. Also, the smoothness should relies on the regularity property of Laplacian operator.\n\nFrom the above example we see that the lower semicontinuity indoes not help since it is in the denominator. Another problem is that the weak limitmay be equal to zero. However, we could have lower semicontinuity of energyif the minimizing sequence converges strongly inFor example, if we consider the subcritical power:\n\nBy Rellich-Kondrachov theorem (seeC.4) the embeddingis compact, then with direct method there exist the minimizer\n\nHistorically, Trudinger gave this restrictive assumption in the paper[Trudinger], that the Yamabe problem could be solved wheneverIn fact, going one step further, he showed the existence of a positive constantsuch that the problem could be solved whenBased on Trudinger\u2019s result, Aubin showed in the paper[aubin1976equations]thatfor everyThis established the following theorem:\n\nSuppose. Then there exists a minimizer ofand hence a solution of the Yamabe problem on M.\n\nThis is one of the three main theorems of the Yamabe problem and serves as the main theorem of this survey. For the other two main theorems, see the survey[Lee]on the proofs ofin dimensionsand higher given, respectively, by Schoen[schoen1984conformal]and Aubin[aubin1976equations].\n\nSECTION: 3The Yamabe Problem on the Sphere\n\nFrom Theorem2.1we understand that the model case of the sphereplays an important role in the proofing of the Yamabe problem. We start this section by discussing a natural question, that is why do we consider the sphere?\n\nSECTION: 3.1Stereographic projection\n\nLetbe the north pole on. Stereographic projectionis defined byforwhere\n\nWe can verify thatis a conformal diffeomorphism. Ifis the standard metric on, andis the Euclidean metric on, then underthe round metric on spherecorresponds to\n\nThis can be written as\n\nWe denote this byin the latter part, where. By means of stereographic projection, it gives the conformal diffeomorphisms of the sphere induced by the standard conformal transformations on the plane, as shown in the diagram\n\nThe group of such diffeomorphisms is generated by the rotations, together with maps of the formwhereis the dilationforCombine with (3), we get the spherical metric ontransforms under dilations to\n\nFor standard metric onwe have\n\nwhereConsider the dilation. Since it\u2019s conformal transformation we get that\n\nwhereis given by (4).\nNotice, then we rewrite as\n\nConsidering the two cases of whetheris at the south pole, letwe get that,\n\nWe see that forconverges weakly to 0, so allconcentrate near the South pole. Now consider this metric on the neighbourhood of any manifold, we have\n\nHence we obtain the restriction in Theorem2.1.\n\nSECTION: 3.2Two important results on the sphere\n\nAfter understanding the importance of the sphere, we now show that the infimum of the Yamabe functional is attained by the standard metricon the sphere\n\nThis was originally independently proved by Aubin[aubin1976problemes]and G.Talenti[talenti1976best]. Here, we will follow the approach by Morio Obata ([obata1971conjectures]) and Karen Uhlenbeck ([Sacks1981TheEO]). It consists of two parts:\n\nThe metricis the standard metric, and it\u2019s conformal diffeomorphism. (Proposition3.0.1.)\n\nThe infimumis attained by a smooth\nmetricin the conformal class of the standard metric(Proposition3.0.2)\n\nThe first part is given by Obata in the survey[obata1971conjectures]about the conformal diffeomorphism on the sphere.\n\nIfis a metric onthat is conformal to the standard round metricand has a constant scalar curvature, then up to a constant scale factor,is obtained fromby conformal diffeomorphism of the sphere.\n\nWe start by showing thatis the Einstein metric (see (1) for the definition of the Einstein metric). By the fact that the standard metrichas a constant sectional curvature and it is an Einstein metric, we get the following.\n\nSinceis traceless,. Then we can directly compute the norm ofby\n\nNotice, thenand. On the other hand,andare both conformal to a flat metric on, and we have. As bothand, the curvature tensor is\n\nThis is the same as the standard metric on, hence by by Killing-Hopf (see Theorem1.1)has constant sectional curvature (not just constant scalar curvature). Therefore,is isometric to the standardby, hence, and this isometry is the desired conformal diffeomorphism.\n\u220e\n\nNow we have shown that the group of metricsis a conformal diffeomorphism. However, they are not compact, that is, the family of metricson the sphere are not uniformly bounded. Therefore, it is crucial to prove the existence of extremals on the sphere. This leads to the other proposition given by Uhlenbeck[Sacks1981TheEO].\n\nThere exists a positivefunctioninthat satisfies.\n\nWithout loss of generality, assume.\nFor(recall), letbe the minimizer of, with(for this part, seeD.1.1). Ifis uniformly bounded for all, then Ascoli-Arzela (see C.7 in the book[evans]) implies that up to the subsequenceconverges to. Therefore, we really care about the case whenis not uniformly bounded. Composing with a rotation, we may assume that eachachieves its supremum at the south pole (Q), and.\nNow letwe define\n\nfor eachFor each, choose the value ofso thatat the south pole. Notice that\n\nwhereis the conformal factor, and at the south pole, so that for each, we get\n\nand\n\nFor simplicity of computation, let us denote\n\nNoticeis part of the conformal translation, this definition will make sure that, if the pull back operator, then\n\nwhich implies\n\nNow we also assume, then\n\nSo there is constant, such that\n\nOn the other hand it is easy to show that, there exist, with\n\nThen we get that\n\nAlso direct computation as below shows that\n\nA brief summarize, so far we get\n\nis bounded in\n\ngoes toas\n\nSince all the functionsare smooth on, we know that eachis also smooth. Also, for anywe have\n\nSo asand,is bounded by, which does not depend on. Therefore, for each,is theneighborhood of the north pole, thenis uniformly bounded on. This implies that up to the subsequence,converges on.\n\nOn the other hand, andfor, so we conclude that at each point of,, for. Also,, so that. By the weak removable singularities theorem (seeD.1), the singularity at pointis weakly removable; hence there is, weakly satisfies the equation. Together with\n\nWe get that\n\nTaking limit, we get\n\nBy definition of, we have that\n\nThisis what we are looking for.\n\u220e\n\nA notable conclusion is thatis the optimal constant for Sobolev embedding inby the above argument. This is helpful in the later part of the concentration compactness lemma (see Lemma4.1).\n\nSECTION: 4Proof of Theorem2.1\n\nNow we proof the main Theorem2.1of this note, that is, when, we can find a minimizer of the Yamabe functional (\u20232) and therefore a solution to the Yamabe problem. We follow the approach of concentration compactness by Lion[lions1984concentration1,lions1984concentration2]in the survey[Neumayer], which plays an important role in the proof.\n\nSupposeis uniformly bounded in, so. Up to subsequences, we can assume\n\nThen\n\nwhereis an at most countable set of points in.\n\nLet\n\nThe existence of the above weak limits are given by Banach-Alaoglu theorem (see Theorem 23.5 in the book[meise1997introduction]). Forwe have, thengiven by the lower semicontinuity. To show\n\none must showa.e. It is not difficult since we havebounded in, that is, forwe havein, which impliesa.e. Therefore, we have. Now, take any. Applying the Sobolev inequality, we have\n\nIfweakly converge toin, then for anywe have\n\nAnd given by the strong convergence ofin, we get\n\nAlso given by the strong convergence ofin, and the fact thatis bounded for all, we have\n\nSo we have that\n\nFollowing from (7) we get that,\n\nRearranging the powers, we have\n\nThis very unnatural thing looks like a reverse H\u00f6lder\u2019s inequality. Applied toapproximating the characteristic function of any open set, (8) shows thatcontrolsnonlinearly:\n\nThis scaling will forceto be supported on a countable set of atoms. Indeed, sinceis a finite measure, it contains at most countably many atoms, say at. For any point, we can take any open setcontainingwith, so that (9) gives us\n\nIn other words,is absolutely continuous with respect toon.\nRecallby the Radon-Nikodym theorem (seeD.2), for-a.e., we have\n\nHence, the support ofis contained onand so\n\nNow take anyand by the reverse H\u00f6lder\u2019s inequality (8), we haveit follows that\n\nNow apply to awith, andon, we find\n\nThis gives us (6) hence concludes the proof.\n\u220e\n\nNow we can start proofing Theorem2.1.\n\nSECTION: 4.1Proof of Theorem2.1\n\nLetbe a minimizing sequence for. Without loss of generality, we may assume that. Up to a subsequence,inandinandwith. Note that if, thenstrongly in. This gives us the lower semicontinuity of the energy. Sinceconverges weakly toin, thenand also\n\nTherefore we get\n\nwhere, and. The concentration compactness lemma4.1implies that\n\nNow together with the assumption that, we have\n\nFor such relation betweenand, we get\n\nThen we getand, which is desired. This shows the spirit of the proof, that is, to find the proper relation between A and B such that the equality holds. This exact relation is given by the concentration compactness lemma (4.1), that is,\n\nand\n\nTherefore, by concentration compactness lemma (4.1), we have\n\nNote that. Recall, we have\n\nThe final equality holds because. Now, sinceand again applying Jensen\u2019s inequality, we have\n\nEquality in (14) implies thator. If, then we have a strict inequality in (13). Therefore. This establishes the existence of a minimizer. Sincefor a.e., we may assume without loss of generality that. The results in the elliptic regularity theory (see[Trudinger]) show thatis smooth, and then the maximum principle ensures thatis positive. Thus, our minimizer is indeed a conformal factor.\n\u220e\n\nSECTION: Appendix ABasics of Riemannian Geometry\n\nMost of this comes from the book[Yau]and the book[CarmoR]. For a more fundamental knowledge of Riemannian geometry, see the book[CarmoR].\n\nSECTION: A.1Curvatures\n\nThe curvatureof a Riemannian manifoldis a correspondence that associates to every pair, a mappinggiven by:\n\nwhereis the Riemannian connection of.\n\nThe curvature measures the non-commutativity of the covariant derivative.\n\nLetbe a 2-dim subspace of the tangent space. Then the sectional curvature is:\n\nwhereis the Riemann curvature tensor defined above. The sectional curvature is an analog of Gauss curvature on 2-dimensional surface; it is important because the Riemannian curvature tensor is uniquely determined by the sectional curvature.\n\nRicci curvature is the contraction of second and last index in curvature tensor:\n\nScalar curvature is the contraction of Ricci curvature with the inverse of metric\n\nThe geometric meaning of takingis to get the average of sectional curvatures of all the 2-d planes passing through\n\nOver normal coordinate, the metriccan be expressed as\n\nAnd the volume form is:\n\nLetbe the product of two Riemannian manifolds, andbe its curvature tensor,,be curvature tensor forandrespectively, then one can relate,andby\n\nwhere.\n\nTo show this, we need the following:\n\nHere part 1 is simply by definition of product Riemannian manifold, part 2 can be shown in local coordinates, and part 3 can be shown by part 1 and part 2 and along with Koszul formula. For more details, see Exercise 1(a) of Chapter 6 in the book[CarmoR].\n\nSECTION: Appendix BThe Conformal Map\n\nFirst, to prove the above definition, we have. Assume, whereis a constant. The standard scalar curvature transformation formula foris\n\nSinceis a constant, we have that, andTherefore,, on the other hand\n\nSo we get the conformal transformation for Hilbert action\n\nAnd the transformation of volume is\n\nTherefore, for, we have\n\nSince we need, we have, it follows that\n\nNow for more general cases let, in whichis a smooth function, then we have\n\nSo\n\nDenote, then the denominator becomes. Then by formula (15), we also get\n\nAnd that\n\nAt last we conclude this note by\n\nSECTION: Appendix CSobolev Space\n\nMost of the materials refer to Chapter 5 of the book[evans].\n\nLet, and letbe a natural number. A functionis said to lie inif its weak derivativesexist and lie infor all. Iflies in, we define thenorm ofby the formula\n\n.\n\nIf, we define its norm to be\n\nIf, the Sobolev conjugate of p is\n\nAssumeThere exists a constantdepending only onand, such that\n\nfor all\n\nAssume.Since u has compact support, for eachandwe have\n\nthen\n\nit follows that\n\nIntegrate (19) with respect toand using the generalized H\u00f6lder inequality, we have\n\nFrom NoteC.1we knowis the Sobolev conjugatewhen p = 1, it follows that\n\nNow consider the case of. Choose, we have\n\nBy the definition of derivative we have\n\nApply H\u00f6lder\u2019s inequality, it follows that\n\nRecall, then\n\nIt follows that\n\nand so\n\nNotice, therefore we have\n\n\u220e\n\nLet M be a bounded open subset of, supposeis. Assumeand. Then, with the estimate\n\nThe constantdepending only onand,and.\n\nLetbe such that. Thenembeds continuously into.\n\nAssumeis a bounded open subset ofandis. Letbe such that. Then\n\nSECTION: Appendix DAnalytic preliminaries\n\nLetbe an open set inand. Supposeis a weak solution ofin, withandfor some.Thensatisfiesweakly on all of.\n\nFor the proof of this theorem, check Proposition 2.7 in the survey[Lee].\n\nForthere exists a smooth, positive solutionto the subcritical equation, for whichand\n\nFor the proof of this proposition see the paper[yamabe]and the survey[Lee].\n\nOn the measurable space, define twofinite measures,and. It states that if(that is, ifis absolutely continuous with respect to, then there exists ameasurable functionsuch that for any measurable set", "text_file": "data\\paper_texts\\2309.02397v4_content.txt"}, {"title": "Megatron: Evasive Clean-Label Backdoor Attacks against Vision\n  Transformer", "authors": ["Xueluan Gong", "Bowei Tian", "Meng Xue", "Shuike Li", "Yanjiao Chen", "Qian Wang"], "published_date": "2024-12-06T04:39:41Z", "summary": "Vision transformers have achieved impressive performance in various\nvision-related tasks, but their vulnerability to backdoor attacks is\nunder-explored. A handful of existing works focus on dirty-label attacks with\nwrongly-labeled poisoned training samples, which may fail if a benign model\ntrainer corrects the labels. In this paper, we propose Megatron, an evasive\nclean-label backdoor attack against vision transformers, where the attacker\ninjects the backdoor without manipulating the data-labeling process. To\ngenerate an effective trigger, we customize two loss terms based on the\nattention mechanism used in transformer networks, i.e., latent loss and\nattention diffusion loss. The latent loss aligns the last attention layer\nbetween triggered samples and clean samples of the target label. The attention\ndiffusion loss emphasizes the attention diffusion area that encompasses the\ntrigger. A theoretical analysis is provided to underpin the rationale behind\nthe attention diffusion loss. Extensive experiments on CIFAR-10, GTSRB,\nCIFAR-100, and Tiny ImageNet demonstrate the effectiveness of Megatron.\nMegatron can achieve attack success rates of over 90% even when the position of\nthe trigger is slightly shifted during testing. Furthermore, Megatron achieves\nbetter evasiveness than baselines regarding both human visual inspection and\ndefense strategies (i.e., DBAVT, BAVT, Beatrix, TeCo, and SAGE).", "arxiv_id": "2412.04776v1", "html_link": "https://arxiv.org/html/2412.04776v1", "search_term": "ti:\"transformers\"", "html_error": "Error processing HTML: HTTP Error 404: Not Found"}, {"title": "DHIL-GT: Scalable Graph Transformer with Decoupled Hierarchy Labeling", "authors": ["Ningyi Liao", "Zihao Yu", "Siqiang Luo"], "published_date": "2024-12-06T02:59:01Z", "summary": "Graph Transformer (GT) has recently emerged as a promising neural network\narchitecture for learning graph-structured data. However, its global attention\nmechanism with quadratic complexity concerning the graph scale prevents wider\napplication to large graphs. While current methods attempt to enhance GT\nscalability by altering model architecture or encoding hierarchical graph data,\nour analysis reveals that these models still suffer from the computational\nbottleneck related to graph-scale operations. In this work, we target the GT\nscalability issue and propose DHIL-GT, a scalable Graph Transformer that\nsimplifies network learning by fully decoupling the graph computation to a\nseparate stage in advance. DHIL-GT effectively retrieves hierarchical\ninformation by exploiting the graph labeling technique, as we show that the\ngraph label hierarchy is more informative than plain adjacency by offering\nglobal connections while promoting locality, and is particularly suitable for\nhandling complex graph patterns such as heterophily. We further design subgraph\nsampling and positional encoding schemes for precomputing model input on top of\ngraph labels in an end-to-end manner. The training stage thus favorably removes\ngraph-related computations, leading to ideal mini-batch capability and GPU\nutilization. Notably, the precomputation and training processes of DHIL-GT\nachieve complexities linear to the number of graph edges and nodes,\nrespectively. Extensive experiments demonstrate that DHIL-GT is efficient in\nterms of computational boost and mini-batch capability over existing scalable\nGraph Transformer designs on large-scale benchmarks, while achieving top-tier\neffectiveness on both homophilous and heterophilous graphs.", "arxiv_id": "2412.04738v1", "html_link": "https://arxiv.org/html/2412.04738v1", "search_term": "ti:\"transformers\"", "html_content": "SECTION: DHIL-GT: Scalable Graph Transformerwith Decoupled Hierarchy Labeling\n\nGraph Transformer (GT) has recently emerged as a promising neural network architecture for learning graph-structured data. However, its global attention mechanism with quadratic complexity concerning the graph scale prevents wider application to large graphs. While current methods attempt to enhance GT scalability by altering model architecture or encoding hierarchical graph data, our analysis reveals that these models still suffer from the computational bottleneck related to graph-scale operations.\nIn this work, we target the GT scalability issue and proposeDHIL-GT, a scalable Graph Transformer that simplifies network learning by fully decoupling the graph computation to a separate stage in advance.DHIL-GTeffectively retrieves hierarchical information by exploiting the graph labeling technique, as we show that the graph label hierarchy is more informative than plain adjacency by offering global connections while promoting locality, and is particularly suitable for handling complex graph patterns such as heterophily. We further design subgraph sampling and positional encoding schemes for precomputing model input on top of graph labels in an end-to-end manner. The training stage thus favorably removes graph-related computations, leading to ideal mini-batch capability and GPU utilization. Notably, the precomputation and training processes ofDHIL-GTachieve complexitieslinearto the number of graph edges and nodes, respectively.\nExtensive experiments demonstrate thatDHIL-GTis efficient in terms of computational boost and mini-batch capability over existing scalable Graph Transformer designs on large-scale benchmarks, while achieving top-tier effectiveness on both homophilous and heterophilous graphs.\n\nSECTION: 1.Introduction\n\nGraph Transformers characterize a family of neural networks that introduce the powerful Transformer architecture(Vaswani et\u00a0al.,2017)to the realm of graph data learning. These models have garnered increasing research interest due to their unique applications and competitive performance(Ying et\u00a0al.,2021; Wu et\u00a0al.,2021; Hussain et\u00a0al.,2022; Zhu et\u00a0al.,2023b). Despite their achievements, vanilla GTs are highly limited to specific tasks because of the full-graph attention mechanism, which has computational complexity at least quadratic to the graph size, rendering it impractical for a single graph with more than thousands of nodes. Enhancing the scalability of GTs is thus a prominent task for enabling these models to handle a wider range of graph data on large scales.\n\nTo scale up Graph Transformers, existing studies explore various strategies to retrieve and utilize graph data efficiently. One representative approach is to simplify the model architecture with a specialized attention module(Wu et\u00a0al.,2022; Ramp\u00e1\u0161ek et\u00a0al.,2022; Wu et\u00a0al.,2023). The graph topology is conserved by the message-passing mechanism, which recognizes edge connections without the need for quadratic computation on all-pair node interactions. However, these modifications introduce another computational bottleneck of iterative graph propagation, which typically has an overhead linear to the edge size and remains challenging for scalable model training.\nAn alternative line of works chooses to embed richer topological information as structured data through different graph processing techniques, such as adjacency-based spatial propagation(Chen et\u00a0al.,2023; Kong et\u00a0al.,2023), polynomial spectral transformation(Ma et\u00a0al.,2023; Deng et\u00a0al.,2024), and hierarchical graph coarsening(Zhang et\u00a0al.,2022; Zhu et\u00a0al.,2023a). Although these models offer a relatively scalable model training scheme, the graph-related operation still persists during their learning pipeline, leaving certain scalability and expressivity issues unsolved as detailed in our complexity analysis.\n\nTaxonomyBatchModelPrecompute TimeTrain TimeRAM Mem.GPU Mem.HeteroVanillaFBGraphormer(Ying et\u00a0al.,2021)NGRPE(Park et\u00a0al.,2022)NKernel-basedNSGraphGPS(Ramp\u00e1\u0161ek et\u00a0al.,2022)YNodeFormer(Wu et\u00a0al.,2022)\u2013YDIFFormer(Wu et\u00a0al.,2023)\u2013NPolyNormer(Deng et\u00a0al.,2024)\u2013YHierarchicalRSNAGphormer(Chen et\u00a0al.,2023)NPolyFormer(Ma et\u00a0al.,2023)YANS-GT(Zhang et\u00a0al.,2022)YGOAT(Kong et\u00a0al.,2023)YHSGT(Zhu et\u00a0al.,2023a)NDHIL-GT(ours)Y\n\nIn this work, we proposeDHIL-GT, a scalable Graph Transformer withDecoupledHierarchyLabeling.\nBy constructing a hierarchical graph label set consisting of node pair connections and distances, we showcase that all graph information necessary for GT learning can be fully decoupled and produced by an end-to-end pipeline before training. The precomputation procedure can be finished within a linearbound, while the iterative learning step is as simple as training normal Transformers withcomplexity, whereandare the numbers of graph edges and nodes, respectively. The two stages achieve theoretical complexities on par with respective state-of-the-art GTs, as well as a substantial boost in practice thanks to empirical acceleration strategies.\n\nOurDHIL-GTis based on the 2-hop labeling technique, which has been extensively studied with scalable algorithms for querying the shortest path distance (SPD) between nodes(Akiba et\u00a0al.,2013; Yano et\u00a0al.,2013; Akiba et\u00a0al.,2015). By investigating its properties, we show that the graph labels construct a hierarchical representation of the graph topology, favorably containing both local and global graph connections. We design a novel subgraph token generation process to utilize the labels as informative input for GT. The data hierarchy benefits GT expressivity in modeling node-pair interactions beyond graph edges, which is superior in capturing graph knowledge under both homophily and heterophily compared to current locality-based GTs.\nIn addition, the built graph labels also offer a simple and fast approach to query pair-wise distance as positional encoding.\nTo this end, graph information is decently embedded into the precomputed data from different perspectives, and all computations concerning graph labels can be performed in a one-time manner with efficient implementation.\nThe GT training stage only learns from the structured input data, which enjoys ideal scalability, including a simple mini-batch scheme and memory overhead free from the graph size.\n\nWe summarize the contributions of this work as follows:\n\nWe proposeDHIL-GTas a scalable Graph Transformer with decoupled graph computation and simple model training independent of graph operations. Both precomputation and learning stages achieve complexities onlylinearto the graph scale.\n\nWe introduce an end-to-end precomputation pipeline forDHIL-GTbased on graph labeling, efficiently embedding graph information with informative hierarchy. Dedicated token generation, positional encoding, and model architectures are designed for representing hierarchical data at multiple levels.\n\nWe conduct comprehensive experiments to evaluate the effectiveness and efficiency ofDHIL-GTagainst current Graph Transformers across large-scale homophilous and heterophilous graphs.DHIL-GTachieves top-tier accuracy and demonstrates competitive scalability regarding time and memory overhead.\n\nSECTION: 2.Related works\n\nVanilla Graph Transformers.\nEarly GTs(Dwivedi and Bresson,2020; Ying et\u00a0al.,2021; Wu et\u00a0al.,2021)are mainly proposed for graph-level learning tasks, typically involving small-scale graphs of less than a thousand nodes. Following the vanilla design, a wide range of positional encoding schemes have been invoked to the self-attention module to encode graph topology information, including graph proximity(Ying et\u00a0al.,2021; Zhang et\u00a0al.,2022; Chen et\u00a0al.,2021), Laplacian eigenvectors(Dwivedi and Bresson,2020; Kreuzer et\u00a0al.,2021; Hussain et\u00a0al.,2022), and shortest path distance(Park et\u00a0al.,2022; Chen et\u00a0al.,2022; Zhao et\u00a0al.,2023).\n\nAs listed inTable1, the critical scalability bottleneck of these models lies in the straight-forward attention mechanism calculating all node-pair interactions in the graph, resultingcomplexity of both training time and memory. If there is positional encoding, additional preprocessing is also demanded withor even higher overhead.\nA naive solution is to randomly sample a subset of nodes and adopt mini-batch learning. However, it largely overlooks graph information and results in suboptimal performance.\n\nKernel-based Graph Transformers.\nKernelization is the method for modeling node-pair relations and replacing the vanilla self-attention scheme. For instance, NodeFormer(Wu et\u00a0al.,2022)employs a kernel function based on random features, and GraphGPS(Ramp\u00e1\u0161ek et\u00a0al.,2022)opts to incorporate topological representation. More expressive kernels are also developed, invoking depictions such as graph diffusion(Wu et\u00a0al.,2023)and node permutation(Deng et\u00a0al.,2024).\n\nAlthough kernelized GTs prevent the quadratic complexity, the nature of the kernel indicates that graph data needs to be iteratively accessed during learning, which is represented by thelearning overhead inTable1. When the graph scale is large, this term becomes dominant since the edge sizeis significantly larger than the node size. Hence, we argue that such a design is not sufficiently scalable.\nAnother under-explored issue is the expressiveness of the neighborhood sampling (NS) strategy for forming node batches in kernel-based GTs. Similar to convolutional GNNs, NS is known to be subject to performance loss on complex graph signals due to its inductive bias on graph homophily(Breuer et\u00a0al.,2020; Zheng et\u00a0al.,2022).\n\nHierarchical Graph Transformers.\nRecent advances reveal that it is possible to remove the full-graph attention and exploit the power of GTs to learn the latent node relations during learning. This is achieved by providing sufficient hierarchical context as input data with node-level identity. The key of this approach is crafting an effective embedding scheme to comply with GT expressivity. To realize this, NAGphormer(Chen et\u00a0al.,2023), PolyFormer(Ma et\u00a0al.,2023), and GOAT(Kong et\u00a0al.,2023)look into representative features using adjacency propagation, spectral graph transformation, and feature space projection, respectively. ANS-GT(Zhang et\u00a0al.,2022)builds graph hierarchy by adaptive graph sampling concerning subgraphs of size, while HSGT(Zhu et\u00a0al.,2023a)leverages graph coarsening algorithms.\n\nHierarchical GTs are applicable to mini-batching with random sampling (RS) as long as their graph embeddings are permutation invariant.\nFurthermore, since graph processing is independent of GT attention, it can be adequately improved with better algorithmic scalability. In most scenarios, the graph can be processed incomplexity in precomputation as shown inTable1. Nonetheless, we note that hierarchical models, except for NAGphormer and PolyFormer, still involve graph-level operations during training, which hinders GPU utilization and causes additional overhead.\n\nScalable Convolutional GNNs.\nThe scalability issue has also been extensively examined for Graph Neural Networks (GNNs) exploiting graph convolutions(Kipf and Welling,2017; Veli\u010dkovi\u0107 et\u00a0al.,2017). Similar to hierarchical GTs, decoupled models propose to separate the graph computation from iterative convolution and employ dedicated acceleration, exhibiting excellent scalability on some of the largest datasets with linear or even sub-linear complexity(Klicpera et\u00a0al.,2019; Wu et\u00a0al.,2019; Chen et\u00a0al.,2020; Liao et\u00a0al.,2022). It is also demonstrated that such strategy is capable of handling heterophily(Li et\u00a0al.,2021; Wang and Zhang,2022; Liao et\u00a0al.,2023). Graph simplification techniques including graph sampling(Chen et\u00a0al.,2018; Chiang et\u00a0al.,2019; Zou et\u00a0al.,2019; Feng et\u00a0al.,2022)and coarsening(Deng et\u00a0al.,2020; Huang et\u00a0al.,2021; Cai et\u00a0al.,2021)approaches are also explored for reducing the graph scale at different hierarchy levels.\nAlthough the high-level idea of scaling up convolutional GNNs is helpful towards scalable GTs, Transformer-based models are unique in respect to their graph data utilization and architectural bottlenecks, and hence require specific designs for addressing their scalability issues.\n\nSECTION: 3.Preliminaries\n\nGraph Labeling.\nConsider a connected graphwithnodes andedges. The node attribute matrix is, whereis the dimension of input attributes.\nThe neighborhood of a nodeis, and its degree.denotes a path from nodeto, and the shortest distanceis achieved by the path with least nodes.\n\nThe graph labeling process assigns a labelto each node, which is a set of pairscontaining certain nodesand corresponding shortest distancesbetween the node pairs. The graph labels compose a 2-hop cover(Cohen et\u00a0al.,2003)ofif for an arbitrary node pair, there existsand.\nGiven an order of all nodes in, we denote each node by a unique indexand useto indicate that nodeprecedes nodein the sequence.\n\nTransformer Architecture.\nA Transformer layer(Vaswani et\u00a0al.,2017)projects the input representation matrixinto three subspaces:\n\nwhere,,are the projection matrices.\nFor a multi-head self-attention module withheads, each attention head possesses its own representations, and the outputacross all heads is calculated as:\n\nwheredenotes the matrix concatenation operation. In this paper, we set the projection dimension.\nIt can be observed thatEqs.1and2for representations ofnodes lead totime and memory overhead. When it only applies to a batch ofnodes, the complexity is drastically reduced to.\n\nSECTION: 4.Hierarchy of Label Graph\n\nOurDHIL-GTaims to retrieve graph hierarchy information from graph labels consisting of node pair connections and distances. In this section, we first introduce the pruned landmark labeling algorithm to efficiently compute graph labels as a 2-hop cover. Then, we analyze that the labeling process favorably builds a graph hierarchy with several useful properties for representing implicit graph information beyond adjacency.\n\nSECTION: 4.1.Pruned Landmark Labeling\n\nBased on the concept of graph labeling inSection3, a straight-forward approach to build the graph labels is to traverse the whole graph for each node successively. This is, however, prohibitive due to the repetitive graph traversal procedure. Hence, we employ the Pruned Landmark Labeling (PLL) algorithm(Akiba et\u00a0al.,2013), which constructs labels with a more efficient search space.\n\nThe PLL algorithm is presented inAlgorithm1. It performs a pruned BFS for each node indexedtofollowing the given order. The algorithm is agnostic to the specific search order. In this work, we follow(Akiba et\u00a0al.,2013)to adopt the descending order of node degrees for its satisfying performance while leaving other schemes for future exploration.\n\nThe PLL is more efficient than full-graph traversal as it prevents the visit to nodesthat have been accessed and labeled with a shorter distanceto the current source node. Intuitively, during the early rounds of pruned BFS starting from nodeswith smaller indices, the traversal is less pruned and can reach a large portion of the graph. These nodes are regarded as landmarks with higher significance and are able to appear in a large number of node labelswherealong with the distance information. On the contrary, for latter nodes with higher indices, the pruned traversal constrains the visit to the local neighborhood.\n\nThus, we reckon that the PLL process naturally builds a hierarchy embedded in the node labels. An exemplary illustration of a real-world graph is displayed inFigure1. The originalchameleongraph is heterophilous, i.e., connected nodes frequently belong to distinct classes. InFigure1(a), different classes are mixed in graph clusters, which pose a challenge for GTs to perform classification based on edge connections. In contrast, nodes in the graph marked by graph labels inFigure1(b)clearly form multiple densely connected clusters, exhibiting a distinct hierarchy. Certain classes can be intuitively identified from the hierarchy, which empirically demonstrates the effectiveness of our utilization of graph labeling.\n\nSECTION: 4.2.Label Graph Properties\n\nThen, we formulate the hierarchy in graph labels by defining a generated graph, namely thelabel graph, as, which is with directed and weighted edges. Its edge set depicts the elements in node labels computed by graph labeling, that an edgeif and only if, and the edge weight is exactly the distance in graph labels. The in- and out-neighborhoods based on edge directions areand, respectively.\n\nWe then elaborate the following three hierarchical properties of the label graph generated byAlgorithm1. Corresponding running examples are given inFigure2. For simplicity, we assume that the original graphis undirected, while properties for a directedcan be acquired by separately considering two label setsandfor in- and out-edges in.\n\nFor an edge, there iswhen, andwhen.\n\nReferring toAlgorithm1, when the current node isand,holds sinceis the direct neighbor of. Hence,is added to labelat this round, which is equivalent to adding edgeto. Similarly,holds when. For example, the edgeinFigure2(a)is represented by the directed edgeinFigure2(b).Property1implies that, i.e., the neighborhood of the original graph is also included in the label graph, and is further separated into two sets according to the relative order of neighboring nodes.\n\nFor a shortest pathin, there isfor eachsatisfying.\n\n(Akiba et\u00a0al.,2013)proves that there isforand. Therefore, considering shortest paths starting with nodeof a small index, i.e.,being a \u201clandmark\u201d node, then succeeding nodesin the path are connected toin. InFigure2(a), the shortest path betweenpassing noderesults in edgesandinFigure2(c), since nodesandare in the path and their indices are larger than node.\nWhen the order is determined by node degree, high-degree nodes appear in shortest paths more frequently, and consequently link to a majority of nodes, including those long-tailed low-degree nodes in.\n\nFor a shortest pathin, if there isand, then.\n\nAccording to the property of shortest path, there is. Hence, the condition of line 8 inAlgorithm1is not met at the-th round when visiting. In other words, the traversal fromis pruned at the preceding node. By this means, the in-neighborhoodis limited in the local subgraph with shortest paths ending at landmarks.\nAs shown inFigure2(d), the shortest path betweenpasses node, indicating thatare not directly connected since their distance can be acquired by edgesand. As a consequence, the neighborhood of nodeinis constrained by nodesand, preventing connections to more distant nodes such asor.\n\nSummarizingProperties1,2and3, the label graph preserves neighboring connections of the original graph, while establishing more connections to a minority set of global nodes as landmark. The hierarchy is built so that long-tailed nodes with high indices are usually located in local substructures separated by landmarks.\nNoticeably, since the label graph is deterministic, it can be computed byAlgorithm1in an individual stage in one time and used throughout graph learning iterations.\n\nSECTION: 5.Methodology\n\nIn this section, we describe the design motivation and approach of theDHIL-GTmodel by respectively elaborating on the proposed modules in its precomputation and learning stages.Figure3illustrates the overview of theDHIL-GTpipeline.\n\nSECTION: 5.1.Subgraph Generation by Labeling\n\nMotivation: Hierarchical GT beyond adjacency.\nCanonical Graph Transformer models(Ying et\u00a0al.,2021; Wu et\u00a0al.,2022; Ramp\u00e1\u0161ek et\u00a0al.,2022; Chen et\u00a0al.,2023)generally utilize graph adjacency for composing the input sequence in graph representation learning. However, recent advances reveal that adjacency alone is insufficient to represent the implicit graph topology. GTs can be improved by modeling node connections not limited to explicit edges, and more hierarchical information benefits learning high-level knowledge on graph data(Zhang et\u00a0al.,2022; Kong et\u00a0al.,2023; Zhu et\u00a0al.,2023a).\n\nUnlike existing hierarchical GTs relying on the original graph, we seek to retrieve structural information from the label graphgenerated byAlgorithm1. As showcased inSection4, the label graph hierarchy processes properties of maintaining local neighborhoods while adding global edges. This is preferable for Graph Transformers as it extends the receptive field beyond local neighbors described by graph adjacency and highlights those distant but important landmarks in the graph for attention modules on node connections. The hierarchical information is especially useful for complicated scenarios, such as heterophilous graphs, where the local graph topology may be distributive or even misleading. Moreover, the edge weight of the label graph, i.e., the shortest distance between node pairs of interest, can serve as a straightforward metric for evaluating relevance with the ego node.\n\nTo leverage the label graph efficiently, we employ a decoupling scheme to prepare the labels and necessary data in a separate stage before training. The graph data is only processed in this precomputation stage and is prevented from being fully loaded onto GPU devices, which intrinsically reduces the GPU memory overhead and offers better scalability to large graphs.\n\nSampling for Subgraph Tokens.Algorithm2describes the precomputation process inDHIL-GT. Given the input graph, we first build the graph labels by PLL as outlined inAlgorithm1. For each node, we generate a token for GT learning, which represents the neighborhood around the node in the label graph. Since the neighborhood size is variable, we convert it into a fixed-length subgraph tokenwithnodes by weighted sampling, as shown in lines 4-9 inAlgorithm2. Neighbors inandare sampled separately with different sizes, asSection4.2shows that these two sets contain nodes of differing importance. The distance to the ego nodeis used as the sampling weight, with hyperparameterscontrolling the relative importance. Note that under our sampling scheme, nodes not connected to the ego node will not appear in the token.\n\nOverall, the subgraph generation process produces a node list of lengthfor each node, representing its neighborhood in the label graph. The relative values of hyperparametersandcan be used to balance the ratio of in-neighbors and out-neighbors in, which correspond to local long-tailed nodes and distant landmark nodes in, respectively. Compared to canonical GT tokens representing the graph node in the context of the full graph,DHIL-GTonly relies on a small but informative subgraph of fixed size. When the graph scales up,DHIL-GTenjoys better scalability as its token size does not increase with the graph size.\n\nSECTION: 5.2.Fast Subgraph Positional Encoding\n\nPositional encoding is critical for GT expressivity to model inter-node relationship for graph learning. In our approach, positional encoding provides the relative identity of nodes within the subgraph hierarchy. We particularly employ shortest path distance (SPD) to token nodes as the positional encoding scheme inDHIL-GT, which is superior as it holds meaningful values for arbitrary node pairs regardless of locality. In comparison, other approaches such as graph proximity and eigenvectors are usually too sparse to provide identifiable information within sampled subgraphs.\n\nConventionally, calculating SPD for positional encoding demandsor higher complexity as analyzed inTable1, which is not practical for large-scale scenarios. Thanks to the graph labeling computation, we are able to efficiently acquire SPD inside subgraphs.\nRecalling the definition of 2-hop cover inSection3, we exploit the following corollary, which ensures the SPD of any node pairs can be effectively acquired on top of PLL labels:\n\nFor any node pair, the shortest path distance can be calculated by:\n\nwhere labelsare computed byAlgorithm1. Note that.\n\nThe second part ofAlgorithm2in line 10-12 depicts the process of further reusing the label graph data structure for managing SPD within node-wise subgraphs. For node pairs of each subgraph, the SPDs are computed and stored as weighted edges that extend the label graph edge set.\n\nTo employ SPD positional encoding, the transformer layer inEq.2is altered with a bias term:\n\nwhere the value of bias entry is a learnable parameter indexed by the node-pair SPD value.\n\nSECTION: 5.3.Model Architecture\n\nDHIL-GTenhances the GT architecture(Ying et\u00a0al.,2021; Zhang et\u00a0al.,2022)to fit the precomputed subgraphs and mini-batch training for large-scale representation learning. Apart from the SPD bias, we also design specific modules to adapt to subgraph hierarchical learning.\nFor each node, given the subgraphproduced byAlgorithm2, input representations are retrieved from the node attributes based on the input node token as, wheredenotes node attributesfor all, andwith hidden dimension.\n\nFor the-th Transformer layer, the representation is updated as:\n\nwhereandstand for layer normalization and feed-forward network, respectively, anddenotes the multi-head self-attention architecture described byEqs.1,4and2.\n\nLastly, a readout layer calculates node-wise attention over the-layer representation among nodes in the fixed-length token:\n\nwhich measures the correlation between ego node and its neighbors in the subgraph. The representation is then aggregated to the ego nodeas output:\n\nwhereis the output classifier.\n\nVirtual Node.\nWe add virtual nodes representing landmarks tosuch thatfor all nodes. It can be observed fromAlgorithm1that virtual nodes are added to everywithout affecting label construction and SPD query. During the learning stage, we set their attributes and attention bias to be learnable. This scheme actually generalizes the global virtual node utilized in(Ying et\u00a0al.,2021), offering graph-level context to node-level representation during representation updates.\n\nMini-batch Capability.\nRemarkably, throughout the Transformer learning stage ofDHIL-GT, input data including subgraph tokens, SPD bias, and node attributes are all readily prepared byAlgorithm2as described in previous subsections. For each node, only indexing operations are performed onandbased on the subgraph token, and no graph-scale computation is required during learning iterations. Therefore, mini-batch training forDHIL-GTcan be easily implemented by sampling batches of ego nodes, and only indexed strides ofandare loaded onto GPU devices.\n\nSmallchameleonsquirreltolokersPre.EpochInferMem.AccPre.EpochInferMem.AccPre.EpochInferMem.ROC AUCDIFFormer\u2217-0.090.380.5037.83\u00b14.54-0.050.050.735.73\u00b11.37-0.1685.80.8874.88\u00b10.59PolyNormer\u2217-0.030.171.140.70\u00b13.38-0.070.491.238.40\u00b11.10-1.2715.59.479.39\u00b10.50NAGphormer0.270.030.030.533.18\u00b14.300.850.080.080.532.02\u00b13.931.590.110.020.579.32\u00b10.39ANS-GT11.21.980.782.841.19\u00b10.6928.14.481.956.637.15\u00b11.107162.373.4210.779.31\u00b10.97GOAT1.990.340.440.435.02\u00b11.156.660.370.580.630.78\u00b10.9136.15.495.875.079.46\u00b10.57HSGT\u22170.010.340.730.332.28\u00b12.430.010.420.740.434.32\u00b10.512.627.768.1217.479.24\u00b10.83DHIL-GT(ours)0.080.030.0054.543.63\u00b12.340.350.680.015.737.16\u00b10.571.90.170.027.279.86\u00b10.47Largepenn94geniustwitch-gamerPre.EpochInferMem.AccPre.EpochInferMem.AccPre.EpochInferMem.AccDIFFormer\u2217-0.530.655.561.77\u00b13.41-0.775.475.484.52\u00b10.36-0.615.144.960.81\u00b10.44PolyNormer\u2217-0.5818.46.379.87\u00b10.06-0.772812.985.64\u00b10.52-1.458921.464.72\u00b10.65NAGphormer2376.142.132.374.45\u00b10.60385.431.042.383.88\u00b10.13161.920.392.361.92\u00b10.19ANS-GT3889424.98.767.76\u00b11.3234092374.958.767.76\u00b11.3212924196.78.661.55\u00b10.45GOAT1332331820.971.42\u00b10.44266428398.980.12\u00b12.323348376321.261.38\u00b10.83HSGT\u2217121151109.367.77\u00b10.27219811417.184.03\u00b10.246823525311.261.60\u00b10.09DHIL-GT(ours)31140.310.278.74\u00b10.45525.40.337.091.06\u00b10.471722.20.157.367.03\u00b12.17Inference of these models is performed on the CPU in a full-batch manner due to their requirement of the whole graph.\n\nSECTION: 5.4.Complexity Analysis\n\nTo characterize the model scalability, we consider the time and memory complexity ofDHIL-GTseparately in the precomputation and learning stages. In precomputation, the PLL labeling and sampling processAlgorithm1satisfies the analysis in(Akiba et\u00a0al.,2013), entailing a complexity offor computing labels of all nodes.\nRegarding the positional encoding, a single SPD query followingCorollary1can be calculated intime within the subgraph. The query is performed at mosttimes for all nodes, which leads to anoverhead forin total. It is worth noting that the empirical number of queries is significantly smaller than the above bound, since the subgraphsare highly overlapped for neighboring nodes.\nThe memory overhead for managing sampled tokens and features in RAM isand, respectively. Note that SPD values are stored as integers, which is more efficient than other positional encoding schemes.\n\nDuring model training, one epoch of-layer feature transformation on all nodes demandsoperations, while bias projection is performed withtime complexity. The GPU memory footprint for handling a batch of node representations and bias matrices isand, respectively, whereis the batch size. It can be observed that the training overhead is only determined by batch size and is free from the graph scale, ensuring favorable scalability for iterative GT updates.\n\nSECTION: 6.Experiments\n\nWe comprehensively evaluate the performance ofDHIL-GTwith a wide range of datasets and baselines. InSection6.2, we highlight the model efficiency regarding time and memory overhead, as well as its effectiveness under both homophily and heterophily.Sections6.3and6.4provides in-depth insights into the effect ofDHIL-GTdesigns in exploiting graph hierarchy. Implementation details and full experimental results can be found inAppendixA.\n\nSECTION: 6.1.Experimental Settings\n\nTasks and Datasets.\nWe focus on the node classification task on 12 benchmark datasets in total covering both homophily(Sen et\u00a0al.,2008; Shchur et\u00a0al.,2018; Hu et\u00a0al.,2020)and heterophily(Lim et\u00a0al.,2021; Oleg Platonov et\u00a0al.,2023), whose statistics are listed inTable5. Compared to conventional graph learning tasks used in GT studies, this task requires learning on large single graphs, which is suitable for assessing model scalability. We follow common data processing and evaluation protocols as detailed inSectionA.1.\nEvaluation is conducted on a server with 32 Intel Xeon CPUs (2.4GHz), an Nvidia A30 GPU (24GB memory), and 512GB RAM.\n\nBaselines.\nSince the scope of this work lies in the efficacy and efficiency enhancement of the GT architecture, we primarily compare against state-of-the-art Graph Transformer models with attention-based layers and mini-batch capability. Methods including DIFFormer(Wu et\u00a0al.,2023)and PolyNormer(Deng et\u00a0al.,2024)are considered as kernel-based approaches. NAGphormer(Chen et\u00a0al.,2023), GOAT(Kong et\u00a0al.,2023), HSGT(Zhu et\u00a0al.,2023a), and ANS-GT(Zhang et\u00a0al.,2022)stand for hierarchical GTs.\n\nEvaluation Metrics.\nWe use ROC AUC as the efficacy metric ontolokersand classification accuracy on the other datasets. For efficiency evaluation, we notice that there is limited consensus due to the great variety in GT training schemes. Therefore, we attempt to employ a comprehensive evaluation considering both time and memory overhead for a fair comparison. Model speed is represented by the average training time per epoch and the inference time on the testing set. For models with graph precomputation, the time for this process is separately recorded. We also feature the GPU memory footprint, which is the scalability bottleneck.\n\nSECTION: 6.2.Performance Comparison\n\nTable2presents the efficacy and efficiency evaluation results on 6 heterophilous datasets, while metrics for 6 homophilous graphs can be found inTable4.\nAs an overview,DHIL-GTdemonstrates fast computation speed and favorable mini-batch scalability throughout the learning process. It also reaches top-tier accuracy by outperforming the state-of-the-art GTs on multiple datasets.\n\nTime Efficiency.\nBenefiting from the decoupled architecture,DHIL-GTis powerful in achieving competitive speed with existing efficiency-oriented GT designs. For baselines with heavy precomputation overhead, including ANS-GT and GOAT,DHIL-GTshowcases speed improvements by orders of magnitude, with up toboost over ANS-GT ongenius. Aligned with our complexity analysis inSection2, the key impact factor ofDHIL-GTis the node sizeand is less affected byandcompared to precomputation in other methods.\nMeanwhile,DHIL-GTis capable of performing the fastest inference even on large-scale graphs, thanks to its simple model transformation without graph-scale operations. Its training speed is also on par with the best competitors, which usually exploit highly simplified architectures. In contrast, models including PolyNormer and HSGT suffer from longer learning times due to their iterative graph extraction and transformation.\n\nMemory Footprint.\nIn modern computing platforms, GPU memory is usually highly constrained and becomes the scalability bottleneck for the resource-intensive graph learning.DHIL-GTexhibits efficient utilization of GPU for training with larger batch sizes while avoiding the out-of-memory issue. In comparison, drawbacks in several model designs prevent them from efficiently performing GPU computation, which stems from the adoption of graph operations. Notably, kernel-based models require full graph message-passing in their inference stage, which is largely prohibitive on GPUs and can only be conducted on CPUs. HSGT faces the similar issue caused by its graph coarsening module. We note that these solutions are less scalable and hinder the GPU utilization during training.\nIn addition, ANS-GT typically demands high memory footprint for storing and adjusting its subgraphs, which exceeds the memory limit of our platform inTable4.\n\nPrediction Accuracy.DHIL-GTsuccessfully achieves significant accuracy improvement on several heterophily datasets such aschameleonandtwitch-gamer, while the performance on other heterophilous and homophilous datasets inTables2and4is also comparable with the state of the art. We attribute the performance gain to the application of the label graph hierarchy inDHIL-GT, which effectively addresses the heterophily issue of these graphs as analyzed inSection4. Since the label graph also preserves edges in the raw graph, the performance ofDHIL-GTis usually not lower than learning on the latter.\nIn comparison, baseline methods without heterophily-oriented designs, including DIFFormer, NAGphormer, and HSGT, perform generally worse on these graphs. This is because their models tend to rely on the raw adjacency or even promote it with higher modularity. As a consequence, node connections retrieved by GT attention modules are restrained in the local neighborhood and fail to produce accurate classifications. On the other hand, while PolyNormer achieves remarkable accuracy on several heterophilous graphs thanks to its strong expressivity, its performance is largely suboptimal on homophilous graphs inTable4.\n\nSECTION: 6.3.Effect of Hyperparameters\n\nWe then study the effectiveness of the label graph hierarchy inDHIL-GTfeaturing the subgraph generation process inFigure4, which displays the\nimpact of sample sizeand sampling exponentscorresponding toAlgorithm2.\nRegarding the total subgraph size, it can be observed fromFigure4(a)that a reasonably largeis essential for effectively representing graph labels and achieving stable accuracy. In the main experiments, we uniformly adopt a constanttoken size across all datasets, as it is large enough to cover the neighborhood of most nodes while maintaining computational efficiency. As a reference, the average neighborhood size among all nodes is 16.0 onciteseerand 31.2 onchameleon.\nWithin the fixed token length, an equal partition for in- and out-neighbors is preferable according toFigure4(a), where impact ofis shown when.\n\nFigure4(b)presents the result of changing the sampling weight factor. Forchameleon, in the plot, negative exponents favoring nodes with small SPD values are more advantageous to the model performance. Nonetheless, the variance is not significant as long as the subgraph effectively covers the neighborhood of the majority of nodes. We hence conclude thatAlgorithm2forDHIL-GTprecomputation does not require precise hyperparameter tuning.\n\nSECTION: 6.4.Ablation Study\n\nTable3examines the respective effectiveness of the hierarchical modules in theDHIL-GTnetwork architecture, where we separately present results on homophilous and heterophilous datasets. It can be observed that the model without SPD bias suffers the greatest accuracy drop, since topological information represented by positional encoding is necessary for GTs to retrieve the relative connection between nodes and gain performance improvement over learning plain node-level features.\n\nInDHIL-GT, the learnable virtual node representation is invoked to provide adaptive graph-level context before Transformer layers, while the attention-based node-wise readout module aims to distinguish nodes inside subgraphs and aggregate useful representation after encoder transformation. As shown inTable3, both modules achieve relatively higher accuracy improvements on the heterophilous graphchameleon, which validates that the proposed designs are particularly suitable for addressing the heterophily issue by recognizing hierarchical information.\n\nDatasetciteseerchameleonDHIL-GT74.91\u201343.63\u2013Node Readout72.21-2.7038.76-4.87Virtual Node71.15-3.7637.08-6.55SPD Bias68.55-6.3636.52-7.11\n\nSECTION: 7.Conclusion\n\nIn this work, we presentDHIL-GTfor leveraging decoupled graph hierarchy by graph labeling. Our analysis reveals that the label graph exhibits an informative hierarchy and enhances attention learning on the connections between nodes. Regarding efficiency, construction and distance query of the label graph can be accomplished withlinearcomplexity and are decoupled from iterative model training. Hence, the model benefits from scalability in computation speed and mini-batch training. Empirical evaluation showcases the superiority ofDHIL-GTespecially under heterophily.\n\nSmallcoraciteseerpubmedPre.EpochInferMem.AccPre.EpochInferMem.AccPre.EpochInferMem.AccDIFFormer\u2217-0.110.131.283.37\u00b10.50-0.070.071.774.65\u00b10.67-0.370.352.775.77\u00b10.40PolyNormer\u2217-0.110.651.480.43\u00b11.55-0.210.861.668.70\u00b10.95-0.866.072.575.80\u00b10.46NAGphormer0.680.010.060.576.96\u00b10.731.260.010.380.562.26\u00b12.103.050.010.040.578.46\u00b11.01ANS-GT432.01.122.085.42\u00b10.5259.911.654.2511.973.58\u00b10.98529143.521.989.53\u00b10.51GOAT10.10.250.932.578.26\u00b10.1711.10.311.042.164.69\u00b10.4357.40.341.615.377.76\u00b10.97HSGT\u22170.11.812.330.581.73\u00b11.950.060.871.230.969.72\u00b11.025.03.894.442488.86\u00b10.46DHIL-GT(ours)0.420.050.00510.185.58\u00b10.180.430.050.0069.674.91\u00b10.642.60.250.059.489.80\u00b10.48Largephysicsogbn-arxivogbn-magPre.EpochInferMem.AccPre.EpochInferMem.AccPre.EpochInferMem.AccDIFFormer\u2217-1.733.793.396.10\u00b10.11-0.894.12.355.90\u00b18.23-1.729.714.231.13\u00b10.48PolyNormer\u2217-0.762.444.196.59\u00b10.16-0.83117.273.24\u00b10.13-2099222.332.42\u00b10.15NAGphormer338.432.431.196.52\u00b10.24184.360.792.367.85\u00b10.178910.32.213.833.23\u00b10.06ANS-GT220363.134.612.596.31\u00b10.28162051092.7211.371.06\u00b10.48---OOM-GOAT4513.712.28.796.24\u00b10.15182348616.569.66\u00b10.7326731161026.127.69\u00b11.32HSGT\u22171240.761.55.096.05\u00b10.50164751420.368.30\u00b10.3218258262912.633.51\u00b11.15DHIL-GT(ours)170.480.0313.396.31\u00b10.42642.020.185.569.17\u00b10.33773914.50.166.733.74\u00b10.24Inference of these models is performed on the CPU in a full-batch manner due to their requirement of the whole graph.\n\nSECTION: References\n\nSECTION: Appendix AAdditional Experiments\n\nSECTION: A.1.Detailed Experiment Settings\n\nDataset Details.Table5displays the scales and heterophily status of graph datasets utilized in our work. Undirected edges twice in the table.chameleonandsquirrelare the filtered version from(Oleg Platonov et\u00a0al.,2023), whileogbn-magis the homogeneous variant.\nWe employ 60/20/20 random data splitting percentages for training, validation, and testing sets, respectively, except forogbn-mag, where the original split is used. Regarding efficacy metrics, ROC AUC is used ontolokersfollowing the original settings, and accuracy is used for the rest.\n\nHyperparameters.\nParameters regarding the precomputation stage for graph structures are discussed inSection6.3. For subgraph sampling, we perform parameter search for relative ratio of in/out neighbors represented byin rage. For sampling weights, we search their values in range.\n\nFor network architectural hyperparameters, we useTransformer layers withheads andhidden dimension for ourDHIL-GTmodel across all experiments. The dropout rates for inputs (features and bias) and intermediate representation are 0.1 and 0.5, respectively. The AdamW optimizer is used with a learning rate of. The model is trained with 500 epochs with early stopping.\nSince baseline GTs employ different batching strategies, it is difficult to unify the batch size across all models. We set the batch size to the largest value in the available range without incurring out of memory exception on ourGPU, intending for a fair efficiency evaluation considering both learning speed and space.\n\nHetero.DatasetNodesEdgesTrainHomo.chameleonsquirreltolokerspenn94geniustwitch-gamerHetero.coraciteseerpubmedphysics84155ogbn-arxivogbn-mag", "text_file": "data\\paper_texts\\2412.04738v1_content.txt"}]], ["ti:\"deep learning\"", [{"title": "Archaeoscape: Bringing Aerial Laser Scanning Archaeology to the Deep\n  Learning Era", "authors": ["Yohann Perron", "Vladyslav Sydorov", "Adam P. Wijker", "Damian Evans", "Christophe Pottier", "Loic Landrieu"], "published_date": "2024-12-06T17:32:53Z", "summary": "Airborne Laser Scanning (ALS) technology has transformed modern archaeology\nby unveiling hidden landscapes beneath dense vegetation. However, the lack of\nexpert-annotated, open-access resources has hindered the analysis of ALS data\nusing advanced deep learning techniques. We address this limitation with\nArchaeoscape (available at https://archaeoscape.ai), a novel large-scale\narchaeological ALS dataset spanning 888 km$^2$ in Cambodia with 31,141\nannotated archaeological features from the Angkorian period. Archaeoscape is\nover four times larger than comparable datasets, and the first ALS archaeology\nresource with open-access data, annotations, and models.\n  We benchmark several recent segmentation models to demonstrate the benefits\nof modern vision techniques for this problem and highlight the unique\nchallenges of discovering subtle human-made structures under dense jungle\ncanopies. By making Archaeoscape available in open access, we hope to bridge\nthe gap between traditional archaeology and modern computer vision methods.", "arxiv_id": "2412.05203v1", "html_link": "https://arxiv.org/html/2412.05203v1", "search_term": "ti:\"deep learning\"", "html_content": "SECTION: Archaeoscape: Bringing Aerial Laser Scanning Archaeology to the Deep Learning Era\n\nAirborne Laser Scanning (ALS) technology has transformed modern archaeology by unveiling hidden landscapes beneath dense vegetation. However, the lack of expert-annotated, open-access resources has hindered the analysis of ALS data using advanced deep learning techniques. We address this limitation with Archaeoscape (available athttps://archaeoscape.ai), a novel large-scale archaeological ALS dataset spanningkm2in Cambodia with 31,141 annotated archaeological features from the Angkorian period. Archaeoscape is over four times larger than comparable datasets, and the first ALS archaeology resource with open-access data, annotations, and models.\n\nWe benchmark several recent segmentation models to demonstrate the benefits of modern vision techniques for this problem and highlight the unique challenges of discovering subtle human-made structures under dense jungle canopies. By making Archaeoscape available in open access, we hope to bridge the gap between traditional archaeology and modern computer vision methods.\n\nSECTION: 1Introduction\n\nAirborne Laser Scanning (ALS) has been celebrated as a \u201cgeospatial revolution\u201d in modern archaeology due to its ability to penetrate vegetation and unveil traces of human activities that may otherwise be concealed or invisible[1,2]. Extensive acquisition campaigns conducted in Southeast Asia[3], Central America[4], and Europe[5,6]have led to a reevaluation of the historical impact of humans on \u201cnatural\u201d landscapes, especially in tropical regions[7]. However, finding archaeological features in vast volumes of ALS data presents a significant challenge. Manual analysis is time-consuming and requires advanced expert knowledge of the studied civilization as well as on-site validation[8].\n\nThe emergence of deep learning offers a promising tool to assist researchers in identifying archaeological patterns, simplifying the exploration of these extensive acquisitions. Yet, the development of specialized models is hampered by the lack of expert-annotated datasets.\nIn response, we introduce Archaeoscape, the largest open-access ALS dataset for archaeological research published to date. Spanningkm2, it comprises 31,411 annotated instances of anthropogenic features of archaeological interest. The dataset includes orthophotos and LiDAR-derived normalized Digital Terrain Models (nDTM), encompassing over 3.5 billion pixels with RGB values, nDTM elevation, and semantic annotations.\n\nTraditionally, U-Net models[9]have dominated archaeological studies. In this paper, we evaluate several recent architectures for semantic segmentation.\nOur findings indicate that identifying ancient features beneath vegetation canopies using ALS still poses significant challenges. These difficulties can be attributed to the subtle nature of the objects sought, which are largely represented by faint elevation patterns. Moreover, certain features can span several kilometers and require extensive spatial context to disambiguate.\nWith Archaeoscape, we aim to challenge the machine learning and computer vision communities to address the rich, impactful, and unsolved problem of ALS-based archaeology. At the same time, we also encourage the archaeological community to adopt open-access policies and explore modern deep learning approaches.\n\ncolspec=X[1,l]X[1,c]X[1,c]X[1,r],\ncell12 = c=2c,\ncell21 = c=4c,\ncolumns=colsep=0pt,\nrows=rowsep=0pt,\nrow3=c,\n\nALS\u00a03D\u00a0point\u00a0cloud & Terrain model   AnnotationsMoundHydrologyTempleBackground\n\nSECTION: 2Related work\n\nIn this section, we explore the advantages of Archaeoscape over existing datasets, highlighting its larger scale and open-access policy (Section2.1), and present the different models evaluated in our benchmark (Section2.2).\n\nSECTION: 2.1ALS archaeology datasets\n\nDeep learning for ALS archaeology is a dynamic field[10].\nIn Table1, we list the main deep learning works on ALS-archaeology.\nArchaeoscape is not only one of the few open-access datasets available but also the largest and most comprehensively annotated by a significant margin.111We consider a dataset to be open-access when data, annotations, and train/test split are accessible, allowing the replication of the results.\n\nALS archaeology datasets typically withhold data, annotations, and code due to legitimate concerns about misuse[11,12], and the absence of established open-access norms in archaeology. However, recognizing the critical role of reproducibility and open access in science, we make Archaeoscape accessible to academic researchers. We implement strict safeguards to protect sensitive archaeological information, as described in Section3.1.\n\nArchaeoscape is the largest ALS archaeology dataset in terms of area covered (km2) and number of annotated instances (31,411). Archaeoscape covers a 2larger surface area and contains 3more instances than the next-largest closed archaeology LiDAR dataset (see Table1). It is also the first such dataset related to the Khmer civilization of Southeast Asia.\n\ncolspec=l ccl ccc,\nhline1,Z = 1pt, hlineY = 0.5pt, gray, rows=rowsep=0pt, row2 = abovesep=2pt, rowY = belowsep=2pt, rowZ = rowsep=2pt, row1 = valign=m,rowsep=2pt, l,\nhline2 = 0.5pt,\n\n& open-access  hi-resRGB  location  extentin km2resolutionin meters  number ofinstancesArran[13]\u2714\u2717United Kingdom  25  0.5  772Litchfield[14]\u2717\u2717USA  50  1  1,866Puuc[15]\u2717\u2717Mexico  23  0.5  1,966AHN[16]\u2717\u2717Netherlands  81  0.5  3,553AHN-2[17]\u2717\u2717Netherlands  437  0.5  3,849Connecticut[18]\u2717\u2717USA  353  1  3,881Dartmoor[19]\u2717\u2717United Kingdom  12  0.5  4,726Pennsylvania[20]\u2714\u2717USA  4  1  4,376Uaxactun[21]\u2717\u2717Guatemala  160  1  5,080Chact\u00fan[22]\u2717\u2717Mexico  230  0.5  10,894Archaeoscape (ours)\u2714\u2714Cambodia  888  0.5  31,411\n\nSECTION: 2.2Semantic segmentation with deep learning\n\nALS archaeology approaches rely predominantly on U-Net-based models[9]. However, the field of semantic segmentation has evolved considerably since its introduction in 2015. We propose to assess the performance of an array of contemporary, state-of-the-art models on the Archaeoscape benchmark.\nModels and pretraining strategies evaluated in Table4.1are denoted inboldthroughout the text for clarity and ease of reference.\n\nConvolutional Neural Networks (CNNs)[23,24], and theU-Net[9]architecture in particular, remain the predominant choice for dense prediction tasks across various application fields due to their simplicity and effectiveness.DeepLabv3[25]improves on this model by using dilated convolution and Spatial Pyramid Pooling[26]to learn multiscale features.\n\nVision transformers harness the versatility and expressivity of transformers[27]to extract rich image features. The Vision TransformerViT[28]model splits the images into small patches, which are embedded with a linear layer, while the final patch encodings are converted into pixel prediction with another linear layer.DOFA[29]embed each input channel conditionally to its wavelength, allowing generalization to new sensors. Alternatively\nhybridHybViTreplaces these linear layers with a combination of convolutional and deconvolutional layers for encoding and decoding patches, respectively. This adaptation is particularly effective on smaller datasets, as the convolutions help capture local feature dependencies more effectively.\n\nSeveral variants of the ViT model use a hierarchical approach to effectively capture spatial features with a large context.\nThe Pyramid Vision Transformer (PVT)[30]applies its attention mechanism according to a nested hierarchical structure, whileSWIN[31]uses overlapping windows of increasing sizes. Building on these concepts,PCPVT[32]introduces a conditional relative position encoding mechanism, andPVTv2[33]also allows for overlapping patches.\n\nRecent advances in self- and weakly-supervised learning have profoundly impacted the efficacy of neural networks.\nThese strategies often use large datasets with text annotations such asCLIP-OPENAI[34]or its open-source counterpartCLIP-LAION2B[35].\nAlternatively,DINOv2[36]learns from large, unannotated image datasets.\nThe recent Masked Auto-Encoder[37]tunes large models by using the pretext task of masked patch reconstruction. This approach has been adapted to address the specific needs of aerial imagery, leading to variants such asScaleMAE[38]which are trained on satellite images.\n\nSECTION: 3Archaeoscape\n\nIn this section, we describe the content of Archaeoscape (Section3.1), as well as its acquisition process (Section3.2).\n\nAngkor, the heart of the medieval Khmer Empire, is often referred to as a \u201chydraulic city\u201d due to its extensive water management infrastructure. This system allowed the Khmer to thrive in a challenging environment, oscillating between monsoon and dry seasons, from the 9th to the 15th century.\nToday, much of the built environment of Angkor and the other cities of this period has disappeared, as virtually all non-religious architecture was built using perishable materials such as wood. What remains is often hidden by dense vegetation or damaged by erosion and modern agricultural practices, rendering these sites nearly invisible at ground level, so that even experts might walk over such sites without realizing it.\nHowever, the advent of LiDAR (Light Detection and Ranging) technology has been transformative, uncovering distinct, often geometric patterns in the topography indicative of ancient occupation and landscape alteration. By combining careful analysis of ALS imagery with targeted ground surveys, this decade-long project has documented tens of thousands of ancient Khmer features, many previously undiscovered, providing a new and expanded perspective on the history of the region.\n\nSECTION: 3.1Dataset characteristics\n\nAs shown in Figure2, the dataset consists of 23 non-overlapping parcels of varying size, ranging fromtokm2, and include archaeologically relevant areas such as ancient temples, cities, and roadways. We present the splits for Archaeoscape\u2019s training (km2,parcels), validation (km2,parcels), and test (km2,parcels) sets. The splits were chosen to respect the global distribution of features and landscapes: densely or scarcely occupied regions, hills or floodplains, large-scale hydraulic engineering sites, monumental temples, andsubtleearthen features.\n\nUnder these constraints, splitting the dataset into spatially distinct regions\u2014as is commonly done in geospatial machine learning\u2014proved impractical. To prevent data contamination all parcels are separated by a least ameter buffer. The test set consists ofremoteparcels, set apart from the others by more thankm, andparcelsadjacentto training and validation sets, covering two use cases: predicting features in a new area under a domain shift, and a realistic scenario in which archaeologists annotate part of an area of interest and train a model to pre-segment the rest.\n\nThere is a valid concern that large-scale annotated ALS data could be misused by malicious actors, leading to the targeted looting or destruction of historical sites[11,12]. The potential for misuse has been a significant factor in the lack of public availability of archaeological datasets. To mitigate this risk and alleviate the concerns of local stakeholders, we propose several measures to balance the benefits of open access with the legal and practical protection of cultural heritage sites:\n\nData partitioning:The data is divided into parcels and stripped of georeferencing and absolute elevation information to prevent spatial identification of remote, less well-known sites. While famous temples such as Angkor Wat may be recognizable, they are already under close protection by the local authorities.\n\nCustom license:The dataset is distributed under a license which forbids re-georeferencing, commercial use, and redistributing the data beyond the intended users.\n\nOpen credentialized access:Access to the dataset requires signing a data agreement form, which holds users legally accountable for misuse. The Appendix contains more details about the license, data access, and the distribution agreement.\n\nWe distribute the data as GeoTIFF files with am resolution and polygon annotations in the GeoPackage format. We associate each pixel with the following values:\n\nRadiometry:RGB values obtained from contemporary orthophotography.\n\nGround elevation:Digital Terrain Model (DTM) obtained with ALS, see Section3.2.\n\nSemantic label:One-hot encoding of the five classes described below.\n\ncolspec=X[2,c]X[2,c]X[2,c]X[1,c],\nrows=rowsep=0pt,\nrow1=belowsep=-2pt,\ncolumns=colsep=0pt,\ncolumn2=colsep=8pt,&\\SetCell[r=3]m{tblr}colspec=cl,\ncolumns=colsep=0pt,\ncolumn1=leftsep=4pt, rightsep=2pt,\ncolumn2=font=,\nrows=valign=m, rowsep=4pt,MoundHydrologyTempleBackgroundImageviewpoint\n\nOne of the most significant undertakings of Archaeoscape is the meticulous annotation by experts, who have individually traced and field-verified a wealth of archaeological features. The annotators employed a granular classification system with 12 feature types. However, to mitigate severe class imbalance and reduce ambiguity, we have streamlined this system into a more manageable 5-class nomenclature, represented in Figure3. We explain these classes below and provide, where applicable, the number of instances and pixel frequency:\n\nTemple (827, 0.2%).Quintessential to the Cambodian landscape, these edifices stand as the most iconic remnants of the Angkorian civilization. The scale of these temples ranges from the monumental Angkor Wat, spanning over one hundred hectares, to much smaller sites marked by little more than a scattering of bricks or stone blocks.\n\nMound (14,400, 8.6%).Manifesting as slight elevations, these artificial earthen features are indicative of a range of human activities. They include habitation and crafting sites, as well as the embankments of canals and reservoirs. Although very numerous, mounds are often concealed by dense vegetation or too subtle to be easily detectable on the ground.\n\nHydrology (16,184, 10.4%).This class groups together various features of Khmer hydro-engineering such as rivers, canals, reservoirs that can reach several kilometers in width, and smaller artificial ponds. These features highlight the Angkorian civilization\u2019s significant investment in water management and have long been of particular interest to archaeologists.\n\nVoid (3,145, 2.5%).This annotation is reserved for areas that are considered ambiguous by expert annotators and for structures excluded from the analysis presented in this paper. Void pixels are removed from supervision and evaluation.\n\nBackground (78.3%).This category encompasses everything else: regions that lack particular archaeological features or are obscured by modern development. Background includes a wide array of non-archaeological elements such as modern agricultural plots and infrastructure.\n\nWhile the annotations are created and distributed as polygons, we treat them as pixel labels, framing the problem of detecting archaeological features as a conventional semantic segmentation task.\n\nSECTION: 3.2Acquisition and processing\n\nALS and orthophotography imagery was obtained during the KALC (2012)[3]and CALI (2015)[39]acquisition campaigns in Cambodia, from which a subset of 888 km2was selected, as described in Section3.1, corresponding to over 13,000 aerial photos and 10 billion 3D points, with a density of 10-95 points per m2, depending on the terrain.\n\nThe data was acquired with Leica LiDAR (ALS60 for KALC, ALS70-HP for CALI) and cameras (RCD105 and RCD30). The instruments were mounted on a pod attached to the skid of a Eurocopter AS350 B2 helicopter flying at 800 m above ground level as measured by an integrated Honeywell CUS6 IMU, and positional information was acquired by a Novatel L1/L2 GPS antenna. GPS ground support was provided by two Trimble R8 GNSS receivers.\n\nNon-terrainpoints (i.e.corresponding to tree canopies, modern buildings) are removed from ALS points with the Terrasolid software[40]. We form a DTM by fitting a triangular irregular network[41]to the remaining points and linearly interpolating the ground point elevation values within each triangular plane on ameter grid. The photos are orthorectified and resampled to the samemeter resolution.\n\nThe endeavor to map Khmer archaeological features has a long history, tracing back to the 19th century, with significant advancements following the availability of aerial imagery in the 1990s[42,43]. Our annotation process builds upon this historical groundwork, but mostly leverages the LiDAR data collected in 2012 and 2015. Our approach relies on an iterative process of manual annotation using a Geographic Information System, QGIS, and targeted ground survey to verify features in the field.\nThese mapping and verification efforts were performed by a shifting team of archaeologists, both local and foreign, who collectively contributed to the analysis and validation of the data. The first pre-LiDAR surveys date back to 1993, and work continued until 2024.\n\nSECTION: 4Benchmark\n\nIn this section, we assess the performance of modern semantic segmentation methods for ALS archaeology. We first detail how we adapt and evaluate these methods (Section4.1), then present our results and analysis (Section4.2), and an ablation study (Section4.3). Finally, we discuss the limitations of our approach (Section4.4).\n\nSECTION: 4.1Baselines and metrics\n\nWe formulate the problem of finding archaeological features as a semantic segmentation task, and benchmark several backbone networks on our dataset.\n\nWe evaluate the prediction of the models with the overall accuracy (OA), class-wise Intersection over Union (IoU), and the unweighted mean of the IoUs (macro-average). For the evaluation, we exclude pixels annotated with the void label.\n\nWe train the evaluated models using the configurations of the official open-source repository and provide more details in the supplementary materials.\nThe predictions on the test set are performed along a grid corresponding to the input size and with% overlap on each side. Only the central portion of each prediction is kept while the border predictions are discarded.\n\nWe use a combination of internal clusters and the HPC GENCI to run our experiments. Reproducing the entire benchmark requires 260 GPU-h with A100 GPUs. We estimate the total cost of our hyperparameters search and initial experiments at 1100 GPU-h.\n\nTo evaluate the performance of modern vision models for ALS archaeology, we adapt several semantic segmentation models to our setting. The changes are minimal:\n\nInputs.Beyond radiometry (RGB), we also incorporate ground elevation derived from the ALS data described in Section3.2. As we consider networks trained on natural images, we modify the first layer to accommodate an extra band and initialize the additional weights randomly according to.\n\nSegmentation head.For all transformer-based methods, we map the final patch embeddings to pixel-level prediction with linear layers, except forHybViTwhich uses transposed convolutions. For CNNs, we use their dedicated segmentation heads, which we initialize randomly.\n\nPre-training and fine-tuning.We consider models pre-trained on ImageNet1K[44]and ImageNet21K[45], but also foundation vision models trained on large external datasets: DINOv2[36], CLIP-OPENAI[34]and LAION-2B[35], and Earth observation datasets[38,29,46].\n\ncaptiondefault\\SetTblrStylefootfont=\\DefTblrTemplatenotedefault\\MapTblrNotes\\UseTblrTemplatenote-tagdefault\\UseTblrTemplatenote-targetdefault\\UseTblrTemplatenote-textdefault{talltblr}[\nnotea =github.com/qubvel/segmentation_models.pytorch,\nnoteb =pytorch.org/vision,\nnotec =timm.fast.ai,\nnoted =huggingface.co/laion,\nnotee =github.com/bair-climate-initiative/scale-mae,\nnotef =https://github.com/zhu-xlab/DOFA,\nnoteg =https://github.com/allenai/satlas,\n]\ncolspec=lllc ccccc c,\nhline1,Z = 1pt, solid, hline5,12,16 = 0.5pt, gray, cell12,3 = r=2l, cell14,Z = r=2c, cell15 = c=5c, hline2 = 5-Y0.5pt, hline3 = 0.5pt, rows=rowsep=0pt, row1,2 = valign=m,rowsep=2pt, row3,5,13,16 = abovesep=2pt, cell31 = r=2valign=m,cmd=,\ncell51 = r=7valign=m,cmd=,\ncell121 = r=4valign=m,cmd=,\n\n& Backbone  pre-training  inputsize  IoU      OAavg  temple  hydro  mound  bkgCNN  U-Net\\TblrNotea  ImageNet1K 224 50.533.332.748.687.688.2\\greycellDeepLabv3\\TblrNoteb\\greycellImageNet1K\\greycell224\\greycell47.6\\greycell19.8\\greycell35.9\\greycell47.5\\greycell87.2\\greycell87.8ViT  ViT-S\\TblrNotec  ImageNet21K224 46.418.5 33.346.687.0 87.5\\greycellViT-S\\TblrNotec\\greycellDINOv2\\greycell224\\greycell41.9\\greycell14.5\\greycell26.1\\greycell40.9\\greycell86.2\\greycell86.7ViT-B\\TblrNoted  CLIP 224 30.33.415.830.383.1 83.4\\greycellViT-B\\TblrNoted\\greycellLAION2B\\greycell224\\greycell32.4\\greycell2.8\\greycell14.4\\greycell28.2\\greycell84.3\\greycell84.6ViT-L\\TblrNotee  ScaleMAE 224 30.40.016.022.882.782.8\\greycellHybViT-S\\TblrNotec\\greycellImageNet21K\\greycell224\\greycell50.4\\greycell32.4\\greycell33.6\\greycell48.0\\greycell87.5\\greycell88.1DOFA\\TblrNotef DOFA  224 39.6 13.4 25.9 33.6 85.5  86.0HViT\\greycellSWIN-S\\TblrNotec\\greycellImageNet21K\\greycell224\\greycell51.9\\greycell33.1\\greycell35.2\\greycell51.4\\greycell88.0\\greycell88.6SWIN-B\\TblrNoteg SatLas  224 49.628.234.048.487.788.3\\greycellPCPVT-S\\TblrNotec\\greycellImageNet1K\\greycell224\\greycell51.7\\greycell33.4\\greycell35.0\\greycell50.6\\greycell88.0\\greycell88.5PVTv2-b1\\TblrNotec  ImageNet1K224 52.132.336.451.488.2 88.7\n\nU-Net\\TblrNotea\\greycellImagineNet1K\\greycell512\\greycell52.8\\greycell31.8\\greycell39.7\\greycell50.7\\greycell89.1\\greycell89.6PVTv2\\TblrNotec  ImagineNet1K  512 52.228.338.053.089.489.9\n\nSECTION: 4.2Results\n\nWe report the quantitative performance of various state-of-the-art semantic segmentation models in Table4.1, and provide qualitative examples in Figure4.\n\nSurprisingly, CNN-based methods such asU-Netoutperform most ViTs on our dataset. We attribute this result toViTs\u2019 reliance on extensive pre-training on RGB images. In our data, the most informative channel is the elevation rather than RGB, as the radiometric information is typically blocked by the dense canopy cover. Indeed, and as shown in Section4.3, models trained on RGB all perform belowmIoU. This distinction may explain why foundation models renowned for their effectiveness on natural images, such asDINOv2orCLIP, fail to adapt to this new setting. EvenScaleMAEandDOFA, which are pre-trained on large amounts of satellite imagery, lead to poor performances.\n\nThe hybrid ViT modelHybViT, which uses convolutions for patch encoding and decoding, performs better. This suggests that integrating local feature processing (typical of CNNs) with a global perspective (a strength of ViTs) is beneficial for interpreting archaeological ALS data. This analysis is further supported by the relatively high performance of hierarchical ViT models, which even surpass CNNs in some cases. We hypothesize that the hierarchical structure of these models aligns well with the dual requirement of our task: to recognize local patterns and to integrate them within a broader spatial context. This capability is particularly advantageous for detecting archaeological features, which often consist of both small objects and expansive, interconnected structures.\n\nIn our experiments, we use the default size of ViTs in all experiments: 224 pixels, equivalent to 112 meters. However, the Archaeoscape dataset includes structures such as basins spanning several kilometres, and which can only be detected with a larger context. When scaling our input size to, we noted a significant improvement in performance, especially with theU-Netmodel. Attempts to further increase the input size did not yield additional performance gains, as the models quickly overfit to the training set.\n\nAs depicted in the top row of Figure4, models trained on our data can detect complex structures, such as the central grid inside the temple moat and the maze-like features outside. However, they miss the broader semantic context,e.g.finding the prominent temple walls while failing to segment the platform.\nIn the middle row, the models detect isolated features and temples with the standard \u201chorseshoe\u201d configuration, while the large ponds are mostly missed, likely due to the limited context window size. In the bottom row, the hilly areas with faint feature elevation pose a significant challenge. This highlights the limitations of current models in handling the varying landscape, scale, and semantic context of archaeological features.\n\nThe performance across models remains relatively low, especially if compared to results achieved on complex computer vision segmentation benchmarks featuring numerous classes. This suggests that contemporary model architectures may not adequately meet the specific challenges of ALS archaeology, which involves interpreting subtle local elevation patterns within a broader spatial context. Furthermore, foundation models for natural images often fail to adapt to the specificity of the data and the new elevation channel, possibly due to their extensive pre-training. This situation highlights the need for bespoke models specifically tailored for ALS data analysis.\n\nSECTION: 4.3Ablation study\n\nWe evaluate the impact of some of the choices made in the design of Archaeoscape through an ablation study.\n\nAirborne LiDAR scans are pivotal for uncovering the subtle elevation patterns of archaeological features like mounds and canals, which are typically not visible in orthophotos, as shown in Figure5. Moreover, dense canopies can obscure or completely hide radiometric information about the ground. Conversely, in less densely forested areas, orthophotos can capture detailed information about archaeological features, complementing LiDAR data. The ablation study results, documented in Table3, highlight the limitations of relying solely on RGB data. Models using only RGB information registered a mean Intersection over Union (mIoU) of about 30%, significantly lower than models also utilizing elevation data. This disparity underscores the inadequacy of RGB data under dense canopy coverage.\nFurthermore, while removing RGB information only moderately affects performance, it particularly affects the detection of temples\u2014\u2013some of which are still standing to this day, and are typically not covered by the canopy.\nThe performance gap between models pretrained with DINOv2 and those pretrained on ImageNet widens without RGB, suggesting that DINOv2 models are highly optimized for RGB processing, whereas ImageNet models adapt better to elevation data.\n\nAdapting models trained on RGB data to handle elevation channels poses challenges. Our approach, detailed in Section4, initialize with small values the weights of the first layer corresponding to the new channel while retaining the pre-trained weights for RGB. In Table3, we evaluate this method against three alternatives: fully random initialization, random initialization of the first layer with other weights retained, and LoRA fine-tuning. Randomly initializing the first layer results in performance akin to training the network from scratch, demonstrating the efficacy of our strategy to leverage pre-existing RGB training.\n\nSECTION: 4.4Limitations\n\nArchaeoscape presents several limitations as a benchmark that should be considered:\n\nDomain shift:The imagery for Archaeoscape has been collected over two campaigns using different equipment. Even with our best efforts to harmonize the dataset and its processing, sensor and meteorological variations may manifest in the data distribution.\n\nAnnotation errors and ambiguity:As they were annotated and field-verified by expert archaeologists, we can affirm that the annotated polygons correspond to actual archaeological features with high confidence. However, there is an inevitable degree of ambiguity regarding the precise shape and boundaries of these features, which often consist of very slight relief sloping gradually into the natural terrain. Moreover, we cannot rule out that background terrain may contain some yet uncovered features that would have eluded detection.\n\nCultural specificity:Archaeoscape aims to serve as a benchmark for vision models for ALS archaeology, but focuses exclusively on the Khmer civilization. We acknowledge that our conclusions may not be universally applicable to other cultural contexts or regions.\n\nSECTION: 5Conclusion\n\nWe have introduced Archaeoscape, the largest published dataset for ALS archaeology featuring open-access imagery and annotations. Focused on the ancient Khmer settlement complexes and temples of Cambodia, our dataset coverskm2and comprises 31,144 individual anthropogenic instances. We provide an extensive benchmark evaluating several state-of-the-art computer vision models for detecting archaeological features within elevation maps and images. Despite formulating the problem as a classic semantic segmentation task, we observe that even usually high-performing models struggle to achieve high scores. We attribute this poor performance to the unique challenges of ALS archaeology, such as the subtlety of the patterns sought, and the importance of large-scale context.\nWe hope that our dataset will encourage the computer vision and machine learning community to propose novel solutions for these unresolved challenges.\n\nSECTION: Acknowledgments and disclosure of funding\n\nThe experiments conducted in this study were performed using HPC/AI resources provided by GENCI-IDRIS (Grant 2023-AD011014781).\n\nThis work has made use of results obtained with the Chalawan HPC cluster, operated and maintained by the National Astronomical Research Institute of Thailand (NARIT) under the Ministry of Science and Technology of Royal Thai government.\n\nThis project is funded by the European Research Council (ERC) under the European Union\u2019s Horizon 2020 research and inovation programme (grant agreement No 866454).\n\nSECTION: References\n\nSECTION: APPENDIX\n\nSECTION: A.1Checklist\n\nFor all authors\u2026\n\nDo the main claims made in the abstract and introduction accurately reflect the paper\u2019s contributions and scope?[Yes]\n\nDid you describe the limitations of your work?[Yes]See Section4.4.\n\nDid you discuss any potential negative societal impacts of your work?[Yes]SeeMisuse Preventionin Section3.1, and the datasheet for dataset in SM.3.\n\nHave you read the ethics review guidelines and ensured that your paper conforms to them?[Yes]\n\nIf you ran experiments (e.g. for benchmarks)\u2026\n\nDid you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?[Yes]See supplementary material.\n\nDid you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?[Yes]See Section4.1and supplementary materials.\n\nDid you report error bars (e.g., with respect to the random seed after running experiments multiple times)?[No]Due to the extensive computational resources required to calculate error bars in time for the submission deadline, we were unable to include them in the current version of the paper. We plan to conduct the necessary experiments over the coming weeks and intend to incorporate error bars in the final camera-ready version of the paper, should it be accepted.\n\nDid you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?[Yes]See Section4.1\n\nIf you are using existing assets (e.g., code, data, models) or curating/releasing new assets\u2026\n\nIf your work uses existing assets, did you cite the creators?[Yes]\n\nDid you mention the license of the assets?[N/A]The code licenses are given on the linked websites.\n\nDid you include any new assets either in the supplemental material or as a URL?[Yes]\n\nDid you discuss whether and how consent was obtained from people whose data you\u2019re using/curating?[Yes]The data was acquired by the EFEO during the KALC and CALI projects, as explained in Section3.2.\n\nDid you discuss whether the data you are using/curating contains personally identifiable information or offensive content?[Yes]The misuse prevention is discussed in Section3.1. See supplementary material for more details.\n\nIf you used crowdsourcing or conducted research with human subjects\u2026\n\nDid you include the full text of instructions given to participants and screenshots, if applicable?[N/A]\n\nDid you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?[N/A]\n\nDid you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?[N/A]\n\nSECTION: A.2Author statement\n\nThe authors hereby acknowledge and accept full responsibility for the content of the Archaeoscape dataset. They confirm that this dataset does not violate any intellectual property rights, privacy rights, or other legal or ethical standards.\n\nThey indemnify and hold harmless the NeurIPS Dataset and Benchmark Track from any claims, damages, or legal actions resulting from the submission, storage, or use of this dataset.\n\nSECTION: A.3Additional ALS archaeology-related datasets\n\nALS data are well-used by archaeologists for its precision and ability to recover the archaeological features[10]. Several recent works have leveraged deep learning techniques to automatically detect features of interest. In TableA.1, we provide a list of such works. Note that this is a very dynamic field, and this list may not be exhaustive.\n\ncolspec=l ccl ccc,\nhline1,Z = 1pt, hlineY = 0.5pt, gray, rows=rowsep=0pt, row2 = abovesep=2pt, rowY = belowsep=2pt, rowZ = rowsep=2pt, row1 = valign=m,rowsep=2pt, l,\nhline2 = 0.5pt,\n\n& open-access  hi-resRGB  location  extentin km2resolutionin meters  number ofinstancesAHN[47]\u2717\u2717Netherlands  437  0.5  N/AMysteries of the Maya[48]\u2717\u2717\u2736Mexico  120  0.5  N/AJ\u00f6nk\u00f6ping[49]\u2717\u2717Sweden  22  1  155M\u00e9galithes de Bretagne[5]\u2717\u2717France  200  0.5  195Bia\u0142owie\u017ca Forest[6]\u2717\u2717Poland  N/A  0.5  211Galicia[50]\u2717\u2717\u2736Spain  N/A  1  306Arran[13]\u2714\u2717United Kingdom  25  0.5  772S\u00e1pmi[51]\u2717\u2714Finland  21  N/A  997MayaArch3D[52]\u2717\u2717Honduras  25  N/A  1,124Litchfield[14]\u2717\u2717USA  50  1  1,866Puuc[15]\u2717\u2717Mexico  22.5  0.5  1,966AHN[16]\u2717\u2717Netherlands  81  0.5  3,553AHN-2[17]\u2717\u2717Netherlands  437  0.5  3,849Connecticut[18]\u2717\u2717USA  353  1  3,881Pennsylvania[20]\u2714\u2717USA  4  1  4,376Dartmoor[19]\u2717\u2717United Kingdom  12  0.5  4,726Uaxactun[21]\u2717\u2717Guatemala  160  1  5,080Lusatia[53]\u2717\u2717Germany  3.4  0.5  6,000Chact\u00fan[22]\u2717\u2717Mexico  230  0.5  10,894Archaeoscape (ours)\u2714\u2714Cambodia  888  0.5  31,411\n\nSECTION: A.4Additional results\n\nFoundation models can be difficult to fine-tune to new datasets or tasks, as they tend to easily overfit. Low rank adaptation (LoRA)[54]can remedy this issue by only learning a low-rank update to the weights of the pre-trained model. Figure FigureA.1provides the performance of PVTv2 and DINO models fine-tuned using LoRA compared to a fully fine-tuned.\n\nWhen fine-tuned with LoRA, the performance of DINO rapidly plateaus and even decreases, showing that the learned features can not be easily adapted from RGB images to terrain models. Conversely, the performance of PVTv2 increases with the rank used for LoRA, but does not reach the performance of a fully fine-tuned network. This suggests that, when fine-tuned to new input and target domains, and particularly when using LoRA, large models can become over-adapted to their source domain and struggle to generalize.\n\nwidth=colspec=X[1,l]X[8,c]X[8,c]X[8,c],\nrows=rowsep=-1pt,\ncolumns=colsep=1pt,\ncolumn3 = colsep=3pt,RGB&nDTMGTPVTv2\n\nWe provide additional visualizations of the mapping outputs generated by our models in FigureA.2. Those once again illustrate the difficulty that arises from large-scale dependency, particularly in water prediction. Columns 2 and 3 are respectively an illustrations of a failure and success case in predicting large bodies of water.\nWhile our model is able to accurately identify most temples and mounds, the reconstruction of the exact shape of religious or settlement complexes remains approximate. Moreover, we observe that the model struggles to detect fainter mounds, although these are still visible to human experts.\n\nSECTION: A.5Implementation details\n\nSECTION: A.5.1Data split\n\nWe designed the split to each contain emblematic archaeological features\u2014large-scale hydraulic engineering sites, monumental temples, subtle features, and typical terrain types\u2014dense and scarce occupation, hills, and floodplains.\nThe class distribution per split is given in TableA.2.\n\nSECTION: A.5.2Data loader\n\nOur data loader loads images of sizepixels at a resolution ofcm, with RGB channels normalized using the dataset mean and variance. The elevation channel is normalized separately, using the mean and variance of each sampled image. We use a batch size of 64 throughout all experiments.\n\nTo sample random images during training, we first randomly select pixels from one of the predefined study areas with a probability proportional to their area. We then take a crop of sizecentered on this pixel. The image is rejected if overof its extent falls out of the area. otherwise, the out-of-area pixels are padded with the image\u2019s mean for each channel.\nFinally, we apply the following augmentations, each activated with an independent probability of:\n\nScale modification:The image is scaled by a random factor betweenand.\n\nRotation: The sample is rotated by an angle randomly chosen betweenandusing bilinear interpolation.\n\nDuring validation, we sample pixels along a regular grid with apixels step. During testing, we use apixels grid, and, when predicted images overlap, only keep the center half of the image,i.e.discarding the 25% border on all sides. We do not employ augmentations during the evaluation.\n\nSECTION: A.5.3Training\n\nWe use the ADAM optimizer with a linear warm-up schedule that increases the learning rate fromtoacross the first two epochs of training. We use aReduceLROnPlateau[55]learning rate scheduler with a patience ofand a decay of.\n\nSECTION: A.6License\n\nThe Archaeoscape dataset is under a custom license, which prevents redistribution and attempts at localizing the data. We provide the full text of the license below.\n\nThe \u00c9cole fran\u00e7aise d\u2019Extr\u00eame-Orient (EFEO) makes the Archaeoscape dataset (the \u201cDATASET\u201d) available for research and educational purposes to individuals or entities (\"USER\") that agree to the terms and conditions stated in this License.\n\nThe USER may access, view, and use the DATASET without charge for lawful non-commercial research purposes only. Any commercial use, sale, or other monetization is prohibited. The USER may not use the DATASET for any unlawful activities, including but not limited to looting, vandalism, and disturbance of archaeological sites.\n\nThe USER may not attempt to identify the location of any part of the DATASET and must exercise all reasonable and prudent care to avoid the disclosure of the locations referenced in the DATASET in any publication or other communication.\n\nThe USER may not share access to the DATASET with anyone else. This includes distributing the download link or any portion of the DATASET. Other users must register separately and comply with all the terms of this Licence.\n\nThe USER must use the DATASET in a manner that respects the cultural heritage of Cambodia and its people, and in compliance with the relevant Cambodian authorities. Any use of the DATASET that could harm or exploit these cultural sites or their environment is strictly prohibited.\n\nThe USER must properly attribute the EFEO as the source of the data in any publications, presentations, or other forms of dissemination that make use of the DATASET.\n\nThis agreement may be terminated by either party at any time, but the USER\u2019s obligations with respect to the DATASET shall continue after termination. If the USER fails to comply with any of the above terms and conditions, their rights under this License shall terminate automatically and without notice.\n\nTHE DATASET IS PROVIDED \"AS IS,\" AND THE EFEO DOES NOT MAKE ANY WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND NON-INFRINGEMENT. IN NO EVENT SHALL THE EFEO OR ITS COLLABORATORS BE LIABLE FOR ANY CLAIM, DAMAGES, OR OTHER LIABILITY ARISING FROM THE USE OF THE DATASET.\n\nSECTION: A.7Datasheet for dataset\n\nSECTION: A.7.1Motivation\n\nFor what purpose was the dataset created?Was there a specific task in mind? Was there a particular gap that needed to be filled? Please provide a description.\n\nThe Archaeoscape dataset is an open-access ALS dataset intended for archaeology. It is simultaneously the largest in terms of its extent and number of annotated anthropogenic features. The intended task is the semantic segmentation of LiDAR-derived terrain maps to find archaeological traces and structures under dense vegetation.\n\nWho created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?\n\nThe different parts of the dataset were acquired during several acquisition campaigns as part of the Khmer Archaeology Lidar Consortium (KALC) and Cambodian Archaeological Lidar Initiative (CALI), joint programs of which the EFEO (Ecole fran\u00e7aise d\u2019Extr\u00eame-Orient) was a member. The curation and benchmarking were performed jointly with the IMAGINE team (A3SI/LIGM, ENPC).\n\nWho funded the creation of the dataset?If there is an associated grant, please provide the name of the grantor and the grant name and number.\n\nThe ALS acquisitions were funded by the following parties:\n\nEuropean Research Council (ERC)\n\n\u00c9cole fran\u00e7aise d\u2019Extr\u00eame-Orient (EFEO)\n\nUniversity of Sydney (USYD)\n\nSoci\u00e9t\u00e9 Concessionnaire d\u2019A\u00e9roport (SCA/INRAP Airport)\n\nHungarian Indochina Company (HUNINCO)\n\nJapan-APSARA Safeguarding Angkor (JASA)\n\nArchaeology & Development Foundation Phnom Kulen Program (ADF Kulen)\n\nWorld Monuments Fund (WMF)\n\nAnd are associated with the following ERC grants:\n\nCALI: \u201cThe Cambodian Archaeological Lidar Initiative: Exploring Resilience in the Engineered Landscapes of Early SE Asia\u201d (Grant agreement ID: 639828)\n\narchaeoscape.ai: \u201cExploring complexity in the archaeological landscapes of monsoon Asia using lidar and deep learning\u201d (Grant agreement ID: 866454).\n\nThe funding for the Archaeoscape annotations is 100% public. The EFEO is an \u201c\u00c9tablissement public \u00e0 caract\u00e8re scientifique, culturel et professionnel\u201d,i.e.a public scientific, cultural or professional establishment which is financed by public funds.\n\nAny other comments?\n\n[N/A]\n\nSECTION: A.7.2Composition\n\nWhat do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?\n\nThe dataset covers several sites in Cambodia of archaeological interest. The dataset comprises ALS-derived elevation maps, orthorectified photography, and manually annotated archaeological features.\n\nHow many instances are there in total (of each type, if appropriate)?\n\nArchaeoscape covers 888 km2and 31,141 individual archaeological features. The dataset is split into 23 non-overlapping parcels, fromtokm2.\n\nDoes the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set?\n\nArchaeoscape covers only a fraction of the full extent of the Khmer Empire at its apogee and of the likely distribution of Khmer archaeological features in the landscape. Those parts of the dataset contained in the training, validation, and test sets have been extensively annotated by archaeological experts. We can affirm with a reasonably degree of confidence that the vast majority of the archaeological features in these splits have been identified and annotated.\n\nWhat data does each instance consist of?\n\nEach parcel is a raster file under the GeoTIFF format with a ground sampling distance ofm. Each pixel is associated with:\n(i) a terrain elevation relative to the lowest point of the file,\n(ii) an RGB value derived from an orthorectified aerial photograph,\n(iii) where available, a label corresponding to one of the sought classes,\n(iv) a binary value indicating whether or not the pixel is in the parcel.\n\nIs there a label or target associated with each instance?\n\n[Yes]We provide dense pixel-precise annotations forkm2corresponding to over 3.5 billion annotated pixels.\n\nIs any information missing from individual instances?\n\n[Yes]The georeferencing information has been stripped from the dataset parcels.\n\nAre relationships between individual instances made explicit (e.g., users\u2019 movie ratings, social network links)?\n\n[No]To prevent their re-georeferencing, we have purposefully removed any information on the relationships between parcels.\n\nAre there recommended data splits (e.g., training, development/validation, testing)?\n\n[Yes]We provide the following data splits: train, validation and test.\nThe test split has been explicitly selected to contain a representative variety of configurations. We implement am buffer between all parcels.\n\nAre there any errors, sources of noise, or redundancies in the dataset?\n\nAs the annotations are made through visual interpretation with quality control, some errors are unavoidable, especially for classes that are visually hard to distinguish. Some unavoidable noise occurs due to the ambiguous boundaries of subtle archaeological features. Internal quality control has been performed to limit such errors. There are no redundancies in the dataset, each parcel covers a distinct area.\n\nIs the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?\n\nThis dataset is self-contained and will be stored and distributed by the EFEO.\n\nDoes the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor\u2013patient confidentiality, data that includes the content of individuals\u2019 non-public communications)?\n\n[No]The data does not contain confidential information. However, to limit potential misuse such as looting or destruction of historical sites, the georeferencing and absolute elevation of the parcels have been removed.\n\nDoes the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?If so, please describe why.\n\n[No]\n\nDoes the dataset identify any subpopulations (e.g., by age, gender)?\n\n[No]\n\nIs it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset?\n\n[No]The nDTM elevation data excludes extraneous points such as modern buildings. The RGB orthophotography resolution of 50 cm/pixel and the aerial perspective prevent the recognition of individuals.\n\nDoes the dataset contain data that might be considered sensitive in any way (e.g., data that reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)?\n\n[No]\n\nAny other comments?\n\nThe safe and ethical release of archaeological data has been the subject of numerous studies[12]. We have implemented the best practices of the field to minimize potential risks of misuse.\n\nSECTION: A.7.3Collection Process\n\nHow was the data associated with each instance acquired?\n\nThe ALS data and photography were acquired from aerial surveys in Cambodia and mapped onto a cartographic coordinate reference system. From this data a subset of 888 km2was selected, corresponding to over 13,000 aerial photos and 10 billion points, with a density of 10-95 points per m2, depending on the terrain.\n\nThe ALS points were filtered to remove noise and classified. The normalized Digital Terrain Models (nDTM) (relative ground elevation) was obtained from the classified ALS point clouds using open-source software. A triangular irregular network was fitted to thegroundpoints (excluding extraneous elements such as tree canopies and modern buildings), with a DTM formed by linear interpolation of the elevation values within each triangular plane based on ameter grid. The same procedure was applied to obtain intensity and return number metadata maps. The photos were orthorectified and resampled to the samemeter resolution.\n\nWhat mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)?\n\nThe data was acquired with Leica LiDAR (ALS60 for KALC, ALS70-HP for CALI) and cameras (RCD105 and RCD30). The instruments were mounted on a pod attached to the skid of a Eurocopter AS350 B2 helicopter flying at 800 m above ground level as measured by an integrated Honeywell CUS6 IMU, and positional information acquired by a Novatel L1/L2 GPS antenna. GPS ground support was provided by two Trimble R8 GNSS receivers.\n\nIf the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?\n\nThe target areas for the LiDAR acquisition campaigns were selected on the grounds of archaeological value and interest by domain experts. A subset of 888 km2presented in this dataset was selected by choosing 23 non-overlapping parcels in the areas where archaeological annotations were deemed complete and finalized, preserving the global distribution of features and landscapes across the training, validation and test sets.\n\nWho was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?\n\nThese mapping and verification efforts were performed by a shifting team of archaeologists, both local and foreign, who collectively contributed to the analysis and validation of the data, with the first pre-LiDAR surveys dating back to 1993, and continuing until 2024. All persons involved were employees and researchers from foreign governmental institutions, such as the EFEO or Sydney University, or employed by the Cambodian governmental authorities, following strictly existing ethical codes and national regulations.\n\nOver what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)?\n\nThe KALC campaign took place in 2012, and the CALI campaign in 2015. The annotations are the result of a continuous effort from 1993 to 2024.\n\nWere any ethical review processes conducted (e.g., by an institutional review board)?\n\n[Yes]Yes, as a part of the CALI and archaeoscape.ai ERC grants.\n\nDoes the dataset relate to people?\n\nThe dataset describes the archaeological remains of anthropogenic structures, but does not directly relate to living people.\n\nDid you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)?\n\n[N/A]\n\nWere the individuals in question notified about the data collection?\n\n[N/A]\n\nDid the individuals in question consent to the collection and use of their data?\n\n[N/A]\n\nIf consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses?\n\n[N/A]\n\nHas an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted?\n\n[Yes]We have studied potential misuse of the data and have taken steps to prevent it, such as removing and obfuscating the location of acquisitions.\n\nAny other comments?\n\n[No]\n\nSECTION: A.7.4Preprocessing, cleaning, and/or labeling\n\nWas any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)?\n\n[Yes]The ALS acquisitions are delivered in the form of 3D point clouds. The ALS points were filtered to remove noise and classified. We have extracted terrain models from thegroundclouds only,i.e.those not belonging to the tree canopies and modern buildings.\n\nWas the \u201craw\u201d data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)?If so, please provide a link or other access point to the \u201craw\u201d data.\n\n[Yes]The data has been saved, but will not be distributed to prevent data re-localization.\n\nIs the software used to preprocess/clean/label the instances available?\n\n[Yes]The annotation software is open source. The anonymized data preprocessing code (without references to specific locations or coordinates) is available.\n\nAny other comments?\n\n[No]\n\nSECTION: A.7.5Uses\n\nHas the dataset been used for any tasks already?\n\n[Yes]As part of the KALC and CALI projects, and for archaeological publications, but not in an open-access fashion.\n\nIs there a repository that links to any or all papers or systems that use the dataset?\n\n[Yes]Such a list will be made available on the website of the project.\n\nWhat (other) tasks could the dataset be used for?\n\nBeyond its use as a difficult semantic segmentation benchmark, the Archaeoscape data holds significant value for archaeologists of the Khmer cultural world who seek to employ machine learning models for feature annotations, and possibly for a wider archaeological audience as well.\n\nIs there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses?\n\nAs the localization of the acquisitions has been removed and obfuscated, the direct archaeological utility of the dataset in its present form isnecessarilylimited.\n\nBy choosing a subset of 23 non-overlapping parcels covering 888 km2, we have removed the spatial and cultural relations between them, which will be a concern for researchers seeking to incorporate that information.\n\nAre there tasks for which the dataset should not be used?\n\n[Yes]Attempting to perform registration and re-localization of the dataset is explicitly forbidden by the dataset license, as well as all commercial use.\n\nAny other comments?\n\n[No]\n\nSECTION: A.7.6Distribution\n\nWill the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created?\n\n[Yes]The dataset will be limited to users who agree to its licence and can provide access credentials, as part of the credentialized open access distribution policy.\n\nHow will the dataset be distributed (e.g., tarball on website, API, GitHub)?\n\nThe data will be available through a web-platform maintained by the EFEO.\n\nWhen will the dataset be distributed?\n\nThe dataset will be distributed upon acceptance of this paper, and will be made public at the camera-ready deadline at the latest.\n\nWill the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)?If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.\n\n[Yes]The dataset will be released under a custom license which forbids its distribution and attempts to localize the data.\n\nHave any third parties imposed IP-based or other restrictions on the data associated with the instances?\n\n[No]\n\nDo any export controls or other regulatory restrictions apply to the dataset or to individual instances?\n\n[No]\n\nAny other comments?\n\n[No]\n\nSECTION: A.7.7Maintenance\n\nWho will be supporting/hosting/maintaining the dataset?\n\nThe EFEO will support and host the dataset and its metadata.\n\nHow can the owner/curator/manager of the dataset be contacted (e.g., email address)?\n\narchaeoscape@efeo.net\n\nchristophe.pottier@efeo.net\n\nIs there an erratum?\n\n[No]There is no erratum for our initial release. Errata will be documented as future releases on the dataset website.\n\nWill the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?\n\nWe do not plan to update this dataset, but may release an expansion in the future.\n\nIf the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were individuals in question told that their data would be retained for a fixed period of time and then deleted)?\n\n[N/A]\n\nWill older versions of the dataset continue to be supported/hosted/maintained?\n\n[Yes]The EFEO is dedicated to providing ongoing support for the Archaeoscape dataset.\n\nIf others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?\n\n[No]Our license forbids third-party redistribution of any portion of the dataset.\n\nAny other comments?\n\n[No]", "text_file": "data\\paper_texts\\2412.05203v1_content.txt"}, {"title": "Modular Duality in Deep Learning", "authors": ["Jeremy Bernstein", "Laker Newhouse"], "published_date": "2024-10-28T17:57:31Z", "summary": "An old idea in optimization theory says that since the gradient is a dual\nvector it may not be subtracted from the weights without first being mapped to\nthe primal space where the weights reside. We take this idea seriously in this\npaper and construct such a duality map for general neural networks. Our map,\nwhich we call modular dualization, forms a unifying theoretical basis for\ntraining algorithms that are a) fast and b) scalable. Modular dualization\ninvolves first assigning operator norms to layers based on the semantics of\neach layer, and then using these layerwise norms to recursively induce a\nduality map on the weight space of the full neural architecture. We conclude by\nderiving GPU-friendly algorithms for dualizing Embed, Linear and Conv2D layers\n-- the latter two methods are based on a rectangular Newton-Schulz iteration\n(Kovarik, 1970; Bj\\\"orck & Bowie, 1971). A variant of our methods was used to\nset speed records for training NanoGPT. Overall, we hope that our theory of\nmodular duality will yield a next generation of fast and scalable optimizers\nfor general neural architectures.", "arxiv_id": "2410.21265v2", "html_link": "https://arxiv.org/html/2410.21265v2", "search_term": "ti:\"deep learning\"", "html_content": "SECTION: Modular Duality in Deep Learning\n\nAn old idea in optimization theory says that since the gradient is adual vectorit may not be subtracted from the weights without first being mapped to theprimal spacewhere the weights reside. We take this idea seriously in this paper and construct such a duality map for general neural networks. Our map, which we callmodular dualization, forms a unifying theoretical basis for training algorithms that are a)fastand b)scalable. Modular dualization involves first assigning operator norms to layers based on the semantics of each layer, and then using these layerwise norms to recursively induce a duality map on the weight space of the full neural architecture. We conclude by deriving GPU-friendly algorithms for dualizing,andlayers\u2014the latter two methods are based on a rectangular Newton-Schulz iteration(Kovarik,1970; Bj\u00f6rck & Bowie,1971). A variant of our methods was used to set speed records for training NanoGPT. Overall, we hope that our theory of modular duality will yield a next generation of fast and scalable optimizers for general neural architectures.\n\nSECTION: 1Introduction\n\nIn this paper, we pursue a rigorous and first-principles theoretical framework for designing neural network training algorithms. We hope that building such a framework will facilitate the design of a next generation of fast and scalable optimizers that are automatically tailored to different neural architectures.\n\nWhile gradient descent is the workhorse of modern machine learning, the most vanilla form of the algorithm does not, in our view, pass a basictype check. For a gradient update to type check, we insist that the gradient must be passed through a duality map before being multiplied by a learning rate and applied to the weights:\n\nWhy? The reason is that the loss function may not be equally smooth in all directions in weight space, and there is no reason for the sizes of different components of the raw gradient vector to respect this heterogeneity. In other words,the geometry of the loss function may be non-isotropic. Insisting on a type check should force the user to become cognizant of this issue and to find a suitable duality map. A good duality map should adjust the size and direction of the gradient to respect the smoothness structure of the loss function.\n\nDuality maps on vector spaces are commonplace in physics and applied math. Examples include themusical isomorphismin differential geometry(Grosse,2022),raising and lowering indicesin general relativity(Carroll,2019)and thebra-ket notationin quantum mechanics(Sakurai & Napolitano,2020). Duality maps are also central to several optimization theories includingmirror descent(Nemirovsky & Yudin,1983),natural gradient descent(Amari,2016)andsteepest descent on a normed space(Boyd & Vandenberghe,2004). Despite the efforts of some prescient papers(Carlson et\u00a0al.,2015b; Flynn,2017), the latter kind of duality map involving normed vector spaces is yet to puncture the deep learning mainstream.\n\nWe believe that duality is a key theoretical concept that will help in building performant large-scale machine learning systems. To support this belief, we show in this paper that two important and seemingly disparate methods in contemporary optimization research may be seen as approximations to a single duality map. These methods aremaximal update parameterization(Yang & Hu,2021), which is aimed at scalable training, andShampoo(Shi et\u00a0al.,2023), which is targeted at fast training. We show inSection4.1that both methods emerge as partial approximations to a single duality map induced by the RMS\u2013RMS operator norm.\n\nThe main contribution of this paper is to describe a procedure for constructing duality maps for general neural architectures. The procedure, which we callmodular dualization, works in three steps:\n\nOperator norms are assigned to individual layers based on the input-output semantics of each layer;\n\nBased on these operator norms, duality maps are constructed for individual layers;\n\nGiven the layerwise duality maps and the structure of the neural architecture, a single duality map is recursively induced on the full weight space of the architecture.\n\nTo instantiate this procedure for a rich family of neural architectures\u2014including convolutional networks and transformers\u2014we write down duality maps for,andlayers. We also provide GPU-friendly algorithms for computing these duality maps. Overall, we hope that modular dualization will help in the principled design of the machine learning systems of the future.\n\nSECTION: 2Related Work\n\nThis paper constructs a duality map for general neural architectures. Our approach is based on assigning operator norms to individual network layers and using these layerwise norms to recursively induce a duality map on the full neural architecture. The most closely related prior work is a series of papers onspectral descent(Carlson et\u00a0al.,2015a;b;2016)and a paper onduality structure gradient descent(Flynn,2017).\n\nSpectral descent has been applied to restricted Boltzmann machines(Carlson et\u00a0al.,2015a)and discrete graphical models(Carlson et\u00a0al.,2016), but let us focus on the more closely related paper on spectral descent for deep learning(Carlson et\u00a0al.,2015b). In that paper, the authors propose assigning the Schatten-norm (a.k.a.\u00a0spectral norm) to individual linear layers. This assignment is based on the observation that neural networks admit natural majorization bounds in the Schatten-norm. The authors call the corresponding duality map for linear layers the \u201c#-operator\u201d\u2014a name presumably inspired by the musical isomorphism(Grosse,2022). The authors propose a cheap approximation to the #-operator based on sketching(Martinsson & Tropp,2020), and they also propose a way to mix RMSprop-style pre-conditioning information(Tieleman & Hinton,2012)into the weight updates. In contrast to our work, the authors only derive duality maps for single linear layers, and these maps are then heuristically extended to all-layer updates. Nonetheless, the authors achieve substantial wall clock speedups using variants of spectral descent to train small networks.\n\nNow, let us turn our attention to duality structure gradient descent(Flynn,2017), which constructs a duality map on the full weight space of the neural architecture based on identifying aFinsler structure(Deimling,1985)inherent to neural networks. Similar to modular dualization,Flynn (2017)\u2019s duality map works by assigning duality maps to each layer and then inducing a duality map on the full weight space. The substantial difference to our approach is thatFlynn (2017)leverages a weighted sum (combination) of layerwise norms to construct his full duality map. This leads to optimization methods that only update a single layer at each iteration, and the methods need to be heuristically extended to achieve all-layer updates. In contrast, we leverage the modular norm(Large et\u00a0al.,2024), which takes a weighted max (combination) of layerwise norms. In turn, our duality map leads directly to more conventional\nall-layer optimizers.\n\nAnother important difference between our work on modular duality and prior work on duality structure gradient descent is that we fully \u201cmodularize\u201d our theory\u2014meaning that our construction is explicitly recursive\u2014and as such it is easy to code up into a software package. In this regard, we are inspired by a line of work that attempts to build optimization algorithms that automatically adapt to the structure of general computation graphs. The earliest work we know of in this category is the PhD thesis ofGrant (2004)on disciplined convex programming, which aims to infer the convexity properties of general functions by breaking them up into subexpressions and applying composition theorems from convex analysis. More recent progress in this vein includes work on universal majorization-minimization algorithms(Streeter & Dillon,2022; Streeter,2023)and related papers on automatic majorization(Tran et\u00a0al.,2015; Bernstein et\u00a0al.,2023).\n\nSECTION: 3Theoretical Preliminaries\n\nIn this section, we introduce duality maps, a means of constructing duality maps based on norms, and finally a norm called themodular normthat is well-suited to describe the geometry of general neural architectures.\n\nSECTION: 3.1Duality Maps\n\nGiven a vector space, we say that a functionis alinear functionalonifis linear. We define thedual spaceto be the set of linear functionals on the vector space. The dual space is itself a vector space provided that addition is defined pointwiseand scalar multiplication is defined pointwisefor any scalar. Byduality mapwe simply mean any function that sends members of the dual vector spaceto the primal vector space. The function need not be an involution.\n\nLetdenote the loss of a differentiable machine learning model with weight space. The Taylor expansion of the loss at weight settingis given by:\n\nObserve that, in the first-order term, the gradientis acting as a linear functional: it is pairing with the weight vectorin a linear way to produce a real number. As such, we shall say that the gradient belongs to the dual weight space:. We shall forbid ourselves from directly subtracting a member of the dual weight spacefrom the weight space. If we would like to conduct a gradient descent update, then we had better find a duality map to send the gradient back to the primal space.\n\nThis restriction may seem absurd! After all, here the weight spaceand its dualare both just. However, insisting upon this type check serves to remind us that the curvature of the loss function may be highly heterogeneous. The next section will show one way to construct duality maps to account for this.\n\nSECTION: 3.2Steepest Descent on a Normed Space\n\nSuppose that we have found anormand asharpness parameterthat serve as a good model of the higher-order terms in the Taylor expansion of the loss function given inEquation3:\n\nIn other words, the norm provides a good characterization of the heterogeneity in curvature of the loss function. Then it makes sense to solve for a weight updateby minimizing the right-hand side ofEquation4. We will show that the minimizer can be expressed in terms of adual normand aduality map:\n\nGiven a norm, the dual normof a vectoris given by:\n\nGiven a norm, we consider the duality map:\n\nwhere, if theis not unique,returns any maximizer.\n\nGiven these definitions, minimizing the expression in the right-hand side ofEquation4can be done using the following standard proposition, for whichBernstein & Newhouse (2024)provide a proof:\n\nFor anythought of as \u201cthe gradient\u201d, anythought of as \u201cthe sharpness\u201d, and any normwith dual normand duality map:\n\nIn words: to find the minimizer of a linear term penalized by a squared norm, we need only evaluate the dual norm and a duality map. In this paper, we focus on constructing a duality map for themodular norm, which is defined on general neural architectures. The next section reviews duality maps for more standard norms.\n\nSECTION: 3.3Basic Norms and Duality Maps\n\nMany basic norms and duality maps are already covered in prior work(Carlson et\u00a0al.,2016;2015a;2015b; Flynn,2017). For some warmup examples, the following duality maps for vector norms are standard:\n\nFor a vector, we have.\n\nFor a vector, we have, where the sign function is applied entrywise and we are free to take.\n\nIn neural networks, the weight spaces of individual layers tend to have matrix structure. And layers with the same shape weight matrix may have semantically different input and output spaces\u2014thinkembeddingversuslinearlayers in a transformer. As such, we will need duality maps for differentinduced operator norms:\n\nGiven a matrixand two normed vector spacesand, the \u201cto\u201d induced operator norm is given by:\n\nFor tensors, we define the duality map via. For linear layers, we will need the duality map for theinduced operator norm. This ends up as a rescaled version of the spectral norm duality map from prior work(Carlson et\u00a0al.,2015b; Flynn,2017).\n\nFor a vector, we define the RMS norm to be the normalized Euclidean norm:. Given a matrix, theinduced operator norm resolves to a rescaled spectral norm:, wheredenotes the standard spectral norm. For a matrixwith reduced singular value decomposition, the corresponding duality map is given by\n\nAnd for embedding layers, we will need the duality map for theoperator norm:\n\nGiven a matrix, theinduced operator norm resolves to the maxnorm of the columns:. For a matrix, the corresponding duality mapsimply normalizes each column ofto have unit RMS norm:for each.\n\nSECTION: 3.4The Modular Norm\n\nThemodular norm(Large et\u00a0al.,2024)is intended to help characterize the heterogeneous curvature of general neural architectures. The construction first defines an abstractmoduletype along with a notion of what is a good, orwell-normed, module. Thencombination rulesare given for constructing new well-normed modules from a library of existing well-normed modules. So modules are a special case ofcombinator patternfrom functional programming(Haskell Wiki Contributors,2007). Modules are also related to amonoidal categoryfrom category theory(Fong & Spivak,2019). We begin by defining the abstract notion of amodule:\n\nGiven input vector space, output vector spaceand weight vector space, a moduleis an object with the following four attributes:\n\na function,, which maps an input and a weight vector to an output;\n\na number,, which is used to set the proportion of feature learning that this module contributes to any supermodule;\n\na number,, which estimates the module\u2019s sensitivity to input perturbations;\n\na norm over the weight space,, sometimes abbreviated to just.\n\nWe shall care most about modules that arewell-normed, which amounts to requiring that the forward function is Lipschitz-continuous in the weights with constant 1 and in the inputs with constant:\n\nLetbe a module on, where the input and output spaces have respective normsand.is well-normed if for all inputsand weights:\n\nTheoperator denotes summation over any shared tensor indices. This definition of well-normed-ness can be used as a guiding principle in the design of a library of atomic (i.e. handwritten) modules. First, norms should be assigned to the input and output space of each module based on the semantics of. Then a normshould be assigned to the module\u2019s weight space and a numbershould be chosen to make the module well-normed. Examples are given inSection4.1.\n\nGiven such a library of well-normed atomic modules, a compound module built through any arbitrary sequence ofmodule compositionsandmodule concatenationsis automatically well-normed(Large et\u00a0al.,2024). And if the atomic modules in the library are not only well-normed but are alsosmoothin an appropriate sense, thenLarge et\u00a0al. (2024)give an automatic procedure for computingsharpness coefficientsfor any compound module built from the library. The relevant definition of module composition is as follows:\n\nConsider modulewith input, output and weight spaceand modulewith input, output and weight space.andare composable if. Their composite modulehas input, output and weight spaceand attributes:\n\n;\n\n;\n\n;\n\ngiven by:\n\nwhere iforis zero, the corresponding term in theis set to zero.\n\nSo the composite norm is taken to be a weighted max over the norms of the two sub-modules, where the weight space of the first module is coupled to the input sensitivity of the second module. The module masses provide freedom to tune the importance of each sub-module in the norm, andLarge et\u00a0al. (2024)prove that module mass provides precise control over the amount of feature learning that can happen in each sub-module.\n\nModule concatenation is defined in a similar way to module composition:\n\nConsider modulewith input, output and weight spaceand modulewith input, output and weight space. We say thatandare concatenatable if their input spaces match:. The tuple modulehas input, output and weight spaceand the following list of attributes:\n\n;\n\n;\n\n;\n\ngiven by:\n\nwhere iforis zero, the corresponding term in theis set to zero.\n\nA shortcoming of the paper byLarge et\u00a0al. (2024)is that the power of the modular norm is not fully leveraged. In particular, the authors domodular normalizationof training, where weight updates to modules are sometimes just na\u00efvely divided by their norm. In this paper we make fuller use of the geometry implied by the modular norm by constructing the corresponding duality map, which we callmodular dualization.\n\nSECTION: 4Modular Dualization\n\nIn this section, we construct a duality map for general neural architectures. Our strategy is to first write down duality maps for atomic modules, i.e.\u00a0individual layers. We then extend to arbitrary compound modules, i.e.\u00a0full neural networks, by showing how duality maps should pass through composition and concatenation.\n\nSECTION: 4.1Duality Maps for Atomic Modules\n\nTo construct a duality map for an atomic module, the idea is to first fix norms on the input and output spaces that respect the semantics of. We should select norms that describe both how large we would like the inputs and outputs to be, and in what geometry we would like the outputs to evolve. Then we place a norm on the weight space such thatis well-normed: this is typically the operator norm (Definition3) induced by the input and output norms. Finally we are in position to solve for the duality map, which we shall call. We now give some examples of this procedure for the basic layer types of,and. The results are summarized inTable1.\n\nWe start with the canonical example of an atomic module:\n\nThemodule sends inputs fromto outputs in. The weight space is given by the matrix space. We endow themodule with attributes:\n\n, the matrix-vector product;\n\n;\n\n, whereis a hyperparameter;\n\n, theinduced operator norm.\n\nSince themodule is intended to map to and from vectors of roughly unitnorm, we place thenorm on both the input and output space:and. Thenis well-normed if the inputs and weights belong to the unit ballsand. Referring back toSection3.3, the duality map corresponding tois then given by:\n\n, where the gradienthas reduced SVD.\n\nThis single duality map recovers essential features of bothmaximal update parameterization(Yang & Hu,2021,P)andShampoo(Gupta et\u00a0al.,2018). In particular, the factor ofinrecovers spectral update scaling(Yang et\u00a0al.,2023)that leads toP. (Initializing such thatalso recoversP initialization scaling.) And the mappingis equivalent to Shampoo without accumulation(Bernstein & Newhouse,2024). As such, we believe that duality maps may help reconcile different strands of deep learning research and provide a unifying basis for fast and scalable training algorithms.\n\nThemodule provides a useful counterpoint to themodule. The difference between the two modules stems from the fact that the input spaces ofandhave different semantics.\n\nThemodule sends inputs fromto outputs in. The weight space is given by the matrix space. We endow themodule with attributes:\n\n, the matrix-vector product;\n\n;\n\n, whereis a hyperparameter;\n\n, theinduced operator norm.\n\nis intended to map from one-hot vectors to vectors of roughly unitnorm, so we place thenorm on the input space and thenorm on the output space:and. Thenis well-normed if the inputs and weights belong to the unit ballsand. Referring back toSection3.3, the duality map foris:\n\nperforms the mappingfor each column index.\n\nFinally, we consider amodule with akernel.has a more involved tensor structure thanand. The calculations work by slicing up the weight tensor into a collection ofmatrices.\n\nThemodule sends inputs fromto outputs in. We think of this as mapping an input image of width, heightand withcolor channels to an output image of width, heightand withcolor channels. The weight space is given by the tensor space, whereis the kernel size. We endowwith attributes:\n\n, wheredenotes 2D convolution;\n\n;\n\n, whereis a hyperparameter;\n\n, the maxnorm over kernel indices.\n\nWe would like pixel intensities in the inputs and outputs to be order one and undergo order one change. We formalize this by taking the input and output norms to be the spatial maximum of the RMS norms of all the color channel vectors:and. Thenis well-normed if the inputs and weights belong to the unit ballsand. Since the duality map for a max of norms decouples into one duality map per sub-norm, the duality map corresponding tois given by:\n\ndoes, wherehas reduced SVD.\n\nSECTION: 4.2Duality Maps for Bond Modules\n\nLarge et\u00a0al. (2024)define another class of basic modules:bond modules. Bonds are handwritten modules without weights. An example of a bond is thenonlinearity. For a bond, the weight space is the zero vector spaceand the modular norm. As such, the corresponding duality map is also. In a software package, one need not write norms or duality maps for bond modules.\n\nSECTION: 4.3Duality Maps for Compound Modules\n\nFirst, given two composable modulesand, the duality map for the compositeis given by:\n\nAnd second, given two concatenatable modulesand, the duality map for the tupleis:\n\nThe proofs ofEquations11and12follow in a straightforward manner fromDefinitions6and7.\n\nSECTION: 5Fast Duality Maps\n\nFor modular dualization to be practically feasible, we need ways of computing duality maps quickly. Inspecting the duality maps listed inTable1, we see thatis easy to implement since it just involves computing vector norms of matrix columns. Butandinvolve the projection:\n\nwhereis the reduced SVD of the matrix. Since computing SVDs can be slow(Carlson et\u00a0al.,2015b; Flynn,2017), here we discuss three fast approximations toEquation13via sketching, iterations for inverse matrix roots, and a family ofrectangular Newton-Schulziterations. Which method works best in practice may depend on the condition number of the matrixor the available computational resources.\n\nSECTION: 5.1Sketching\n\nSketching is a randomized method(Martinsson & Tropp,2020)that can be used to build low-rank approximations to the SVD.Carlson et\u00a0al. (2015b)already used sketching to provide a fast approximation to their-operator. More recent papers have experimented with sketching in the context of Shampoo-type algorithms(Feinberg et\u00a0al.,2023). A potential downside of approximatingEquation13via sketching is that randomized SVD methods usually try to accurately approximate the largest singular values of a matrix(Martinsson & Tropp,2020, Section 11.2)while the value ofEquation13may lie in its action on the small singular values.\n\nSECTION: 5.2Iterations for Inverse Matrix Roots\n\nGiven a full rank matrixwith reduced SVD, we have that:\n\nThis provides a route to approximatingEquation13since one can compute inverse matrix roots such asvia Newton iteration(Laki\u0107,1998). This is discussed in Chapter 7 ofHigham (2008)\u2019s book and also seeAnil et\u00a0al. (2020)\u2019s paper. Care must be taken with inverses whenever the matrixis ill-conditioned.\n\nSECTION: 5.3Rectangular Newton-Schulz Iteration\n\nWe developed a \u201crectangular Newton-Schulz iteration\u201d for computingby adapting Equation 5.22 inHigham (2008)\u2019s book for computing the \u201cmatrix sign function\u201d. We later discovered that this iteration has a long history(Kovarik,1970; Bj\u00f6rck & Bowie,1971). In short, the method works by first normalizing the matrixaccording to(or alternatively) and then iterating:\n\nthen as, the sequence. To see this, one can plot the univariate cubic functionand see that, for, iterating this cubic will pushcloser and closer to. The final step is to realize that the effect of the iteration inEquation15is to apply this cubicto each singular value of. This shows that the spectral normalizationis stronger than what is required: we need only ensure thathas singular values no greater thanfor the iteration to converge.\n\nThe iteration inEquation15has the advantage over sketching that it always works on all singular values, and since the iteration does not compute inverse matrix roots it is well-behaved even on low-rank matrices.\n\nFinally, there are in fact a family of degreepolynomial iterations of the form\n\nfor suitablethat could be used instead ofEquation15. One should choose coefficientsso that the univariate polynomialis a suitable approximation to. One may try to further accelerate the iteration by \u201ctuning\u201d the coefficientsempirically.\n\nSECTION: 6Discussion\n\nThis paper develops the theory ofmodular dualityand the procedure ofmodular dualizationas means to construct duality maps for general neural architectures. Here, we comment on implications and connections.\n\nSECTION: 6.1A Type System for Deep Learning\n\nPart of the inspiration for this work is the idea of building a fully-fledgedtype systemfor deep learning. We think that activation spaces should be typed by their intended norm and the intended size of activations in that norm. This information would help in the construction of well-normed modules (seeSection4.1). Modules should be typed according toDefinition4. And, as suggested in the introduction, gradients should be explicitly typed as dual vectors. A duality map should flip the type of a dual vector to a primal vector. We plan to use the Modula deep learning package(Large et\u00a0al.,2024)as a testbed for these ideas.\n\nSECTION: 6.2Neural Network Speedrunning\n\nWe believe that the ideas in this paper can help in the design of faster training methods. In fact, a new NanoGPT training speed record was recently set(Jordan,2024)using a Newton-Schulz-based duality map. We communicated the method to Keller Jordan through our workshop paper(Bernstein & Newhouse,2024).\n\nSECTION: 6.3Modular Duality: A Unifying Theoretical Framework for Fast and Scalable Training\n\nAn important topic in contemporary optimization research is the design of fast and scalable training methods for neural networks. In fact, the theme of the Optimization for Machine Learning workshop at this year\u2019s NeurIPS conference is \u201cscaling up optimization\u201d(OPT,2024). Two popular methods in this research space aremaximal update parameterization(Yang & Hu,2021,P), which allows for increasing network width without changing the optimal learning rate, andShampoo(Gupta et\u00a0al.,2018), a variant of which(Shi et\u00a0al.,2023)won a speed challenge at the inaugural AlgoPerf optimization competition(Dahl et\u00a0al.,2023).\n\nWe showed inSection4.1that essential features of bothP and Shampoo are recovered from the single duality map. We think that, on a basic theoretical level,P and Shampoo should be viewed as partial approximations to this duality map. This observation helps putP and Shampoo on a consistent theoretical footing, orients the methods with respect to overlooked prior work on spectral descent(Carlson et\u00a0al.,2015b)and duality structure gradient descent(Flynn,2017), and suggests new ways to generalize these methods to arbitrary layer types and network architectures via the modular norm and modular dualization.\n\nSECTION: 6.4On the Alignment of Activations and Updates\n\nRecent work(Yang et\u00a0al.,2023; Everett et\u00a0al.,2024; Large et\u00a0al.,2024)has singled out the following question as important to the design of scalable deep learning systems:to what extent do gradient updates to neural network layers align with incoming activation vectors?This question is important since it helps inform how large weight updates need to be to induce a certain amount of change in layer outputs. Duality maps such asandmay help simplify the answer to this question, since they project gradients to scaled semi-orthogonal matrices for which all singular values have the same magnitude.\n\nSECTION: 6.5A Numerical Paradox:The Weights Don\u2019t Change!\n\nPast work(Lee et\u00a0al.,2019; Jesus et\u00a0al.,2021)has pointed out an apparent paradox in deep learning: the weights seem to move a vanishing amount from initialization in the limit of large network width. This finding has motivated a substantial amount of work on linearized training dynamics(Jacot et\u00a0al.,2018). We attempted to resolve this paradox in prior work by showing that the weights move a roughly constant amount at any width when the change is measured in spectral norm(Yang et\u00a0al.,2023). But duality maps change the story again:ramps up the stable rank of updates, so the weights should move a non-trivial relative amount at large widtheven in the Frobenius norm\u2014provided the batch size is not too small.\n\nSECTION: 7Conclusion\n\nThis paper has proposed a recursive procedure calledmodular dualizationfor building duality maps for general neural architectures. The procedure unifies past strands of optimization research on Shampoo(Gupta et\u00a0al.,2018)andP(Yang & Hu,2021). Partial implementations have already led to significant wall-clock speedups in transformer training(Jordan,2024). The rectangular Newton-Schulz iteration provides a GPU-friendly and numerically stable means of dualizing under theoperator norm, while avoiding some of the downsides of sketching-based approaches(Carlson et\u00a0al.,2015b). Overall, we hope that our theory ofmodular dualityprovides a clarifying toolkit for the design and analysis of deep learning systems.\n\nSECTION: Acknowledgements\n\nMany ideas in this paper were developed jointly with Tim Large before he left to work at a tech company.\nWe are grateful to Phillip Isola for invaluable discussions. We also thank Jack Gallagher, Keller Jordan, Simo Ryu, Rogier Brussee, Tongzhou Wang, Victor Butoi, Jeffrey Cider and Volkan Cevher for helpful conversations.\n\nSECTION: References", "text_file": "data\\paper_texts\\2410.21265v2_content.txt"}, {"title": "Comprehensive Analysis and Improvements in Pansharpening Using Deep\n  Learning", "authors": ["Mahek Kantharia", "Neeraj Badal", "Zankhana Shah"], "published_date": "2024-12-06T09:55:37Z", "summary": "Pansharpening is a crucial task in remote sensing, enabling the generation of\nhigh-resolution multispectral images by fusing low-resolution multispectral\ndata with high-resolution panchromatic images. This paper provides a\ncomprehensive analysis of traditional and deep learning-based pansharpening\nmethods. While state-of-the-art deep learning methods have significantly\nimproved image quality, issues like spectral distortions persist. To address\nthis, we propose enhancements to the PSGAN framework by introducing novel\nregularization techniques for the generator loss function. Experimental results\non images from the Worldview-3 dataset demonstrate that the proposed\nmodifications improve spectral fidelity and achieve superior performance across\nmultiple quantitative metrics while delivering visually superior results.", "arxiv_id": "2412.04896v1", "html_link": "https://arxiv.org/html/2412.04896v1", "search_term": "ti:\"deep learning\"", "html_content": "SECTION: Comprehensive Analysis and Improvements in Pansharpening Using Deep Learning\n\nPansharpening is a crucial task in remote sensing, enabling the generation of high-resolution multispectral images by fusing low-resolution multispectral data with high-resolution panchromatic images. This paper provides a comprehensive analysis of traditional and deep learning-based pansharpening methods. While state-of-the-art deep learning methods have significantly improved image quality, issues like spectral distortions persist. To address this, we propose enhancements to the PSGAN framework by introducing novel regularization techniques for the generator loss function. Experimental results on images from the Worldview-3 dataset demonstrate that the proposed modifications improve spectral fidelity and achieve superior performance across multiple quantitative metrics while delivering visually superior results.\n\nSECTION: IIntroduction\n\nOwing to payload, cost and data constraints, remote sensing satellites are unable to capture images having both high spatial and spectral resolution.Therefore satellites provide two types of images a high spatial resolution panchromatic image and a high spectral-resolution multispectral image. Both these images are taken by sensors looking over the same area at the same angle and hence requires no registration task. Pansharpening or Panchromatic sharpening aims to combine these two images to achieve higher spatial resolution while preserving specific spectral attributes.\n\nOver the years, many algorithms have addressed this task. These techniques are mailnly divided into component substitution, multiresolution analysis, sparse representation and variational approaches. While these traditional techniques are relatively easy, they face several difficulties such as spectral distortions, spatial detail injection limitations, and limitations based on theoretical assumptions. Performance of these methods is also variable across different sensor data, and land cover characteristics.\n\nWith recent advances made by deep neural networks for\nimage processing applications, researchers have also explored the avenue for pan-sharpening. Deep learning has become a popular technique for image processing tasks, including pansharpening, due to its ability to learn complex nonlinear mappings between input and output images. In the context of pansharpening, deep learning can be used to learn the complex relationships between low-resolution multispectral images and high-resolution panchromatic images. Unlike traditional methods that rely on handcrafted features and linear models, deep learning can automatically learn the most discriminative features from the input data, and use them to generate high-quality output images.\n\nMany researchers have used deep-learning for pansharpening. Inspired by image super-resolution[1], Masi et al.[2]constructed a three-layer convolutional neural network for pan-sharpening.[3]trains ResNet architecture in high-pass domain and adds MS input to it\u2019s output.[4]uses a Conditional GAN to generate pansharpened image. Even though these methods are a great improvement over traditional methods they still show spectral distortions. In this paper, we propose a perceptual loss function that reduces this spectral distortion. We hypothesize that by minimizing the distance between the pansharpened and original MS in the high-level feature subspace, a more spectrally accurate image can be generated. To do this we calculate the gram matrix of the output of the bottom-most layer of the generator from PSGAN and use L1 loss between the two as an enriching term in the generator loss function.\n\nThis paper is organized as follows. In section II various traditional as well as deep-learning based pansharpening methods are briefly reviewed. Section III presents the methodology used to improve pan-sharpening. Experimental results and comparisons are provided in Section IV. Conclusions are drawn in Section V.\n\nSECTION: IIRelated Work\n\nThis section briefly reviews Component substitution, Multiresolution analysis and Deeplearning based pansharpening methods\n\nSECTION: II-AComponent Substitution\n\nComponent Substitution methods are based on projection of the multispectral images into a space, where the spatial and spectral components can be separated. The spatial component is then replaced by the panchromatic image. Since greater correlation between the replaced component and the panchromatic image leads to lesser distortion, the panchromatic image is histogram matched with the replaced component. The process is completed by doing inverse transformation to bring back the multispectral image to the original space.\n\nSome of the techniques belonging to this class are IHS, GIHS, Brovey transform, PCA, Gram-Schmidt analysis.\n\nIHS colour model is used as it separates the spatial information (intensity component) from the spectral information (hue and saturation). Hence, it is possible to manipulate the spatial information while keeping the same spectral information.\n\nThe hue component describes the color in the form of an angle between 0 to 360 degrees. It is determined by the relative proportions of red, green and blue The saturation describes the purity of the color, how much color is dilated with white light and its value ranges between 0 and 1. The IHS solid adds the dimension of intensity with black at the bottom and white at the top. Shades of gray run along the axis of the solid.\n\nIHS yield adequate spatial enhancement but introduces spectral distortion. In[7], it is demonstrated that saturation component changes after changes in the intensity value. The product of intensity and saturation is a constant value. Hence, saturation and intensity are inversely proportional to each other. The new saturation value is expanded if Pan value is less than internsity,Ivalue and it is compressed when Pan value is greater than theIvalue. Studying the relative spectral response, the authors find that the RGB bands do not fall within the same range of the panchromatic band.\n\nFurthermore, the response of the Pan is expanded beyond the NIR band. Since the spectral response of Pan andIare bound to change, to reduce the colour distortion, they introduce a generalized IHS method that responds to the NIR band.\n\nBrovey transform[8]normalizes the spectral bands before they are multiplied with the panchromatic image. It retains the corresponding spectral features of each pixel and transforms the luminance information into a panchromatic image which then gets replaced by the histogram high resolution panchromatic image.\n\nHowever, the Brovey transform method assumes that the spectral response of the PAN image represents the overall spectral content of the MS image. However, this assumption may not hold true in all cases, leading to spectral distortion in the sharpened image.\n\nThe MS image is taken as an input to principle component analysis procedure. The first principle component represents the maximum variance direction of the data. The Pan data is histogram matched with the first principal component. The results of which are used to replace the first principle component and the data is retransformed back to its original space.\nThe justification used to replace the first component is that the first principle component will have information which is common to all bands which is the spatial information while spectral information unique to any of the bands is mapped to the other components. However some of the spatial information may not be mapped to the first component, depending on the degree of correlation and spectral contrast existing among the MS bands[8].\n\nThe Gram-Schmidt (GS) algorithm is commonly used in remote sensing to orthogonalize matrix data or bands of a digital image. This process removes redundant or correlated information that is contained in multiple bands, resulting in a more accurate outcome.\n\nThe multispectral (MS) bands are resampled or interpolated to the same scale as the panchromatic (PAN) band.A lower-resolution panchromatic (PAN) band is simulated and used as the first band of the input to the GS transformation. Then each MS band vector is projected onto the (hyper)plane established by the previously determined orthogonal vectors. Pansharpening is accomplished by replacing the first vector with the histogram-matched PAN before the inverse transformation is performed.\nTwo methods are used for creating a low resolution PAN[8]. In the first method the LRMS bands are combined into a single lower-resolution PAN (LR PAN) as the weighted mean of MS image. The second method simulates the LR PAN image by blurring and subsampling the observed PAN image. However, the first method suffers from spectral distortion and the second method sufferes from low sharpeness. In order to avoid this drawback an enhanced GS method is used, where the LR PAN is generated by a weighted average of the MS bands and the weights are estimated to minimize the MMSE with the downsampled PAN.\n\nGS is a generalization of PCA in which the first principle component can be choosen and the other components are made to be orthagonal to one another and the first component.\n\nGSA assumes that the orthogonalized multispectral bands preserve the original spectral information. However, the orthogonalization process can lead to spectral distortion in the sharpened image. This distortion may result in colour shifts, an inaccurate representation of the original spectral content, or the introduction of artificial spectral artefacts. Also, GSA prioritises the enhancement of spatial resolution by utilising the panchromatic image. However, this can come at the expense of spectral resolution, potentially leading to a loss of fine spectral details in the sharpened image.\n\nSECTION: II-BMRA\n\nAnother class of methods are Multiresolution analysis based methods that aim to extract the spatial information (high-frequency detail) from the PAN image by wavelet transform, Laplacian pyramid, etc., in the first step and then inject it to the up-sampled MS images to generate the fused image\n\nA high pass filter is used to obtain high-pass information from the Pan image. The HPF results are added pixel by pixel to the lower spatial resolution MS image\n\nSECTION: II-CDeeplearning based methods\n\nWhile these traditional techniques are relatively easy, they face several difficulties such as spectral distortions, spatial detail injection limitations, and limitations based on theoretical assumptions. Performance of these methods is also variable across different sensor data, and land cover characteristics.\n\nWith developments in machine learning (ML) and deep learning (DL) in the last decades, these technologies started to be widely used in image processing, such as image classification, image segmentation, object\ndetection super-resolution, pan-sharpening and reconstruction.\n\nResearchers in[2]have build upon architecture proposed in[1]for super-resolution problem and converted it to solve the pansharpening problem by leveraging to it the huge domain-specific knowledge available.\nThe 4-band Multispectral components are upsampled and interpolated and are stacked with the panchromatic band to form the 5-component input.\nIn addition to this, the authors add more planes corresponding to some well-known radiometric indices.\n\nMean Square error between the pansharpened image and its reference is used as the loss function\n\nwhereis the reference image,is the pansharpened image andis the number of batch size\n\nThe design of PNN is relatively simple and needed to be improved. But deep neural networks are difficult to optimize.[9]demonstrated the same problem and devised a clever solution that allowed the layers to learn residual functions with respect to the layer inputs instead of learning the unreferenced functions from scratch. This allowed training over 2000 layers with increasing accuracy.\n\nThis approch was implemented in the task of Pansharpening by the authors of[3]and is called PanNet. The researchers trained the ResNet in the high-pass domain to preserve spatial features and simply added the upsampled MS input to the model output to preserve spectral features.\n\nPanNet uses the same loss function as PNN.\n\nBoth the approaches above used Euclidean distance between the predicted and reference image as a loss function which would cause blurring effects.\n\nAnother important breakthrough in the DL field is Generative Adversarial Networks[10]where a generative model tries to generate an image like the real image and is pitted against an adversary: a discriminative model that learns to determine whether an image is generated or real. This later lead to[11]where Conditional GANs have been used to for image to image translation.\n\nIn[4]a Generative Adversarial Network(PSGAN) was first applied for pansharpening. This network consisted of a two-stream input generator inspired by the U-NET[12]architecture and a fully convolutional discriminator similar to[11]. The work also demonstrated that l1 loss produced better results than l2 loss.\n\nThe loss function of the generator and the discriminator are:\n\nwhereis the LRMS image,is the HRPAN image,is the HRMS image and G and D are the Generator and Discriminator respectively.\n\nSECTION: IIIProposed methodology\n\nOur method builds upon PSGAN. Apart from the L1 loss betweenandwe propose three new loss functions:\n\nSECTION: III-ALoss function based on SAM\n\nIn order to reduce the spectral distortions seen in PSGAN method, we devised a loss function like SAM (Spectral Angular Mapper)[13]and used it as a regularizing term is the generator loss function.\n\nwhereis output of.\n\nSECTION: III-BSam loss on both resolutions\n\nWe applied the above loss function on both reduced and original resolutions and used it as a regularizer in the generator loss function.\n\nwhereis obtained by downsamplingby r.\n\nSECTION: III-CPerceptual loss\n\nWe created a perceptual loss function that reduces this spectral distortion. We hypothesize that by minimizing the distance between the pansharpened and original MS in the high-level feature subspace, a more spectrally accurate image can be generated.\n\nIn order to generate the high-level feature subspace, we take a network with same architecture as the generator and add dropout layers to add noise. We train this network on MS images and take the L2 norm between the generated and input as loss function.\n\nWe take L2 norm between these two high-level feature subspaces taken from the bottleneck layer of the pretrained U-NET as a regularizer to the generator loss function.\n\nSECTION: III-DGram matrix based perceptual loss\n\nInstead of directly taking L2 norm like in the previous method, we calculate the gram matrix of the high level features of the generated and the original image and the euclidean distance between them is minimized.\n\nSECTION: III-EGram matrix based reconstruction loss\n\nWe create a regularizer which is based on minimizing the distance between gram matrix of the reference and pansharpened patches. We use this loss along with generator loss and give both the same weightage\n\nIn all of the above cases we use the loss functions as regularizers and the final loss function of the generator becomes:\n\nIn order to calculatewe employ hyperparameter tuning using gradient descent.\n\nSECTION: IVExperiments\n\nSECTION: IV-ADataset and Performance measure\n\nWe train and test our network on dataset acquired from Worldview-3 satellite. The spatial resolution is 0.34m for PAN and 1.38m for MS. The dataset consists of images taken over three cities: Paris, Vegas and Shanghai. Wald\u2019s protocol[14]is followed to downsample both the MS and PAN by a factor of 4 so that the resulting pansharpened image can be compared with the original MS image. Anti-aliasing is used for downsampling as it blurs the patches before the downsampling process. Hence, there is no need to apply a smoothing kernel. The training dataset consists of patches of size 256x256 from the datasets of Paris and Vegas. While the patches from Shanghai are used for testing.\n\nPopular quantitative Evaluation metrics used are:\nSAM: Spectral Angle Mapper[13], ERGAS: Global adimens, relative synthasis error[15]Q4: 4 band average universal image quality index[] and[16]SSIM: Structural Similarity index measure[17]\n\nSECTION: VConclusion\n\nThis work introduces several new regularization techniques for the generator loss function in PSGAN. Experimental results demonstrate that the Gram matrix-based reconstruction loss significantly enhances overall performance across most metrics, with the exception of SAM, where the SAM-based loss achieves the best improvement. On smaller datasets, the perceptual loss function shows notable enhancements compared to PSGAN, although the Gram matrix-based perceptual loss leads to a slight performance degradation.\n\nSECTION: Acknowledgment\n\nThe authors would like to thank everyone at the IAQD department of SAC, ISRO for their kind cooperation and encouragement. We would also like to thank the department for providing the resources. These resources have played a significant role in enabling us to conduct research, access relevant literature, and acquire necessary datasets for experimentation and analysis.\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.04896v1_content.txt"}, {"title": "Rethink Deep Learning with Invariance in Data Representation", "authors": ["Shuren Qi", "Fei Wang", "Tieyong Zeng", "Fenglei Fan"], "published_date": "2024-12-06T08:52:26Z", "summary": "Integrating invariance into data representations is a principled design in\nintelligent systems and web applications. Representations play a fundamental\nrole, where systems and applications are both built on meaningful\nrepresentations of digital inputs (rather than the raw data). In fact, the\nproper design/learning of such representations relies on priors w.r.t. the task\nof interest. Here, the concept of symmetry from the Erlangen Program may be the\nmost fruitful prior -- informally, a symmetry of a system is a transformation\nthat leaves a certain property of the system invariant. Symmetry priors are\nubiquitous, e.g., translation as a symmetry of the object classification, where\nobject category is invariant under translation. The quest for invariance is as\nold as pattern recognition and data mining itself. Invariant design has been\nthe cornerstone of various representations in the era before deep learning,\nsuch as the SIFT. As we enter the early era of deep learning, the invariance\nprinciple is largely ignored and replaced by a data-driven paradigm, such as\nthe CNN. However, this neglect did not last long before they encountered\nbottlenecks regarding robustness, interpretability, efficiency, and so on. The\ninvariance principle has returned in the era of rethinking deep learning,\nforming a new field known as Geometric Deep Learning (GDL). In this tutorial,\nwe will give a historical perspective of the invariance in data\nrepresentations. More importantly, we will identify those research dilemmas,\npromising works, future directions, and web applications.", "arxiv_id": "2412.04858v1", "html_link": "https://arxiv.org/html/2412.04858v1", "search_term": "ti:\"deep learning\"", "html_content": "SECTION: Rethink Deep Learning with Invariance in Data Representation(Tutorial Proposal)\n\nIntegratinginvarianceinto datarepresentationsis a principled design in intelligent systems and web applications. Representations play a fundamental role, where systems and applications are both built on meaningful representations of digital inputs (rather than the raw data). In fact, the proper design/learning of such representations relies on priors w.r.t. the task of interest. Here, the concept ofsymmetryfrom theErlangen Programmay be the most fruitful prior \u2014 informally, a symmetry of a system is a transformation that leaves a certain property of the system invariant. Symmetry priors are ubiquitous, e.g., translation as a symmetry of the object classification, where object category is invariant under translation.\n\nThe quest for invariance is as old as pattern recognition and data mining itself. Invariant design has been the cornerstone of various representations inthe era before deep learning, such as the SIFT. As we enterthe early era of deep learning, the invariance principle is largely ignored and replaced by a data-driven paradigm, such as the CNN. However, this neglect did not last long before they encountered bottlenecks regarding robustness, interpretability, efficiency, and so on. The invariance principle has returned inthe era of rethinking deep learning, forming a new field known as Geometric Deep Learning (GDL).\n\nIn this tutorial, we will give ahistorical perspectiveof the invariance in data representations. More importantly, we will identify those research dilemmas, promising works, future directions, and web applications.\n\nSECTION: 1.Topic and Relevance\n\nThe topic of this tutorial is a historical review of the invariance in data representations. The scope of this tutorial covers 1) the invariance in the era before deep learning, on old-fashioned invariant designs from various hand-crafted representations; 2) the invariance in the early era of deep learning, on the slump of the invariance principle and the success of the data-driven paradigm; 3) the invariance in the era of rethinking deep learning, on the revival of the invariance principle and the emergence of geometric deep learning as a way to bridge the research gap. For the depth within each era, the research dilemmas, promising works, future directions, and web applications will be sorted out.More details are expanded inSection 2.\n\nThe presenters are qualified for a high-quality introduction to the topic. We have extensive research experience and strong publication records in representation backbones and downstream applications of pattern recognition and data mining.More details are expanded inSection 3.\n\nThis tutorial is timely, due to the general limitations of today\u2019s intelligent systems and their web applications with respect to being only data-driven. Also, the invariance perspective (technology focus) and the historical perspective (broad horizons) are rarely seen in the tutorial tracks of related conferences.\n\nThis tutorial is relevant to the Web Conference. From a technological perspective, representations play a fundamental role in intelligent systems and their wide range of downstream web applications. From a practical perspective, the currently popular data-driven paradigm has led to bottlenecks in intelligent systems and their web applications, regarding robustness, interpretability, efficiency, and so on. Understanding invariance in data representations is helpful in facilitating better web applications.\n\nSECTION: 2.Content\n\nOver the past decade, deep learning representations, e.g., convolutional neural networks (CNN) and transformer, have led to breakthrough results in numerous artificial intelligence (AI) tasks, e.g., processing human perceptual information, playing board games, and generating realistic media. Without exception, these successful programs are consistent with the principle of empirical risk minimization and rarely involve other realistic factors. More recently, their applications are expanding to more real-world scenarios, e.g., medical diagnostics, self-driving cars, online services, and web platforms. In such scenarios, the robustness, interpretability, and efficiency of AI systems are crucial: 1) robustness means the performance of system is stable for intra-class variations on the input; 2) interpretability means the behavior of system can be understood or predicted by humans; 3) efficiency means the real-time availability and energy cost during human-computer interaction.\n\nIntegrating invariant structures into representations is a principled design towards robust, interpretable, and efficient AI systems(Bronstein et\u00a0al.,2017). Specifically, representations play a fundamental role, where the system is generally built on meaningful representations of digital inputs (rather than the raw data)(Bengio et\u00a0al.,2013). Note that the proper design/learning of such representations in fact relies on priors w.r.t. the task of interest. Here, the concept of symmetry from theErlangen Program(Klein,1893)may be the most fruitful prior \u2014 informally, a symmetry of a system is a transformation that leaves a certain property of system invariant(Weyl,2015). Symmetry priors are ubiquitous, e.g., translation as a symmetry of the object classification where object category is invariant under translation(Fukushima and Miyake,1982).\n\nWe focus on historical perspectives on the invariance or symmetry in the development of data representations(Balntas et\u00a0al.,2019).\n\nGoing back to the era before deep learning, symmetry priors (e.g., invariance and equivariance) w.r.t. geometric transformations (e.g., translation, rotation, and scaling) have been recognized as main ideas in designing representations(Lowe,2004). However, these hand-crafted representations are all fixed in design, relying on (under)-complete dictionaries, and therefore fail to provide sufficient discriminability at larger scales, e.g., ImageNet classification task(Russakovsky et\u00a0al.,2015).\n\nAs we enter the early era of deep learning, a cascade of learnable nonlinear transformations achieves over-complete representations of strong discriminative power for larger-scale pattern recognition and data mining tasks. As a textbook view now, representations should be learned not designed(LeCun et\u00a0al.,2015). Therefore, early learning representations are equipped with very few symmetry priors, typically just translation equivariance. Hence, these representations lack robustness, interpretability, and efficiency guarantees(Garrido et\u00a0al.,2024), e.g., the presence and understanding of adversarial perturbations(Buckner,2020). Note that the compatibility between invariance and discriminability has emerged as a tricky problem when moving towards real-world AI(Taddeo et\u00a0al.,2019).\n\nNow in the era of rethinking deep learning, the invariance principle has returned, forming a new field known as Geometric Deep Learning (GDL) \u2013 endowing a pattern recognition and data mining system with the basic symmetry structure of the physical world, and harmonizing knowledge-driven invariant representations and data-driven deep representations(Bronstein et\u00a0al.,2017). The GDL research extends the scope of previous invariant theories from geometric, algebraic, and group, while showing the potential for uniformly improving the robustness, interpretability, and efficiency of the deep representation techniques(Wang et\u00a0al.,2023).\n\nDespite the above GDL ideals, the current community of invariance faces the following research challenges:\n\nAt the theoretical level, classical invariance is based on certain global assumptions. As for more informative local and hierarchical invariants in pattern recognition and data mining (i.e., going partial and deep), there is a challenge on corresponding theoretical expansion.\n\nAt the practical level, classical invariance is often used in certain low-level pattern recognition and data mining tasks. As for higher-level tasks with symmetry prior, there is a challenge on corresponding practical expansion.\n\nWe can respond to the above challenges with a long-term research. One idea is to extend the successful structure of modern deep learning to knowledge approach, exploring the discriminative potential of hand-crafted representations. Another idea is to embed the experience of invariant theory into modern learning representations, replacing the black-box nature of typical learning with controllability. Such theoretical efforts at the representation level are expected to have a wide range of web-related applications.\n\nPattern recognition and data mining on graphs. A uniqueness of geometric deep learning is the applicability to data types, covering typical grids and surfaces, as well as generalized sets and graphs. A good representation on sets or graphs implies, in fact, a full exploitation of their complex geometric priors. Due to the nodes and connectivity properties that are natural to the web, a range of web problems relies on invariant representations of sets or graphs.\n\nRecommender systems and social networks. Recommender systems are typically implemented by data mining on data of user-user graph (social networks), user-item graph (interactions), and item-item graph. Invariance is an important prior to achieve compact representations of graph data.\n\nCybersecurity and information forensics. The automated generation and online distribution of misinformation is a real threat. Unlike typical tasks, cybersecurity tasks have a distinctly adversarial nature: the adversary actively creates obstacles to forensics, i.e., escape attacks. Invariance is an important prior to achieve robust and interpretable representations against escape attacks.\n\nEfficient web services. As deep learning models are used more on the server side, high inference costs have become a significant barrier for network service providers. Invariance is an important prior to avoid over-parameterization in the representation model on the server side.\n\nNon-learning end-side deployments. Even on the end side where computational resources are limited, sometimes expensive deep models are unavoidable due to the domain adaptive capabilities of end-to-end learning. Invariance is an important prior to achieve non-learned but task-adaptive representations, by compressing discriminative-irrelevant feature spaces.\n\nPhysical consistency enhancement for large generative models. Automatic content generation is currently a popular web application due to the fact that large models are usually only suitable to be placed on the server side. It has been known that large generative models are inefficient and unprotected in fitting physical structures. Invariance is an important prior to constrain large model representations to conform to physical and geometric patterns.\n\nSECTION: 3.Organizers\n\nSECTION: Shuren Qi\n\nDr. Shuren Qi is currently a Postdoctoral Fellow with Department of Mathematics, The Chinese University of Hong Kong. His research focuses on robust and explainable representations in Geometric Deep Learning, with applications in Trustworthy AI and Science AI. He has authored 14 papers in top-tier venues such as ACM Computing Surveys and IEEE Transactions on Pattern Analysis and Machine Intelligence. His works offer new designs of invariant representations \u2014 from global to local and hierarchical assumptions. The following research records by Shuren Qi are directly relevant to the tutorial topic:\n\nS. Qi, Y. Zhang, C. Wang, J. Zhou, X. Cao. A survey of orthogonal moments for image representation: Theory, implementation, and evaluation. ACM Computing Surveys 55 (1), 1-35, 2023.\n\nS. Qi, Y. Zhang, C. Wang, J. Zhou, X. Cao. A principled design of image representation: Towards forensic tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence 45 (5),\n5337 - 5354, 2022.\n\nS. Qi, Y. Zhang, C. Wang, T. Xiang, X. Cao, Y. Xiang. Representing noisy image without denoising. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024, in press.\n\nS. Qi, Y. Zhang, C. Wang, Z. Xia, X. Cao, J Weng, F. Fan. Hierarchical invariance for robust and interpretable vision tasks at larger scales, arXiv preprint (under review), 2024.\n\nC. Wang, S. Qi*, Z. Huang, Y. Zhang, R. Lan, X. Cao, F. Fan. Spatial-frequency discriminability for revealing adversarial perturbations. IEEE Transactions on Circuits and Systems for Video Technology, 2024, in press.\n\nPresentations\n\nThe Invariant Representation Towards Trustworthy Artificial Intelligence, invited talks at The Chinese University of Hong Kong and Harbin Institute of Technology.\n\nSECTION: Fei Wang\n\nDr. Fei Wang is currently an Associate Professor of Health Informatics in Department of Population Health Sciences, Weill Cornell Medicine, Cornell University. His major research interest is data mining and its applications in health data science. He has published more than 250 papers in AI and medicine, which have received more than 21.3K citations (GoogleScholar:link). His H-index is 70. His papers have won 8 best paper awards at top international conferences on data mining and medical informatics. His team won the championship of the NIPS/Kaggle Challenge on Classification of Clinically Actionable Genetic Mutations in 2017 and Parkinson\u2019s Progression Markers\u2019 Initiative data challenge organized by Michael J. Fox Foundation in 2016. Dr. Wang is the recipient of the NSF CAREER Award in 2018, the inaugural research leadership award in IEEE International Conference on Health Informatics (ICHI) 2019. Dr. Wang is the chair of the Knowledge Discovery and Data Mining working group in American Medical Informatics Association (AMIA).\n\nThe following research records by Dr. Fei Wang are directly relevant to the tutorial topic:\n\nF. Wang, R. Kaushal, and D. Khullar. Should health care demand interpretable artificial intelligence or accept \u201cblack box\u201d medicine? Annals of Internal Medicine 172, 59-60, 2020.\n\nF. Wang, L. P. Casalino, and D. Khullar. Deep learning in medicine-promise, progress, and challenges. JAMA internal medicine 179(3), 293-294, 2019.\n\nC. Zang, F. Wang. MoFlow: An invertible flow model for generating molecular graphs. KDD 2020.\n\nC. Zang, F. Wang. Differential deep learning on graphs and its applications. KDD 2020.\n\nM. Sun, S. Zhao, C. Gilvary, O. Elemento, J. Zhou and F. Wang. Graph convolutional networks for computational drug development and discovery. Briefings in bioinformatics, 21(3), 919-935. 2020.\n\nTutorials\n\nAI in Precision Medicine: Towards Knowledge Empowered Intelligence over \u201cSmall\u201d Data. Tutorial at 34th AAAI. 2020. (4 hours)\n\nRecent Advances in Graph Analytics and Its Applications in Healthcare. Tutorial at the 26th KDD. 2020 (4 hours, with Peng Cui, Jian Pei, Yangqiu Song, Chengxi Zang)\n\nDifferential Deep Learning on Graphs and its Applications. Tutorial at 34th AAAI. 2020. (4 hours, with Chengxi Zang)\n\nLearning From Networks: Algorithms, Theory, & Applications. Tutorial at the 25th KDD. 2019 (7 hours, with Xiao Huang, Peng Cui, Yuxiao Dong, Jundong Li, Huan Liu, Jian Pei, Le Song, Jie Tang, Hongxia Yang, Wenwu Zhu)\n\nData Analytics with Electronic Health Records. Tutorial at 29th AAAI. 2015. (4 hours)\n\nData Analytics in Healthcare: Problems, Solutions and Challenges. Tutorial at 23rd CIKM, 2014. (3 hours)\n\nFeature Engineering for Health Informatics. Tutorial at The 18th PAKDD. Tainan, Taiwan. 2014 (3 hours)\n\nDr. Fei Wang has served as senior program committee member/area chair of conferences including AAAI, IJCAI, KDD, CIKM, ICDM, SDM. He also reviews major medical and interdisciplinary journals including Nature Medicine, Nature Communications, Annals of Internal Medicine, etc.\n\nSECTION: Tieyong Zeng\n\nDr. Tieyong Zeng is currently a Professor at the Department of Mathematics, The Chinese University of Hong Kong. Together with colleagues, he has founded the Center for Mathematical Artificial Intelligence (CMAI) since 2020 and served as the director of CMAI. His research interests include image processing, optimization, artificial intelligence, scientific computing, computer vision, machine learning, and inverse problems. He has published around 100 papers in the prestigious journals such as IEEE Transactions on Pattern Analysis and Machine Intelligence and International Journal of Computer Vision. He is laureate of the 2021 Hong Kong Mathematical Society (HKMS) Young Scholars Award.\n\nThe following research records by Dr. Tieyong Zeng are directly relevant to the tutorial topic:\n\nJ. Liu, M. Yan, and T. Zeng. Surface-aware Blind Image Deblurring. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(3), 1041-1055, 2021.\n\nJ. Liu, W. Liu, J. Sun, T. Zeng. Rank-one prior: Real-time scene recovery, IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(7), 8845-8860, 2022.\n\nSECTION: Fenglei Fan\n\nDr. Fenglei Fan is currently a Research Assistant Professor with Department of Mathematics, The Chinese University of Hong Kong. His primary research interests lie in NeuroAI and data mining. He has authored 26 papers in flagship AI and medical imaging journals. He was the recipient of the IBM AI Horizon Scholarship. He was also selected as the award recipient for the 2021 International Neural Network Society Doctoral Dissertation Award. His primarily-authored paper was selected as one of few 2024 CVPR Best Paper Award Candidates (26 out of 1W+ submissions) and won the IEEE Nuclear and Plasma Society Best Paper Award.\n\nThe following research records by Dr. Fenglei Fan are directly relevant to the tutorial topic:\n\nF. Fan, J. Xiong, M. Li, G. Wang. On interpretability of artificial neural networks: A survey. IEEE Transactions on Radiation and Plasma Medical Sciences, 5(6), 741-760, 2021.\n\nZ. Dong, F. Fan*, W. Liao, J. Yan. Grounding and enhancing grid-based models for neural fields, CVPR, 2024, in press (This paper gets full-graded review, CVPR 2024 Best Paper Award Candidate).\n\nF. Fan, R. Lai, G. Wang. Quasi-equivalency of width and depth of neural networks. Journal of Machine Learning Research, 2023, in press.\n\nF. Fan, M. Li, R. Lai, F. Wang, G. Wang. On Expressivity and Trainability of Quadratic Networks. IEEE Transactions on Neural Networks and Learning Systems, 2023.\n\nTutorial\n\nIntroducing Neuronal Diversity into Deep Learning. Tutorial at 37th AAAI. 2023. (1 hour and 45 minutes)\n\nSECTION: 4.Schedule\n\nIntroduction (20 min)\n\nThe bottlenecks of deep learning\n\nThe potential of invariance\n\nPreliminaries of invariance (20 min)\n\nConcepts of invariance and symmetry in physics and mathematics\n\nCases of invariance and symmetry in pattern recognition and data mining tasks\n\nFormalizations of invariance and symmetry\n\nInvariance in the era before deep learning (40 min)\n\nInvariance of global representations\n\nInvariance of local sparse representations\n\nInvariance of local dense representations\n\nInvariance in the early era of deep learning (40 min)\n\nHierarchical representations with data driven\n\nInvariance of hierarchical representations \u2013 symmetry breaking\n\nInvariance in the era of rethinking deep learning (40 min)\n\nGeometric deep learning as a way to bridge the gap\n\nThe good, the bad, and the ugly\n\nConclusions and discussions (20 min)\n\nReview this tutorial\n\nHighlight research opportunities\n\nSECTION: 5.Style and Audience\n\nThis tutorial is lecture style. The intended audience for this tutorial mainly include researchers, graduate students, and industrial practitioners who are interested in exploring a new dimension of data representations.\n\nThe audience is expected to have the basic knowledge on deep learning, pattern recognition, and data mining. However, the tutorial will be presented at\ncollege junior/senior level and should be comfortably followed\nby academic researchers and industrial practitioners.\n\nAfter this tutorial, the audience are expected to 1) have a comprehensive understanding of basic invariance concepts; 2) learn a history of invariance before and after the era of deep learning; 3) know the bottlenecks in the data-driven-only paradigm, the revival of invariance, and the geometric deep learning; and 4) explore novel research opportunities in this area, and master how to use or even design invariance representations for their tasks.\n\nSECTION: 6.Tutorial Materials\n\nThis tutorial provides attendees with lecture slides, as well as a reading list of selected papers. We would like to state that such materials are free of any copyright issues.\n\nSECTION: 7.Video Teaser\n\nThis tutorial has a video trailer, available atlink.\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.04858v1_content.txt"}, {"title": "Deep Learning and Machine Learning: Advancing Big Data Analytics and\n  Management with Design Patterns", "authors": ["Keyu Chen", "Ziqian Bi", "Tianyang Wang", "Yizhu Wen", "Pohsun Feng", "Qian Niu", "Junyu Liu", "Benji Peng", "Sen Zhang", "Ming Li", "Xuanhe Pan", "Jiawei Xu", "Jinlang Wang", "Ming Liu"], "published_date": "2024-10-04T02:50:58Z", "summary": "This book, Design Patterns in Machine Learning and Deep Learning: Advancing\nBig Data Analytics Management, presents a comprehensive study of essential\ndesign patterns tailored for large-scale machine learning and deep learning\napplications. The book explores the application of classical software\nengineering patterns, Creational, Structural, Behavioral, and Concurrency\nPatterns, to optimize the development, maintenance, and scalability of big data\nanalytics systems. Through practical examples and detailed Python\nimplementations, it bridges the gap between traditional object-oriented design\npatterns and the unique demands of modern data analytics environments. Key\ndesign patterns such as Singleton, Factory, Observer, and Strategy are analyzed\nfor their impact on model management, deployment strategies, and team\ncollaboration, providing invaluable insights into the engineering of efficient,\nreusable, and flexible systems. This volume is an essential resource for\ndevelopers, researchers, and engineers aiming to enhance their technical\nexpertise in both machine learning and software design.", "arxiv_id": "2410.03795v2", "html_link": "https://arxiv.org/html/2410.03795v2", "search_term": "ti:\"deep learning\"", "html_content": "", "text_file": "data\\paper_texts\\2410.03795v2_content.txt"}, {"title": "Deep Learning-Enabled ISAC-OTFS Pre-equalization Design for\n  Aerial-Terrestrial Networks", "authors": ["Weihao Wang", "Jing Guo", "Siqiang Wang", "Xinyi Wang", "Weijie Yuan", "Zesong Fei"], "published_date": "2024-12-06T03:30:53Z", "summary": "Orthogonal time frequency space (OTFS) modulation has been viewed as a\npromising technique for integrated sensing and communication (ISAC) systems and\naerial-terrestrial networks, due to its delay-Doppler domain transmission\nproperty and strong Doppler-resistance capability. However, it also suffers\nfrom high processing complexity at the receiver. In this work, we propose a\nnovel pre-equalization based ISAC-OTFS transmission framework, where the\nterrestrial base station (BS) executes pre-equalization based on its estimated\nchannel state information (CSI). In particular, the mean square error of OTFS\nsymbol demodulation and Cramer-Rao lower bound of sensing parameter estimation\nare derived, and their weighted sum is utilized as the metric for optimizing\nthe pre-equalization matrix. To address the formulated problem while taking the\ntime-varying CSI into consideration, a deep learning enabled channel\nprediction-based pre-equalization framework is proposed, where a\nparameter-level channel prediction module is utilized to decouple OTFS channel\nparameters, and a low-dimensional prediction network is leveraged to correct\noutdated CSI. A CSI processing module is then used to initialize the input of\nthe pre-equalization module. Finally, a residual-structured deep neural network\nis cascaded to execute pre-equalization. Simulation results show that under the\nproposed framework, the demodulation complexity at the receiver as well as the\npilot overhead for channel estimation, are significantly reduced, while the\nsymbol detection performance approaches those of conventional minimum mean\nsquare error equalization and perfect CSI.", "arxiv_id": "2412.04751v1", "html_link": "https://arxiv.org/html/2412.04751v1", "search_term": "ti:\"deep learning\"", "html_content": "SECTION: Deep Learning-Enabled ISAC-OTFS Pre-equalization Design for Aerial-Terrestrial Networks\n\nOrthogonal time frequency space (OTFS) modulation has been viewed as a promising technique for integrated sensing and communication (ISAC) systems and aerial-terrestrial networks, due to its delay-Doppler domain transmission property and strong Doppler-resistance capability. However, it also suffers from high processing complexity at the receiver. In this work, we propose a novel pre-equalization based ISAC-OTFS transmission framework, where the terrestrial base station (BS) executes pre-equalization based on its estimated channel state information (CSI). In particular, the mean square error of OTFS symbol demodulation and Cram\u00e9r-Rao lower bound of sensing parameter estimation are derived, and their weighted sum is utilized as the metric for optimizing the pre-equalization matrix. To address the formulated problem while taking the time-varying CSI into consideration, a deep learning enabled channel prediction-based pre-equalization framework is proposed, where a parameter-level channel prediction module is utilized to decouple OTFS channel parameters, and a low-dimensional prediction network is leveraged to correct outdated CSI. A CSI processing module is then used to initialize the input of the pre-equalization module. Finally, a residual-structured deep neural network is cascaded to execute pre-equalization. Simulation results show that under the proposed framework, the demodulation complexity at the receiver as well as the pilot overhead for channel estimation, are significantly reduced, while the symbol detection performance approaches those of conventional minimum mean square error equalization and perfect CSI.\n\nSECTION: IIntroduction\n\nAs the key technology extending coverage and promoting ubiquitous connectivity, the aerial-terrestrial network has garnered widespread attention in recent years[1,2,3]. The operation of the aerial-terrestrial network depends on exceptional wireless communication and sensing capabilities, which enable data transmission, trajectory tracking, and intrusion detection[4], etc. However, the limited spectrum resources make achieving great connections and sensing in the aerial-terrestrial network challenging, necessitating higher spectral efficiency techniques to meet this demand. The integrated sensing and communications (ISAC)\ntechnology is regarded as a promising candidate[5,6,7]. ISAC technology utilizes a unified waveform to simultaneously achieve communication and sensing functions. That is to say, the waveform transmitted by the transmitter can be used for deliver data symbols to the communication receiver, while also being used for sensing parameter estimation at the sensing receiver. This unified approach shares hardware resources, spectrum resources, and signal processing algorithms, leading to benefits such as improved spectrum and energy efficiency, and reduced hardware costs. It should be noted that the contradiction between the communication signal and the sensing signal poses challenges in the ISAC waveform design[8]. Therefore, the ISAC waveform needs to achieve a balance between communication and sensing performance.\n\nNote that the aerial-terrestrial integrated network can involve the high-mobility scenarios. In such cases, the wireless channels may transition from stable to highly dynamic, or from time-invariant channel to fast time-variant channel, degrading communication and sensing performance[9]. For example, the performance of ISAC systems using multi-carrier waveforms (e.g., orthogonal frequency division multiplexing) can be severely degraded by inter-carrier interference[10]. Hence, how to achieve efficient communication and precise sensing under high mobility has emerged as a critical challenge for aerial-terrestrial ISAC networks.\n\nAmong the possible candidates, orthogonal time frequency space (OTFS) exhibits high reliability compared to other commonly used waveforms[11]. Specifically, in OTFS modulation, the communication channel parameters and sensing parameters are of the same form, making it more suitable as an ISAC waveform. Hence, it received attention from researchers in recent years[12,13,14]. For example, in[15], an OTFS-based ISAC system was proposed, allowing for highly accurate range-velocity profiles without requiring large bandwidth transmissions or long-duration frames. The authors in[16]addressed the challenge of precisely estimating of user state information alongside reliable data transmission in high-mobility environments. The authors in[17]presented a deep learning-based algorithm for target detection using ISAC-OTFS waveforms, where this approach eliminates the need for pre-computing any statistical parameters. Given that ISAC systems typically has to sense of environmental parameters to perform specific tasks, some studies are shifting their focus from mere target detection to parameter estimation. For example, in[18], the authors minimized the bit error rate (BER) of the OTFS-based ISAC system while ensuring accurate sensing parameter estimation. The authors in[19]proposed a generalized likelihood ratio test (GLRT) based multi-target detection and delay-Doppler-angle estimation algorithm for multiple-input-multiple-output OTFS radar sensing. Based on this, the trade-off between sensing SNR and communication rate was achieved by jointly optimizing the spatial and delay-Doppler (DD)-domain waveforms. Note that the aforementioned works all assumed channel state information (CSI) is perfect; however, in practical applications, due to the high mobility of the aerial node, the AP may only be able to obtain outdated CSI, which will definitely degrade the communication performance.\n\nTo address the above issue, some works have been done to refine CSI for OTFS communication systems. Specifically, the authors in[20]used the sensing echoes to assist in beam alignment and beam tracking, where Kalman filtering was employed to predict the angle-domain information for updating beamforming strategies. A convolutional long-short term memory recurrent neural network (CLRNet) was then proposed in[21]by introducing convolutional neural network modules for spatial feature extraction and long short-term memory (LSTM) modules for capturing temporal dependencies. In[22], based on deep learning, the roadside unit utilized vehicle angular parameters obtained from sensing echoes to predict beam directions for the next time instant, thereby achieving precise beam pairing with minimal latency. These works focused on channel prediction in the spatial domain, while there have been little works studying methods to obtain the delay and Doppler shift information of OTFS. The authors in[23]reconstructed the topology between communication nodes using the estimated sensing parameters of communication targets. Based on kinematic formulas, the reconstructed topology was then adopted to assist in predicting the complete channel, including delay and Doppler shift, without requiring channel estimation. In addition to leveraging topology relationships, the work in[24]capitalized on past channel information across multiple time slots to forecast the future communication precoding matrix, thereby minimizing the frame error rate and achieving ultra-reliable low latency communications. Note that the literature discussed above[20,21,22,23,24]primarily concentrates on enhancing communication performance, but the sensing aspect was not addressed. Additionally, these works focused on the beamforming or precoding design and such algorithms may not be suitable for the aerial receivers (e.g., unmanned aerial vehicle (UAV)) with limited signal processing capabilities. This is because the complex OTFS channel estimation and equalization algorithms still need to be designed at the aerial receiver, which leads to increased signal processing delays at the aerial receiver. In sum, under the outdated CSI scenario, the investigation of the OTFS waveforms to balance the performance between communication and sensing while ensuring low processing complexity at the UAV side still needs to be addressed.\n\nIn this work, we explore the ISAC transmission framework for an OTFS-based aerial-terrestrial ISAC network. By taking into account the impact of channel outdatedness caused by high mobility and the limitations of communication signal processing capabilities at the UAV, a channel prediction-based pre-equalization (CPBPE) framework for ISAC waveforms is proposed to achieve a trade-off between communication and sensing performance. The main contributions of this paper are as follows:\n\nFor the aerial-terrestrial network with one terrestrial access point (AP), one UAV UE, and multiple scatters, an ISAC transmission framework based on OTFS waveform is developed, which includes a pre-equalization module to reduce the complexity at the UE side. Under this transmission framework, the mean square error (MSE) of OTFS data symbols and the Cram\u00e9r-Rao lower bound (CRLB) of sensing parameter estimation are analyzed. Based on this, we formulate a weighted MSE and CRLB optimization problem under the transmission power constraint to balance the communication and sensing performance.\n\nSince the original optimization problem is difficult to solve directly, and it also faces the issue of outdated CSI, by leveraging deep learning, we come up with a CPBPE framework. Specifically, the CPBPE framework includes a parameter-level channel prediction module with a low-dimensional input space, which directly corrects outdated CSI by utilizing the decoupled OTFS channel parameters, a CSI processing module used for the post-processing of the parameter-level channel data, and a residual-structured deep neural network (DNN)-based module for the pre-equalization design that considers both communication and sensing performance.\n\nSimulation results show that the proposed framework effectively enhances communication and sensing performance compared to pre-equalization design using outdated CSI and can approximate the performance achieved with true CSI. This indicates the robustness of the proposed framework for pre-equalization in high-mobility scenarios. Furthermore, we examine the processing complexity at the UAV, which has been greatly reduced.\n\nThe remainder of this paper is organized as follows. SectionIIpresents the system model, including the pre-equalization framework for ISAC signals. SectionIIIanalyzes the performance metrics of the proposed transmission framework and then formulates the optimization problem, considering the weighted communication and sensing performance. The CPBPE framework, including the channel prediction module, CSI processing module, and pre-equalization design module, is elaborated in SectionIV. In SectionV, numerical results are discussed to evaluate the communication and sensing performance of the proposed algorithms under different parameter settings. Finally, the paper is concluded in SectionVI.\n\nThis paper adopts the following notations. A boldface capital letter, a boldface lowercase letter, and a calligraphy letter are used to denote a vector, a matrix, and a set, respectively.denotes theidentity matrix. The transpose and Hermitian transpose are denoted byand, respectively.denotes the Kronecker product.represents converting a matrixto a vector column by column.represents taking the average ofwithas the variable.denotes a diagonal matrix.represents the discrete Fourier transformation matrix of size.denotes the 2-norm of its argument.\n\nSECTION: IISystem Model\n\nWe consider an ISAC-enabled aerial-terrestrial network under the high-mobility communication scenario, which consists of one AP, one UAV UE, and several scatters, as shown in Fig.1. Therein, the AP communicates with the UE while sensing it. In this work, the OTFS waveform is employed for communication and sensing. Similar to[25,26,27], both the AP and UEs are assumed to be equipped with a single antenna.\n\nSECTION: II-AISAC Pre-equalization Framework\n\nTo reduce spectrum resource occupancy and signal processing complexity of UE, we come up with an OTFS-based ISAC pre-equalization framework. As the name suggests, pre-equalization is a scheme that mitigates the impact of the channel at the AP in advance, thereby avoiding the complex channel estimation and equalization at the UE. Specifically, the comparison between the block diagrams of our ISAC pre-equalization framework and conventional OTFS system is illustrated in Fig.2. Different from the conventional scheme, the OTFS modulated signalis firstly pre-equalized by the pre-equalization matrixat AP and then transmitted to UE, which directly performs symbol detection without channel estimation and channel equalization. In the meantime, the AP senses the UE based on the received echo signals. This means a a unified waveform is required to simultaneously fulfill both communication and sensing functions. In this work, we primarily focus on pre-equalization design at AP to accommodate both communication and sensing. The following design principles are taken into account during pre-equalization design:\n\nThere is a contradiction between the randomness of the communication signal and the determinism of the sensing signal. Therefore, pre-equalization design needs to achieve a trade-off between communication and sensing performance under a specific requirement, thus improving the spectrum resource utilization.\n\nNote that waveform design generally requires the instantaneous CSI and sensing parameters, but in practical applications, often only outdated CSI and sensing parameters are available. Therefore, it is essential to design an algorithm that accounts for outdated CSI and sensing parameters to minimize the impact of pre-equalization design inaccuracies on communication and sensing performance.\n\nSECTION: II-BSensing Model\n\nLetdenote the original OTFS modulated symbol in the DD domain. Therein, the modulated information symbols inare assumed to be independent and identically distributed (i.i.d.) with unit power. Then, multiplywith the pre-equalization matrixmentioned inII-A, the vector form of ISAC signal symbol in DD domaincan be obtained, i.e.,\n\nBased on the principle of OTFS modulation, the signal vector at the-th time slot in time-delay (TD) domaincan be obtained by performing the inverse symplectic finite Fourier transform (ISFFT) and inverse discrete Fourier transform (IDFT) to. Mathematically,can be expressed as\n\nThe received sensing signal vectorat the AP in the TD domain is written as\n\nwhereis the additive white Gaussian noise (AWGN) vector at the AP,denotes the TD domain equivalent sensing channel in time slot, respectively. According to[28,29], the sensing channel is given by\n\nwhereis the total number of sensing paths,is the channel attenuation at the-th path.andare the delay tap and Doppler tap of the-th path, respectively. Their relationship with the real delayand Doppler shiftcan be expressed as, respectively,\nwhereis the distance between the AP and the target,is the carrier frequency,is the radial velocity of the target relative to the AP,is the number of delay grids, andis the number of Doppler shift grids,,,is the subcarrier spacing, andis the time slot duration. In (4),characterizes as the Doppler influence, where.\n\nwhere, and\n\nwheredenotes the length of the cyclic prefix. The-th element ofis\n\nBy performing Fourier transformation and symplectic finite Fourier transformation to the TD-domain received sensing signal, we have the DD-domain received sensing signalat the-th time slot as\n\nwhereis the DD-domain AWGN vector with powerat the AP, andis the DD-domain equivalent sensing channel in the-th time slot, which can be expressed as\n\nSECTION: II-CCommunication Model\n\nThe TD-domain received communication signal at UE is\n\nwhereis the i.i.d AWGN vector at the UE,denotes the communication channel in TD domain, expressed as\n\nwhereis the total number of communication paths.,, andare the channel attenuation, delay tap, and Doppler tap of the-th path at the-th time slot, respectively. Similarly, by performing Fourier transformation and symplectic finite Fourier transformation to the TD-domain received signal, we have the DD-domain received communication signalat the-th time slot expressed as\n\nwhereis the communication channel in the DD domain, which can be expressed as\n\nandis the i.i.d AWGN vector with powerin DD domain at UE.\n\nSECTION: IIIProblem Formulation\n\nIn this section, we first analyze the performance metrics of interest, i.e., the CRLB of the sensing parameters (the real part and imaginary part of the channel attenuation, Doppler shift, and delay) and the MSE of the demodulated symbols. Based on the analysis, a multi-objective optimization problem is formulated to investigate the trade-off between sensing and communication performance via pre-equalization design.\n\nSECTION: III-APerformance Metrics\n\nCRLB is defined as the MSE lower bound of the unbiased estimate for the sensing parameters, i.e.,\n\nwhereis the sensing parameter set to be estimated, andis the estimated value of. In this work, the sensing parameters of interest include,, Doppler shift, and delay; hence,is expressed as\n\nAccording to[30], the CRLB can be derived as the inverse of the Fisher Information Matrix (FIM), i.e.,\n\nwhereis expressed as\n\nThe elementof (21) is\n\nwhereandrepresent the-th and-th element of, respectively,is the covariance matrix of, andrepresents the channel matrix of the-th sensing path. For natation simplicity,will be denoted asby omitting the superscriptand subscriptsand, i.e.,\n\nwhose-th element is denoted as\n\nThe derivation ofin (22) w.r.t,,, andcan be then calculated as (26), (27), (28), and (III-A1), where.\n\nTo further simplify the expression of the CRLB, we rewritein (22) as\n\nwhere the step (a) leverages the i.i.d. property of AWGN, with its covariance matrix represented as. The derivation of step (b) makes use of the circular nature of matrix trace. The derivation of step (c) is based on the assumption that the modulated information symbols inare i.i.d. with unit power.\n\nCombing (25) with (21) and then substituting back to (20), we can obtain the CRLB of.\n\nMost of the previous works on ISAC waveform optimization adopted SINR-based metrics for evaluating communication performance, such as the sum rate. However, these metrics are unsuitable for the proposed pre-equalization framework. This is because, although such metrics can achieve optimal reception from the power perspective, they do not account for the constellation angle distortion under the direct symbol demodulation scheme employed at the UE. Therefore, in this paper, a more suitable metric, namely, MSE of communication symbols, is adopted to evaluate communication performance.\n\nMSE of communication symbols is defined as the mean squared error between the DD-domain received communication signaland the modulated information symbol vector. Mathematically, it is expressed by\n\nwhereis the power normalization factor used to normalize the power of the received communication signalto the power of the modulated information symbol vector.\n\nSECTION: III-BOptimization Problem Formulation\n\nTo achieve a trade-off between communication and sensing performance, we consider using the weighted sum of the two metrics as the optimization objective. By properly setting the weights of communication and sensing performance, pre-equalization design can be tailored to meet various communication and sensing requirements. Specifically, the pre-equalization design problem can be formulated as\n\nwhereis the weighting factor of communication performance,is the weighting factor of sensing communication, andis the maximum transmission power, which is limited by the hardware of the AP.\n\nThe complicated objective makes the formulated problem (31) challenging to solve. First, since the optimization variable is continuous, the optimal solution cannot be obtained via exhaustive searching. In addition, this work considers high-mobility scenarios where instantaneous perfect CSI and sensing parameters are unavailable, leading to the following problems: On one hand, the pre-equalization design based on the outdated CSI results in suboptimal results. On the other hand, the computation of the CRLB relies on true sensing parameters, which are unavailable before sensing parameter estimation. Note that such a critical issue has been largely overlooked in existing optimization works.\n\nSECTION: IVProposed Algorithms\n\nIn this section, we present our developed algorithm for solving the optimization problem in (31). We first describe the overall CPBPE framework, which is then followed by a detailed description of the channel prediction module, CSI processing module, and pre-equalization module.\n\nSECTION: IV-AOverall Framework\n\nRecently, deep learning has demonstrated significant capabilities in solving complicated problems. It can leverage the offline training mechanism to improve the generalization ability of the network; in the online deployment phase, only one low-latency forward propagation is required to obtain the pre-equalization matrix solution, effectively addressing rapidly changing channel conditions. Therefore, the deep learning approach is adopted in this work to solve the original optimization problem.\n\nThe overall framework is illustrated in Fig.3. Specifically, to address the outdated CSI and sensing parameters in high-mobility scenarios, we first develop a communication and sensing channel parameter prediction network to forecast the channel coefficients in the delay-Doppler domain. By leveraging the temporal dependence of OTFS channel parameters, the channel parameters in the next time slot can be predicted. This not only ensures that the pre-equalization design is based on almost real-time CSI and sensing parameters but also reduces the overhead for obtaining CSI. Based on the predicted CSI and sensing parameters, the AP facilitates pre-equalization design with DNN, which enables direct demodulation of signals at the UAV.\n\nNote that instead of merging channel parameter prediction with pre-equalization, we decompose the above process into three modules, i.e., channel prediction module, CSI reconstruction module, and pre-equalization design module. This comes from the fact that the three modules of the network can be trained independently, thereby reducing the overall training complexity of the network. In the channel prediction module, based on the sparsity of the OTFS channel and the strong correlation between the OTFS channel and mobility, only a few channel parameters, such as channel attenuation values, delays, and Doppler shifts, are required. After inputting the above parameters in the past few slots into the prediction network, the channel and sensing parameters for the next time slot can be predicted. These predicted parameters are then fed into the CSI processing module to be processed and then provide an initial value for the pre-equalization design module, thereby reducing the complexity of the pre-equalization network training process.\n\nSECTION: IV-BChannel Prediction Module\n\nTo design the pre-equalization matrix in problem (31), we first need to obtain the CSI and the sensing parameters of the-th time slot. To effectively reduce the overhead of channel estimation and reduce the impact of outdated CSI, we come up with a channel prediction module based on the neural basis expansion analysis for interpretable time series forecasting (NBEATS) network[31], which relies on the channel information of the past few time slots.\n\nThe core idea of the module is as follows. First, the prediction module leverages the sparsity of OTFS channels to decouple the channel into a series of parameters. This enables a parameter-level channel prediction using a low-dimensional input space, thereby reducing complexity. Furthermore, it decomposes time series through multiple fully connected layers, with each layer fitting partial features of the channel sequence. This approach allows the network to extract, separate, and integrate channel variations from multiple scales. Consequently, the prediction does not rely on period characteristics, which are difficult to maintain for channel parameters strongly associated with mobility. This enables the effective fitting of rapidly changing channel features with fewer time steps, even in the absence of clear periodicity. The details of the module are described below.\n\nLeveraging the sparsity of OTFS channels, the channel parameters, i.e., delays, Doppler shiftsand channel attenuation parameters, are adopted as the input of the prediction network instead of the whole channel matrix as considered in[24]. This greatly reduces the dimensionality of the network input, effectively lowering the network overhead. In this way, the communication or sensing channel parameter set at the-th time slot is denoted as\n\nTo predict the channel parameter set at the-th time slot, the channel parameter setsof the pasttime slots are fed into the prediction network, whereis expressed as\n\nThe adopted prediction network structure is illustrated in Fig.3\u2460.\n\nThe channel prediction network consists of several stacks, which are connected as shown in Fig.3\u2460. Each stack extracts one type of feature from the channel parameter variations and passes the unlearned feature variations to the next stack for processing, ultimately achieving the learning and prediction of channel parameter features at different scales. The channel parameter prediction results of each stack are summed up to form the predicted channel parameter set, i.e.,\n\nEach stack is composed ofblocks. For each block, the input is a channel parameter sequenceof length, containing a portion of the channel features. The output consists of two parts, i.e., one part is a fitted channel parameter sequence of length, representing the features of the channel parameter sequence that the block can fit, and the other part is the predicted channel parameter valuefor the next slot. Note for the first block in the entire model, its input is the original channel parameter sequence.\n\nDifferent blocks are connected based on the doubly residual stacking principle[31]. In other words, each blockcontains two residual branches, i.e., one branch computes the difference between the fitted channel parameter sequence and the block\u2019s input, outputting the channel parameter sequence that the block failed to fit, which is then passed to the next block for fitting. The other branch sums the predicted channel parameter valueof the blockwith those from other blocks, serving as the channel parameter prediction output for the stack. This process is represented as\n\nInside each block, the network is composed of two parts. The first part is a 4-layer fully connected network with a channel parameter sequenceas input. The output of this fully connected network is input into two other fully connected networks. One of these fully connected networks is responsible for fitting the input channel parameter sequenceof length. The output result is linearly mapped to output the fitted channel parameter sequence. The other fully connected network\u2019s output is linearly mapped to predict the channel parametersfor the next time slot. The linear mapping process is expressed as follows:\n\nwhereandare the learnable weight matrices for channel parameter fitting and prediction.andrepresent the output of the fully connected layers for channel parameter fitting and prediction.andare the learnable bias vectors for channel parameter fitting and prediction, respectively.\n\nFor the channel prediction module, the network implementation process is divided into two stages, i.e., offline training and online prediction. In the offline training stage, a large set of channel time series data is used to train the network. Notably, the channel parameter dataset is generated using ray-tracing based on the UE\u2019s trajectory, with periodic sampling over a continuous period. In this way, more realistic channel parameters are collected, thereby ensuring the practical applicability of the developed network.\n\nThe loss function of the offline training process is designed based on the MSE between predicted channel parameters and real channel parameters, denoted as\n\nSubsequently, we consider using six stacks, each containing three blocks in our channel prediction module. We utilized the Adam optimizer to iteratively train the channel prediction network with a learning rate of 0.001 until convergence.\n\nDuring the online prediction phase, simply input the channel parametersestimated from the previous time step into the channel prediction module. Then, a single forward computation is done to predict the channel parametersfor the current time slot.\n\nSECTION: IV-CCSI Processing Module\n\nThe CSI processing module is mainly designed to post-process the output of the channel prediction module and provide an initial value for the pre-equalization design module, thereby reducing the complexity of learning and training. First, the predicted channel parameters from the channel prediction module, including channel attenuation, delay, and Doppler shift, are used to reconstruct the communication channelaccording to (17). Subsequently, the inverse of the communication channel matrixis computed.\n\nSECTION: IV-DPre-equalization Design Module\n\nThe output of the CSI processing module and sensing parameters can then be used to aid the pre-equalization matrix design. Therein, we leverage a neural network to design the pre-equalization matrix as shown in Fig.3\u2462. The details of the module are described below.\n\nThe inverse matrix of the predicted communication channel is concatenated with the transmitted data symbols with the following processing process: Firstly, the real and imaginary parts of the inverse matrix of the predicted communication channel are separated and reconnected along dimension 1 to form a tensor of size. Subsequently, this tensor is flattened into a one-dimensional tensorof size. Additionally, the real and imaginary parts of the DD-domain modulation symbolsare separated and reconnected along dimension 1 to form a one-dimensional tensorof size. Next,andare concatenated via a concatenate layer to form a one-dimensional tensorof size.\n\nThe network consists of multiple dense layers and a residual connection structure. First,is input into the first dense layer through the Tanh activation function. The advantage of using the Tanh activation function lies in its ability to handle negative numbers and maintain a mean value of zero, which is suitable for a pre-equalization matrix that has centralized characteristics. Next, the output of this dense layer is fed into three consecutive dense layers, each with a Tanh activation function. To expedite convergence, we additionally adopt a residual connection method, linking the inverse matrix of the predicted communication channel with the output of the dense layers. More specifically, the output of several consecutive dense layers is connected to the input of the first layer via the residual connection, and the inverse matrix of the predicted communication channel is modified for both communication and sensing performance by directly adding the outputs. Finally, the output is reshaped to sizeby the reshaping layer. The detailed parameter of the proposed DNN-based pre-equalization design network is listed in TABLEI.\n\nThe output of the last reshape layer is a matrix with a size of, which is denoted as. The firstrows of the matrixcorrespond to the real part of the pre-equalization matrix before power normalization, and the subsequentrowscorrespond to the imaginary part of the pre-equalization matrix before power normalization.\n\nDue to constraints on transmission power in (32), power normalization on the output of the neural network is performed as follows\n\nThen, the real-value outputs are reconnected to the complex-valued pre-equalization matrix as\n\nSimilar to the channel prediction module, the implementation process involves two phases, i.e., offline training and online deployment. During the offline training phase, to minimize the objective problem in (31), we design the following loss function\n\nwhereis the L2 regularization term, which is used to prevent the model from overfitting.is the parameter of the DNN-based pre-equalization design network.is the weighting factor of the L2 regularization term.\n\nThe channel information processed by the CSI processing module is used as input for the network, and the sensing parameters are used as labels in the loss function. Based on them, the neural network is then trained offline using the Adamax optimizer with the loss functionand a learning rate of 0.001 until the network converges.\n\nDue to the well-established offline training, during the online deployment phase, only one forward propagation is required to complete the prediction of the pre-equalization matrix for the next time slot considering both communication and sensing performance. Therefore, the pre-equalization matrix can be rapidly obtained by inputting the inverse of the channel matrix and the modulated information symbols. After usingto pre-equalize the transmitted symbols, the sensing performance of the transmitted waveform can be improved, and at the same time, the UE can also directly demodulate the signal without equalization, thus significantly reducing the complexity of the UE.\n\nSECTION: VSimulation Results\n\nIn this section, the numerical results are presented to demonstrate the effectiveness of the proposed scheme. We consider one AP, one UE, which also serves as a sensing target, andscatters. The AP is fixed at coordinates, while the UE and scatterers move around the AP at speeds of up to. These positions and motion patterns are incorporated into the Wireless Insite[32]software to generate corresponding channels as the dataset, including delays, Doppler shifts, and channel attenuation parameters. The dataset consists of 640 trajectory groups, which are divided into training and testing sets in a ratio of 4:1. Moreover, the size of one OTFS symbol is set asand[33,34], respectively.\n\nTo verify the performance of the proposed CPBPE algorithm, we compare our algorithm with the following schemes, i.e.,\n\nMinimum mean square error (MMSE) equalization[35]:Under this scheme, the MMSE equalization, which is calculated using the communication channel matrix of perfect CSI, is performed at the UE side. In this work, MMSE is used as an upper-bound algorithm for communication performance.\n\nPre-equalization with perfect CSI:This is the ideal case, where only the pre-equalization design network proposed by this paper is utilized to design the pre-equalization matrix. The network takes the communication channel matrix with perfect CSI as input.\n\nPre-equalization with outdated CSI:Under this scheme, channel prediction is not performed, and the CSI from the previous time slot is used as input to the pre-equalization network.\n\nSECTION: V-APerformance of Channel Prediction\n\nWe first examine the performance of channel prediction. Fig.4compares the accuracy of the NBEATS-based OTFS channel prediction module with the common time-series prediction algorithm, including long short-term memory recurrent neural network (LSTM)[36], patch time series Transformer (PatchTST)[37], and Autoformer[38]. Due to the varying magnitudes of OTFS channel parameters, the mean absolute percentage error (MAPE) is employed as the accuracy evaluation metric, expressed as\n\nwhereandare the true and predicted values of the parameters, respectively, andis the number of samples.\n\nAs shown in the figure, for the prediction of,,, and, the NBEATS algorithm always demonstrates the best MAPE performance. The worst prediction performance for the NBESTS algorithm is observed for the delay, but its MAPE is still below 5 %. In addition, for the predictions ofand, the MAPE is significantly less than 1 %. Overall, this demonstrates that selecting the NBEATS algorithm for OTFS channel parameter prediction is feasible.\n\nSECTION: V-BEffect of Iteration Number\n\nThis subsection explores the relationship between the number of iterations during the offline training phase of the pre-equalization design network and the ISAC performance. From Fig.5, it can be observed that for communication MSE performance, convergence is achieved at approximately 20 iterations. However, for sensing CRLB performance, it takes about 60 iterations to reach convergence. Furthermore, communication MSE initially decreases and then slightly increases during training. This is because, in the early iterations of training, the network primarily focuses on optimizing communication MSE performance. As communication MSE performance stabilizes, the network begins to prioritize optimization of sensing CRLB performance. This indicates that when communication performance is more important in the OTFS-based ISAC system, it may not be necessary to perform a large number of iterations to meet performance requirements. However, if a strict trade-off between communication and sensing performance is desired, more iterations are required. Although the offline training phase takes some iterations, during online deployment, the designed CPBPE network does not require as many iterations and only a single forward propagation is required to complete the design of the pre-equalization matrix considering both communication and sensing performance. The ability of this network to provide rapid outputs helps overcome outdated pre-equalization matrix designs in high-mobility scenarios.\n\nSECTION: V-CEffect of Transmit Power\n\nWe then investigate the effect of transmit power on the optimized performance of communication and sensing. Fig.6presents the optimized communication MSE versus the transmit power under different schemes, where only the communication performance is considered (i.e.,). As illustrated in Fig.6, with increasing transmit power, the MSE of all algorithms shows a decreasing trend. This is because higher transmission power results in a higher SNR for communication symbol demodulation at the UE. Additionally, it can be observed that our proposed algorithm, utilizing the predicted channel for pre-equalization design, not only approaches the communication MSE performance under perfect channel information, indicating the accuracy of our channel prediction algorithm, but also approaches the performance of MMSE equalization based on perfect CSI. This shows the excellent performance of our proposed pre-equalization design scheme in significantly reducing complexity while maintaining significant symbol detection performance. Additionally, the performance curve of pre-equalization designed with outdated CSI shows a significant gap compared to the other three curves. This indicates that pre-equalization designs based on outdated CSI can degrade communication performance, making accurate CSI prediction essential for pre-equalization.\n\nFig.7presents the optimized CRLB versus the transmit power under different schemes, where only the sensing performance is considered (i.e.,). From this figure, as transmit power increases, the sensing CRLB of all schemes decreases. This is because higher transmit power indicates a higher signal-to-noise ratio for echoes, leading to improved sensing accuracy. Additionally, we can see from the figure that the performance based on our CPBPE algorithm almost overlaps with the best CRLB performance achieved by the scheme under perfect sensing parameters. When compared to the scheme based on outdated CSI for pre-equalization design, there is an improvement of aroundin CRLB performance. This demonstrates that prediction is necessary for achieving higher accuracy in CRLB for sensing. It is worth noting that even with the scheme based on outdated sensing parameters, significant sensing performance gains compared to the scheme without pre-equalization can still be obtained. This validates the ability of the proposed pre-equalization design module to achieve sensing performance gains even under less accurate estimation.\n\nSECTION: V-DTrade-off Between Communication and Sensing Performance\n\nFig.8illustrates the trade-off between communication and sensing performance achieved by different algorithms. As seen in the figure, when the communication weighting factoris large, the proposed algorithm achieves an MSE of the order of. As the communication weighting factordecreases, the sensing CRLB of the CPBPE algorithm is reduced by more than 65 %, indicating a significant improvement in sensing accuracy. This demonstrates that the designed weighting factor allows different pre-equalization designs to adapt to varying communication and sensing requirements.\nMoreover, the trade-off curve of the proposed CPBPE algorithm nearly overlaps with that of the design based on perfect CSI and sensing parameters, while the trade-off curve of the pre-equalization design based on outdated CSI always lies outside the trade-off curve of the proposed CPBPE algorithm. This means that, under any communication and sensing requirements, the proposed method can overcome the impact of outdated CSI, achieving a good balance between communication and sensing performance. Note that the CRLB performance of the pre-equalization design based on outdated CSI improves overall with the increase in communication symbol demodulation MSE. However, when the communication weighting factoris very small, the CRLB increases. This is because when the gap between outdated and perfect CSI and sensing parameters is larger, the better the optimization result based on the outdated CSI, the poorer communication and sensing performance when applied with real CSI. This again underscores the necessity of obtaining accurate CSI.\n\nSECTION: V-EEvaluation of Signal Processing Complexity at the UE\n\nIn traditional communication frameworks as shown in Fig.2, the UAV UE needs to perform channel equalization before symbol detection, which requires matrix inversion operations. This results in high computational complexity, i.e.,, whereis the modulation order. In contrast, the proposed CPBPE scheme only requires a complexity ofat the UE. The comparison of the signal processing complexity of the proposed framework with the traditional framework (i.e., performing channel equalization at the UE side) is plotted in Fig.9. As shown in the figure, the processing complexity increases with the increase in symbol length. Moreover, the complexity gap between the traditional communication scheme and the proposed CPBPE scheme widens significantly. It is worth noting that given that energy consumption is proportional to complexity, the proposed method greatly reduces energy consumption at the UE, thereby enhancing the battery life of the UAV.\n\nSECTION: VIConclusions\n\nIn this work, we have proposed a pre-equalization design based on the OTFS waveform for an ISAC-enabled aerial-terrestrial network. To overcome the influence of outdated CSI and sensing parameters, relying on deep learning, a CPBPE framework was developed to effectively design a more precise pre-equalization matrix. The pre-equalization design enabled the receiver to require only simple power normalization for symbol detection, significantly reducing the overhead and complexity associated with channel estimation and equalization at the UE. Simulation results showed that the CPBPE framework enhances both communication and sensing performance compared to pre-equalization based on outdated CSI. Moreover, it closely approximated the performance achieved with perfect CSI and sensing parameters, demonstrating its robustness and suitability for high-mobility scenarios.\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.04751v1_content.txt"}, {"title": "Code generation and runtime techniques for enabling data-efficient deep\n  learning training on GPUs", "authors": ["Kun Wu"], "published_date": "2024-12-06T03:20:03Z", "summary": "As deep learning models scale, their training cost has surged significantly.\nDue to both hardware advancements and limitations in current software stacks,\nthe need for data efficiency has risen. Data efficiency refers to the effective\nhiding of data access latency and the avoidance of unnecessary data movements.\nMajor challenges arise from the growing disparity between GPU memory bandwidth\nand computational throughput, imminent GPU memory capacity limitations, and\ninefficiencies in the PyTorch software stack, including a lack of\ndevice-specific PCIe transfer optimizations and high-level domain-specific\nabstractions. To effectively mitigate these data inefficiencies for deep\nlearning training, this dissertation analyzes data inefficiency in\nrepresentative deep training tasks, specifically in graph neural networks\n(GNNs) and large language models (LLMs). It then proposes novel runtime and\ncode generation techniques to mitigate these challenges and implements these\noptimizations seamlessly within the PyTorch stack while maintaining strong\nprogrammability and interoperability. First, PyTorch-Direct is devised to\nincorporate the GPU-centric PCIe data transfer paradigm in PyTorch for GNN\ntraining. Next, Hector intermediate representation (IR) and its code generator\nare proposed to introduce domain-specific high-level abstraction and\nsystematically address memory-intensive performance challenges for relational\nGNNs. Finally, in LLM training, the throughput has been increasingly\nconstrained by GPU memory capacity. To mitigate this, the SSDTrain offloading\nframework is designed and implemented. Together, these contributions show that\ncode generation and runtime techniques can systematically mitigate the data\nmanagement bottlenecks in deep learning training, which stem from the\ndata-intensive nature of workloads and the oversimplification inherent in the\ndeep learning training software stack.", "arxiv_id": "2412.04747v1", "html_link": "https://arxiv.org/html/2412.04747v1", "search_term": "ti:\"deep learning\"", "html_content": "SECTION: Code Generation and Runtime Techniques for Enabling Data-Efficient Deep Learning Training on GPUs\n\nAs deep learning models scale, their training cost has surged significantly. Due to both hardware advancements and limitations in current software stacks, the need for data efficiency has risen. Data efficiency refers to the effective hiding of data access latency and the avoidance of unnecessary data movements. Significant challenges arise from the growing disparity between GPU memory bandwidth and computational throughput, imminent GPU memory capacity limitations, and inefficiencies in the PyTorch software stack, including a lack of device-specific PCIe transfer optimizations and high-level domain-specific abstractions.\n\nTo effectively mitigate these data inefficiencies for deep learning training, this dissertation analyzes data inefficiency in representative deep training tasks, specifically in graph neural networks (GNNs) and large language models (LLMs). It then proposes novel runtime and code generation techniques to mitigate these challenges and implements these optimizations seamlessly within the PyTorch stack while maintaining strong programmability and interoperability.\n\nFirst, Hector intermediate representation (IR) and its code generator are devised to introduce domain-specific high-level abstraction and systematically address memory-intensive performance challenges for relational graph neural networks (RGNNs). The performance challenges stem from RGNN\u2019s inherent memory intensiveness, the gap between the programming interface and the kernel APIs, and the high kernel optimization cost due to kernel coupling with layout and heterogeneity. Using a general matrix multiply\u00a0(GEMM) template and a traversal template, Hector achieves up to a 43.7speed-up in training and inference compared to the state-of-the-art systems. Linear operator reordering and compact tensor materialization further achieve up to 3.8speed-up compared to the Hector unoptimized code.\n\nSecond, PyTorch-Direct is introduced to incorporate the GPU-centric PCIe data transfer paradigm in PyTorch for GNN training. PyTorch-Direct significantly reduces CPU utilization, resulting in higher end-to-end training performance. For the input datasets and GNN architectures evaluated, PyTorch-Direct decreases the overall training time by up to 38.2%.\n\nFinally, in LLM training, the throughput has been increasingly constrained by GPU memory capacity. To mitigate this, the SSDTrain offloading framework is designed and implemented. Since activations take most of the GPU memory, SSDTrain offloads activations to Non-Volatile Memory Express (NVMe) SSDs with a direct GPU\u2013SSD data path and good interoperability. The evaluation shows that SSDTrain reduces activations peak memory use by up to 47% with negligible overhead. We further analyze how the reduced activation memory use may be leveraged to increase throughput by increasing micro-batch size and reducing pipeline parallelism bubbles.\n\nTogether, these contributions demonstrate that code generation and runtime techniques can systematically mitigate the data management bottlenecks in deep learning training, which stem from the data-intensive nature of workloads and the oversimplification inherent in the deep learning training software stack.\n\nTo my parents, for their unconditional love and support.\n\nSECTION: LIST OF ABBREVIATIONS\n\nArtificial Intelligence\n\nArithmetic Logic Unit\n\nApplication Programming Interface\n\nThe \u201cA TENsor\u201d Library\n\nByte\n\nBidirectional Encoder Representations From Transformers\n\nBatched Matrix Multiplication\n\nBasic Linear Algebra Subroutines\n\nConvolutional Neural Network\n\nCoordinate Format\n\nCentral Processing Unit\n\nCompressed Sparse Row\n\nCompute Unified Device Architecture\n\nDeep Graph Library\n\nDirect Memory Access\n\nDeep Learning Recommendation Model\n\nDynamic Random-Access Memory\n\nDomain-Specific Language\n\nDisk Writes Per Day\n\nExtract, Transform, and Load\n\nFirst In, First Out\n\nFloating-Point Operations\n\nFloating Point\n\nBillion\n\nGraph Attention Network\n\nGPUDirect Storage\n\nGaussian Error Linear Unit\n\nGeneral Matrix-Matrix Multiplication\n\nGeneral Matrix-Vector Multiplication\n\nGlobal Interpreter Lock\n\nGraph Neural Network\n\nGenerative Pre-Trained Transformer\n\nGraphics Processing Unit\n\nThe \u201cGraph SAmple and AggreGatE\u201d Algorithm\n\nGeneralized SpMM\n\nGeneralized SDDMM\n\nHigh Bandwidth Memory\n\nHeterogeneous Graph Transformer\n\nHigh-Performance Computing\n\nInput/Output\n\nInstructions Per Cycle\n\nIntermediate Representation\n\nInstruction Set Architecture\n\nJoint Electron Device Engineering Council\n\nJEDEC Standard\n\nJust-In-Time\n\nThousand\n\nKey-Value\n\nThe Level-One / Texture Cache\n\nLayer Normalization\n\nLarge Language Model Meta AI\n\nLarge Language Model\n\nLoad-Store Unit\n\nMillion\n\nMulti-Level Intermediate Representation\n\nMulti-Level Cell\n\nMultilayer Perceptron\n\nMatrix Multiplication\n\nMilliseconds\n\nMixture-of-Experts\n\nMean Squared Error\n\nNot-And\n\nNon-Volatile Memory Express\n\nNVMe over Fabrics\n\nOut-Of-Memory\n\nQuadrillion\n\nPetabytes Writes\n\nPeripheral Component Interconnect Express\n\nProgram/Erase\n\nPython Enhancement Proposal\n\nParallel Thread Execution\n\nPyTorch Geometric\n\nRedundant Array of Independent Disks\n\nRapid Analytics on Platforms In Data Science\n\nRectified Linear Unit\n\nRelational Graph Attention Network\n\nRelational Graph Convolutional Network\n\nRelational Graph Neural Network\n\nRandom Number Generator\n\nRecompute-Offload-Keep\n\nSeconds\n\nStreaming ASSembler\n\nSampled Dense-Dense Matrix Multiplication\n\nSingle Instruction, Multiple Threads\n\nSingle-Level Cell\n\nStreaming Multiprocessor\n\nStandard Performance Evaluation Corporation\n\nSPEC High-Performance Group\n\nSPEC Open System Group\n\nSingle Program, Multiple Data\n\nSparse Matrix Dense Matrix Multiplication\n\nStructured Query Language\n\nSolid-State Drive\n\nServer PCIe Module\n\nTrillion\n\nText-To-Text Transfer Transformer\n\nThe Tensor Algebra Compiler\n\nHyperbolic Tangent Function\n\nTensor Core\n\nTotal Cost of Ownership\n\nTorcH Python\n\nTriple-Level Cell\n\nTensor Processing Unit\n\nTensor Virtual Machine\n\nMicroseconds\n\nUnified Virtual Memory\n\nWrite Amplification Factor\n\nAccelerated Linear Algebra\n\nTranscendental and Data Type Conversion Unit\n\nZero Redundancy Optimizer\n\nZoned Namespaces\n\nSECTION: Chapter 1Introduction\n\nIn recent years, deep learning models have demonstrated remarkable capabilities in learning from vast amounts of data, leading to broad adoption and superior performance in various real-world applications, e.g., recommender systems[1,2], content creation[3,4], etc. As these models continue to achieve transformative results, their scale has proliferated to further enhance their competence. However, this increase in model complexity has caused a significant rise in training cost: The training cost of frontier models has grown at 2.4annually in the last eight years. For example, training GPT-4, one of the largest models to date, incurred approximately US$100 million. Notably, the growth in training compute cost has outpaced that of inference, with the former being nearly double the latter[5].\n\nAs a result, deep learning training has increasingly posed pressing data efficiency challenges to the software computing stack. Data efficiency means effectively hiding data access latency and avoiding unnecessary data accesses. Several factors contribute to the growing importance of data efficiency.\n\nFirst, the rapid growth of large-scale models has driven an increase in GPU computational power that far outpaces improvements in data transfer bandwidth. As shown in Figure1.1, for recent GPUs for deep learning, FP16 throughput\u00a0(yellow dotted line, right vertical axis) has increased not only faster than memory bandwidth\u00a0(blue dotted line, left vertical axis), but also faster than the device\u2019s PCIe bandwidth\u00a0(purple dotted line, left vertical axis) and device-to-device interconnect bandwidth\u00a0(orange dotted line, left vertical axis). Moreover, hardware-accelerated lower-precision computation has enlarged the gap between the computational throughput and memory bandwidth: FP16 multiply costs 70% less energy compared with FP32 multiply[6]. At the same time, FP16 data transfers only reduce energy usage by 50% due to their proportionality to transfer size. Consequently, the training processes of most deep learning models are memory-bound. This is illustrated in Figure1.2, where the characteristics of Nvidia B100 are compared with the deep learning training production workloads reported by Google TPU architects[7]. All models except the reported LLM workload are bound by memory. Even in LLM training, memory-intensive operations account for a significant portion of the overall execution time[8,9].\n\nSecond, GPU memory capacity alone cannot sustain the growth in computational throughput, necessitating additional buffer and domain-specific PCIe transfer optimizations. Due to the limited capacity of GPU memory, deep learning training predominantly relies on mini-batches, where the entire training dataset is stored outside the GPU, and only a small subset is transferred during each step. For graph neural networks\u00a0(GNNs), the generic mini-batch transfer scheme\u2014where the CPU prepares the mini-batch input and initiates direct memory access (DMA) PCIe transfer\u2014introduces significant performance overhead due to fine-grained, gather-style random accesses. This can even lead to the loss of scalability[16]. As EMOGI demonstrates[17,16,18], an optimized GPU-centric transfer scheme avoids these issues by programming the GPU to use zero-copy techniques, allowing it to gather features and perform PCIe transfers simultaneously.\n\nFor large language models\u00a0(LLMs), the growth in GPU memory capacity and main memory capacity has struggled to keep up with the increasing demands driven by GPU throughput. In contrast, SSDs offer large storage capacity, and their growth has kept up with these demands. Section5.2.1details the reasoning. Given this, we choose to offload tensors to SSDs for LLM training to overcome GPU memory limitations. Nevertheless, SSD bandwidth is limited, and the gap between SSD bandwidth growth and GPU computational throughput growth continues to widen, as shown in Figures1.1and1.3. It is essential to carefully manage data transfers to prevent training throughput from being constrained by SSD bandwidth. We propose techniques for selecting which tensors to offload and hiding transfer latency. To evaluate the trade-off between performance and memory savings, we compare different strategies, i.e., tensor recomputation, offloading, and keeping tensors in GPU memory. These are detailed in Chapter5.\n\nDespite the importance of data efficiency, several obstacles exist to address it within the current PyTorch-based deep learning training software stack.\nAs one of the most popular deep learning frameworks, PyTorch offers an intuitive interface through the dynamic Python language. It abstracts away the complexity of CUDA-accelerated systems, making them user-friendly and fostering a robust ecosystem in the deep learning community. However, PyTorch is by no means a \u201csilver bullet\u201d[21]. Instead, the PyTorch stack design is largely compute-oriented. This focus creates significant challenges when attempting to tackle data efficiency, a new paradigm requiring optimizing data access alongside computation.\n\nOne significant challenge posed by PyTorch\u2019s compute-centric design, which we encountered while integrating the EMOGI PCIe transfer scheme[17,16,18]for GNNs, is its assumption that both the input and output of each operator reside on the same device to which the operator is dispatched. However, the optimized transfer scheme uses the GPU to gather node features and perform PCIe transfer simultaneously as an integral process, demanding the input to be in the host memory.\nTo adopt such a data-efficient scheme and retain the PyTorch programming interface, the PyTorch runtime code has to be recompiled with the addition of a full-fledged new tensor type, its special host memory allocator, and its set of new dispatch rules.\nChapter4details our solution for incorporating EMOGI PCIe transfer into PyTorch.\n\nSecond, although PyTorch incorporates high-performance math libraries and uses them for corresponding operators[22,23], it lacks high-level abstraction to capture domain-specific semantics. This limitation makes it difficult to safely optimize the code to eliminate redundant data movement and achieve more efficient execution schedules. For example, in relational GNNs\u00a0(RGNNs), a common operation is producing a per-edge vector, edge message, by multiplying the source node features with a weight matrix specific to the edge type. Since edges with the same edge type and source node will get the same edge message as the result, repetitive computation and output footprint can be eliminated. However, existing frameworks with generic GNN abstraction cannot leverage these optimization opportunities because they lack the necessary abstraction to capture and track edge-type-specific information. Chapter3details how a code generator with domain-specific intermediate representation\u00a0(IR) enables the optimization discussed above, compact materialization.\n\nIn this thesis, we show that code generation and runtime techniques can systematically mitigate the data management bottlenecks in deep learning training, which stem from the data-intensive nature of workloads and the oversimplification inherent in the deep learning training software stack.\n\nTo prove the dissertation statement, the dissertation examines the data inefficiency in representative scenarios in GNNs and LLMs, proposes runtime and code generation techniques to mitigate such inefficiency, and implements transparent incorporation into the PyTorch stack with good programmability and interoperability.\nThe contributions of this dissertation are as follows:\n\nHector IR and code generator for end-to-end RGNN training and inference[24]. RGNN execution faces significant performance challenges due to inherent memory intensiveness, the gap between the programming interface and the kernel APIs, and the high kernel optimization cost due to kernel coupling with layout and heterogeneity. To systematically address these issues, we present Hector. Hector generates optimized CUDA kernels to eliminate redundant data movement within GPU and reduces GPU memory footprint. The IR design decouples the model semantics, data layout, and operator-specific schedule and expresses these opportunities to integrate them into the design space. Based on a general matrix multiply\u00a0(GEMM) template and a traversal template, Hector already achieves up to 43.7speed-up in training and inference compared to state-of-the-art systems. Linear operator reordering and compact tensor materialization obtain up to 3.8speed-up compared to the Hector unoptimized code. Chapter3details Hector.\n\nPyTorch-Direct, a GPU-centric data access paradigm for GNN training[25,16,26]. Training GNNs on large graphs that do not fit in GPU memory suffers from significant throughput and CPU utilization overhead. By enabling GPUs to efficiently access complicated data structures in host memory directly without CPU intervention, PyTorch-Direct significantly reduces CPU utilization in GNN training, resulting in higher end-to-end training performance. For the input datasets and GNN architectures evaluated, PyTorch-Direct decreases the overall training time by up to 38.2%. One of its key advantages is the minimal required programmer effort: Users can take full advantage of the benefits that PyTorch-Direct provides by modifying at most two lines of their original code. Chapter4details PyTorch-Direct.\n\nSSDTrain activations111In deep learning, activations are the tensors produced in forward propagation to be used for gradient computation in the backward propagation.offloading framework for LLM training[27]. After mitigating the data inefficiency in CUDA kernels and PCIe transfers, we take the next step to address higher-level data inefficiency, particularly challenges in overlapping kernels and transfers. Notice that LLM training systems are increasingly constrained by GPU memory, with activations being one of the primary culprits. We propose SSDTrain to address this by offloading activations to Non-Volatile Memory Express (NVMe) SSDs. We demonstrate its viability in large-scale systems by modeling. We incorporate into SSDTrain a direct GPU\u2013SSD data path and good interoperability. To fully overlap computation with data transfer, SSDTrain features asynchronous data transfer, tensor deduplication, forwarding, and adaptive offloading. The evaluation shows SSDTrain reduces the activations peak memory use by up to 47% with negligible overhead. We introduce the recompute-offload-keep (ROK) curve to show runs with SSDTrain\u2019s offloading are on the efficient frontier in the design space. We further analyze how the reduced activation memory use may lead to increased throughput by increasing micro-batch size and reducing pipeline parallelism bubbles. Chapter5details SSDTrain.\n\nThe remaining chapters serve the following purposes:\n\nChapter2introduces the background of this dissertation, involving GNNs, LLMs, the Nvidia GPU architecture and programming model, and the PyTorch computing stack.\n\nChapter6presents a final discussion on PyTorch-Direct, Hector, and SSDTrain. Then, it explains the future work on top of this dissertation.\n\nChapter7concludes this dissertation.\n\nSECTION: Chapter 2Background\n\nThis chapter provides the background knowledge necessary for understanding the subsequent chapters.\nReaders may choose one or more sections to read for a particular chapter or skip the sections they are familiar with.\nSection2.1introduces GNNs, which are the focus ofChapters3and4.\nSection2.2delves into transformers, offering context for Chapter5.\nSection2.3covers the architecture, programming model, programming interface and compilation flow for Nvidia GPUs.\nSection2.4overviews the Python language.\nSection2.5introduces the PyTorch computing stack.\n\nSECTION: 2.1Graph Neural Networks\n\nInspired by the success of convolutional neural networks\u00a0(CNNs)[28], people devised GNNs as a new type of neural network that applies similar filters to graphs[29,30,1,31,32,33,34].\nWhile CNNs excel at extracting features from grid-like data such as images, GNNs are designed to propagate and transform features according to the structure of graphs, allowing them to retain relational information between entities represented by nodes and edges.\nGNNs are increasingly applied in diverse domains, including social network analysis and recommender systems[30,1,32], etc.\n\nGNNs have shown significant advantages in graph representation learning[35,30,1], where the goal is to embed graph-structured information into low-dimensional dense vectors.\nThe trained model can produce vectors for specified nodes or edges. Then, tasks, e.g., node classification, link prediction, etc., can be performed by only relying on them rather than all the raw data in the graph, e.g., the adjacency list, node features, etc.\nAs Hamilton et al.[35]noted, traditional algorithms, e.g., DeepWalk[36]and node2vec[37], cannot generalize to perform inference on unseen nodes or edges during training, and their representation power is limited.\nIn comparison, GNNs offer a more powerful and flexible approach, capable of addressing these limitations and enabling inductive learning for new graph data.\n\nA widely-used GNN model is graph convolutional network\u00a0(GCN)[32].\nFormally, a GCN layer is defined as, wheredenotes the trainable weight matrix of the-th layer,is a non-linear activation function andis the node representation at layer. In particular, the node input features are denoted as.is the adjacency matrix normalized by node degrees:\n\nwhereis node\u2019s out degree andis the in degree of node.\n\nSECTION: 2.2Transformer-Based Large Language Models\n\nLLMs now drive a wide range of applications, including chatbots[3], search[38], content generation[4], reasoning[39], etc. These models, when sufficiently large in size, demonstrate emergent abilities[40]and thus the ability to handle complicated tasks. Consequently, LLMs today can be as large as containing hundreds of billions of parameters. Furthermore, model designers are driven to continue to scale up the size of LLMs, carrying more parameters.\n\nMost LLM architectures, including GPT[41], are transformer-based[42]. As Figure2.1(a) shows, the GPT model consists mainly of multiple transformer layers. Before transformer layers, GPT takes in the tokenized text and maps the tokens into dense vectors with positional information. The task determines the last part of the model architecture.\nFor instance, a classifier could be added for text classification tasks.\nFigure2.1(b) shows that each transformer layer is primarily made up of an attention block and a multi-layer perception\u00a0(MLP) block. Attention blocks\u00a0(Figure2.1(c)) compute a weight, called attention, for each token pair, and produce dense vectors for each token via weighted summation. The MLP blocks transform the vector of each token into a new vector.\n\nGPT is a decoder-only model because it only involves transformer decoder layers. A transformer encoder layer has the same structure as the transformer decoder layer except that the latter imposes causality on the attention mask in Figure2.1(d): the causal mask ensures that the new vectors produced by the attention block for each token depend only on vectors of tokens, not after this token. By this categorization, transformer models are classified as (1)\u00a0encoder-only, e.g., BERT[43], (2)\u00a0decoder-only, e.g., GPT, Llama[44], and (3)\u00a0encoder-decoder, e.g., T5[45]. In encoder-decoder models, the transformer decoder layers take in both outputs from the encoders and another text and apply two attention blocks\u2014the self-attention block is applied to the new text, and the cross-attention block is applied among the tokens in the sequence from the encoder and tokens in the new text.\n\nParallelizing LLM training involves partitioning and/or replicating the model and the data into different GPUs[46]. Pipeline parallelism, data parallelism, and model parallelism are the three levels of parallelism available to all LLM models and widely adopted in frameworks, e.g., Megatron, DeepSpeed, and PyTorch 2.0[47,48,49].\nPipeline parallelism partitions the model into several chunks of layers and places them on different GPUs. In a step, when the GPUs finish their layers, the output is passed to the GPUs owning the next layers.\nData parallelism replicates the models in different groups of GPUs and assigns separate micro-batches to each group.\nAt the end of a step, the gradients in each group are aggregated to update all the model replicas.\nModel parallelism shards a weight tensor and puts shards onto different GPUs. Each GPU performs a part of the computation using its shard for the corresponding operator.\nGiven the system scale and interconnect, all or a few among the three levels may be used.\nZero Redundancy Optimizer\u00a0(ZeRO)[50]further reduces memory use with data parallelism by sharding the optimizer states, and/or optionally the gradients and parameters and stores the shards across these GPUs.\n\nSECTION: 2.3Nvidia GPU Architectures and Programs\n\nWhile the CPU is designed to minimize the latency of each operation, the GPU is a massively parallel processor optimized to maximize throughput[51]. To support this computational parallelism, each GPU device is equipped with memory that has very high bandwidth, reaching an order of magnitude of TB/s after high-bandwidth memory\u00a0(HBM) is adopted. Just as each CPU chip contains multiple cores, each Nvidia GPU contains hundreds of cores, called streaming multiprocessors\u00a0(SMs). The structure of an SM is illustrated in Figure2.2. In an SM, the scheduler selects instructions ready for execution, which are then dispatched by the dispatcher unit to various function units. The function units include floating-point units, arithmetic logic units\u00a0(ALUs), tensor cores, transcendental and data type conversion units\u00a0(XUs), and load-store units\u00a0(LSUs). LSUs are responsible for transferring data between the register file and memory, while the other function units operate on values stored in registers. For fast on-chip memory, each SM also contains its own L1 cache and a scratchpad, called shared memory.\n\nA common way to program Nvidia GPUs with a parallel computing workload is to create a CUDA C++ program. Functions executed on Nvidia GPUs are called kernels. During the execution of a kernel, a massive number of threads execute the same logic specified in the kernel\u2019s CUDA C++ function definition. CUDA C++ well matches Nvidia GPUs\u2019 single instruction, multiple threads\u00a0(SIMT) execution model: At each time during execution, all threads that are being executed in an SM execute the same instruction. Threads within a CUDA kernel are organized into blocks, and each block is scheduled onto one SM, where it remains until all threads within the block finish executing. To hide latency, programmers typically aim for high occupancy, i.e., ensuring that each SM is assigned a large number of threads.\n\nThe Nvidia compiler, nvcc, compiles CUDA kernels into the PTX\u00a0(Parallel Thread Execution) intermediate language. The machine code executed by the GPU is in a proprietary instruction set architecture\u00a0(ISA), called SASS\u00a0(Streaming ASSembler). Translation from PTX to SASS can occur at compile time or runtime via the GPU driver.\n\nSECTION: 2.4The Python Language\n\nPeople in the deep learning community use Python extensively. Python is easy to use due to its simplicity, expressiveness, and powerful features. One of the key features of Python is its interpreter, so users do not need to compile their code before execution. Additionally, Python\u2019s dynamic typing, known as duck typing, frees programmers from declaring the type of each variable and allows variables to change types during execution, simplifying development. Python also features rich ecosystems with widely-used package managers, e.g., Python\u2019s built-in Pip, Anaconda, etc.\n\nTo illustrate how friendly Python is, we write a Python program to sort tuples in ListingLABEL:lst:sort_second_python. In contrast, ListingLABEL:lst:sort_second_cppshows the C++ program doing the same job. Both programs sort therecordsvariable according to each tuple\u2019s second element. Therecordsvariable stores the name and address of each person as a two-string tuple. The tuples will be sorted according to the second string, i.e., the addresses. Both programs execute three steps: 1)\u00a0defining therecordsvariable, 2)\u00a0sorting, and 3)\u00a0printing the sorted records.\n\nAs shown, the Python code is more expressive and quicker to develop due to several factors. First, it does not need compilation and an entry pointmain()function. Second, Python does not require the type declaration of each variable. Third, Python has a simpler lambda function syntax and supports containers in the built-inprint()function.\n\nOne of the biggest concerns of Python is the significant serialization penalty caused by the global interpreter lock\u00a0(GIL) in multithreading programs. As the most widely-used Python implementation, CPython[58]uses GIL to ensure thread safety. To mitigate this, frameworks work around GIL. One method is to put performance-critical logic in the C++ framework libraries and release the GIL once the control flow goes outside the Python code[59]. Another direction is to remove the GIL from the Python implementation. Although there are some alternate GIL-free Python implementations[60]to CPython, many frameworks rely on CPython-specific implementation details, making it challenging to migrate these frameworks to such alternatives. These limitations led to the Python Enhancement Proposal (PEP) 703 to make GIL optional[61]in CPython, which has been accepted recently.\n\nSECTION: 2.5The PyTorch Computing Stack\n\nThanks to its intuitive, dynamic, and flexible programming interface, PyTorch is friendly to both users and developers who build packages on top of PyTorch. As a result, PyTorch has gained much popularity since its inception.\n\nBy default, PyTorch executes code eagerly, meaning operations are computed immediately as they are called. For example, ListingLABEL:lst:pytorch_nested_moduledefines a simple classifier model with mean squared error\u00a0(MSE) as the loss function and executes a training step. The model contains a nested module,linear_with_activation, and a loss functionloss_fn().linear_with_activationis made up of a linear layer and a hyperbolic tangent activation function. Figure2.3illustrates both the forward-propagation and backward-propagation computational graphs for this example. As shown in step\u00a0(1) of ListingLABEL:lst:pytorch_nested_module, the modules are defined as subclasses oftorch.nn.Module. In the class definition, the initialization method__init__()initializes the parameters of layers and submodules. The methodforward()defines the forward propagation logic of this module. Step\u00a0(2) constructs the model, consisting of the nested modulelinear_with_activationand the MSE loss functionloss_fn. Step\u00a0(3) executes a training step, i.e., one forward propagation and one backward propagation pass.xis the input data,yis the predicted labels, andy_expectedis the ground-truth labels.\n\nNotice that users only need to define the forward propagation logic, as shown in FigureLABEL:lst:pytorch_nested_module. PyTorch\u2019s auto-differentiation mechanism handles the computation of gradients without requiring users to manually specify the backward propagation logic. During forward propagation, PyTorch records activations, i.e., the intermediate tensors, and weights required for backward propagation and constructs the corresponding computational graph. In Figure2.3, for example,yandy_expectedare the activations stored for the backward propagation process ofMSELoss,MSELossBackward. During backward propagation, PyTorch executes the backward-propagation computational graph. PyTorch calls precompiled kernels to execute operators in forward propagation and backward propagation.\n\nAnother way to provide auto differentiation uses just-in-time\u00a0(JIT) compilation. For example, to perform auto differentiation, JAX 1)\u00a0captures the forward propagation functions\u2019 IR through trace-based JIT, 2)\u00a0generates the gradient functions\u2019 IR via transformation, and 3)\u00a0compiles CUDA binaries using XLA[62]. Similarly, JIT-based approaches are adopted by PyTorch JIT[63], Mathematica[64], Zygote[65], CLAD[66], and Enzyme[67].\n\nFigure2.4shows the PyTorch computing stack for GNNs and LLMs, the primary workloads addressed in this dissertation. At the top of the stack are the GNN models and LLM models. Users can use distributed optimizers, e.g., DeepSpeed for LLMs and DGL for GNNs. Both single-GPU execution and distributed optimizers are built on top of PyTorch, although DGL also relies on its library for graph-related operations. At the bottom of the stack are the CUDA runtime and math libraries. The stack comprises five layers from the top to the bottom: (i)\u00a0Model definition specifies the model architecture and pre-trained parameters. (ii)\u00a0Distributed optimizers provide mechanisms for device-level parallelism and communication. (iii)\u00a0Python frameworks offer layer definition, dataloading, and profiling utilities. (iv)\u00a0The C++ runtime provides a GIL-free context, auto differentiation mechanism, and functionality of operator dispatching. (v)\u00a0Hardware-specific binaries provide the binaries to execute operators on devices. These binaries may leverage vendor-optimized libraries and provide support for new hardware, e.g., tensor cores on Nvidia GPUs. At this level, support for new operators can be added to the stack by creating new PyTorch extensions and registering them during runtime. PyTorch extensions usepybind11[68]to allow the new C++ code to interact with the Python runtime.\n\nSECTION: Chapter 3Hector: An Efficient GPU Programming and Compilation Framework for Relational Graph Neural Networks\n\nSECTION: 3.1Introduction\n\nGNN-specific machine learning frameworks, e.g., DGL[69]and PyTorch Geometric\u00a0(PyG)[70], are optimized specifically for homogeneous graphs.\nFor example, they implement several highly-optimized operations, e.g., sparse-dense matrix multiply\u00a0(SpMM) and sampled dense-dense matrix multiply\u00a0(SDDMM), to speed up the execution[71].\nMost of these operators and optimizations are for homogeneous graphs[72,73,71].\nHowever, real-world graphs are typically heterogeneous by nature and contain multiple types of nodes and edges.\nFor example, a citation graph may represent entities involving authors, articles, etc., as nodes of different types;\nthe edges may model various types of relations, e.g., an article citing the others.\nRecently, to incorporate the information provided by such heterogeneity,\nRGNNs[74,75]are proposed to define dedicated parameters and data paths for each type.\n\nRGNN poses three major challenges to the existing GPU computation stack due to its inherent computation patterns, the gap between the programming interface and the kernel APIs, and the high cost of kernel code optimizations due to its coupling with data layout and heterogeneity.\n\nThe first challenge with GNN implementations on GPUs stems from their need to traverse graphs and scatter/gather tensor data in order to use high-performance GEMM kernels to implement message passing.\nIn RGNN, message passing is the procedure in each layer where an edgewise operation is followed by a nodewise aggregation operation. In other words, messages are passed through edges to the destination nodes. We show how message passing works in models in Section3.2.1.\nDuring message passing, the graph structure and data layout significantly impact the memory access patterns and execution throughput[76,77].\u00a0(Examples and details are in Section3.3).\nFurthermore, as the connectivity of the input graph is involved in the gather computation, the computation patterns of GNNs are affected not only by the model definition but also by the graph. Such data-dependent behavior precludes any one-size-fits-all optimization strategy when executing GNNs. Additionally, RGNN introduces new complications into the design space due to the need for the operations to account for heterogeneity. We detail this in Section3.2.\n\nThe second challenge in RGNN implementation stems from the lack of an abstraction layer between the programming interface and kernel APIs, resulting in extra data movement. A typical example is an edgewise typed linear layer.\nWe detail the context and cause of the extra data movement in the edgewise typed linear layer in Section3.2.3.\nBut essentially, an edgewise typed linear layer multiplies one of the vectors on each edge with the layer weight dedicated to the edge type.\nTo achieve this, many existing PyTorch-based systems materialize a temporary three-dimensional edgewise weight tensor, where the slice corresponding to each edge is the weight matrix of its edge type.\nThis temporary weight tensor is huge, causing redundant data access and memory footprint.\nHector avoids such unnecessary copying activities by having typed linear transformations operate on views of tensors, a feature that PyTorch lacks, and decouples the materialization of its operands\nfrom the source-level expression\u00a0(Section3.3.2).\n\nThird, code generation is necessary.\nHigh-performance neural network implementations have historically been based on pre-built libraries, e.g., cuBLAS[78].\nGNNs make this less practical because the number of kernels to optimize is multiplied by the number of adjacency-matrix storage format choices such as Blocked-Ellpack[51].\nFor instance, cuSPARSE only supports the latter in a few APIs[79].\nThe typed edges and nodes of RGNN further exacerbate the problem, which makes the traditional pre-built libraries even less adequate and compels framework developers to either painstakingly develop optimized layers from scratch or settle for slow implementation.\nFor example, it took more than a month for a full-time engineer to implement and deploy the typed linear layer of RGNN in DGL[80].\nAnother consequence is the performance degradation caused by limited kernels due to high implementation costs. For example, the DGLHeteroConvoperator uses a Python native loop to separately launch kernels for each of the relation types in a heterogeneous graph, leading to serial execution of small GPU kernels that underutilize GPU resources on small graphs.\n\nTo systematically address these challenges, we propose Hector, a two-level IR and an associated code generator framework.\nThe higher-level IR, called inter-operator level IR, defines the model semantics as sets of operators and expresses layout choices in a decoupled manner. At the lower level, the intra-operator level IR provides the facility to express template specialization and lower them to CUDA kernels.\nWe further propose two optimizations, i.e., compact materialization\u00a0(Section3.3.2) and linear operator reordering\u00a0(Section3.3.2).\nWe show in the corresponding Sections how these two optimizations are conveniently enabled by the two-level IR design.Sections3.3.2,3.3.3and3.3.4further the design and rationale of the two-level IR.\n\nIn short, Hector 1)\u00a0represents the key properties of RGNN models to capture opportunities to reduce memory accesses in inter-operator scheduling and materialization, 2)\u00a0generates code flexibly with proper data access schemes to eliminate redundant data movement, and 3)\u00a0expresses model semantics, data layout, and operator-specific optimization in a decoupled manner to reduce programming effort. To the best of our knowledge, Hector is the first to propose a multi-level IR to capture RGNN-specific opportunities involving cross-relation inter-operator optimizations and tensor data layout with consideration of the type dimension added by RGNNs.\nThe contribution of Hector is as follows:\n\nWe propose the Hector two-level IR and code generation framework to systematically optimize and generate GPU kernels for RGNN training and inference. It bridges the gap between the programming interface and the kernel generation process, decouples models, data layout, and operator-specific schedule from each other, and leverages optimization opportunities from the three aspects.\n\nWe devised the Hector code generator based on two generalized CUDA templates, i.e., a GEMM template and a node and/or edge traversal template. The generated code achieves up to 9.9speed-up in inference and up to 43.7speed-up in training compared to the best among the state-of-the-art systems[81,82,83]when running RGCN, RGAT, and HGT[74,84,75]on heterogeneous datasets provided by DGL and OGB packages[85,86,87,88,89,90]. Hector also encountered fewer out-of-memory\u00a0(OOM) errors, which is significantly alleviated by the optimization mentioned in Contribution3.\nIn fact, with compaction enabled, Hector incurs no OOM error for all the datasets tested.\n\nWe devised two optimizations: compact tensor materialization and linear operator reordering.\nThe best combination of options varies across models and datasets and further obtains up to 3.8speed-up in inference and 2.7speed-up in training compared to our basic generated code mentioned in Contribution2.\nThrough profiling, we found that the improved memory efficiency allows Hector to accommodate larger computations and improve GPU hardware utilization for forward propagation. In contrast, backward propagation does not benefit from larger input due to its latency-bound nature caused by atomic updates and outer products.\n\nSECTION: 3.2Background and Motivation\n\nRGNNs extend GNNs to model different node and edge types for relational graph data.\nFor example, extended from GCN, a relational graph convolutional network\u00a0(RGCN) layer is defined as\n\n, wheredenotes neighbors of nodein relation,is the-th layer node representation of.is the weight for relation.is a problem-specific normalization factor.\nFigure3.1shows an example of how output features are produced in the message passing formulation equivalent to Formula3.1:\nThe forward propagation of an RGNN layer could be divided into \u2460 the edge message generation stage and \u2461 the node aggregation stage.\nFor simplicity, we focus on the output featureof node: To obtain, \u2460 a messageis generated for each incoming edge, and \u2461 the edge messages go through weighted aggregation and an activation functionto produce.\nNotably, to obtain the output feature of node, the input feature ofitself is applied to theand added to the transformed neighbor features. We call this a virtual self-loop because it could be seen as if each node now has a new edge to itself.\n\nRelational graph attention network\u00a0(RGAT)[84]and heterogeneous graph transformer\u00a0(HGT)[75]are shown in Figure3.2. Attention is introduced in these more complex models:\nAttention is produced in the message generation stage together with edge messages.\nSimilar to the normalization factor, it is a scalar that emphasizes the message associated with the same edge during the subsequent node aggregation stage. However, attention is learned, as it is produced by operations among weights and features.\n\nIn addition to describing GNNs in two stages\u2014message generation and node aggregation\u2014a popular formulation uses the SpMM and SDDMM pair. The DGL[69]paper has proven that GNN message passing can be expressed as generalized SpMM\u00a0(g-SpMM) and generalized SDDMM\u00a0(g-SDDMM) operations, with their backward propagation also following the same structure. SpMM computes the product of two matrices,, where the left matrixis sparse and in a sparse matrix format. The right matrixis dense. Notice that each row inis a weighted aggregation of specific rows inaccording to:\n\nwhereis the vector of\u2019s-th row, andis the vector of\u2019s-th row. g-SpMM generalizes SpMM in three ways: (1)\u00a0the scalaris generalized to data corresponding to the edge, (2)\u00a0the product operatoris generalized to a message function that produces a vector after taking as input the data of the edgeand thevector of node, and (3)\u00a0the summation operatoris generalized to a custom aggregation function.\n\nSDDMM selectively computes the product of two dense matrices based on a sparse matrix:\n\nwhereis the vector of\u2019s-th row,is the vector of\u2019s-th column, andis the sparse matrix. g-SDDMM generalizes SDDMM in two ways: (1)\u00a0the scalaris generalized to data corresponding to the edgeand (2)\u00a0the two product operatorsare generalized to one message function that produces a vector after taking as input the data of the edge, thevector of node, and thevector of node.\n\nIn non-graph neural networks,\nmost linear operators, e.g., convolution, can be efficiently implemented with GEMM kernels.\nGEMM takes up most of the execution time due to its cubic complexity.\nWhile some operators can be optimized by transformations, e.g., Winograd for convolution layers[91], the operators are still computation-intensive after such computation reduction.\nGPUs are excellent at GEMM because the latter\u2019s high computation complexity allows leveraging the massive parallel compute units on GPUs. At the same time, the input data could be sufficiently reused to allow the memory bandwidth to keep up with the computation throughput.\n\nIn contrast, GNNs spend a much larger portion of their execution time on memory-intensive, non-GEMM operations[76,77]. One major source of memory-intensiveness is the sparsity of graphs: to be not bound by the memory bandwidth, Nvidia H100 GPU requires the data reuse of single-precision float to be at least 16 times. However, the average degree of a graph often falls below this threshold, e.g., the graph datasets in Table3.3. The heterogeneity of RGNNs further exacerbates the issue due to lowered data reuse by the introduction of dedicated weights to different edge types and node types, as shown in Figure3.2.\n\nWe use an edgewise typed linear layer as an example to walk through the various performance overheads in the existing computation stack, as summarized in Figure3.4.\nEdgewise typed linear layer applies a typed linear operator on each edge to one of its vectors. The weight of the linear operator used in the computation depends on each edge\u2019s type. For example, the edge message in an RGCN layer\u00a0(Figure3.1) or an RGAT layer\u00a0(Figure3.2), is produced by a typed linear layer.\n\nA typed linear layer is typically implemented using batched matrix multiply\u00a0(BMM) or segment matrix multiply\u00a0(segment MM)[92].\nFor example, PyGFastRGCNConvimplemented typed linear layers using BMM to unleash parallelism. However, a temporary tensor must be created from the weight tensor due to the lack of support for indirect addressing by PyTorch tensor APIs: the typed linear layer could be denoted aswhere,andare input feature, output feature of nodeand the weight of node\u2019s type. The middle dimension ofandare needed to make the operation a matrix multiply. However, there is currently no support for specifyingas one of the arguments to an operator in PyTorch;\none must createbefore the typed linear layer.\n\nSegment MM requires presorting features by types. Then, the node/edge feature tensor is in the form of segments of features of the same type: the segment MM kernel then applies the corresponding weight tensor of the type to each segment.\nIf neither BMM nor segment MM can be employed, one may fall back to multiple matrix multiplies, leading to higher device API overhead and GPU under-utilization.\n\nAnother type of inefficiency is suboptimal math library calls. PyTorch has routines to handle various scenarios, e.g., a tensor is strided in memory layout or isNestedTensor, a pack of tensors. Consequently, PyTorch sometimes performs BMM by launching multiple general matrix-vector multiplies\u00a0(GEMVs) kernels, which also leads to API overhead and GPU under-utilization.\n\nLastly, CUDA math libraries were initially developed for large inputs and may not be efficient for small inputs[78].\n\nTo better illustrate the points, Figure3.3breaks down HGT and RGAT inference time on FB15k and MUTAG.\nSection3.4.1details the system configurations and datasets.\nThis experiment measured Graphiler[82], which executed compiled TorchScript code and delivered the best inference performance among the existing systems tested in Hector.\nFigure3.3shows that indexing and copying take up a significant portion, and the portion of GEMM operations, i.e., MM vs. Other compute, varied with graphs.\nBy profiling, we found that the CUDA API overhead is 22% of the time of the critical path, which is the sum of the API overhead and kernel duration. This is partly due to a huge number of kernel launches caused by 1)\u00a0libraries calling a series of kernels to fulfill an API invocation and 2)\u00a0some operators calling separate sets of kernels for each types in the graph.\n\nIn contrast, Hector 1)lowers more of the logic to GEMM,\nand 2)\u00a0assembles kernels with flexible access schemes togather and scatter data on the flyto eliminate redundant data movement. Consequently, Hector does not replicate weights during computation. As shown,this strategy achieves better performance than using hand-optimized kernels with dedicated functions to data movement, e.g., in Graphiler.\n\nTo address the performance challenges in RGNN systems due to both RGNN\u2019s inherent computation pattern and the system design, we propose the Hector IR and code generation framework. By the IR design thatdecouplesandexpressesthe model semantics, data layout, and operator-specific schedules, Hector opens up these opportunities and the integration of all three aspects into the design space.\nTable3.1shows the feature comparison of Hector with existing systems.\n\nGraphiler\n\nSeastar\n\nHGL\n\nHector\n\nSECTION: 3.3Design and Implementation\n\nHector consists of a programming interface, a code generator, and Python modules.\nThe code generator takes in the model definition and generates both CUDA kernels and host functions that configure and invoke the CUDA kernels.\n\nFigure3.5uses an example to illustrate the workflow. The input is an excerpt of DGL code invoking a typed linear layer on the input node features. Applying the@hector.compiledecorator triggers a transpiling pass to lower the code into Hector inter-operator level IR. In this example, the typed linear transformationtyped_linearcan be efficiently implemented as GEMM kernels. To this end, Hector lowers the transform to an operator instance derived from the GEMM template at the inter-operator level. After the analysis and optimizations at the inter-operator level, Hector further lowers the code to a detailed GEMM specification at the intra-operator level. The GEMM outputcollects edge data generated from the node data. The first inputis the weight matrix, and the second inputis the collection of features of all the source nodes of the edges involved. The intra-operator level IR indicates that the GEMM operation should use the default tile width of 16 and be carried out without scatter, gather, or transpose applied to input or output matrices. Eventually, Hector generates a segment MM\u00a0(Section3.2.3) kernel,gemm_1.\nThe Layout Choices section of Figure3.5shows the default layout choice.etype_ptrspecifies the offsets of each segment of different type.row_idxis the source node index array in the COO format. The result tensore[\"msg\"]has the number of edges as the number of rows, and the number of the columns is the input dimension of the hidden layer. We detail in Section3.3.2an optimization technique, compact materialization, that is opened up by the decoupled layout choices from the inter-operator level IR.\n\nThe generated code is compiled into a shared library where host functions are exported through thepybind11utilities.\nHector falls back to existing routines in PyTorch when certain operators are not yet supported.\nDuring runtime, the precompiled functions are loaded and registered as subclasses of PyTorchautograd.Function.\n\nThe inter-operator level IR follows the Python grammar but involves some new constructs, as listed in Table3.2. ListingLABEL:lst:ir_exampleillustrates how the attention calculation in a single-headed RGAT layer could be expressed using the inter-operator level IR.\nLines 10-16 shows a code segment that generates attention values for all edges of graphgand then invoke theedge_softmax(g)function that spans lines 1 through 9. As shown in ListingLABEL:lst:ir_example, the message generation and aggregation stages are expressed as for-each edge loops starting from line\u00a02, line\u00a08, and line\u00a010, and for-each node loop starting from line\u00a04. To accumulate data from the incoming edges of each node n, then.incoming_edges()iterator is used. Notably, the data layout that specifies how to access the input and output data per edge or node as well as the incoming edges associated with each node, is abstracted away in ListingLABEL:lst:ir_example.\n\nHector provides a decorator,@hector.compile, to take the existing PyG or DGL forward propagation logic and generate code for it, as exemplified by the input in Figure3.5. The decorator, when applied to a method, invokes a simple transpiling pass that replaces the PyG and DGL method calls, e.g., SpMM/SDDMM, with an implementation in the inter-operator level IR, and replaces supported constructs from PyG and DGL with expressions in Hector IR.\nSimilarly to statically-typed compilers in other Python packages[93,94], the function to compile can use most of the Python features except dynamic ones, e.g., assigning objects of different types to the same variable. We support a few types as the function arguments for heterogeneous graphs, involvingTensoranddict[str, Tensor]objects, i.e.,dictobjects where the keys arestrobjects and the values areTensorobjects.\n\nBesides, one can use the Hector inter-operator level IR itself to express the model, as exemplified by ListingLABEL:lst:ir_example.\n\nThe Hector inter-operator level IR deliberately abstracts away the data layout from the model semantics. As exemplified by ListingLABEL:lst:ir_example, the IR only expresses the association of variables with nodes or edges, e.g.,e[\"att\"]andn[\"att_sum\"], without dictating the mapping of elements in the conceptual variable to the memory space.\n\nIn Hector, we devised compact materialization, which is a technique enabled by the decoupling between model semantics and data layout.\nNote that certain edge data are determined by sparse combinations of source node features and edge types, e.g.in Figure3.2. Rather than computing and storing such data for each edge, we instead compute and store the data once for eachpair that actually exists, reducing the resources spent on computing and storing common subexpressions.\nAs exemplified in Figure3.7, the materialized tensor involves seven rows when each row vector corresponds to amsgof an edge.\nAlternatively, the system can materialize the tensor with only five rows, where each row vector corresponds to amsgof anpair.\nWe call the former vanilla materialization and the latter compact materialization.\nFor the vanilla scheme, the row number is the edge index specified by the sparse adjacency. For the compact scheme, it is a unique non-negative integer assigned to each. We precompute this mapping and store it in a CSR-like format. Hector does not create the temporary weight tensor, as explained in Section3.2.3.\nIn summary, compact materialization is a technique to eliminate repetitive identical computations and results in edgewise operators. It is applicable when an edgewise operator depends only on the source node data and edge type, and its output has the shape of. After this optimization, the output shape is reduced to, and repetitive computation is eliminated.\nSection3.4.3provided further analysis of the effects of compact materialization on memory footprint reduction.\n\nBesides tensor materialization, the multi-level IR design also allows data layout optimizations involving 1)\u00a0architecture-specific optimizations, e.g., padding, and 2)\u00a0various sparse adjacency encoding.\nAt the inter-operator level, data layout specifications are decoupled from the model semantics and do not influence the transform passes at this level.\nHowever, they determine the data access scheme and make a difference when generating CUDA code at the intra-operator level.\nHector inter-operator level IR bookkeeps the specifications, which are passed to the intra-operator level during lowering.\nThe intra-operator level operator instances choose the data access scheme corresponding to the data layout specifications while assembling the kernel code.\nWe leave the exploration of data layout optimizations to future work and detail our plan in Section3.6.\n\nLinear operator reordering is an inter-operator level optimization. When a linear operator, e.g., linear layer and dot product, is followed by another linear operator, their order may be switched.\nFor example, forattsas shown in Figure3.8(c), we may calculatefirst instead. Its profitability can be determined by counting the number of multiplication and addition involved in the two GEMMs before and after the order is changed. For now, we implement the pass to switch the orders of two linear operators whenever this produces an operator between weights, because it reduces the complexity by reducing one of its factors, the number of nodes/edges, to the size of hidden dimension. For simplicity, rewritten operator instances use PyTorch BMM to compute the product of weights and apply PyTorch slicing when necessary.\n\nLoop transformation at this level is augmented with the graph-semantic-specific equivalence rule: a for-each loop over the edges is equivalent to a for-each loop nest iterating over all the incoming/outgoing edges of all destination or source node. Loop transformation is applied during the lowering pass to canonicalize and fuse loops in order to more thoroughly identify kernel fusion opportunities.\n\nTo lower the IR to the intra-operator level, Hector greedily lowers every eligible operator to instances derived from GEMM templates\u00a0(Section3.3.3). Then, it fuses each remaining region and lower them to as few traversal instances\u00a0(Section3.3.3) as possible.\nTo achieve this, Hector scans the code three times. Each time, it attempts to lower operators to instances of a specific preference level. During the first pass, it attempts to lower operators to GEMM-template-derived instances. In the next pass, it attempts the traversal-template-derived instances. The third pass will lower all the remaining operators to PyTorch function calls.\nDuring each pass, whenever an operator can be lowered, Hector marks the operator itself, together with all subsequent operators that can be fused into it, with the lowering decision.\nAfter all the operators have been examined in a pass, the marked operators are lowered and fused. Before the second pass, it canonicalizes the for loops and fuses loop nests whenever possible to discover kernel fusion opportunities.\n\nThe intra-operator level IR serves between the inter-operator level IR and the generated CUDA code. At this level, the IR should encode specifications to emit CUDA code and provide sufficient information specific to each operator invocation to the transform and lowering passes at the inter-operator level.\nThe code transformation components at this level provide the methods to generate specialized CUDA code for the operators, to apply operator-specific schedules, and to return necessary information on operator selection and kernel fusion feasibility to the passes at the inter-operator level.\n\nHector\u2019s code generator ultimately lowers the IR to two basic constructs, the GEMM template and the traversal template.\nAlgorithms3.1and3.2illustrate the edge traversal template and the GEMM template.\nThe node traversal template is similar to Algorithm3.2, and we will revisit it in Section3.3.4.\nFor simplicity, function template specialization refers to routines specialized for the specific instances derived from the two templates and involve 1)\u00a0function arguments, e.g., number of rows, etc., 2)\u00a0special registers, e.g.,threadIdx, and 3)\u00a0loop variables.\n\nWe base the code generation on GEMM and traversal templates because RGNNs involve not only sparse operations but also multiple dense operations to project vectors across different semantic spaces.\nThe GEMM template serves edgewise and nodewise linear transformations, as exemplified by the computation of RGAT edge messages in Figure3.7. The GEMM template is defined as a matrix multiply augmented with custom gather and scatter schemes. It is formulated aswhere,,are output, input, and weights, respectively;,, andare scatter list, gather list, and the type of the nodes or edges, respectively.\nThe traversal template performs generic nodewise or edgewise operations. It serves operators that cannot be lowered to GEMM templates, e.g., edgewise dot products.\n\nAs shown in Algorithm3.1, the GEMM template is based on tiled matrix multiplication. The GEMM template starts with the work assignment per block during theGetRange<kid>subroutine\u00a0(line 1).\nTheidxTileRowandidxTileColwhose range is determined byGetRange<kid>is used to position the workload.\nTypically, it is the coordinate of the tile of the output matrix.\nFactors that affect\u2019s loading scheme,LoadXToShmemIfInRange<kid>, and\u2019s,LoadWToShmemOrRegistersIfInRange<kid>, involve whether gather lists or transpose needs to be applied on the fly\u00a0(lines 4-5).\nGather listin the Input section is sometimes needed to locate the rows in the source matrix: For example, in Figure3.7(a),row_idxis needed in step \u2460.\nThe required information will be passed during the lowering.\nThe operator instance then accordingly chooses the data access scheme code piece for kernel code generation.\nThe storing schemeStoreCIfInRange<kid>depends similarly on whether a scatter list will be applied.\nAtomic intrinsics are used in the case of multiple simultaneous updaters.\n\nIn the traversal template, as shown in\nAlgorithm3.2, the edge type, node indices retrieval scheme in lines\u00a05-7 depend on the sparse adjacency encoding.\nSimilarly to the GEMM template, when a row vector needs to be loaded or stored, the tensor materialization scheme determines how the row is located in the materialized tensor.\nAll statements are initially inserted into the innermost loop.\nAfter Hector finishes the loop transformations, it then defines work assignment on line\u00a01 in Algorithm3.2for the operator instance derived from the traversal template using a simple scheme. For example, if the loop nest is three levels, as exemplified by Algorithm3.2, we assign the outermost loop, i.e.,idxEdgeoridxNodeloop, to each thread block and the two inner loops to the multi-dimensional threads in each block.\n\nAt the intra-operator level, the templates work for any sparse adjacency encoding as long as specific interfaces are implemented. For example, the edge traversal shown in Algorithm3.2works as long as the function template specializationGetEType<kid>,GetSrcId<kid>, andGetDstId<kid>are implemented: If the sparse adjacency is COO,GetSrcId<kid>is a subscript operator applied to the row indices array. If it is CSR, thenGetSrcId<kid>is a binary search in the row pointer array.\n\nCentral to the code generator is the two-level IR.\nInter-operator level IR optimizations address the opportunities brought in by heterogeneous relation types. These optimizations manipulate operators and their connections. A high-level IR abstracts away the low-level details that can complicate or even hinder the transformations.\nIntra-operator level IR optimizations reduce the data movement by generating access schemes in kernels rather than using specialized kernels and dedicated indexing/copying kernels. These optimizations manipulate low-level data access and schedule details, and thus are better supported by a low-level IR.\n\nThe two-level IR enables concerted but decoupled choices of intermediate data layout and compute schedules.\nFor example, in Figure3.5, the semantics of the model are decoupled from the layout choices.\nHector implements the model semantics and layout choices in intra-operator level IR with specific access schemes.\nThe next few paragraphs explain how the two-level IR design facilitates operator-specific optimizations, operator selection, and kernel fusion.\n\nEach instance derived from the GEMM template provides the option to apply a coarsening factor in, to choose the tile size, and to apply__launch_bounds__that limits the number of registers in exchange for more active warps.\nThe coarsening factor is the number of elements each thread deals with in the loading, computing, and storing stages. When applied, each block still works on the same assignment, but its number of threads shrinks by the factor[51].\nWe also allow a per-row scalar to be applied to the tiles of matrix.\nThis eliminates the extra memory-intensive traversal to perform weighted vector summation by attention or norm.\n\nAs for the traversal template, similarly to the discussion in Section3.3.2, we incorporate graph-semantic-aware loop transformation rules that allow Hector to leverage graph semantics to open up the trade-off between more data reuse opportunities and greater parallelism. As mentioned in Section3.3.3, initially, all statements are in the innermost loop in each instance derived from the traversal template.\nLoop hoisting is performed to enhance data reuse: The template features insertion points before and after the end of each loop level. For each statement, Hector finds the outermost level where it can be placed before applying the template.\nIn addition, the template also provides a partial result aggregation method, which is applied during lowering by default, to reduce global memory traffic by accumulating results within a thread and within a warp before atomically adding them to the data in global memory.\n\nTransformation and lowering passes at the inter-operator level need information about operator instances, specifically operator preference and the feasibility of kernel fusion.\nPreference level is the mechanism Hector uses to select the operator instance when there are multiple candidates. For example, an operator instance derived from the GEMM template may have an alternative derived from the traversal template but the alternative would lead to lower performance due to much lower data reuse.\nFor good performance, operator instances derived from the GEMM template are assigned a higher preference level than those derived from the traversal template unless otherwise specified. Instances that fall back to PyTorch have the lowest preference level.\n\nOperator instances also provide methods to determine the feasible operators to be fused within the IR. Operator instances derived from the GEMM template can be fused with the consumer if 1)\u00a0the latter multiplies the row vectors in the GEMM output with scalars and 2)\u00a0the two operators are in the same loop\u00a0(nest). Operator instances derived from the traversal template can be fused with each other as long as they are in the same loop\u00a0(nest).\nIf the inter-operator level pass finds that some temporary variables are created and merely used inside the fused operator, it passes that knowledge to the method so that the variable no longer needs to be created in the global memory.\n\nSimilarly to PyTorch, Hector supports auto-differentiation by maintaining the backward propagation counterparts of the operators.\nHector first emits the backward propagation via inter-operator level IR and removes unused gradients and their computation.\nThe lowering and code generation schemes are similar to those in forward propagation.\nHowever, additional processing is needed because the PyTorch auto-differentiation requires the backward propagation methods to be paired with the forward propagation methods in theautograd.Functiondefinitions. To achieve this, Hector bookkeeps the kernel calls in each forward propagation method. For each forward propagation method, Hector puts all the corresponding backward propagation kernel calls in the body of the backward propagation method.\n\nThe code generation procedure emits code based on the CUDA kernel specifications detailed in the form of intra-operator IR. Kernel code generation is fairly straightforward and is implemented using a template-based approach.\nHector then emits the host functions that configure grids and blocks, gets raw pointers from thelibtorchat::Tensorreferences, and launches the corresponding kernel. The host functions are exported viapybind11utilities.\n\nThe Hector performs a pass that scans all the functions generated to collect a list of preprocessing required for the input dataset, involving transposition, converting COO to CSR, etc. The code generator then emits the preprocessing code.\n\nLinear operator reordering and compact materialization are specific to RGNNs. Linear operator reordering is specific to RGNNs because RGNNs typically require linear projections from different semantic spaces, introduced by the heterogeneity of node types and edge types, to a common space before further operations.\nCompact materialization is specific to RGNNs because of the additional tensor dimension brought in by different node types and edge types.\n\nSome of the intra-operator IR optimizations could benefit ordinary GNNs, which can be treated as a special case of RGNNs whose relation type number is one. Intra-operator level IR allows specification of both data access schemes and schedules, thus allowing flexible code generation to accommodate different dense or sparse tensor layouts, a need that often arises from compact materialization. However, the ability to generate code for different data access schemes and schedules can be beneficial when compiling ordinary GNNs.\n\nSECTION: 3.4Evaluation\n\nWe evaluate Hector with the following questions to answer.\n\nHow does the performance of Hector compare with state-of-the-art systems? How does Hector achieve it?\n\nHow much improvement do the two optimizations detailed inSections3.3.2and3.3.2, compaction materialization and linear operator reordering, make?\n\nAny architectural insights for GPU for RGNNs?\n\nSection3.4.2answers Q1.Section3.4.3answers Q2 and further analyzes the performance implications of the two optimizations through a case study.Section3.4.4addresses Q3.\n\nTo assess performance, we measure the inference and training time of Hector and other systems on a single-GPU computer. Its hardware components include one Intel Core i9-9900K CPU, 128 GB dual-channel memory, and one Nvidia RTX 3090 GPU with 24 GB memory. The operating system is Ubuntu 18.04.5, with kernel version 5.4.0-135. The CUDA and driver versions are 12.1 and 530.30.02, respectively. PyTorch and DGL versions are 2.0.1 and 1.1.1, respectively.\n\nAs shown in Table3.3, we use public datasets from DGL[69]and OGB[85].\nWe measure (1)\u00a0inference and (2)\u00a0training time on three RGNN models, RGCN[74], RGAT[84], and HGT[75], comparing with previous systems, involving DGL[69], PyG[70], Seastar[81], Graphiler[82], and HGL[83].\nWe ported Seastar artifacts to the same version of CUDA and Python packages as Hector depends on because one of Seastar\u2019s dependencies, dgl 0.4, used an API deprecated since CUDA\u00a011.\n\nFor RGCN, RGAT, and HGT, excluding comments, Hector took in 51 lines in total and produced more than 3K lines of CUDA kernel code, 5K lines of other C++ code to define host functions, and 2K lines of Python code to define subclasses of PyTorchautograd.Function. The implementation also involves 2K lines of Python code providing common utilities.\n\nTo best align with the hyper-parameters prior work used in its evaluation, we set the input and output feature dimensions as 64 and the number of heads as 1.\nWe measure the inference and training time of the single layer used.\nIn training, to obtain a loss, we compute the negative log-likelihood loss by comparing the output with a precomputed random label tensor.\nFor each case, we run the full graph inference and training for at least 10 epochs and average the elapsed time.\nTo align with the existing system, nodes are presorted to enable segment MM for typed linear layers.\n\nFor the performance of DGL and PyG, we measure all public implementations of these models from DGL, PyG, and Graphiler artifacts.\nPyG provides two RGCN convolution layers:RGCNConvplaces nodes in segments of the same type but launches separate kernels for each of the node types, leading to device underutilization.FastRGCNConvreplicates weights and usesbmm(). It is consistently faster than theRGCNConvimplementation.\nSimilarly, DGL\u2019s built-in segmentMM-based RGCN layer is faster than other DGL implementations.\nFor HGT, the DGL segmentMM-basedHGTConvprimitive generally has the best performance.\nIn the cases where some variants encounter OOM errors, we choose the best among those that run without issues.\nSome cases are missing due to insufficient operator support, such as HGL on HGT and Graphiler on training. We do not measure HGL in inference because it is designed to optimize training.\n\nFigure3.9shows that Hector\u2019s best-optimized code consistently outperforms state-of-the-art systems. It achieves up to 9.9speed-up in inference and up to 43.7speed-up in training against the best state-of-the-art systems. On geometric average, Hector gets 1.79, 8.56, 2.87speed-up in inference via RGCN, RGAT, and HGT, respectively, and 2.59, 11.34, 8.02speed-up in training RGCN, RGAT, and HGT, respectively. The performance advantage is larger in small graphs, demonstrating thatgenerating a single kernel that performs the computation across multiple edge types boosts the performance on small graphs compared to existing systems that run many small kernels.\n\nWe see close performance achieved by Graphiler in RGCN and HGT inference. Graphiler leverages PyTorch utilities to produce TorchScript binaries before execution and utilizes edgewise parallelism for edgewise computation. Similarly toRGCNConv, it places node features into segments of the same type but runs separate kernels to perform a typed linear transformation. DGL and PyG, under similar configurations, achieve competitive performance. However, when it comes to RGAT, Graphiler suffers from performance degradation. Because Graphiler relies on pre-programmed fused kernels to deliver a significant portion of the performance boost[82], we postulate that the degradation is due to the non-exhaustiveness of these pre-programmed kernels[95].\nThis reflects the drawbacks of compiler design without a code generation mechanism. By contrast, with two-level IR and a code generator, Hector achieves better performance, showing thatgenerating kernels with flexible access scheme that gather and scatter data on the fly eliminates redundant data movement and outperforms indexing/copying followed by hand-optimized GEMM and sparse kernels. Besides, it is challenging to extend Graphiler\u2019s approach to training due to TorchScript\u2019s limited auto-differentiation support. For example,dictobject creation is not supported, but it is a common way to express nodewise and edgewise data.\n\nBy comparing Hector with Seastar, which lowers all logic to sparse kernels, we realize thatsparse kernel code generation alone is not efficient in RGNNs: it is better to lower to GEMM kernels as much as possible.\n\nThere are two reasons why Hector is more efficient in device memory usage. First, Hector only keeps a single copy of weights, as discussed in Section3.3.2. Replicating weights also affects backward propagation because the gradient of each copy will be derived, occupying extra memory. Second, our compact materialization reduces memory and redundant computation, as explained in Section3.4.3.\n\nNotably, even without compact materialization or linear operator reordering, Hector still consistently outperforms existing systems, as Table3.4shows. In addition, the unoptimized Hector code triggers fewer OOMs than existing systems, with the only exception where the RGAT inference is run on mag and wikikg2.\nFor comparison, we also show the statistics of the best optimized Hector code in Table3.4.\n\nNow, we study the effects of compact materialization and linear operator reordering. They are detailed inSections3.3.2and3.3.2.\nWe investigate their effects on RGAT and HGT.\n\nTable3.5shows the speed-up on top of Hector unoptimized code by these two optimizations. Due to compact materialization, Hector no longer triggers OOM errors when running RGAT on mag and wikikg2. In addition, in some cases, the layout speeds up the execution due to the common subexpression elimination brought forth by the layout. Compact materialization is hardly possible without a code generation scheme or an IR design that decouples the model semantics, data layout, and operator-specific schedule.\nBesides,data layout choice, compact materialization, in particular, allows further performance enhancementwhile prior work usually focuses on improving the schedule given a specific sparse matrix format. This is shown by the significant speed-ups in the \u201cC[ompact]\u201d columns in Table3.5.\n\n*Normalized by the performance with compact materialization\u00a0(C) because the unoptimized version triggers OOM errors.\n\nTo study how compact materialization reduces the memory footprint, we illustrate the Hector DRAM usage without compact materialization in Figure3.10(b) and the portion of DRAM usage with compact materialization in Figure3.10(a). For simplicity, we define the entity compaction ratio as the number of uniquepairs divided by the number of edges. Figure3.10(b) shows that the memory use of inference and training is highly proportional to the number of edges of the datasets. Figure3.10(a) shows that compact materialization significantly reduces DRAM usage in all datasets. The memory footprint ratio of compact materialization compared with the memory footprint of the unoptimized code correlates with the entity compaction ratio. The memory footprint ratio is higher than the entity compaction ratio, as the memory footprint consists of edgewise data, nodewise data, and weights, whereas the compaction applies to edgewise data only. Besides, in case the average degrees are larger, the memory footprint ratio reduces more significantly, getting closer to the entity compaction ratio.\n\nTo better understand the performance benefits of optimizations, Figure3.11studies two cases.\nThe entity compaction ratio of AM and FB15k are 57% and 26%, respectively. On AM, the time GEMM instances take is greatly reduced. By comparison, in FB15k, compaction brings less performance improvement due to the less significant GEMM reduction.\n\nIn short,due to the data-dependent nature of computation in RGNNs, there is no one-size-fits-all optimization strategy. However, as shown in Table3.5, enabling compaction and reordering obtains fairly good performance consistently and is the best fixed strategy on average in all four scenarios, i.e.,. If Hector presumably chooses the best configuration in every run, it could further get 1.06, 1.33, 1.02, and 1.08speed-up in the four scenarios above, respectively. We leave autotuning to future work.\n\nWe show the average time of unoptimized Hector in Figure3.12. We also profile generated kernels when running Hector on RGAT on bgs and am, as shown in Figure3.13.\n\nOne thing to note is the sublinear time increase in Figure3.12: when the input and output dimension doubles, the amount of computation and memory accesses becomes close to 4those of the original, but the time increase is typically lower than 2of the original. The reason is increased computation throughput when the size increases, as corroborated by Figure3.13. Moreover, we observed higher throughput when the graph scale increases, e.g., from bgs to am in Figure3.13. Similarly, we witnessed the cuBLAS throughput increases steadily when we keep the right matrix size as (64, 64) and increase the number of rows of the left matrix from 1M\u00a0(217) to 8M\u00a0(220). These suggest thatan RGNN system should be memory-efficient in order to accommodate larger models and datasets to fully utilize the massive resources on GPUs. By eliminating unnecessary data copies, Hector achieves better memory efficiency than state-of-the-art systems.\n\nThe instruction per cycle\u00a0(IPC) charts in Figure3.13indicate the traversal kernels are generally latency-bound: on RTX 3090, IPC is ideally four as each SM has four schedulers. Backward propagation kernels have lower throughput due to worsened latency and increased memory bandwidth consumption by doubled memory accesses compared to forward propagation. In backward propagation, backward traversal kernels compute gradients using atomic updates, therefore hindering the throughput; GEMM kernels also, on average, have lower performance due to outer products that compute the delta of weights.\n\nSECTION: 3.5Related Work\n\nGeneral GPU-accelerated GNN libraries.DGL[69]and PyG[70]are among the most popular GNN Python packages that enable easy development and evaluation of GNN models. DGL[69]proposes to implement GNN as SpMM/SDDMM operations. PyG\u2019s key scheme is scatter and gather operations that switch between edge-parallel regions and node-parallel regions. Hector instead built upon GEMM and traversal templates. By lowering the operators to GEMM as much as possible, Hector obtains better RGNN performance.\nBesides, DGL, PyG, and work based on them do not currently provide inter-operator level IR. Hector shows the benefit of capturing\ninter-operator and inter-relation opportunities, e.g., linear-operator reordering, by operator rewrite at the inter-operator level IR. Systems without IR at this level eagerly execute operators without support for such optimizations.\n\nGNN end-to-end compilers.Seastar[81]proposes a vertex-centric compiler stack to generate performant kernels throughout the model\u2019s training and/or inference. Graphiler[82]proposes to program the message passing data flow graph and devises several TorchScript transforms to emit highly optimized inference code. Similarly, HGL[83]is an RGNN compiler. These prior arts 1)\u00a0expose PyTorch tensors as operands of all operations to users and 2)\u00a0replicate weight to unleash parallelism due to a lack of support for flexible data access schemes and/or code generation. Thus, they suffer more or less from memory inefficiency and performance degradation.\nAlthough the general concept of multi-level IR is not new, Hector proposes new optimizations appropriate for each level and effective in reducing data movement and code bloat in the current state of practice:\nLinear operator reordering and compact materialization are two key and novel features to capture and eliminate repetitive computation across edge types. Section3.3.7discussed the generalizability of Hector.\n\nKernel code optimization.FeatGraph[71]proposes a code optimization framework on top of TVM[96]for user-defined-function-enabled SpMM and SDDMM. Some work proposed optimizations for specific GNN kernels. GE-SpMM[72,97], and work[98]propose optimized schedules for SpMM. Others involve Seastar[81], PyTorch-Direct[16], and TLPGNN[99]. As Hector shows, SpMM/SDDMM is not the only essential kernel in end-to-end RGNN execution. Hector is orthogonal to these prior arts as they can be incorporated into Hector as operator-specific schedules or new templates.\n\nCode generation.SparseTIR[73]and TACO[100]propose IR and code generator for sparse tensor operations.\nMLIR[101]proposes multi-level IR design for deep learning.\nAligned with this direction, FusedMM[102]unifies the SpMM and SDDMM CPU kernels.\nHector is different as a higher-level compiler that optimizes the type of operators and generates efficient kernels to handle multiple edge/node types in the RGNN execution. SparseTIR and TACO are tensor-level compilers for sparse operators that may or may not specialize in deep learning.\nWhile we do not intend to reinvent the general-purpose sparse tensor code generator for completeness or performance, some of these works inspire us. They may be incorporated to enhance the Hector code generator.\n\nSECTION: 3.6Discussion on Extensibility\n\nHector is designed as an extensible framework to prototype and evaluate new techniques. First, inter-operator optimizations can be prototyped as inter-operator level passes. Second, data layout optimizations can be supported by adding the corresponding intermediate data and adjacency access schemes discussed in Section3.3.2.\nThird, kernel optimizations can be prototyped as a kernel template and operator instances based on it. Alternatively, they can be implemented as operator-specific schedules.\n\nTable3.6shows how the proposed compiler could be extended to support common kernel optimizations from the high-performance computing community. Each row in the table shows an example of the new feature to support in bold, followed by an approach to add support to it in our system. For example, to enable row reordering to balance the load, we can add a schedule option at the intra-operator level such that, when enabled, the compiler remaps the row loop index to the row index.\n\nWe focused Hector on single-GPU performance. The kernels Hector generated could serve distributed systems, e.g., DistDGL[111]. Since performance improvement results from the reduction of data movements and memory footprints, it also applies to distributed systems.\n\nIn Hector, we craft the code generators on our own for quick prototyping and focus on high-level optimizations. As Hector establishes our understanding of what constructs are needed in the code generators for the traversal kernels, we think it viable to incorporate TACO for the code generation in the future because TACO provides a mature compiler infrastructure that enables the expression and application of optimizations[112]for sparse tensor operations in a principled manner, e.g., loop transformations. However, RGNN scenarios still pose several open challenges to TACO, especially in edge-centric operations. Take the edgewise dot product when computingin Figure3.2as an example. First, to balance the workload, we evenly split the edgewise loop and assign them to threading blocks. If we specify the source-node-wise loop and destination-node-wise loop as two dimensions in the TACO iteration space, we need to fuse these two loop levels to form the edgewise loop to split, but such loop fusion between two loop levels of iteration variables is not supported by TACO yet. Alternatively, we can specify the edgewise loop index as one dimension in the iteration space. In this case, we need indirect addressing to retrieve node data: We need to retrieve \u2460 the source/destination node index by edgewise loop index and then \u2461 the node data. However, indirect addressing is not natively supported in TACO and thus poses the second challenge.\n\nSECTION: 3.7Conclusion\n\nRGNNs are graph neural networks with dedicated structures for modeling the different types of nodes and edges in heterogeneous graphs. While RGNNs have been increasingly adopted in many real-world applications due to their versatility and accuracy, they pose performance and system design challenges: inherent memory-intensive computation patterns, the gap between the programming interface and kernel APIs, and heavy programming effort required to optimize kernels caused by their coupling with data layout and heterogeneity. To systematically address these challenges, we propose Hector, a novel two-level intermediate representation and its code generator framework that\n(a)capturesthe key properties of RGNN models, and opportunities to reduce memory accesses in inter-operator scheduling and materialization,\n(b)generatescode with flexible data access schemes to eliminate redundant data copies, and\n(c)decouplesmodel semantics, data layout, and operators-specific optimizations from each other to reduce programming effort.\nBy building on one GEMM template and a node/edge traversal template, Hector achieves up to 9.9speed-up in inference and 43.7speed-up in training compared with the state-of-the-art public systems on select models, RGCN, RGAT, and HGT, when running heterogeneous graphs provided by DGL and OGB.\nIn addition, Hector does not trigger any OOM exceptions in these tests.\nWe also propose linear operator reordering and compact materialization to further accelerate the system by up to 3.8.\nAs an indicator of the reduction of programming effort, Hector takes in 51 lines of code expressing the three models and generates 8K lines of CUDA and C++ code.\nThrough profiling, we found that higher memory efficiency allows Hector to accommodate larger input and, therefore, attain higher throughput in forward propagation. In contrast, backward propagation is bound by latency introduced by atomic updates and outer products.\n\nSECTION: Chapter 4PyTorch-Direct: Enabling GPU-Centric Data Access for Very Large Graph Neural Network Training\n\nSECTION: 4.1Introduction\n\nCompared with traditional neural networks, GPU-accelerated systems in large-scale GNNs suffer from performance penalties caused by low effective PCIe bandwidth.\nThe scale of graphs in real world is way larger than the tens of gigabytes of capacity the GPU device memory offers;\nTherefore, raw data of the graph is stored in the host memory, and during each mini-batch, the input to the model is transferred to the GPU.\n\nFigure4.1illustrates the data layout and transfer during the training of a GNN model.\nThe features of all nodes in the graph are stored in a two-dimensional array, as shown on the left in Figure4.1.\nThey encode prior knowledge and stay constant during training.\nIn this example, the vector for node 9 is to be output.\nIts neighbors and two-hop neighbors are sampled and required as the input for the graph.\nThese sampled neighbors are scattered in the node features array.\nUnfortunately, transferring the scattered data to GPUs with the existing deep neural network (DNN) libraries is not straightforward.\nInitiating a DMA call on each data fragment is too expensive; therefore, the CPUs must first gather the scattered data before the transfer.\nFor small graphs, this inefficiency can be bypassed by simply loading the whole features into GPU memory, but real-world graphs can go beyond billions of nodes[1]and thus far exceed the GPU memory capacity.\n\nConventional wisdom would argue that since the graph feature data is in host memory, the CPU should have a significant latency and bandwidth advantage over GPUs in performing the gather operations on these features. However, with their ability to issue a massive number of concurrent memory accesses to tolerate latency, GPUs have recently been shown to be effective in accessing data with irregular structures like graphs in the host memory[17]. If successful, having the GPUs perform gather operations also eliminates the need to perform a data copy from the CPU to the GPU after the feature data has been gathered.\nIt is, therefore, desirable to explore the use of GPUs to perform feature gather accesses to significantly reduce end-to-end GNN training time. This chapter presents PyTorch-Direct, a GPU-centric data access design for GNN training.\n\nPyTorch-Direct adopts zero-copy, in which the node features array is stored in host-pinned memory and can be accessed by GPU kernels directly.\nIn a zero-copy access, the GPU sends a PCIe read request to the host memory at the time the GPU core dereferences the pointer.\nContrary to the usual belief, after careful optimization on access pattern, zero-copy access yields close to peak PCIe bandwidth[17].\nMoreover, it removes the redundant data copy in the host memory incurred during a block transfer.\nFigure4.2(b) shows the transfer procedure after adopting zero-copy access.\nComparing it with the original procedure in Figure4.2(a) shows that 1) redundant data copy is eliminated, and 2) finer-granularity zero-copy access replaces block transfer.\n\nNevertheless, incorporating zero-copy into PyTorch is non-trivial.\nPyTorch does not support zero-copy.\nNor did PyTorch take cross-device access into consideration in its tensor abstraction.\nSpecifically, every tensor in PyTorch is bound to a specific device, as illustrated in Figure4.2(b).\nSuch device binding governs the computation device and the physical location of the result tensor.\n PyTorch-Direct devises and implements a full-fledged new tensor type, the unified tensor, accessible by both CPU and GPU.\nIt is underlain by zero-copy access, enabling the scheme in Figure4.2(b), and is seamlessly integrated into the PyTorch APIs and runtime.\nChanges needed to adopt it in GNN scripts are minimal.\n\nThe contributions of PyTorch-Direct are as follows:\n\nWe identify inefficient host-to-GPU data transfer patterns in existing GNN training schemes that cause high CPU utilization and increase end-to-end training time.\n\nWe propose\na GPU-centric data access paradigm with a novel circular shift indexing optimization for GNN training to reduce training time and CPU utilization.\n\nWe seamlessly incorporate the proposed system level changes into a popular DNN library, PyTorch, with a comprehensive implementation to benefit a broad range of GNN architectures.\n\nSECTION: 4.2Background and Related Work\n\nOne shortcoming of early GNN models is the large memory footprint.\nAs inspired by the Laplacian filter, they usually involve an adjacency matrix in each of their hidden layers, which scales up as the size of the graph increases.\n\nTo mitigate this, GraphSAGE[30]proposes neighbor sampling along with mini-batch.\nIt takes in the node pairs chosen in the mini-batch, their sampled neighbors, and multi-hop neighbors rather than the nodes in the whole graph.\nThis dramatically reduces the memory footprint.\nA GraphSAGE model includes two to three aggregation layers, which can be mean, pooling, LSTM, etc.\nFigure4.1shows an example of neighbor sampling on node 9.\nNode indices are represented in hexadecimal.\nNeighbors of node 9 are sampled, constituting the input of the second aggregation layer.\nSimilarly, the neighbors of these neighbors are sampled as the input for the first aggregation layer.\nThe input of the first aggregation layer is node features from the graph.\nConsequently, the sampled node features are scattered in the node features tensor, as the illustration on the left shows.\nGathering needs to be done before DMA block transfer in the original mini-batch input transfer scheme, as Figure4.2(a) shows.\n\nIn GNN training, the input features are located in a two-dimensional array where the row indices are the identifiers of nodes and the columns are the features of each node.\nIn Figure4.2, we show a case of retrieving the node features of the neighboring nodes during the GNN training.\nDue to the structural discrepancy between the graph and the array, accessing the features of neighboring nodes in the graph results in accessing rather unpredictable and non-sequential rows of the Feature Array.\n\nA straightforward approach to sending these non-consecutive rows to the GPU is to call data copying functions likecudaMemcpy()multiple times, once for each row.\nUnfortunately, making multiple calls to data copying functions incurs significant overhead and can be highly inefficient.\nWhen the input graphs are small, one can bypass this issue by simply placing\nthe entire feature array into the GPU memory at the beginning of GNN training.\nHowever, in reality, it is not reasonable to assume that the entire feature array can always fit into the GPU memory.\n\nCurrently, the solutions for training GNNs on huge graphs can be divided mainly into two categories:\n1) Only the immediately necessary features for the current mini-batch are gathered by the CPU and then sent to the GPU memory[1].\n2) Before training, partition the input graphs into multiple smaller subgraphs that can be fit into the GPU memory and then train on them one by one[113,114].\nIn the former category, the CPU can become a bottleneck, slowing down the training pipeline. In the latter category, the subgraphs inevitably lose some of the distinct structural patterns of the original graphs[115].\nPyTorch-Direct addresses these deficiencies by enabling the GPU to directly gather all the needed features from the host memory on demand.\n\nTo facilitate GNN development, efforts are made to create frameworks that incorporate commonly required functionalities in GNN training based on popular Python-based DNN libraries such as PyTorch and TensorFlow.\nDGL[69]is developed based on MXNet, PyTorch, and TensorFlow.\nPyTorch-Geometric[70]is a PyTorch-based GNN framework.\nStellarGraph[116]and Spektral[117]are based on TensorFlow and Keras API. In PyTorch-Direct, we demonstrate the benefit of our approach by extending PyTorch.\n\nThere is rich literature addressing the challenges of large-scale GNNs.\nWhile this body of work highlights the demands and issues with large-scale GNNs, the novelty of PyTorch-Direct is unique: it mitigates the PCIe bottleneck for these applications by proving the close-to-peak effective PCIe bandwidth of zero-copy in node features gathering and transfer and devising the new APIs and runtime modifications to integrate into PyTorch.\nWorks[118,119,114,113,120]propose new models to mitigate the memory footprint, such as graph partitioning, layer sampling, etc. They change the algorithm and empirically may worsen the accuracy[121].\nIn comparison, PyTorch-Direct applies to all GNN models using neighbor sampling.\nWork[122]proposes a general GNN processing framework able to utilize multiple GPUs and conduct high-level optimizations, including kernel fusions and dataflow optimizations. Still, it does not account for the PCIe transfer efficiency.\nWork[111]devises a distributed CPU-only GNN processing system, which does not exploit the massive parallelism of GPUs.\n\nBesides, there is much research on large-scale graph processing systems.\nWork[123]utilizes unified memory and static graph ordering to mitigate irregular data access, but our work also applies to dynamic graphs. Work[124]proposes a subgraph generation algorithm and uses DMA engines to perform host-to-device transfer.\n\nThere are three ways to transfer data among the CPU and GPUs, i.e., API calls for DMA transfers, on-demand paging by Unified Virtual Memory (UVM)[125,52,126,127,128], and zero-copy access[127,17,129].\n\nThe first way is through explicit API calls.\nHost logic in the program invokes the corresponding APIs to perform data transfer.\nThe two most commonly used APIs arecudaMemcpy()andcudaMemcpyAsync(), which perform synchronous and asynchronous data copy, respectively.\nThe programmer must also specify data movement direction, e.g., host to device, device to device, etc.\nWhen the API is invoked, the driver uses the DMA engine to perform the transfer.\nIf the source data are in the host memory, they will be first copied to a pinned region in the host memory by the CPU, causing extra data movement[128,130].\nAs Pearson et al.[128]measured, the effective bandwidth is very low when the transfer size is a few KBs and reaches 50% of the peak bandwidth only when the transfer size is at least 217to 219bytes.\nGiven that each node feature typically takes around 1 KB, the host must first gather the node features into a temporary array in host memory before DMA transfer to well utilize PCIe bandwidth.\n\nOn-demand paging is the second way. CUDA provides UVM[127]to reduce the burden of memory management.cudaMallocManaged()calls allocate UVM-managed memory regions, which can be accessed by either the host or GPUs in the system.\nDuring a miss, the driver transparently migrates the page from the remote to the local memory.\nThe migration granularity is between 4 KB and 2 MB.\nSince Pascal architecture, Nvidia GPUs use the page faulting mechanism to handle missing pages when accessing a location not in its device memory[131,125].\nUVM provides the programmers with convenience.\nEspecially, they do not need to explicitly perform deep copies for every referenced memory region.\nBut UVM is not designed to maximize performance.\nAs Chien et al.[132]have measured, page faults by unified virtual memory cause non-negligible negative impacts on bandwidth.\nBesides, in GNN, in particular, only a few node features may be accessed per page migration, reducing the effective bandwidth.\nFurthermore, since the total size of all node features is way larger than the device memory, it may cause excessive eviction, further aggravating the originally severe PCIe bottleneck.\n\nThe third method is zero-copy access.\nGPU can access any data in the system as long as it is pinned and memory-mapped into the GPU\u2019s address space.\nIn zero-copy access, GPU sends the request through the interconnect to get data, without explicit copying or migration in the previously mentioned two mechanisms.\nWhen accessing host memory, the GPU issues at most cacheline-sized, i.e., 128 bytes, data requests onto PCIe[17].\nThere are three APIs or combinations that enable zero-copy in a memory region[25], but for simplicity, we choosecudaMallocHost()in PyTorch-Direct.\n\nSECTION: 4.3Motivation\n\nIn current implementations of deep learning frameworks, the host-to-GPU data loading process is CPU-centric.\nWhen data that needs to be processed by the GPU is scattered in host memory, it is the CPU\u2019s responsibility to gather the data fragments before calling a DMA. Figure4.2(a) shows the four main steps of this CPU-centric approach. The CPU first reads (gathers) the features, i.e., relevant rows of the Feature Array in this example, into its cache (\u2460),\nit then writes them into consecutive locations in a temporary buffer (\u2461)\nbefore it calls a data copy function to set up a DMA operation (\u2462)\nand finally, the DMA hardware on the GPU reads the data from the temporary buffer in host memory into a corresponding buffer in the GPU memory (\u2463).\n\nIn Figure4.3, we show the impact of this CPU-centric data loading approach on GNN training.\nAs a comparison, we use AlexNet[133]and ResNet-18[134]as CNN examples and GraphSAGE[30]and graph attention network (GAT)[135]as GNN examples.\nWe use Torchvision[136]for CNN training and DGL backed by PyTorch for GNN training.\nWhile the time spent for data loading is less than 1% of the CNN training time, it consumes 47% and 82% of the GNN training time for GrapSAGE and GAT, respectively.\nAs the vertical axis on the right of Figure4.3shows, CPU utilization is also much higher in GNN training.\nThis happens partly because the data gathering part of the code is multithreaded and tries to maximize the throughput and thus minimize latency.\nAdditionally, multi-threading is also used to maximize the performance of graph traversal and subgraph generation during data loading.\n\nIn short, in GNN training, unlike CNN training, data loading incurs significant time and resource overheads. In PyTorch-Direct, we aim to reduce this overhead from inefficient use of CPU resources in gather operations.\nWe propose a GPU-centric approach to accessing data for GNN training based on the direct host-memory-access capability of modern GPUs (Figure4.2(b)).\nModern GPUs have their own address translation units and can access host memory directly.\nIf GPUs are connected over PCIe, they can simply generate PCIe read/write I/O\nrequests to the host.\nFrom the programmer\u2019s point of view, accessing host memory can be simply done by dereferencing unified memory pointers, just like dereferencing device memory pointers.\n\nThis direct access feature is different from the conventional unified virtual memory (UVM) method, which is based on page migration.\nIn UVM, the data transfer between the host and GPU is done in page granularity, which is at least 4 KB per page in modern computing systems.\nWhenever a required page is missing from the GPU, the CPU needs to handle the page fault through a hardware interrupt service.\nSince the minimum data transfer granularity is a page and the hardware interrupt service process is costly, the performance of the UVM method depends on the applications\u2019 spatial and temporal localities[137].\nWhen dealing with highly irregular data structures such as a graph, using UVM incurs excessive page faults and I/O amplification[123,17,124].\n\nIn the following section, we describe our implementation of PyTorch-Direct, which enables GPU-centric data accesses for the PyTorch DNN library.\nWe mainly focus on PyTorch in PyTorch-Direct due to its straightforward and intuitive way of binding data to a certain physical location from the user\u2019s perspective.\nHowever, the main idea of the GPU-centric data accessing mechanism can still be applied to other DNN frameworks, such as TensorFlow.\n\nSECTION: 4.4Design and Implementation\n\nThis section describes the design and implementation of PyTorch-Direct.\nFirst, we provide an overview of design goals and introduce a new type of tensor, i.e.,the unified tensor, which incorporates new concepts in need.\nWe then discuss the unified tensor API and its advanced configurations.\nFinally, we describe our implementation and optimizations.\n\nPyTorch-Direct aims to enable GPU out-of-memory training and inference for GNN while incorporating the direct access feature to improve data access performance.\nTo achieve this, PyTorch-Direct presents to the developers several API features centered around a new type of tensor called \u201cunified tensor\u201d.\nIt is a new, independent type parallel to PyTorch native GPU or CPU tensors from both the user interface perspective and its implementation in the runtime system. We have developed all the supporting code that allows unified tensors to be used as a full-fledged tensor type in all PyTorch runtime activities such as memory allocator,torch.deviceclass, dispatch, etc. This makes it extremely easy for the application developers to adapt their PyTorch code to use unified tensors.\n\nUnified tensors are at the core of the PyTorch-Direct design, which enables GPUs to directly operate on the host memory.\nAll CUDA and CPU C++ kernels in PyTorch runtime can directly access unified tensors by simply dereferencing their memory pointers.\nIn comparison, PyTorch native CPU tensors can only be accessed by CPU, and CUDA tensors can only be accessed by GPU, thus limiting the type of computation devices that can participate in processing these tensors.\nUnified tensors eliminate these limitations.\n\nBy default, PyTorch-Direct allocates the unified tensors in the host memory and allows GPUs to directly access them over the PCIe.\nSince the unified tensors are located in the host memory, their sizes can grow beyond the GPU memory size.\nFrom the CPU\u2019s perspective, accessing the unified tensors is identical to accessing CPU tensors.\n\nApplication developers can adapt their PyTorch code to use unified tensors with minimal changes to their code.\nIn ListingLABEL:unified-example-original, we show a simplified example of GNN training in PyTorch.\nAfter loading all the features into host memory, in every training step, it sends the features in the mini-batch to the GPU by callingto(\"cuda\")before invoking thetrainfunction (lines 10\u201313).\n\nThe procedure with the unified tensor is shown in ListingLABEL:unified-example.\nIn this example, to migrate to the unified tensor scheme, the developer only needs to remove theto(\"cuda\")invocation onfeatures[neighbor_id]and instead invoketo(\"unified\")onfeaturesat the beginning.\nThe features of the whole graph are now stored in a unified tensor that can hold data beyond the GPU memory capacity.\nAfter that, GPU kernels that are launched by thetrain()function can directly accessfeaturessince it can access a unified tensor and its derived tensors.\nTherefore,to()calls are not needed anymore.\nSection4.4.2describes more about the API design, including advanced configurations.\n\nAs a full-fledged tensor type, unified tensor facilitates a clean implementation of complicated rules in runtime systems and easy future extensions.\nFor example, PyTorch-Direct clearly defines the whole set of rules to resolve computation placement and output tensor placement for computation that involves unified tensors, as detailed in Section4.4.3. Thanks to the completeness of the unified tensor, this ruleset is well integrated into the PyTorch runtime system. The implementation details are discussed in Section4.4.4,\n\nPyTorch-Direct APIs are designed to provide an interface to unified tensors in the idiomatic PyTorch manner.\nTable4.1demonstrates the typical use of unified tensor APIs.\nDevelopers can create a unified tensor by copying from another tensor via PyTorch built-into()method oftorch.Tensor.\nIt can also be created from scratch by specifying thedeviceargument as the unified device in PyTorch APIs, such astorch.ones.\nThe user can check if a tensor is of unified type by checking theis_unifiedattribute.\n\nUnified tensors can be computed with CPU or CUDA tensors, providing great flexibility.\nMeanwhile, they are free from redundant data movements since the CPU and GPU can directly access their underlying memory without creating temporary copies.\nBy contrast, in the native PyTorch API, CPU tensors typically cannot work with CUDA tensors because of the device binding unless additional routines to handle them have been implemented in the PyTorch runtime system.\nFor example, the subscript operator allows a CUDA tensor to be indexed by a CPU tensor, and binary and comparison operators accept GPU scalar and CPU scalar as the two operands.\n\nThough unified tensors can be accessed by both CPU and GPUs, we need to define scheme to determine the computation device and the location of result tensors.\nEspecially, this scheme may be complicated in scenarios where the operator involves more than two tensors or a hybrid of native tensors and unified tensors.\n\nIn the original PyTorch, the dispatch mechanism determines the computation device and result tensor type based on input tensor metadata before executing the operator.\nWe followed the same idea and integrated a set of lightweight rules into the existing dispatch mechanism.\nThis allows it to be better integrated into the PyTorch runtime and leads to low overhead in performance and programmer effort to adopt the unified tensor.\nThere might be more sophisticated ideas, such as computational graphs, but they may drastically change the APIs or cause a bigger performance overhead.\n\nEach unified tensor is designated an affinity mode, either host-affinity or GPU-affinity.\nIn the simplest scenario where an operator is applied to a unified tensor in host-affinity mode, the computation and results tensors are placed on the host during execution.\nSimilarly, if this happens to a unified tensor in GPU-affinity mode, the computation and result are placed on the GPU.\nThe reasoning behind the two modes is simple.\nIn GNN mini-batch input transfer, we want the results to stay on the GPU as they are consumed by kernels executing the GNN on the GPU, thus avoiding unnecessary data transfers over PCIe.\nTherefore, the output tensor should be of CUDA type.\nThis is what the GPU-affinity mode is for.\nOn the other hand, the host-affinity mode allows the result tensor to stick to the unified tensor type and allows for more preprocessing.\nOne can switch a host-affinitive unified tensor to GPU-affinity mode once the preprocessing is done.\n\nSwitching the affinity mode of a unified tensor can be done easily by a new tensor method.\nIt does not incur movements.\n\nTable4.2shows the complete set of rules.\nThe number of scalar CPU tensors influences the placement to stay consistent with the existing PyTorch dispatch logic.\nThe other factor is if CUDA tensors are participating in the operator because, in that case, the only feasible computation devices are GPUs.\n\nWhile offering seamless API integration into the existing PyTorch design, this project also integrates it into the PyTorch runtime C++ code in a neat, modular, and extensible way.\n\nThe goal of implementation is to realize the flexibility and performance benefits of the unified tensor while keeping modifications to existing logic as minimal as possible, especially with the large number of operator definitions.\n\nThe core object in the PyTorch runtime system isat::Tensor.\nEvery PyTorch tensor (torch.Tensorobject) is aTHPVariable111\u201cTHP\u201d stands for TorcH Python[138].object in C++ runtime code, which is the wrapper class combining anat::Tensorobject with Python metadata.\nThe PyTorch runtime dispatches each method call to the proper definition according to the device and data types of the tensor arguments.\nA PyTorch method operating on tensors eventually goes into a function ofat::Tensor222\u201cat\u201d stands for the \u201cA TENsor\u201d library[139]..\n\nPyTorch-Direct implements the unified tensor mechanisms in the PyTorch runtime as a complete type of tensor.\nThis makes the design modular, extensible, and well-integrated into the PyTorch runtime code.\nA new memory allocator is implemented to govern the memory allocation for all unified tensors.\nIt adapts the allocation pool mechanism from the PyTorch CUDA allocator to reduce the number of CUDA API invocations.\n\nTwo dispatch keys are added, corresponding to the two affinity modes mentioned in Section4.4.3.\nDispatch keys specified by each tensor inform the dispatcher to dispatch the operator to the correct backend to get executed.\nThe introduced zero-copy memory allocator uses PyTorch\u2019s pooling idea in PyTorch\u2019s original CUDA to reduce API invocations.\nBesides, auxiliary logic in the build system and runtime is modified to incorporate the changes.\nOnly the device-checking logic needs to be changed for most operator definitions, as it now needs to recognize the new unified tensor type.\n\nThis project is first developed on top of PyTorch 1.6.\nAround 2.6K lines of code are added or modified to incorporate the complete mechanism detailed in this section.\nTo support the latest CUDA microarchitecture in Section4.5, we then migrated the minimal functional part to nightly PyTorch 1.8.\n\nTo achieve efficient PCIe data transfer, memory requests from the GPU threads in the same warp should be aligned and merged to the GPU cacheline (128-byte) granularity[17].\nHowever, the default PyTorch GPU indexing function does not guarantee memory alignment unless the input feature tensors are naturally aligned with the GPU cacheline size.\nIn Figure4.4, we depict a simplified working mechanism of the default PyTorch GPU indexing function.\nIn this specific example, we scale down the warp size (32 threads in real) and the GPU cacheline size (128 bytes in real) by a factor of eight.\nWe assume each feature is 4 bytes, and each node has 11 features.\nNow, due to the size mismatch between the cacheline (16-byte) and the node feature (44-byte), misaligned accesses can occur.\n\nIn the example of Figure4.4, assume that the GPU needs to access nodes 0, 2, and 4.\nTo achieve this, each thread accesses a single feature.\nFor example, the first 11 threads access the 11 features of node 0; the following 11 threads access the 11 features of node 2, and so on.\nThis looks simple in a logical view on the left side of Figure4.4, where we highlight the accesses of threads 11\u201321 to features of node 2.\nHowever, when we redraw the access patterns based on cacheline and warp alignments on the right side of Figure4.4, we see that the accesses are fragmented into multiple cachelines and warps.\n\nTo solve the problem of misaligned access patterns, we use a circular shift method as described in Figure4.5.\nIn this method, all threads calculate the required index offset values to make aligned accesses.\nIn the case of Figure4.5, the threads need to do a right shift by an offset of one.\nThe threads on the edges check the boundary conditions and make additional adjustments by adding or subtracting the length of the node feature so that they do not accidentally access the other node features.\nWhen the indexed values are written to the output, the output indices are also identically adjusted to maintain the ordering.\nWith the optimization, PyTorch-Direct reduces the number of total PCIe requests from seven to five in this case.\nInside the PyTorch GPU indexing kernel, we check the input tensors and apply this optimization only when the input tensors are unified tensors and the feature widths are not naturally aligned to 128-byte granularity.\nAll these adjustments are automatically made due to our modifications to PyTorch source code. As such, no programmer effort is required to solve the memory alignment problem.\n\nSECTION: 4.5Evaluation\n\nThis section evaluates PyTorch-Direct performance using a well-defined microbenchmark and end-to-end GNN training.\nUsing the microbenchmark, we demonstrate that (1) PyTorch-Direct is faster than the baseline PyTorch approach in accessing features from the GPU under different combinations of data sizes and systems and (2) the effectiveness of our optimized memory alignment mechanism.\nIn GNN training, we show the benefit of using PyTorch-Direct for faster training.\n\nDatasets.The datasets we use for the GNN training evaluation are shown in Table4.3.\nFor the sk-2005[140], twitter7[141], and wikipedia_link_en[142]datasets, we have created them from existing real-world graphs but with synthetic feature values just for the purpose of training time evaluation.\nDatasets reddit[30], ogbn-products, and ogbn-papers100M[85]are commonly used datasets in the field for comparing the training accuracies between different GNN architectures.\n\nTest System.The platforms we have used for the evaluation are described in Table4.4.\nWe use NVIDIA 450.51.05 driver and CUDA 10.2 on the evaluation platforms.\nSystem2 and System3 configurations are only used in Section4.5.2.\n\nMicrobenchmark.We would like to answer the following questions with the microbenchmark:\n\nHow does increasing the feature size affect the PyTorch-Direct performance? The feature sizes vary greatly across datasets. For example, while a node of ogbn-products[85]has 100 features, a node of reddit[30]has 602 features.\n\nHow does increasing the number of features to be copied affect the PyTorch-Direct performance? Depending on factors such as the connectivity of the input graph and the batch size, the number of neighboring nodes that need to be fetched per batch can vary.\n\nHow well does the alignment optimization as discussed in Section4.4.5work with misaligned input features?\n\nWhat is the performance impact of using PyTorch-Direct on different systems?\n\nThe microbenchmark is designed to mimic the behavior of the data gathering and copy processes in the GNN training.\nThe microbenchmark uses a random number generator (RNG) to generate random indices, which are used to index feature values.\nThe total number of items is fixed to 4M for all experiments.\n\nGNN Training.In this evaluation, we use GraphSAGE[30]and GAT[135]implementations from DGL.\nBoth implementations have all necessary utilities (e.g., subgraph generation) to perform GNN mini-batching, which makes it suitable to work even if the input graphs cannot fit into the GPU memory.\nThe features are located in host memory, and during training, only the immediately required features are transferred to the GPU memory.\nIn the baseline implementation with PyTorch, the required features are gathered by the CPU and then copied to the GPU memory through DMA.\nIn the PyTorch-Direct implementation, the entire features are located in the unified tensor and the GPU directly accesses only the immediately required features.\nBesides the data movement parts, the core training algorithms of the DGL implementations are left unmodified.\n\nThe result of copying different numbers of features with various sizes is shown in Figure4.6.\nThe ideal case only includes the pure data transfer time under the theoretical peak bandwidth of the interconnect.\nDue to the lack of system memory, we do not run the(256K, 16KB)setup with System3.\nWith the baseline PyTorch approach, the performance varies greatly depending on the system configurations.\nWhile the slowdowns in System2 are about 3.31to 5.01, the slowdowns in System1 are about 1.85to 2.82.\nOn the other hand, with PyTorch-Direct, we can consistently reach near the ideal performance regardless of the system configuration unless the data transfer volume is very small.\nWhen the total data transfer volume is very small, the overall execution time is dominated by the CUDA API calls and kernel launch overheads.\nExcept for the(8K, 256B)case, the baseline PyTorch approach shows 1.85to 3.98slowdowns, while PyTorch-Direct shows only 1.03to 1.20slowdowns compared with the ideal case.\nOverall, PyTorch-Direct shows about 2.39of performance improvement on average compared to the baseline PyTorch approach.\n\nTo evaluate the impact of the memory alignment optimization in PyTorch-Direct, we measure data access times for various feature sizes from 2048-byte to 2076-byte in a 4-byte stride.\nThe result is shown in Figure4.7.\nFor thePyD Na\u00efvecase, we use the unmodified GPU indexing kernel from PyTorch, and the kernel has no knowledge of memory alignment.\nFor thePyD Optimizedcase, the optimization from Section4.4.5is applied.\n\nFigure4.7shows that PyTorch-Direct reduces the data access time significantly compared to the PyTorch baseline.\nHowever, the benefit is limited without the memory-alignment optimization.\nFor example, when the feature size is 2052 bytes, thePyD Na\u00efveprovides only 1.17of performance improvement overPy, while thePyD Optimizedprovides 1.95of performance improvement.\nBased on the results, we observe the optimization provides a consistent benefit over the PyTorch baseline (averagely 1.93) regardless of the data alignment.\n\nIn Figure4.8, we compare the breakdown of the training epoch time when using unmodified DGL implementations in PyTorch vs. PyTorch-Direct.\nIn the GAT training, we do not runskdataset due to the DGL\u2019s out-of-host-memory error for both PyTorch and PyTorch-Direct cases.\nSimilar to the microbenchmark results in Section4.5.2, we observe about 47.1% reduction in the feature copy times.\nThe other portions of the training epoch times remain almost identical to the baseline case.\nPyTorch-Direct gives less benefit for datasets with smaller feature sizes (e.g.,paper) because the feature copy time is smaller in the end-to-end training time.\nSimilarly, GAT training is computationally heavier than GraphSAGE, and therefore, we observe less benefit of PyTorch-Direct.\nOverall, we observe between 1.01to 1.45speedup when we use PyTorch-Direct in GNN training.\n\nSECTION: 4.6Conclusion\n\nWith the increasing adoption of GNNs in the machine learning community, GPUs have become essential to accelerate GNN training.\nHowever, training GNNs on massive graphs that do not fit in GPU memory is still a challenging task.\nUnlike conventional neural networks, mini-batching input samples\nin GNNs requires complicated tasks such as traversing neighboring nodes and gathering their feature values.\nWhile this process accounts for a significant portion of the training time, existing GNN implementations using popular deep neural network libraries such as PyTorch are limited to a CPU-centric approach for the entire data preparation step.\nThis \u201call-in-CPU\u201d approach negatively impacts the overall GNN training performance as it over-utilizes CPU resources and hinders GPU acceleration of GNN training.\nTo overcome such limitations, we introduce PyTorch-Direct, which enables a GPU-centric data-accessing paradigm for GNN training.\nIn PyTorch-Direct, GPUs can efficiently access complicated data structures in host memory directly without CPU intervention.\nOur microbenchmark and end-to-end GNN training results show that PyTorch-Direct reduces data transfer time by 47.1% on average and speeds up GNN training by up to 1.6.\nTo minimize programmer effort, we introduce a new \u201cunified tensor\u201d type along with necessary changes to the PyTorch memory allocator, dispatch logic, and placement rules.\nAs a result, users need to change at most two lines of their PyTorch GNN training code for each tensor object to take advantage of PyTorch-Direct.\n\nSECTION: Chapter 5SSDTrain: Enhancing Large Language Model Training Throughput by Using SSDs to Keep Activations\n\nSECTION: 5.1Introduction\n\nGPU memory capacity has become a bottleneck for the continued growth of LLMs.\nAs Figure5.1shows, the increase of GPU memory capacity is around 60% slower than the LLM size scaling speed and the GPU FP16 throughput improvement. About 80% of the GPU memory used to train recent LLMs consists of activations[143,144], the intermediate tensors produced by forward propagation and reused in backward propagation.\nFurthermore, the memory needed for activations is growing more rapidly than any other memory use, making GPU memory a more severe constraint for future LLM training (see Section5.2.1for details).\n\nCommon mitigations are to reduce batch size or through gradient accumulation. With gradient accumulation, a batch is divided into micro-batches that are processed separately between gradient updates. Although gradient accumulation has been adopted by many LLMs[145,47,146], the GPU computation stack is not designed for small inputs, and both mitigations lead to device under-utilization[147,148]and suboptimal math library performance[149].\nIntuitively, a smaller batch size might reduce total training computation through faster convergence. However, LLM trainers have identified a critical batch size for each model, below which convergence speed increases negligibly or even decreases[150,151]. Notably, critical batch size grows during training as training loss is reduced.\n\nAnother common approach to reducing GPU memory use is activation checkpointing. With this strategy, only some activations are kept in GPU memory, while others are flushed and then recomputed during backward propagation. For a model withlayers, activation checkpointing can reduce memory requirements fromto[152]. However, as we show in Section5.2.1, even this reduction is insufficient to eliminate the bottleneck posed by the GPU memory limits for future LLMs.\n\nThis chapter proposes SSDTrain, a software framework that offloads activations to NVMe SSDs and reloads activations just before they are needed in backward propagation. SSDTrain can fully overlap activation transfers with computation, reducing activation memory usage without incurring significant performance overhead.\n\nSSDs are a more attractive target than main\u00a0(CPU) memory for several reasons. First, as illustrated in Figure5.2, clusters and cloud instances[154,155,156]typically have limited host memory capacity (100\u2013250 GB per GPU), while SSDs offer much greater capacity. Host memory capacity is further consumed by input data, checkpointing buffers, and other training management buffers, leaving even less capacity for activation offloading. In contrast, as modeled in Section5.3.6, the activation size per GPU per training step in large LLM models can reach hundreds of GBs or even TBs, exceeding the capacity of host memory. Additionally, as Section5.2.1will detail, SSD capacity is increasing faster than the main memory, making SSD the more viable choice in the future. Second, host memory bandwidth is shared across training management tasks and offloaded computation[157,158,159]running on the host CPU\u00a0(Please see further elaboration onSwapping and offloadingin Section5.5). This shared usage can make host memory bandwidth both limited and unpredictable[160]for saving and restoring activations. In contrast, the SSD bandwidth can be dedicated to activation offloading during training.\nThird, SSDs are more elastic, both by adding more SSDs and even PCIe switches if necessary\u2014as well as through the use of optional remote high-throughput storage[161,162]. Such elasticity allows the data centers to keep up with the fast-growing size of activations. In contrast, the memory capacity of GPU cloud instances and cluster nodes is much more challenging to extend.\n\nSSDTrain makes the following main contributions:\n\nTo address the GPU memory capacity issue and the resulting GPU under-utilization during LLM model training, we design and implement the SSDTrain framework to offload activations in LLM training to NVMe SSDs. We demonstrate the viability of SSDTrain on large-scale systems by modeling the performance, estimated SSD lifespan, and the required per-GPU PCIe bandwidth.\n\nWith all code in Python except for a tiny CUDA memory allocation API hooking library, SSDTrain works with the latest PyTorch and distributed frameworks, including Megatron[47]and DeepSpeed[48]. We developed and tested SSDTrain with Megatron-DeepSpeed[163]on a two-GPU node with seven Intel Optane SSDs.\n\nBecause SSDTrain overlaps the data transfer entirely with computation, it incurs almost no performance overhead. To achieve this, we introduce several optimization techniques, including tensor deduplication, tensor forwarding, and adaptive offloading algorithm.\n\nEvaluation shows SSDTrain achieves almost the same training time per step as the original system without SSDTrain while reducing the activations peak memory use by up to 47%. We introduce the recompute-offload-keep (ROK) curve to compare the SSDTrain offloading with two other tensor placement strategies, keeping activations in memory and layerwise full recomputation. SSDTrain has the same performance as keeping activations in memory and a lower memory peak than activation checkpointing.\n\nWe further analyze how the reduced activation memory use may be leveraged to increase throughput by increasing micro-batch size and reducing pipeline parallelism bubbles.\n\nSECTION: 5.2Background and Motivation\n\nAs Figure5.12of Section5.4will show, the GPU memory capacity limits the model throughput. By offloading the activations to SSDs, SSDTrain can alleviate this limitation and improve the per-GPU model throughput. An important question is whether the GPU memory capacity will continue to be the limiting factor of per-GPU model throughput according to the trend of LLM scaling. This section shows that the historical trend will make GPU memory capacity an even more critical limiting factor of the per-GPU model throughput.\n\nNeural scaling laws[164,150,151]guide LLM scaling as computing power increases. We follow these laws in our reasoning.\nThe whole-system GPU compute throughput, whereis the number of parameters andis the number of tokens in a batch[165]. The Chinchilla scaling law[164]concludes that the optimal model design follows, which impliesto saturate the GPU throughput. Whole-system GPU memory use consists of two parts: activations, which require, whereis the hidden dimension in the layers and is a slow-growing function of, e.g.,, and all other memory use,, including parameters, gradients, and optimizer states. Comparing the factors, we can deduce that (1)grows faster than, and (2) whole-system memory use, which is dominated by the activations, grows slightly slower than the compute throughput(approximated).\nHowever, Figure5.1shows that the historical growth rate of GPU memory capacity (red dotted line) is less than 50% of that of the compute throughput (yellow dotted line). Therefore,GPU memory capacity will become increasingly inadequate for saturating the compute throughput, and memory for activations will continue to dominate the GPU memory usage.\n\nWhat about activation checkpointing? Revisiting the prior equation,whereis the number of layers. Activation checkpointing makes the new activations memory use. Sinceandgrow whenincreases and,still grows faster than.\n\nFigure5.3illustrates the trend of the main memory capacity and Figure5.4illustrates the SSD capacity\u2019s trend. As shown, the growth of the main memory capacity still falls behind the demand to sustain GPU throughput growth. On the contrary, the SSD capacity better keeps up with such demand.\n\nTrends in price, latency, and bandwidth have led to the widespread adoption and integration of SSDs into cloud instances and clusters[154,155,156].\nThe random write latency of flash has been reduced to tens of microseconds[167], and NVMe SSD data rates are now a few GB/s.\n\nSSD endurance remains a concern: how long will SSDs last in a write-intensive scenario such as activation offloading?\nSSD endurance is determined by the type and number of cells, write amplification factor (WAF), and over-provisioning.\nSSD cells can be purposed to store one bit, i.e., single-level cells (SLCs), or multiple levels, e.g., triple-level cells (TLCs).\nGenerally, the more bits a cell stores, the shorter its lifetime in program/erase (P/E) cycles. WAF is the ratio of media write amount to host write amount\u2014SSD writes pages at a time but erases blocks of pages, a coarser granularity. Erasing a partially empty block requires the remaining valid pages to be relocated, causing write amplification.\nIn turn, vendors adopt over-provisioning to reserve some blocks for wear leveling, evening out the writes across blocks.\n\nTable5.1samples current SSD models. TheD7-P5620represents a mainstream data center model with 144-layer\u00a0(L) TLC cells and a rating of three disk writes per day\u00a0(DWPD). The FL6 andD7-P5810SSDs are designed for write-intensive scenarios and have much higher endurance. Notably, SSD endurance rating uses the JESD testing method[168], performing random writes after tough preconditioning. In our scenario, the writes are large and sequential, as each tensor being offloaded is easily hundreds of MBs. Such writes are more endurance-friendly than those used to determine the JESD rating.\nFor example,three-DWPDSSDs generally allow about 2.5as many sequential writes as expected from the JESD rating[169,170,171]. Vendor guidelines[172,173,174]and empirical data[175]corroborate this difference.\nSection5.3.6conducts modeling to demonstrate why mainstream data center SSDs similar toD7-P5620are viable options to support the deployment of SSDTrain in a large-scale LLM training system.\n\nTo offload tensors to SSDs with high performance, SSDTrain utilizes GPUDirect Storage (GDS), which enables a direct data path between GPU and local or remote NVMe SSDs[182]. By eliminating the need to involve the CPU for the bounce buffer, GDS enhances bandwidth and reduces both latency and CPU load.\n\nSSDTrain aims to mitigate the training overhead caused by the GPU memory capacity limit, e.g., device underutilization, large pipeline bubble time fraction, etc. In contrast, most existing projects incorporate the offloading mechanism to execute larger models than the original system can fit without offloading at the cost of performance. To this end, there are three differences between SSDTrain and existing work: SSDTrain offloads (a)\u00a0activations to (b)\u00a0the SSDs (c)\u00a0with negligible performance overhead. To the best of our knowledge, SSDTrain is the first work that leverages SSD to offload activations for LLM training.\n\nTo take a closer look at the uniqueness of SSDTrain, let us compare it with related work Stronghold[183]and ZeRO-Infinity[184]. For difference\u00a0(a), SSDTrain offloads activations. Data in LLM training can be categorized into mutually exclusive types: parameters, optimizer states, gradients, and activations.\nIn contrast, existing work offloads other data than activations. E.g., Stronghold offloads parameters and gradients. Although ZeRO-Infinity offloads many types of data, when it comes to activations, only a subset as defined in the activation checkpoints, is optionally offloaded. Activations are the intermediate tensors produced in the forward propagation and kept for gradient computation. They are consumed in the backward propagation immediately after the forward propagation. Due to the high computing cost, gradient computation is best done by GPUs. In comparison, parameter updates associated with parameter and gradient offloading are also light and suitable for CPUs, which is why some work leverages the CPU computing power to update the gradients to improve overall throughput.\n\nFor difference\u00a0(c), neither ZeRO-Infinity nor Stronghold is designed to hide long data transfer latency. With activation checkpointing in the CPU memory enabled, at the beginning of the backward propagation of each layer, ZeRO-Infinity loads its checkpoint from the CPU memory and waits until it is done. The data transfer latency is in the critical path. Because Stronghold overlaps data transfer with computation, Stronghold\u2019s evaluation performs significantly better than ZeRO-Infinity. Nevertheless, Stronghold exhibits performance degradation compared with the no-offloading Megatron due to the long transfer latency when using NVMe, as Figures 11 and 14 of Stronghold\u2019s publication show. In contrast, SSDTrain incurs no performance degradation as it overlaps the computation and data transfer well and uses GDS to reduce the SSD access latency.\n\nIn summary, SSDTrain must tackle unique challenges, including (i)\u00a0the micro-second level SSD latency and (ii)\u00a0the short interval between producing activations in the forward propagation and their consumption in the backward propagation. To achieve this, SSDTrain uses GDS to reduce SSD access latency and carefully schedules data movement so that the computation hides the latency.\n\nTable5.2compares the features of earlier LLM systems supporting activation offloading and SSDTrain:\n\nDirect GPU\u2013SSD data path.As Section5.1mentions, transfer via CPU interferes with CPU workloads, affecting efficiency.\n\nAsync data transfer.These systems either block the training computation when loading the offloaded data or synchronize at each layer. Consequently, the I/O latency is exposed in the critical path. SSDTrain hides the I/O latency by overlapping I/O with GPU computation.\n\nInteroperability.Since LLM training requires a synergy of Python packages and the ecosystem is rapidly evolving, it is vital for the offloading feature to have good interoperability with other components in the same library or other libraries. SSDTrain relies on process-local alternation to PyTorch execution and can work with distributed frameworks, such as Megatron and DeepSpeed. In contrast, DeepSpeed\u2019s offloading features, e.g., ZeRO-Infinity, are available only in certain ZeRO stages. ZeRO stage determines what is sharded. For example, stage-3 ZeRO in Fig.5.10sharded optimizer states, gradients, and weights across the data parallel ranks. Flexgen and LLM in a Flash have their own runtime and do not work with distributed frameworks.\n\nFlexgen\n\nLLM in a Flash\n\nZeRO-Infinity\n\nSSDTrain\n\nSECTION: 5.3Design and Implementation\n\nSSDTrain implements atensor cacheto manage the offloading and reloading of tensors, facilitating the release of memory and the prefetch of tensors back to memory before they are needed for backward propagation.\nFigure5.5demonstrates how SSDTrain works using PyTorch as an example.\nSSDTrain launches its threads\u00a0(separate from PyTorch\u2019s execution threads) to store tensors\u00a0(\u2460) and to load them back\u00a0(\u2464). In forward propagation\u00a0(F), offloading of an activation starts once the operator producing it finishes\u00a0(\u2460). When activations are reused in backward propagation\u00a0(B), prefetching\u00a0(\u2464) occurs in the reverse order of layers as recorded during forward propagation\u00a0(\u2461). If the last layer begins backward propagation immediately after its forward propagation\u00a0(L3 in micro-batch 2 in the example) , SSDTrain keeps the layer\u2019s activations in GPU memory instead of offloading them\u00a0(\u2463). SSDTrain keeps individual records for each micro-batch. Upon micro-batch changes\u00a0(\u2461), SSDTrain switches its record to the one corresponding to the new micro-batch.\n\nFigure5.6shows the SSDTrain software components.\nThe tensor cache manages the activations and performs tensor offloading and loading. To achieve this, PyTorch hooks are used to alter PyTorch execution. Section5.3.2details the design and implementation of the tensor cache. SSDTrain has the SSD offloader that targets NVMe SSDs within the same node and the CPU offloader that targets host memory. Each offloader encapsulates the logic to transfer CUDA tensors to and from an offloading target. The SSD offloader leverages the GDS python binding, kvikio[187]. Using theLD_PRELOADlibrary interposition mechanism, CUDA malloc hook is a shared library that alters CUDA memory allocation and free API calls so that the memory is properly registered and deregistered for best GDS performance. This allows us to keep the PyTorch CUDA cached memory allocator for easy comparison with the baseline, without replicating its implementation in a PyTorch pluggable memory allocator or modifying the PyTorch runtime C++ code. The CPU offloader is for future work on clusters with massive remote SSD storage. It is backed by an allocator with pre-allocated host-pinned memory. The pool size is determined by profiling the first training step. New API calls are added to Megatron\u2019s and DeepSpeed\u2019s schedulers so that the tensor cache could get hints about stage changes and micro-batch changes, e.g., \u2462 and \u2463 in Figure5.5. The following paragraph details hinted DeepSpeed\u2019s scheduler as an example.\n\nTo use SSDTrain, moderate code additions are needed in the existing script:configure_tensor_cache()in Algorithm5.1shows the logic to configure tensor cache before training. The logic registers the PyTorch hooks, bookkeeps the parameters to not offload them when they are registered onto the computational graph, and monkey-patches[188]the schedulers.\nWith the dynamicity of PyTorch, monkey-patch overrides a defined function by assigning the custom implementation to the defined function in a package.deepspeed_exec_schedule()shows the hints added to DeepSpeed\u2019s pipeline scheduler. Before and after the execution of each command, APIs are called to notify the tensor cache about the upcoming stage\u00a0(line 13) and the completion of an action\u00a0(line 15). Accordingly, the tensor cache can prefetch data or wait for I/O to complete. Megatron\u2019s scheduler is patched similarly.\n\nSSDTrain extends naturally to distributed settings such as use with ZeRO, because frameworks like DeepSpeed and Megatron divide the workload into processes built on top of PyTorch\u2019s built-in tensor functionality.\nBy working below PyTorch and keeping each process\u2019 activities local, SSDTrain applies directly to distributed launches.\n\nTo benefit from tensor offloading, the GPU memory that the offloaded tensors own must be released when the tensors are not in use. However, by default, PyTorch stores a reference to all the activations on the computational graph, disallowing the GPU memory to be reclaimed. The tensor cache alters the PyTorch execution so that the identifiers, not the references, of the activations are registered on the computational graph; upon PyTorch\u2019s reusing the activation tensor, the tensor cache uses the identifier from the computational graph as the key to return the requested tensor. In forward propagation, when the tensor finishes offloading, the tensor cache no longer holds a reference to it, allowing its memory to be reclaimed by Python garbage collection once the Python control flow gets out of the function scope where the tensor object is used. In the backward propagation, the tensor cache holds a reference to the tensor by loading it from the SSD before its use; when all the module scopes the tensor is referred to have been finished, the reference is no longer held, allowing its memory to be reclaimed.\n\nIn short, the tensor cache is the in-memory structure that manages the references to all activations and keeps track of activations\u2019 states, including whether they are being offloaded, the path in the file system, etc.\n\nAs Algorithm5.2shows, the tensor cache relies on the three PyTorch hook pairs to alter its execution behavior.\n\nThe forward hook pair works in the forward propagation: The start of a module triggers the forward pre-hook, and the finish of a module triggers the forward hook.\nThe tensor cache maintains the current scope stack using the forward hook pair: Upon entrance to a module, the module is pushed to the stack;\nwhen the module exits, it is popped out.\n\nThe backward hook pair is similar.\nWhen entering a module, the tensor cache prefetches activations in upcoming modules. Section5.3.4details prefetching.\nWhen exiting a module, the tensor cache removes it from the scope lists of all activations. Activations no longer in use are removed, whose memory will be released by garbage collection.\n\nWhen a tensor is to be registered onto the computational graph, the pack hook is called to produce a value to be registered instead.\nWhen the tensor is reused, the unpack hooks are called to take in the object on the computational graph and return the original tensor.\nFigure5.7illustrates the tensor cache\u2019s activity when triggering the pack or unpack hook.\nWhen the multiply operatorfinishes (\u2460), the pack hook is called\u00a0(\u2461) on the inputxand parametersw.\nTensor cache has a record of parameters and accordingly returnswto let it be registered on the graph as is.\nThe tensor will also be returned as is if the tensor is on CPU or it is too small\u00a0(line 12 in Algorithm5.2).\nAs line 16 in Algorithm5.2shows, the tensor cache does not offload tensors but only keeps a record when the module is to be kept in the memory or in backward propagation.\nThe first condition holds true when the adaptive offloading algorithm determines to keep the last few modules in GPU memory\u00a0(Section5.3.5).\nThe second condition is true when an activation-checkpointing-enabled function does recomputation in the backward propagation to reproduce the activations.\nFor tensorxin Figure5.7, the tensor cache stores it to the SSDs\u00a0(\u2462) and returns a tensor identifier.\nWhen the unpack hook is triggered\u00a0(\u24b7), in the backward propagation\u00a0(\u24b6), the tensor cache either waits until the prefetch finishes(\u24b8), and eventually returns the tensor.\n\nTensor cache has aget_id()method to assign a unique identifier to each tensor.\nThe shortcoming of PyTorch nativeid()is that its returned value is related to the GPU memory address. As SSDTrain offloads activations, the latter will be cleared by garbage collection once the control flow goes out of its use scope.\nThe GPU memory address may be reused, causing identifier collision.\nTo solve this,get_id()combines the timestamp when it first processes the tensor with the tensor shape as the unique identifier. Whenget_id()processes a tensortfor the first time,get_id()adds the current timestamp as an additional attribute to the tensor\u2019s underlying storaget.untyped_storage()instead oft.\nThis is because sometimes PyTorch creates newtorch.Tensorobjects representing the identical tensor.\nAll futureget_id()calls get the attribute value.\nThis deduplicating scheme helps prevent redundant I/Os.\n\nPyTorch registers all needed tensors in backward propagation into the computational graph, including activations and parameters. As SSDTrain focuses on offloading activations, the tensor cache excludes the model parameters. To achieve this, before training, the tensor cache records the identifiers of all model parameters\u00a0(line 4 in Algorithm5.1). As linear layers store the transpose of the parameter tensors for backward propagation, the unique identifiers of the transpose are recorded. One benefit of ourget_id()scheme is that the identifier for the transpose of the same parameter tensor remains consistent across steps. This is because the transpose uses the original tensor\u2019s underlying storage, to which we already assigned a timestamp before training.\n\nThe tensor cache has two thread pools\u2014one for storing tensors and the other for loading tensors. The jobs submitted to each thread pool are executed in first-in-first-out\u00a0(FIFO) order.\n\nTo hide the I/O latency, the tensor cache starts prefetching each activation before the corresponding module\u2019s backward propagation.\nThe activations in the last module are kept in GPU memory, so they need not be prefetched.\nThis simple scheme suffices because, in PyTorch, the CPU submits GPU kernel launches and memory operations ahead of GPU execution. Prefetching schemes are equivalent as long as there are always I/O tasks in the GPU job queue to keep PCIe busy.\n\nUpon loading a tensor, if it is still being stored, the tensor cache will return its original in-memory reference to skip loading from SSD. We call this data forwarding. For example, in Figure5.7, when the PyTorch engine retrieves tensorxfrom theMulBWDnode, if it is still being stored to the SSDs, it is in memory. Instead of loading the tensor, the tensor cache returns its in-memory reference by converting the weak reference to a reference and storing the obtained reference in the tensor cache for the future if it is used in other scopes.\n\nOne insight we got during SSDTrain is that the activation offloading should target minimizing the peak memory usage so that the same system could accommodate a configuration with larger activations without triggering out-of-memory\u00a0(OOM) errors. Offloading tensors after the peak is not helpful. In Figure5.8, the blue curve is the memory footprint without offloading; it illustrates that GPU memory usage peaks at the beginning of the backward propagation. The black curve shows the memory footprint with offloading, where the peak is delayed by the in-progress offloading jobs and new intermediate tensors created in backward propagation. Excessive tensor offloading may keep the tensor reference even after its last use in backward propagation, delaying the reclamation of its memory. To reduce unnecessary offloading after the peak, we devised adaptive offloading with two features.\n\nFirst, when a thread is assigned a storing job, the thread will check if the tensor was forwarded. If so, the job will be canceled. Second, as illustrated in Figure5.9, we devise an algorithm to choose a module from which the offloading is paused. We profile a step to collect: (1) the data transfer size and computation time of each MLP block and attention block, and (2) the forward propagation\u2019s computation time, data transfer time, and total data transfer amount. Suppose moduleis the last module to offload in a step. The required data transfer bandwidth is to finish offloading for all the modules beforeand both offloading and reloading for moduleby the time the backward propagation of modulebegins. With the estimate that the backward propagation time is twice the forward propagation time, the required data transfer bandwidth can be calculated by the collected numbers. It should be no larger than the write bandwidth in the measured forward propagation.\n\nTo confirm whether our design is viable in large-scale training systems, particularly regarding SSD endurance and required bandwidth, we conduct performance modeling to obtain the forward propagation time per training step and the size of activations produced in the process.\n\nWe extend the performance model packagellm-analysis[8].\nTo estimate the forward propagation time,llm-analysismodels each transformer layer as a simple pipeline,,\nwheredenotes any layers inside a transformer layer. When ZeRO is enabled, the ZeRO communication time is assumed to be perfectly pipelined with the non-ZeRO computation and memory operations at the level of the transformer layer.\n\nWe model the required PCIe write bandwidth per GPU as the total amount of activations divided by half the training time. As Section5.3.5explains, some activations may be written at the early stages of the backward propagation to reduce the needed PCIe bandwidth. We also assume that the training step timeis three times the forward propagation time. The lifespan is then projected aswhereis the lifetime writes allowed by the SSD endurance rating, andis the size of activations per training step. We validated theformula with profiled activations size in experiments in Section5.4. We assume four Solidigm D7-P5620 12.8TB\u00a0(Table5.1) for each GPU and assume the WAF is 2.5 in JESD rating and 1 in our scenario.\n\nWith these, we obtain Figure5.10. We use the system configurations and measured floating point throughput from Megatron-LM[47]. The GPUs are A100 PCIe. Among all cases, the projected lifespan is over three years, and the PCIe write bandwidth per GPU is no greater than 12.1 GB/s.\nMoreover, when the system size scales up, the required PCIe write bandwidth reduces, and the projected lifespan increases. This occurs because larger systems imply increased communication overhead and reduced computation efficiency, thus slowing down training iterations on each GPU. Similar effects are observed when the model size scales up because larger model size leads to longer compute latency with increased\ndata reuse and therefore less bandwidth requirement. Section5.4.4discusses the effect of scaling up in detail.\n\nWe also estimate the maximal size of activations each GPU produces in one step: We compute the maximal micro-batch size by assuming only two layers in a row are in GPU memory at the same time while all other activations are offloaded. Then, the activation maximal micro-batches produce in a step are the largest activations offloading could open up, which are shown as diamond marks in Figure5.10. The maximal activations size per GPU ranges from 0.4 TB to 1.8 TB, while the micro-batch size ranges from eight to 32. Activations so large can no longer be held by the main memory\u00a0(Figure5.2), and therefore, SSD is the only choice as an offloading target.\n\nTo further increase SSD endurance, the data retention period can be relaxed: NAND flash gets 86P/E cycles when the data retention period is relaxed from three years to one day[189,190,191,192]. This technique was not leveraged in the reasoning of this subsection, but we discuss its impact on cost in Section5.4.4.\n\nSECTION: 5.4Evaluation and Discussion\n\nWe evaluate SSDTrain and answer the following questions:\n\nHow well does SSDTrain hide the I/O latency?\n\nHow much does SSDTrain reduce peak memory usage?\n\nHow does SSDTrain effects translate into advantages as a design choice?\n\nSection5.4.2answers Q1 and Q2 by comparing SSDTrain with execution without SSDTrain. To answer Q3, we examine the design space in Section5.4.3and discuss various implications in multiple aspects in Section5.4.4.\n\nWe use a machine with two A100 PCIe GPUs and seven Intel P5800X SSDs, as Table5.3specifies. The SSDs are organized into two RAID0 arrays: one with three SSDs and the other with four SSDs. Each array is the dedicated offloading target of one of the A100 GPUs. We measured the memory usage of the A100 with four SSDs during the evaluation. For consistent performance, the GPUs are locked at base frequency. The latest Megatron-DeepSpeed[163]is installed, incorporating DeepSpeed techniques into Megatron and ensuring interoperability.\n\nWe measure the system pretraining performance on three models: BERT[43]as an encoder-only model, GPT[41]as a decoder-only model, and T5[45]as an encoder-decoder model.\nWe use the OSCAR corpus[193,194]as the dataset.\n\nBefore we further explain the model setup, we clarify the batch taxonomy. Like other deep learning models, LLM model training typically uses mini-batches, smaller subsets of the training data. Before Section5.4, we use the terms \u201cmini-batch\u201d and \u201cbatch\u201d interchangeably.\n\nHowever, the introduction of data parallelism complicates this terminology: Now, the samples processed in each training step are partitioned into several groups, and each group is assigned to a data-parallel rank. To avoid confusion, in cases where data parallelism is enabled, we refer to all the samples in each training step as aglobal batch, and we refer to the samples assigned to one data-parallel rank amini-batch. Such cases where data parallelism is enabled are only in Section5.4.4.\n\nMicro-batch is at the lowest level of the batch taxonomy. When gradient accumulation is enabled, a global batch or a mini-batch is further divided into smaller groups for concurrent processing. Similarly, when pipeline parallelism is enabled, such a phenomenon may occur. Each group of samples is called amicro-batch. In particular, micro-batch refers to the samples processed in one operator kernel launch.\n\nWe use the two A100 GPUs for tensor parallelism. The number of micro-batches per step is fixed at one because without pipeline parallelism, in each training iteration, Megatron-DeepSpeed will not start a new micro-batch before both forward propagation and backward propagation of the previous micro-batch are done. A micro-batch number larger than one only brings in gradient accumulation and does not affect the activation offloading pattern. In other words, unless stated otherwise, the micro-batch size is equivalent to global batch size throughout Section5.4.\nThroughout Section5.4, no ZeRO technique is used. Besides, the optimizer states, i.e., what stage-1 ZeRO shards only, may be shared across other dimensions than across the data-parallel ranks: In Megatron or Megatron-DeepSpeed, this is enabled by the--use-distributed-optimizerargument, which we also do not enable in experiments across Section5.4.\nIn our experiments, the hidden dimension is from 8,192 to 16,384, and we use typical hyper-parameters[43,44,45]for hidden dimensions within this range. The attention head dimension is 128. The text sequence length is 1,024. For T5, the number of decoders is half the number of layers, rounded down. FlashAttention-2[195]is used with or without SSDTrain for optimized attention computation.\n\nAs each A100 has only 40 GB of device memory, to explore the design space closer to that in real-world training systems with A100 80 GB and later GPUs[47,143], we make several mitigations. First, we use FP16 instead of mixed precision, eliminating the FP32 weight copy. Second, we use SGD instead of Adam as the optimizer to reduce the memory use by optimizer states. The two measures only affect accumulation operations and weight updates, thus imposing a constant bias in the training step time and memory usage in execution with or without SSDTrain.\n\nTo understand SSDTrain\u2019s impact on execution time and peak memory usage, we measure the step time of BERT, T5, and GPT and the memory peak during forward and backward propagation. The collected metrics of the system with SSDTrain and without are compared in Figure5.11. For each model, we collected three scenarios with different (hidden dimension, number of layers): (8192, 4), (12288, 3) and (16384, 2). As shown, SSDTrain has almost no performance overhead in all cases. Although SSDTrain and its optimizations introduce additional CPU-executed logic, the performance comparison indicates that this logic is not on the critical path. Instead, GPU computation defines the critical path, and the CPU\u2019s role lies primarily in launching new GPU jobs before current GPU operations are complete. Thus, the CPU is underutilized, and SSDTrain\u2019s extra work does not lead to delays in new tasks reaching the GPUs.\nRegarding the activations\u2019 memory use, SSDTrain effectively reduces the peak by 28%\u201340% in these cases.\n\nNotice that throughout Section5.4, neither ZeRO nor the Megatron\u2019s optimizer state sharding, i.e., the feature enabled by the--use-distributed-optimizerargument, are enabled. Both stage-1 ZeRO and Megatron\u2019s optimizer state sharding affect only the weight update stage and have no effect on SSDTrain activation offloading and reloading. As a feature for data parallelism, ZeRO may be enabled when data parallelism is enabled. Data parallelism is typically introduced when the number of GPUs exceeds 100[47,196].\nAs to be further explained in the discussion onImpact of Upscalingin Section5.4.4, data parallelism with or without ZeRO will not negatively affect SSDTrain performance.\n\nSSDTrain opens up offloading activations to SSDs as an option besides keeping activations in the GPU memory and activations checkpointing. We compare the three strategies here by plotting the runs on the recompute-offload-keep\u00a0(ROK) curve.\nFigure5.12shows the ROK curve for training two 3-layer BERT models, one with a hidden dimension of 12,288 and the other with a hidden dimension of 14,336. In a ROK curve, each training run is represented by a point. The x-axis is the activations memory peak, and the y-axis is the model throughput. Model throughput[47]refers to the number of algorithmic computations done in unit time regardless of software and hardware implementation, e.g., whether the activations are recomputed.\nIn these two cases, SSDTrain reduces the GPU activations memory peak, allowing a larger micro-batch size to attain higher throughput. Given the same micro-batch size, SSDTrain offloading attains the throughput the same as the throughput when the activations are kept in memory. Meanwhile, SSDTrain gets a lower activations memory peak than the recomputation. Compared with keeping the activations in memory, SSDTrain can double the micro-batch size with the same activations memory budget. Alternatively, people could leverage SSDTrain to run a bigger model or use fewer GPUs.\n\nOther than the three strategies, before FlashAttention[197], Megatron[144]proposed selective recomputation:\nnoting that in the transformer layer, the operations performed by the core attention module\u00a0(the whole gray box in Figure2.1) require less computation but create a large intermediate tensor when compared with the MLP block, the work recomputed only the core attention module.\nAs we adopt FlashAttention, the core attention module is done in one kernel, eliminating these intermediate tensors. The effect of selective recomputation with FlashAttention has a negligible impact on the performance and the peak memory usage for activations.\n\nTo understand the accuracy of the performance model in Section5.3.6, we compare the offloaded amount by SSDTrain with the model estimate. As shown in Table5.4, the figures are close. We also compute the required PCIe write bandwidth using half of the measured training time. The PCIe write bandwidth is reduced as the hidden dimension gets larger. Typically, a model with more than 60B parameters has a hidden dimension of no less than 8K[44,164]. The PCIe write bandwidth of the BERT models aligns with the estimate in Section5.3.6.\n\nWhen LLM systems scale up, the computation efficiency decreases due to more cross-node communication.\nSection5.2.1demonstrates that the whole-system activations sizegrows slower than the whole-system GPU throughput, i.e.,. Therefore, the bandwidth required to fully overlap the computation with the SSD accesses is reduced.\nIn short, LLM scaling is essentially a weak scaling scenario, and SSD I/O latency is easier to hide when scaled up.\n\nAs shown in Table5.4, the required SSD throughput per GPU to fully offload tensors is negatively correlated with the hidden dimension of the LLM model, a factor of the model scale. Since most computation is GEMM, theoretically, the required SSD throughput per GPU is approximately inversely proportional to the hidden dimension of the LLM model, assuming the GPU model and computational efficiency are the same. The evaluation shows that the SSDTrain offloading performs well with two GPUs and a hidden dimension of 8K. Given that all data transfers SSDTrain offloading incurs are within the node, this configuration pressured the system more than some larger configurations, e.g., four GPUs per node and hidden dimension as 16K.\n\nIn Table5.5, we further project the impact of upscaling on the write bandwidth per GPU usingllm-analysis. We follow typical parallelism configurations[47,196]when the number of GPUs is less than 100: Initially, all GPUs are dedicated to tensor parallelism, and as the number of GPUs increases, we gradually increase the pipeline parallelism factor. In all projected cases, the write bandwidth per GPU is smaller than the corresponding original two-GPU case shown in Table5.4. Notice that Table5.5does not study the effect of data parallelism, which is typically adopted when the number of GPUs exceeds 100. Vanilla data parallelism only affects the weight update stage and does not affect the write bandwidth because SSD offloading and reloading only happen during forward and backward propagation. A configuration with ZeRO-enabled data parallelism has no greater required write bandwidth than the corresponding configuration without data parallelism because the introduced communication operations may delay forward propagation and backward propagation.\n\nTo further understand how larger micro-batch size improves the performance, we compare the no-offloading cases in Figure5.12(a) to the same configurations with global batch size as one and break down the throughput improvement in Table5.6. The improvement comes from higher kernel throughput and time-saving by weight update, where weight update saving is consistently the primary source. Such a benefit is very relevant to large-scale LLM training systems. The micro-batch size is usually set as one or two in Paxml[198]and BLOOM[146]pretraining. For these two models, the micro-batch size is set small in exchange for smaller bubbles introduced by the pipeline parallelism. The bubble time percentage is inversely proportional to the number of micro-batches. For example, in the BLOOM training system, the tensor parallelism factor is four, and the pipeline parallelism factor is 12. In each training step, each data-parallel rank is assigned a mini-batch with 32 samples. When the micro-batch size is no less than four, the ideal pipeline bubble time percentage is no less than 11.5%. However, the weight update and gradient accumulation cost is inversely proportional to the micro-batch size. When the micro-batch size is one or two, the cost is enormous. SSDTrain allows larger micro-batch sizes given the same activation memory budget, thus beneficial to these pipeline-parallelism-enabled training systems.\n\nThis work focuses on offloading activations. When the size of weights gets larger, it becomes more desirable to offload weights. SSDTrain can be configured to offload weights as well. As Section5.3.2explained, the tensor cache keeps a record of all the weights and ignores them when the pack hook is triggered. The tensor cache may be modified to offload weights in a profitable situation. For each operator, e.g., a matrix multiply, the amount of computation, weight size, and input size can be determined from the model specification without execution. SSDTrain can decide whether to offload one or both according to the GPU FP16 throughput and SSD write bandwidth. A reasonable starting strategy is to offload as much as possible while staying within SSD write bandwidth.\n\nFurthermore, SSDTrain could be extended to generate an optimized plan for all operators in the model before the execution by framing the decision-making process into an optimization problem and solving it. Offloading weights works when the pipeline parallelism factor is small. When the pipeline parallelism factor is large, careful planning is needed to determine what to offload because some weights are immediately reused by the later micro-batches.\n\nNotice that offloading weights to the main memory, together with weight update to the CPU, has been explored in prior work[184,157]. In the future, it may be explored to use SSDTrain together with any of the prior work to offload activations to the device memory and offload weights to the main memory. We leave the discussion to the elaboration onSwapping and offloadingin Section5.5.\n\nWe study the SSD cost associated with adopting SSDTrain offloading in LLM systems.\nTo obtain the endurance in Figure5.10, each A100 priced at US$10K[199]is paired with a total of US$6.4K worth of SSDs.\nIn the evaluation, we allocate seven Intel Optane P5800X for the two A100s. Although P5800X is more expensive than the models in Table5.1, the price per PBW is comparable at US$10.27[200].\nWe can further reduce the cost to a few percentage points by relaxing the data retention period: For example, for all cases shown in Figure5.10, Figure5.13shows that using four Samsung 980 PRO 1 TB for each A100 provides more than two years of SSD lifespan. The corresponding SSD cost is US$360 per A100[201,202].\nTo have more durable storage for other data, the system may restrict the activation offloading to dedicated SSDs or utilize hardware equipped with Zoned Namespaces\u00a0(ZNS) standard[203,204]to confine the wear within designated zones of physical blocks on the same SSD.\n\nAnother significant cost factor is electricity. Each SSD costs around 20 watts, whereas a single GPU can easily draw several hundred watts. Taking other factors, e.g., commissioning, cooling, etc., into consideration[205,206], the total cost of ownership\u00a0(TCO) of each GPU is an order of magnitude higher than its corresponding SSDs[207,208].\n\nWill NVMe SSDs continue to be a good offloading target in the future? As shown in Figure1.1, historically, the PCIe bandwidth per lane has grown faster than the minimum requirement to keep up with the FP16 throughput, i.e.,of the growth rate of FP16 throughput. The PCIe bandwidth has continued to grow rapidly, with new standards frequently released that double the bandwidth per lane of the previous version.\nWhenever a new PCIe standard is adopted, SSD vendors promptly release SSD products that provide the increased bandwidth aligned with the new PCIe standard.\nTherefore, the analysis we have done and all the conclusions we have drawn on SSDs in Chapter5will still be valid in the future.\n\nFirst,Cost Analysisshows the cost of SSDs in a system with SSDTrain enabled is an order of magnitude lower than that of GPUs. Therefore, adopting SSDs to enable SSDTrain, whether as an upgrade to existing on-premises clusters or in new on-premises machines, is profitable. The power supply should not be a problem: Typically, clusters have sufficient power redundancy to support upgrades that add new SSDs[205]. However, adopting SSDs in cloud instances may not be profitable if high-throughput SSDs are too costly[161].\n\nSecond, some clusters may have a lower SSD-to-GPU ratio.\nTwo measures can be taken for such clusters. First, a portion of the processes on the node can use the CPU offloader (Figure5.6) to offload tensors to the CPUs. Second, the adaptive offloading mechanism (Figure5.9) measures the I/O bandwidth and determines the amount of tensors to be offloaded so as not to delay the training process.\n\nLet us conduct a case study on DGX H100 systems[209]. Each DGX H100 node is equipped with dual-socket CPUs and eight GPUs. Within each DGX H100 node, in addition to 10 local NVMe SSDs, a significant number of PCIe lanes are allocated to storage network adapters that enable high-performance access to NVMe over Fabrics (NVMe-oF)[210]. Since GDS supports both local NVMe SSDs and remote NVMe-oF SSDs, SSDTrain is compatible with both types of storage in the DGX H100 system. GDS still provides acceleration[211,212]when the remote SSDs are purposed as a distributed file system, e.g., Lustre, and optimized file format is used, e.g., HDF5. Users can choose to offload activations to local SSDs when their bandwidth and capacity are sufficient. If additional bandwidth or capacity are needed, users may utilize remote SSDs when available and/or choose the host memory as an additional target, as discussed above.\n\nSECTION: 5.5Related Work\n\nSwapping and offloading.Many LLM systems with offloading abilities are inference-only[213,185,186]. In inference, weights and KV-cache never change and are reused across iterations; researchers leverage this to enhance locality and memory efficiency. However, in LLM training, the weights are updated in each iteration, and all tensors change across the iterations. Some work avails offloading features[184]for training but is mostly designed to accommodate larger models in a smaller system at the cost of performance. They lack the asynchronous data transfer ability to maintain performance.\n\nAnother direction is to offload data and the associated computation to the CPU[158,157,159]. The offloaded computation is relatively light, and the offloaded data include gradients, sparse elements in the weights, etc.\nRecognizing this direction, SSDTrain is made orthogonal because we offload the activations to SSDs via GDS to minimize the interference with the CPU. Activations are for gradient computation, which is compute-intensive and best done solely on GPUs.\n\nBefore the massive adoption of LLMs, there is work on offloading data for deep learning[214,215,160,216,217]. Most of them offload data to main memory while some[160]enable the GPU\u2013SSD data path. LLM training is unique because massive parallelism and its implications on the memory use of optimizer states, gradients, and weights are fundamental to the design space. SSDTrain naturally supports multiple GPUs. Besides, we demonstrated its viability on clusters and introduced the ROK curve to help with the design choice. On the other hand, LLMs have such a high demand for computing power that it stimulates rapid development in specialized hardware, e.g., transformer engine[218], and distributed frameworks. This is why we ensure good interoperability. In contrast, most earlier work in this direction is bound to a specific PyTorch version or a custom runtime with support to select layers.\n\nQuantization and sparsity.Some work on offloading uses quantization and/or sparsity to reduce the I/O size[185,186,160]. To reduce computation, algorithms have been proposed to quantize parameters and introduce sparsity into the model[219,220,221,222,223]. Mixture-of-Experts\u00a0(MoE)[224]is in this direction as it sparsifies the token-to-neuron connection in the MLP to the token-to-expert connection. Some algorithms introduce structured sparsity, e.g., N:M[225]sparsity and 2:4[226]sparsity. On the other hand, there are frameworks and specialized kernels to accelerate models with quantization and/or sparsity[227,228,229,230]. Some kernels leverage specialized hardware, e.g., Ampere tensor core[231,232].\nThese techniques are orthogonal to SSDTrain and can be used to alternate the model and accelerate the computation while using SSDTrain. Notably, given the hardware, the reuse factor to fully overlap the computation with PCIe transfer will change according to the new numerical format or sparsity access pattern. We believe that SSDTrain\u2019s adaptive offloading algorithm helps optimize the offload amounts in these cases.\n\nOptimized kernels.Previous work develops optimized kernels to accelerate LLM[197,195,233]. Some kernels utilize special hardware[234]. SSDTrain\u2019s interoperability ensures it can be used easily with these and upcoming techniques.\n\nSECTION: 5.6Conclusion\n\nThe growth rate of the GPU memory capacity has not been able to keep up with that of the size of LLMs,\nhindering the model training process. In particular, activations\u2014the intermediate tensors produced during forward propagation and reused in backward propagation\u2014dominate the GPU memory use. To address this challenge, we propose SSDTrain to efficiently offload activations to high-capacity NVMe SSDs. This approach reduces GPU memory usage without impacting performance by adaptively overlapping data transfers with computation. SSDTrain is compatible with popular deep learning frameworks such as PyTorch, Megatron, and DeepSpeed and employs techniques such as tensor deduplication, forwarding, and adaptive offloading to further enhance efficiency. We extensively tested popular LLMs such as GPT, BERT, and T5. The results demonstrate that SSDTrain effectively reduces 47% of the activation peak memory usage. At the same time, SSDTrain perfectly overlaps the I/O with the computation and incurs negligible performance overhead. We introduce the ROK curve to compare the SSDTrain offloading with two other tensor placement strategies, keeping activations in GPU memory and layerwise full recomputation. SSDTrain achieves better memory savings than layerwise full recomputation while retaining the performance of keeping the activations in memory. We further analyze how SSDTrain increases training throughput by increasing micro-batch size and reducing pipeline bubbles.\n\nSECTION: Chapter 6Discussion and Future Work\n\nBefore concluding this dissertation in Chapter7, this chapter provides a final discussion on the contributions made in this work and introduces potential future directions. Section6.1examines the advantages and limitations of various approaches to integrate techniques into the PyTorch stack. Section6.2explains how future work can further our investigation into data-efficient deep learning training. Lastly, Section6.3elaborates on how the optimizations proposed in this dissertation can be applied to other data-intensive workloads, particularly tabular data analysis.\n\nSECTION: 6.1Discussion on Integrating Techniques into the PyTorch Stack\n\nIn this dissertation, we propose and implement three projects: Hector, PyTorch-Direct, and SSDTrain. All are incorporated into the PyTorch stack in different ways.\nPyTorch-Direct wraps the zero-copy-enabled dispatch ruleset into a full-fledged unified tensor type and incorporates that into the PyTorch C++ runtime, which requires recompiling the PyTorch source code\u00a0(Section4.4.1).\nHector generates the kernels, compiles them as a PyTorch extension library, and loads them before training. The code generator and auxiliary logic, e.g., graph loading, are in Python\u00a0(Section3.3.1).\nSSDTrain has all logic in Python, except for an interposed library to register memory in GDS during device memory allocation and deregister the memory during deallocation\u00a0(Section5.3.1). The software components of the three works are shown in Table6.1.\n\nSimilarly to Hector, most of the literature incorporating changes into the PyTorch runtime creates Python extension libraries to achieve this, e.g., DeepSpeed[48], Megatron[47], Graphiler[82]. Similarly to PyTorch-Direct, some projects make changes to the PyTorch source code and recompile it to incorporate extensive modifications to the PyTorch runtime. For example, FlashNeuron[160]introduces the tensor offloading mechanism into PyTorch. PopTorch[235]incorporates support for GraphCore\u2019s accelerator, which requires adding a new dispatch key.\n\nUnlike Python extension libraries and interposed libraries, modifying and recompiling the PyTorch source code usually requires consistent efforts to keep up with the latest PyTorch changes in the long run, especially when the changes are maintained in an out-of-tree repository. Merging modifications to the official PyTorch repository will alleviate such consistent efforts, if possible. Therefore, for research projects, modifying the PyTorch source code is advisable only when the other two methods are insufficient in adding the required functionality, e.g., adding new dispatch keys.\nIn light of this, our SSDTrain project is carefully developed without modifying the PyTorch source code, unlike other projects such as FlashNeuron, as discussed in Section5.5.\nAs for the PyTorch-Direct project, changes in the PyTorch source code are required to incorporate the GPU-centric paradigm in exchange for keeping PyTorch\u2019s original programming interface. We have worked with the DGL team to integrate the particular optimized transfer scheme into the DGL repository[236,237,238,239,240]so that the optimized scheme can be activated through explicit new APIs without the need to recompile modified PyTorch source code.\n\nSECTION: 6.2Further Exploration in Deep Learning Training\n\nCost modeling and inter-operator scheduling are two key areas to deepen our exploration in deep learning training. Cost modeling helps choose the optimized design in the efficient frontier of the design space complicated by data efficiency.\nInter-operator scheduling helps hide the latency of memory accesses and data transfers with other operators.\n\nFor Hector, devise algorithms to select layouts, optimizations, and schedules according to model, input graph, and GPU architecture.One of the most important compiler research problems is the algorithm that makes choices among candidates in the design space. It remains an open problem how the data-dependent sparse operations and layout choices fit in the cost model and layout choices. Pertinently, in various applications in high-performance computing (HPC) with multiple layout and kernel choices, researchers have developed heuristics to make the optimized choice[241,242,243]. Besides, the specific microarchitecture of each GPU model also makes a difference due to the architecture-specific features available, e.g., asynchronous loading to shared memory since Ampere[244], and different microarchitecture characteristics in each model. Therefore, it is meaningful to investigate their impact and incorporate them into decision-making.\n\nFor SSDTrain, devise algorithms to pick the optimized design choice in the combined design space of both LLM parallelism strategies and tensor placement strategies.SSDTrain demonstrates that offloading opens up design choices on the efficient frontier, given a parallelism strategy. With the memory savings from SSDTrain offloading, we may choose a new LLM parallelism strategy with higher throughput at the cost of more per-GPU memory use. For example, as mentioned, the larger amount of activations SSDTrain allows to accommodate can be allocated to enlarge the number of micro-batches and/or to enlarge the micro-batch size.\nOn the other hand, pipeline parallelism brings about bubbles of idleness of the device, which could be mitigated by a larger number of micro-batches[47].\nBoth the throughput boost by increased micro-batch size and that by increased number of micro-batches saturate at a point, leaving the optimized strategy to allocate activations memory given parallelism configurations an intriguing question to explore. A broader and more general challenge is how to systematically explore the combined design space of both LLM parallelism and tensor placement strategies and find the optimized design choice. In addition to throughput, TCO is an essential target. For example, it is valuable to understand the minimal SSD requirements for a particular scenario and the upgrade cost from an existing cluster configuration.\n\nIn its latest systems software stack, Nvidia provides CUDA Graph as a performant task graph runtime. CUDA Graph reduces the launch overhead of kernels and schedules and executes tasks in the graph while their dependencies are preserved. We use CUDA Graph for low-overhead inter-operator scheduling.\n\nHide memory latency of sparse operations by enhancing intra-SM parallelism via CUDA Graph.We have observed that both GNNs and LLMs involve a mixture of sparse operations and dense operations: for GNNs, we have broken down the models to GEMM kernels and traversal kernels; for LLMs, the layers are typically dense if neither specific design, e.g., mixture-of-experts[245], is performed nor pruning is done, but the output of the activation layers is typically sparse by its nature.\n\nThe mixture of dense and sparse operations allows us to hide the memory latency of sparse operations by running dense and sparse operations in parallel. In particular, we will break down sparse and dense operations into smaller kernels and schedule them so that both dense and sparse kernels are run on the same SM simultaneously. For example, GEMM and SpMM can be broken down by partitioning the input matrices into blocks and performing matrix multiplication among block pairs before reduction. To reduce launch overhead, we use the CUDA graph to manage task dependencies and execute the series of kernels.\n\nFor GNNs, optimize data movement in mini-batch training.Graphs not fitting into GPU memory must stay in host memory or even storage during RGNN execution. In each step, the subgraphs are sampled and transferred to the GPU. With knowledge of graph semantics, data layout, and operator-specific schedules, Hector can help improve the scheduling of sampling and data transfer and generate CUDA kernels that gather data from the host memory on the fly[16].\n\nDuring backward propagation, the system needs to compute the gradient of both the weights and the input for each layer. This doubles the cost compared to forward propagation. On the other hand, the computation of the two gradients uses identical tensors, creating an opportunity yet to be leveraged to reuse data across the calculation of the two gradients.\n\nSECTION: 6.3Applying Techniques to Tabular Data Analysis\n\nData tables have been widely adopted in data analytics and machine learning pipelines. Data analytics aims to gain insights from massive data, where data tables are a core data structure. In SQL database systems, tables are essential elements to organize raw data and outputs of each query; in many data processing libraries and languages, such as pandas and R, data tables are the fundamental class as well. In machine learning pipelines, data tables hold the data, at least during preprocessing, before input to the machine learning model. The preprocessing stages involve ETL (extract, transform, and load) and feature engineering. Preprocessing may reoccur in data streaming scenarios or when iteratively refining the algorithm. Data processing takes a substantial amount of time: 80%\u201490% of the work time of a data scientist is dedicated to processing data[246,247].\n\nThanks to the high bandwidth of device memory and a massive number of processing units, GPUs could greatly help analytical workloads that typically involve many simple homogeneous operations. Aligned with this direction, many GPU-optimized databases have been established recently, involving Brytlyt, Kinetica, OmniSci (formerly MapD), SQream, etc.[248]Nvidia released the RAPIDS Python suite to allow developers to run end-to-end data analytics and data science pipelines on the GPU[249]. Central to it is the cudf package, which is the CUDA equivalent of the data table Python package pandas. In cudf, in-memory data tables are in columnar format. Other packages in RAPIDS, e.g., BlazingSQL, cuGraph, etc., enable SQL queries, graph analytics, etc., by using cudf to store data in data tables.\n\nSimilarly to deep learning training, tabular data analysis is data-intensive[250]. Data table operations typically have small arithmetical intensity, e.g., comparing the values of two columns and light arithmetic computation of a few cells for each row. Besides, real-world tabular data analysis usually involves massive data, rendering the limited GPU HBM memory capacity a problem[251].\n\nThe techniques proposed in this dissertation can also be applied to tabular data analysis. As an example, the following explains how code generation with flexible data access schemes proposed by the Hector project could help tabular data analysis with indexes.\nIndex is an essential optimization in tabular data analysis[252,253]: Index stores the presorted results of a column or multiple columns and the mapping from the result to the row index in the original table. Many data table operations can be accelerated using the index to save computation.\nNevertheless, GPU-accelerated tabular data analysis software has limited support on indices[254,255].\nBy introducing a Hector-like code generator with optional indirect addressing by index, the software gets 1)\u00a0optimized kernel development cost without the need to maintain kernel variants of the same operator and 2)\u00a0free of intermediate data tables when doing indirect addressing.\nSuch optimization is aligned with kernel fusion for tabular data analysis[250,256], and the optimizations to avoid materialization of intermediate data tables[257]. However, none of these existing projects support the index.\n\nSECTION: Chapter 7Conclusion\n\nDue to both demand from the workload and hardware advancements, it becomes increasingly critical to ensure data efficiency in deep learning training.\nData inefficiency in deep learning training arises from the data-intensive nature of workloads and the oversimplification inherent in the PyTorch computing stack.\nTo effectively mitigate data inefficiency for deep learning training, this dissertation analyzes data inefficiency in representative deep training tasks, specifically in GNNs and LLMs. It then proposes novel runtime and code generation techniques to mitigate these challenges and implements these optimizations seamlessly within the PyTorch stack while maintaining strong programmability and interoperability.\n\nFirst, this dissertation devises the Hector IR and code generator. By introducing domain-specific high-level abstraction and code generation, Hector systematically addresses significant performance challenges due to the inherent memory intensiveness, the gap between the programming interface and kernel APIs, and the high kernel optimization cost due to the kernel coupling with layout and heterogeneity.\n\nThen, this dissertation designs and implements PyTorch-Direct to incorporate the GPU-centric PCIe data transfer paradigm in PyTorch for GNN training. PyTorch-Direct significantly reduces CPU utilization, resulting in higher end-to-end training performance.\n\nLast, LLM training systems are increasingly constrained by GPU memory, with activations being one of the primary culprits. This dissertation creates the SSDTrain activations offloading framework with a direct GPU\u2013SSD data path and good interoperability.\n\nThis dissertation proves that code generation and runtime techniques can effectively mitigate data inefficiency in deep learning training.\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.04747v1_content.txt"}, {"title": "Deep Learning in Single-Cell and Spatial Transcriptomics Data Analysis:\n  Advances and Challenges from a Data Science Perspective", "authors": ["Shuang Ge", "Shuqing Sun", "Huan Xu", "Qiang Cheng", "Zhixiang Ren"], "published_date": "2024-12-04T14:07:11Z", "summary": "The development of single-cell and spatial transcriptomics has revolutionized\nour capacity to investigate cellular properties, functions, and interactions in\nboth cellular and spatial contexts. However, the analysis of single-cell and\nspatial omics data remains challenging. First, single-cell sequencing data are\nhigh-dimensional and sparse, often contaminated by noise and uncertainty,\nobscuring the underlying biological signals. Second, these data often encompass\nmultiple modalities, including gene expression, epigenetic modifications, and\nspatial locations. Integrating these diverse data modalities is crucial for\nenhancing prediction accuracy and biological interpretability. Third, while the\nscale of single-cell sequencing has expanded to millions of cells, high-quality\nannotated datasets are still limited. Fourth, the complex correlations of\nbiological tissues make it difficult to accurately reconstruct cellular states\nand spatial contexts. Traditional feature engineering-based analysis methods\nstruggle to deal with the various challenges presented by intricate biological\nnetworks. Deep learning has emerged as a powerful tool capable of handling\nhigh-dimensional complex data and automatically identifying meaningful\npatterns, offering significant promise in addressing these challenges. This\nreview systematically analyzes these challenges and discusses related deep\nlearning approaches. Moreover, we have curated 21 datasets from 9 benchmarks,\nencompassing 58 computational methods, and evaluated their performance on the\nrespective modeling tasks. Finally, we highlight three areas for future\ndevelopment from a technical, dataset, and application perspective. This work\nwill serve as a valuable resource for understanding how deep learning can be\neffectively utilized in single-cell and spatial transcriptomics analyses, while\ninspiring novel approaches to address emerging challenges.", "arxiv_id": "2412.03614v2", "html_link": "https://arxiv.org/html/2412.03614v2", "search_term": "ti:\"deep learning\"", "html_content": "SECTION: Deep Learning in Single-Cell and Spatial Transcriptomics Data Analysis: Advances and Challenges from a Data Science Perspective\n\nThe development of single-cell and spatial transcriptomics has revolutionized our capacity to investigate cellular properties, functions, and interactions in both cellular and spatial contexts. Despite this progress, the analysis of single-cell and spatial omics data remains challenging. First, single-cell sequencing data are high-dimensional and sparse, often contaminated by noise and uncertainty, obscuring the underlying biological signals. Second, these data often encompass multiple modalities, including gene expression, epigenetic modifications, metabolite levels, and spatial locations. Integrating these diverse data modalities is crucial for enhancing prediction accuracy and biological interpretability. Third, while the scale of single-cell sequencing has expanded to millions of cells, high-quality annotated datasets are still limited. Fourth, the complex correlations of biological tissues make it difficult to accurately reconstruct cellular states and spatial contexts. Traditional feature engineering-based analysis methods struggle to deal with the various challenges presented by intricate biological networks. Deep learning has emerged as a powerful tool capable of handling high-dimensional complex data and automatically identifying meaningful patterns, offering significant promise in addressing these challenges. This review systematically analyzes these challenges and discusses related deep learning approaches. Moreover, we have curated 21 datasets from 9 benchmarks, encompassing 58 computational methods, and evaluated their performance on the respective modeling tasks. Finally, we highlight three areas for future development from a technical, dataset, and application perspective. This work will serve as a valuable resource for understanding how deep learning can be effectively utilized in single-cell and spatial transcriptomics analyses, while inspiring novel approaches to address emerging challenges.\n\nKeywordsSingle-cellSpatial transcriptomicsDeep learning\n\nSECTION: 1Introduction\n\nThe advancement of single-cell and spatial transcriptomics techniques has facilitated in-depth investigations of cellular characteristics, functions, and interactions, considering both cellular activity and spatial context within tissues. Single-cell RNA sequencing (scRNA-seq) quantifies gene expression at the cellular level, thereby elucidating cellular composition, gene expression patterns, and molecular characteristics[174,54]. Recognized as the Method of the Year by Nature Methods in 2013[8], scRNA-seq has significantly advanced research into complex biological questions, including mechanisms of disease resistance[114,194], tissue heterogeneity[97,219], targeted therapies[221], and embryonic development[196]. However, tissue dissociation disrupts spatial cell distribution and intercellular interactions, thereby constraining our understanding of the intricate processes occurring within multicellular organisms.\n\nSpatial transcriptomics (ST) generates spatially resolved transcriptomic data to create detailed tissue maps at the subcellular level. This technique represents a significant advance in the field of transcriptomics, transitioning from cellular resolution to spatially sub-cellular resolution. In recognition of its importance in biomedical research, Nature Methods named spatially resolved transcriptomics as the Method of the Year in 2020[139].\n\nSingle-cell and spatial transcriptomics are crucial for studying the microenvironment at cellular and spatial resolutions, respectively. However, the complexity of biological tissues and the limitations of current sequencing techniques present significant analytical challenges. In this review, we discuss four major challenges in single-cell and spatial transcriptomics from a data science perspective: data sparsity, diversity, scarcity, and correlation. Our aim is to elucidate the origins of these challenges, explore potential solutions, and provide insights into the underlying mechanisms of the methodology. In terms of data sparsity, we examine issues such as the curse of dimensionality, noise, and uncertainty. Concerning data diversity, we categorize the integration of single-cell and spatial transcriptomics data into two primary types: multimodal integration and multi-source integration. When dealing with data scarcity, we focus on missing data annotations and missing modalities. Finally, from a data correlation perspective, we analyze methods for modeling spatiotemporal dependencies and incorporating prior knowledge.\n\nWith the increasing volume and diversity of data, traditional analysis techniques for bulk RNA sequencing are becoming increasingly inadequate for single-cell and spatial transcriptomics[54]. Deep learning (DL), a powerful tool for modelling large-scale, high-dimensional complex data, has demonstrated its versatility across numerous scientific domains, including small molecule modeling[147,19], protein structure prediction[38,150], and drug development[31], etc.\n\nRecent reviews[51,15,58,23]of deep learning (DL) applications in single-cell data have introduced methods such as multilayer perceptrons (MLP)[184], autoencoder (AE)[82], generative adversarial network (GAN)[68], convolutional neural network (CNN)[110], and graph neural network (GNN)[160]. These reviews explored both traditional and DL methods across various stages of the scRNA-seq analysis pipeline. But they do not summarize current technological advances and challenges from a data science perspective. Moreover, existing data analysis techniques may not always be effective for addressing novel problems as the number of modalities continues to grow. This review aims to discuss four major data science challenges and exploring relevant methods within these contexts. We highlight DL techniques by comparing them with traditional machine learning approaches and emphasize their advantages, particularly when integrated with statistical frameworks. Each algorithm is discussed alongside its mathematical foundations, focusing on both similarities and differences. Finally, we outline future directions in three key areas: the application of novel AI methodologies, the development of fair and robust benchmark datasets with biologically interpretable evaluation metrics, and the exploration of DL applications in practical scenarios. This review provides a comprehensive overview of DL applications in single-cell and spatial transcriptomics data analysis from a data science perspective, offering insights that could inspire innovative solutions to emerging challenges in biological and medical research. The overall structure of the article is shown in Fig.1.\n\nSECTION: 2Transcriptomic data\n\nBulk RNA sequencing (RNA-seq) provides average gene expression profiles at the tissue level, limiting its ability to accurately represent cellular heterogeneity. Consequently, it becomes challenging to discern whether the observed differences are due to changes in cellular composition or variations in gene expression (Fig.2). The scRNA-seq addresses this limitation by profiling gene expression at the single-cell level. Additionally, spatial transcriptomics integrates sequencing data with spatial context, providing a more comprehensive understanding of tissue construction and function.\n\nSECTION: 2.1Single-cell transcriptomes\n\nIn 2009, scRNA-seq technology emerged, making it possible to study the transcriptomes of individual cells. Single-cell sequencing technology requires four main steps: (1) isolation of single cells (2) reverse transcription (3) cDNA amplification (4) sequencing library preparation and sequencing.\n\nIsolation of single cells.Isolation of single cells refers to the process of separating individual cells from a complex tissue or cell population. Accurate and reliable capture is essential for single-cell sequencing. The dissociation methods mainly include mechanical dissociation, enzymatic dissociation and chemical dissociation. Target cells are selected from single-cell suspensions based on specific characteristics such as size, fluorescence, or surface labeling.\n\nFluorescence-activated cell sorting (FACS)[80]is a widely used high-throughput technique that labels cells with fluorescent dyes or antibodies targeting specific molecules on or within the cell. In flow cytometry, a laser excites a fluorescence marker, which emits a signal that is measured to quantify the molecular content. However, this method is less effective for cells with low marker expression and struggles to distinguish a subset of cells with similar fluorescence markers. Typical sequencing methods include Smart-seq[71], VASA-seq[158], FLASH-seq[73].\n\nMagnetic-activated cell sorting (MACS)[143]is another high-throughput isolation technique that separates and enriches specific cell types by binding magnetic beads to target cell proteins. The beads are conjugated with antibodies or other ligands, but unlike FACS, MACS isolates cells based on surface protein expression rather than gene expression, sorting them into positive and negative populations. MACS is primarily employed for the initial enrichment of cells and does not facilitate precise single-cell sorting as FACS does.\n\nMicrofluidic-based techniques exploit the inherent physical properties of cells for separation. These properties encompass cell size, shape, electrical polarizability, electrical impedance, density, deformability, magnetic susceptibility, and hydrodynamic characteristics[69]. In droplet-based microfluidics, individual cells are encapsulated within small droplets that are suspended in an immiscible fluid. Techniques such as InDrop[103], Drop-seq[136], and 10x Chromium[223]build upon this methodology. Microwell-based scRNA-seq methods\u2014such as CytoSeq[53], Seq-Well[64], and Microwell-seq[76]\u2014involve placing cells into discrete wells to ensure that each well contains either a single cell or none at all.\n\nReverse transcription.RNA cannot be directly sequenced within the cell. After cell lysis, the released RNA must be reverse transcribed to generate complementary DNA (cDNA). The poly(A) tailing method employs an oligo-dT primer that binds to the 3\u2019-poly(A) tail of mRNA, thereby facilitating its reverse transcription into cDNA. During this process, additional nucleotide sequences, such as cell-specific barcodes and uniform molecular identifiers (UMIs) for mRNA, are incorporated to uniquely label each cell and distinguish individual mRNA molecules.\n\ncDNA amplification.Since mRNA is typically present in very low quantities within individual cells, it often proves inadequate for sequencing purposes. Therefore, cDNA amplification is necessary to produce sufficient amounts for subsequent library preparation. The most widely utilized method for this process is PCR-based amplification.\n\nSequencing library construction.The first step in library preparation involves converting nucleic acids into a sequencing library, where DNA or RNA molecules are ligated to platform-specific adapters.\n\nSECTION: 2.2Spatial transcriptomes\n\nSpatial transcriptomic techniques can be broadly categorized into two main types based on whether positional information is encoded before sequencing: (1) next-generation sequencing-based methods and (2) imaging-based methods.\n\nNext-generation sequencing-based approaches encompass both the earlier microdissection-based techniques and the more widely adopted barcode-based approaches. Microdissection techniques isolate regions of interest through physical segmentation or optical selection, followed by collection for library preparation and sequencing. Microdissection-based techniques include tomo-seq[95], STRP-seq[161], Geo-seq[33], PIC-seq[65], TIVA[130], NICHE-seq[140]. Of these, PIC-seq, TIVA, and NICHE-seq achieve cellular-level resolution[174]. However, physical segmentation is often performed manually, making it time-consuming. Additionally, optical selection requires the insertion of specialized markers into living cells or model organisms, which limits its application to FFPE human samples. Accurately locating spatial locations remains a significant challenge, often resulting in relatively low spatial resolution.\n\nThe barcode-based sequencing technique, inspired by scRNA-seq, utilizes cell barcodes to capture poly-adenylated RNA molecules in situ before reverse transcription. This process is facilitated by a capture probe that incorporates a spatial barcode, a unique molecular identifier (UMI), and poly-T oligonucleotides, followed by the synthesis of complementary DNA (cDNA). Spatial barcodes operate analogously to cellular barcodes, ensuring an accurate mapping of transcriptomes obtained from tissue slices back to their original locations. The spatial resolution of this method depends on the distance between adjacent spots, achieving a maximum resolution of approximately, which facilitates subcellular analysis. However, enhancing spatial resolution often leads to compromises in detection sensitivity and gene coverage. Examples of barcode-based sequencing technique include 10x Visium[170], Slide-seq (V2)[171], HDST[191], Stereo-seq[32], Seq-scope[37,101], Decoder-seq[26]. Moreover, Open-ST[162]can generate ST in 3D.\n\nIn contrast to barcode-based techniques, image-based methods directly leverage in-situ spatial information without the need for spatial barcodes. Techniques such as in situ hybridization (ISH) and in situ sequencing (ISS) utilize gene-specific complementary DNA or RNA probes that bind to target sequences within fixed cells or tissues. Subsequently, spatial mapping of gene expression is accomplished by imaging, typically employing fluorescence or other markers. However, the number of detectable transcripts is constrained by optical limitations, allowing to detect only a few hundred targets. Ongoing technological advances aim to enhance the multiplexing capability. For example, ISH-based MERFISH[207]utilizes 3-color imaging and can analyze approximately 10,000 genes with only 23 rounds of imaging. Other techniques include seqFISH+[49].\n\nRNA techniques based on in situ sequencing (ISS) can be divided into targeted and untargeted approaches. Targeted ISS involves the binding of RNA or cDNA targets with specifically designed probes, such as padlock probes, followed by rolling-circle amplification (RCA) to replicate these targets for sequencing. In contrast, the untargeted ISS transcribes the transcript into cDNA using standard reverse transcription, which is followed by DNA amplification and sequencing. This approach does not require pre-selection of target genes, but may exhibit lower detection efficiency. Examples of untargeted ISS include STARmap[198], and ExSeq[6].\n\nSECTION: 2.3Database\n\nThe volume of sequencing data has grown exponentially with the rapid advances in single-cell and spatial transcriptomics technologies, highlighting the need for curated databases, robust analysis pipelines, and effective visualization tools. This review collects 12 large-scale single-cell sequencing databases (Table1) and 7 spatial transcriptomics databases (Table2). A concise overview is provided in Table3.\n\nSingle-cell omics highlights the significance of spatial context, which will increasingly be incorporated to develop multi-omics databases. The establishment of such a database not only emphasizes the integration of data sets from diverse sources, but also necessitates data preprocessing, analysis, visualization, user interaction, and other critical components.\n\nSECTION: 3Challenges and DL Methods in Single-Cell Data Analysis\n\nSECTION: 3.1Data Sparsity\n\nSingle-cell transcriptomes typically encompass tens of thousands of genes and exhibit considerable variability in expression across individual cells. Many genes remain inactive within a particular cell type, and even within the same cell type, certain genes may be transiently unexpressed due to the dynamic nature of the transcriptome and fluctuations in the cell cycle state. Consequently, gene expression matrices tend to be highly sparse, which poses challenges in modeling feature spaces, including issues related to the curse of dimensionality, noise, and uncertainty (Fig.5). The overall structure of this section is shown in Fig.4.\n\nThe curse of dimensionality arises from the exponential increase in the volume of the space as the number of dimensions grows, which leads to sparsity and makes it more difficult to cover the space effectively with a limited number of observations. Consequently, the similarity between data points diminishes. To address this challenge, feature selection and dimensionality reduction techniques are frequently employed. Traditional methods for dimensionality reduction include parameterized approaches such as principal component analysis (PCA)[135], along with variants like scPCA[22]and FastRNA[111]. Additionally, non-parametric methods such as t-distributed stochastic neighbor embedding (t-SNE)[200,121,104]and uniform manifold approximation and projection (UMAP)[18,100]are also widely utilized.\n\nNonparametric methods aim to map high-dimensional data into a lower-dimensional space while preserving local structure, typically described in terms of probabilities or metric learning. In contrast, parametric methods model the relationships between data points through explicit mathematical formulations with parameters estimated from training data. DL is a powerful parametric approach that employs neural networks for automated modeling, enabling end-to-end parameter learning directly from data. Compared to traditional methods, DL has demonstrated superior effectiveness in managing complex and high-dimensional feature spaces[145,89,133]. Scvis[44], a nonlinear dimensionality reduction method based on the variational autoencoder (VAE) framework, integrates generative modeling with variational inference (Fig.5(a)). By using two distinct neural network structures, it facilitates bidirectional mapping from high-dimensional data to low-dimensional (i.e., cell embedding) space, thereby preserving the global structure of the data. Consider a high-dimensional scRNA-seq dataset, which comprisescells, wheredenotes the gene expression vector for each cell. It is assumed that the observed data is generated from a low-dimensional prior distribution given by; this is typically modeled as a factorized standard normal distribution expressed as, through a transformation parameterized by. This parameter is difficult to compute directly and is instead approximated using a neural network. For each cell, the generative distribution can be expressed as the following integral:\n\nHowever, computing the posterior distributionbased on the observed data is intractable. To address this issue, a variational distributionis introduced as an approximation. It is assumed that the variational distribution follows a multivariate Gaussian distribution characterized by meanand standard deviation, both of which are functions of, parameterized by a neural network. The model is then optimized to ensure that similar cells exhibit analogous posterior distributions. Consequently, the low-dimensional latent space effectively preserves the distance relations of the high-dimensional data, leading to efficient dimensionality reduction.\n\nfor tree=\nforked edges,\ngrow\u2019=0,\ndraw,\nrounded corners,\nnode options=align=center,,\ntext width=2.7cm,\ns sep=6pt,\ncalign=edge midpoint,\ntext=black,\n,\n[\nData Sparsity,\n[\nCurse ofDimensionality, for tree= top_class\n[\nParameterizationMethods,\nfor tree=fill=red!45,generation\n[\nPCA-based,\ngeneration_more\n[\nscPCA[22];FastRNA[111],\ngeneration_work\n]\n]\n[\nNeural network-based,\ngeneration_more\n[\nscvis[44];Molho D. et al.[145];Hwang H. et al.[89],\ngeneration_work\n]\n]\n]\n[\nNon-parameterizationMethods,\nfor tree=fill=red!45,generation\n[\ntSNE-based,\ngeneration_more\n[\nWang Y. et al.[200];Linderman G. C. et al[121];Kobak D. et al.[104],\ngeneration_work\n]\n]\n[\nUMAP-based,\ngeneration_more\n[\nSeurat v3[18];Kim G. et al.[100],\ngeneration_work\n]\n]\n[\nEnsemble-based,\ngeneration_more\n[\nEDGE[175],\ngeneration_work\n]\n]\n]\n]\n[\nNoise Issues,\nfor tree=top_class\n[\nBatch effect correct,\nfor tree=fill=green!45,gpa\n[\nNNM-based,\ngpa_wide\n[\nHaghverdi L. et al.[72];Hie B. et al.[81],\ngpa_work\n]\n]\n[\nNeural network-based,\ngpa_wide\n[\nCLEAR[75];BERMUDA[197],\ngpa_work\n]\n]\n]\n[\nImputation,\nfor tree=fill=blue!45,gpa\n[\nMatrix factorization,\ngpa_wide\n[\nALRA[122],\ngpa_work\n]\n]\n[\nNearest neighbor,\ngpa_wide\n[\nMAGIC[188],\ngpa_work\n]\n]\n[\nStatistical-based,\ngpa_wide\n[\nScImpute[118];SAVER[88],\ngpa_work\n]\n]\n[\nNeural network-based,\ngpa_wide\n[\nDCA[50];scIGAN[209],\ngpa_work\n]\n]\n[\nBenchmark=fill=gray!45,top_class\ntopclass_wide\n[\nscIMC[41];SAE-Impute[14],\ntopclass_wide\n]\n]\n]\n]\n[\nUncertainty,\nfor tree= top_class\n[\nStatistical-based=fill=green!45, pretraining_more,\npretraining_more\n[\nscVI[128],\npretraining_work\n]\n]\n]\n]\n\nBiological noise in scRNA-seq data arises from the intrinsic randomness of biological systems and variations in cellular states. In contrast, experimental noise reflects non-biological fluctuations due to technical limitations or random errors. Systematic biases, commonly referred to as batch effects, occur due to discrepancies in experimental conditions, instruments, reagents, and procedures across data batches. Furthermore, technical constraints or low capture efficiency often lead to missing values, resulting in a large number of zeros in the gene expression matrix. These zeros can obscure the true biological signal, a phenomenon known as dropout events. Batch effects and dropout events always co-occur in real-world datasets. Research has increasingly focused on batch effect correction and imputation methods to address these challenges.\n\nBatch effect correction improves the comparability of datasets derived from different batches, ensuring that observed differences reflect genuine biological variation. This process involves mapping the identical cell types from various experiments to a common region within a latent space. Traditional methods, such as nearest neighbor matching (NNM)[72,81], address this issue by aligning representations across batches. DL-based approaches focus on the hidden space where samples are mapped to semantic representations, facilitating better learning of the underlying patterns. These methods preserve essential biological signals while filtering out irrelevant features through data reconstruction. The CLEAR algorithm[75], which is based on contrastive learning, improves latent representations by constructing positive and negative sample pairs during training (Fig.5(b)). Similarly, BERMUDA[197]aligns batch distributions using the maximum mean discrepancy (MMD) loss, facilitating the integration of data in the latent space.\n\nImputation methods are designed to reconstruct missing gene expression values by distinguishing technical noise from real biological zeros, relying on observed data patterns. Various approaches have been developed to address this issue, including traditional matrix factorization techniques, similarity modeling, statistical approaches, and DL strategies. Matrix factorization methods mitigate noise by preserving the dominant low-rank signal while discarding extraneous components. Traditional matrix factorization techniques are based on singular value decomposition (SVD) and non-negative matrix factorization (NMF), such as ALRA[122]. Similarity-based approaches leverage relationships between cells to infer missing values. Actually, they leverage gene expression profiles from other cells. An example is MAGIC[188], which employs a k-nearest neighbor algorithm to smooth and impute data based on local similarities. ScImpute[118]also relies on local structure for imputation. Statistical modeling methods employ predefined or probabilistic models to fit observed data. For example, SAVER[88]assumes gene expression follows a negative binomial distribution modeled through a Poisson-gamma mixture. It estimates the parameters using an empirical Bayesian approach with Poisson LASSO regression, and outputs the posterior means as imputed values. DL methods reframe traditional matrix operations as neural network layers, transforming parameter estimation into an optimization problem. Common frameworks include autoencoder-based models and generative architectures that adaptively learn complex patterns to enhance imputation accuracy.\n\nAutoencoders (AEs) are widely used for imputation, effectively integrating dimensionality reduction and denoising within a unified framework. As unsupervised learning models, AEs transform high-dimensional input data into a lower-dimensional latent space, preserving essential features while eliminating redundant information. This latent representation provides contextual insights that facilitate the imputation process. The decoder reconstructs the original input from this compact representation with the objective of generating an output that closely resembles the initial data. For example, DCA[50]employs an autoencoder with a ZINB noise model to infer key parameters such as the mean, dispersion, and dropout probabilities associated with gene expression data (Fig.5(c)). The decoder produces a denoised reconstruction that is well aligned with the modeled data distribution. Variants of AEs, such as variational autoencoders (VAEs)[102], conditional autoencoders[167], and sparse autoencoders[148], offer additional flexibility tailored to specific applications.\n\nGenerative models, such as GANs, address the limitations of similarity-based methods that often lead to over-smoothed imputations. GAN-based models are designed to learn the underlying data distribution and generate new samples that closely resemble the denoised data. ScIGAN[209]generates synthetic single-cell profiles instead of directly estimating missing values from observed data. This strategy minimizes overfitting to dominant cell types while improving imputation for rare cell populations. The distinctive design of scIGAN involves transforming real gene expression data into a two-dimensional image representation. The generator synthesizes gene expression profiles from latent variables, whereas the discriminator distinguishes between real and synthetic images. Both networks are trained adversarially and their performance is refined by iterative competition.\n\nWe have collected 12 methods, including scImpute[118], SAVER[88], ALRA[122], MAGIC[188], scTSSR[92], DCA[50], DrImpute[67], DeepImpute[10], AutoImpute[178], scIGANs[209], scGAIN[70], to evaluate their imputation performance on five benchmark datasets[41,14]. The results demonstrate that DCA and scIGANs each achieved the highest imputation consistency across the two benchmarks, with both methods displaying robust clustering performance in four out of five datasets (Fig.3).\n\nUncertainty issues typically arise from factors that contribute to ambiguity during analysis, decision-making, or prediction. This is caused by insufficient information, measurement errors, inaccurate model assumptions, or other sources of variability. In addition to the aforementioned approaches for addressing batch effects and dropout events, uncertainty quantification can enhance model selection and performance evaluation. This procedure facilitates the mitigation of uncertainties arising from experimental data and model assumptions[61]. An example is scVI[128], which explicitly incorporates batch annotations and addresses batch effects through conditional independence assumptions (Fig.5(d)). This approach effectively isolates batch-related factors from the data, thereby reducing the uncertainties associated with batch differences and improving gene expression analysis. scVI models the observed expressionof each genein each cellas a random sample from a zero-inflated negative binomial distribution (ZINB) denoted as. Here,represents a low-dimensional Gaussian vector that captures biological differences between cells;is a one-dimensional Gaussian variable that accounts for variation due to differences in capture efficiency and sequencing depth, serving as a cell-specific scaling factor; anddenotes the batch annotation of the cell (if available). Employing variational inference and reparameterization techniques, scVI optimizes the posterior distribution via a variational lower bound. By incorporating sources of uncertainty, including cell-specific and batch-dependent features, this approach effectively preserves the information inherent in the original data. In contrast, posterior correction methods may rely on fixed assumptions, which can lead to the loss of critical information or introduce bias.\n\nSECTION: 3.2Data diversity\n\nfor tree=\nforked edges,\ngrow\u2019=0,\ndraw,\nrounded corners,\nnode options=align=center,,\ntext width=2.7cm,\ns sep=6pt,\ncalign=edge midpoint,\ntext=black,\n,\n[\nData Diversity,\n[\nMultimodalData Alignment, for tree= top_class\n[\nMulti-omicsData Alignment,\nfor tree=fill=red!45,generation\n[\nFeature selection,\ngeneration_more\n[\nSeurat v3[18];LIGER[204];iNMF[60];scAI[94],\ngeneration_work\n]\n]\n[\nVAE-based,\ngeneration_more\n[\nMultiVI[11];Cobolt[66];scMVAE[230];scMM[144];scMVP[113],\ngeneration_work\n]\n]\n]\n[\nAlignment ofSC and ST Data,\nfor tree=fill=red!45,generation\n[\nMarker-based,\ngeneration_more\n[[159],\ngeneration_work\n]\n]\n[\nLatent space-based,\ngeneration_more\n[\nSeurat[173];LIGER[204];Harmony[105],\ngeneration_work\n]\n]\n[\nStatistical modeling,\ngeneration_more\n[\nRCTD[25],\ngeneration_work\n]\n]\n[\nNeural network-based,\ngeneration_more\n[\nsoScope[112],\ngeneration_work\n]\n]\n]\n[\nIntegration ofOther Omics Data,\nfor tree=fill=red!45,generation\n[\nNeural network-based,\ngeneration_more\n[\nSpatialData[138];STAligner[226];ST-Net[78],\ngeneration_work\n]\n]\n]\n]\n[\nIntegration of Multi-Source Data,\nfor tree= top_class\n[\nVAE-based=fill=green!45,gpa,\ngpa_wide\n[\nDAVAE[85],\ngpa_work\n]\n]\n[\nGAN-based=fill=green!45,gpa,\ngpa_wide\n[\nscAEGAN[98],\ngpa_work\n]\n]\n[\nGraph-based=fill=green!45,gpa,\ngpa_wide\n[\nGLUE[30],\ngpa_work\n]\n]\n[\nBenchmark=fill=gray!45,top_class\ntopclass_wide\n[\nLuecken et al.[131],\ntopclass_wide\n]\n]\n]\n]\n\nThe \"central dogma\" delineates the flow of genetic information from DNA to RNA and subsequently to proteins, establishing a foundational framework for understanding how gene expression influences cellular functions. Omics data, obtained through high-throughput techniques, provide a systematic characterization of the various molecular components within an organism, including the genome, transcriptome, proteome, and metabolome. Recent advances in multichannel sequencing now allow simultaneous measurement of multiple types of omics data. Current transcriptome-focused multimodal techniques include combinations such as gDNA-mRNA[157,42], mRNA-methylation[7,84], mRNA-ATAC[27,35,134], mRNA-proteome[63,172], and mRNA-methylation-ATAC[199,39]. The observed heterogeneity encompasses intra-sample heterogeneity (resulting from differences in sequencing depth, coverage, and data type), inter-sample heterogeneity (caused by variations in experimental design, sample handling, and sequencing protocols), and variability across species and individuals. The diversity and complexity inherent in single-cell data present significant challenges, particularly when it comes to aligning and integrating paired and unpaired datasets. \"Paired\" data refers to multimodal datasets derived from the same sample, whereas \"unpaired\" data consists of multimodal datasets from different samples or platforms. Multi-omics analysis integrates these diverse data types to facilitate a comprehensive understanding of organismal heterogeneity and regulatory mechanisms. The overall structure of this section is shown in Fig.6.\n\nThis section focuses on the alignment of multimodal data, including multi-omics data as well as paired single-cell and spatial transcriptomics data, with the goal of uncovering intrinsic patterns in the alignment of homologous data.\n\nMulti-omics data alignment aims to align similar features while preserving the unique characteristics of each modality. LIGER[204]employs non-negative matrix factorization (NMF) to uncover latent structures in the data, extracting both shared and modality-specific gene expression patterns while minimizing distances between datasets (Fig.7(a)). iNMF[60]builds on LIGER by enabling online learning for enhanced data integration. Other comparable methods include scAI[94]. MultiVI[11]adopts a VAE framework where the encoder processes different molecular attributes, such as protein abundance, to generate modality-specific latent representations, denoted asand. The cell state is estimated as the average of these two representations:, thereby forming a joint latent space of multiple modalities. Modality-specific decoders then generate the observed values using a negative binomial distribution for transcriptomic data and a Bernoulli distribution for chromatin accessibility data. A constraint is imposed on the latent space to minimize the distance between these two representations. Additional VAE-based models include Cobolt[66], scMVAE[230], scMM[144], and scMVP[113]. scMVP maximizes the likelihood of jointly generated probabilities across multi-omics data by introducing a Gaussian mixture model (GMM) prior to obtain a shared latent embedding. Each modality is encoded separately using an asymmetric GMM-VAE model that incorporates two clustering consistency modules to align each imputed dataset while preserving the shared semantic information.\n\nSpatial mapping in ST involves aligning scRNA-seq data with physical spatial domains, matching the geometry of the spatial data. Traditional methods have aimed to reconstruct key marker genes by assuming continuity in gene expression or using local alignment information[159]. However, these methods are hindered by limited capture rates, significant dropout events, and sparse gene distribution, making them error-prone and unable to generalize across different experimental settings. More recent approaches, such as Seurat[173], LIGER[204], and Harmony[105], integrate scRNA-seq with ST data through shared latent spaces and mutual nearest neighbors (MNN). This integration facilitates the transfer of cell-type labels while enhancing weak ST signals. RCTD[25]integrates reference scRNA-seq data to model the average gene expression profiles of different cell types. It utilizes a hierarchical model to estimate the proportion of each cell type within individual spatial spots. The method applies a Poisson distribution to estimate gene expression counts and employs maximum likelihood estimation (MLE) for parameter estimation. soScope[112]adopts a multimodal DL framework that integrates spot-level omics maps (transcript, histone, DNA, protein), spatial relationships, and high-resolution morphological images. It jointly infers high-resolution spatial maps using a variational Bayesian inference network (Fig.7(d)).\n\nMany open-source frameworks have been developed to facilitate the alignment and integration of multimodal spatial omics data, including SpatialData[138], SSGATE[132], STAligner[226]and SpatialGlue[127]. For example, SpatialGlue[127]can be used to integrate three modalities, including spatial epigenome\u2013transcriptome and transcriptome\u2013proteome modalities. For aligning 2D slices, STAligner[226]employs a triplet-based approach to identify mutual nearest neighbors that exhibit similar gene expression patterns across different slices. This method facilitates coordinate registration of stacked consecutive slices, enabling 3D tissue reconstruction (Fig.7(c)). ST-Net[78]integrates paired H&E-stained pathology images with ST data to train an end-to-end neural network for predicting spatially resolved transcriptomics from pathology images.\n\nRecent studies have highlighted advancements in ST techniques[86], although the field is still challenged by a trade-off between spatial resolution and measurement throughput. spatial transcriptomics, bridging imaging and sequencing, holds great potential for integrating diverse modalities from histopathology and single-cell data, offering deeper insights into spatial organization, microenvironmental interactions, and histopathology. Moving forward, the integration of multimodal data remains a key challenge in single-cell and spatial transcriptomics analysis.\n\nIntegration of unpaired datasets, such as those derived from different samples or sequencing platforms, usually requires alignment of independent feature spaces. In this context, cross-modal integration aims to mitigate discrepancies in embeddings of the same cell type across heterogeneous modalities. Seurat v3[173]identifies common anchors (cell pairs) across datasets based on features (such as genes) that exhibit high variability among cells, integrating data using these anchors (Fig.7(b)). It can integrate scRNA-seq with scATAC-seq, allowing for an investigation into chromatin differences.\n\nGenerative DL models have been widely employed to capture complex semantic relationships in multi-source datasets. For example, DAVAE[85]integrates large-scale unpaired data through three essential components: a variational approximation network, a generative Bayesian neural network, and a domain adversarial classifier. This setup benefits the learning of latent representations, enhances data fitting, and mitigates batch effects to accurately represent cellular biological states across diverse datasets.\nSimilarly, scAEGAN[98]combines autoencoders (AE) with conditional generative adversarial networks (cGAN) to facilitate the translation between different single-cell datasets. It effectively transforms the dataset by using AE to remove random noise, while employing cGAN with recurrent consistency regularization. GLUE integrates omics-specific autoencoders with graph-based coupling and adversarial alignment to model regulatory interactions between omics layers, supporting integrated regulatory inference for unpaired multi-omics datasets (Fig.7(b)).\n\nRecent benchmark for multi-source data integration has collected 19 methods for data integration on seven benchmark datasets, as detailed by Luecken et al.[131], indicates that NMF-based and nearest neighbor approaches exhibit superior performance in average bio-conservation and batch correction across all datasets, respectively. We also observed the effectiveness of the VAE-based and GAN-based model in preserving biological consistency (Fig.8).\n\nSECTION: 3.3Data scarcity\n\nThe overall structure of this section is shown in Fig.9.\n\nfor tree=\nforked edges,\ngrow\u2019=0,\ndraw,\nrounded corners,\nnode options=align=center,,\ntext width=2.7cm,\ns sep=6pt,\ncalign=edge midpoint,\ntext=black,\n,\n[\nData Scarcity,\n[\nMissing Data Annotation, for tree= top_class\n[\nStatistical modeling=fill=green!45,gpa,\ngpa_wide\n[\nscDesign3[168],\ngpa_work\n]\n]\n[\nGAN-based=fill=green!45,gpa,\ngpa_wide\n[\nGRouNdGAN[229],\ngpa_work\n]\n]\n[\nBenchmark=fill=gray!45,top_class\ntopclass_wide\n[\nPratapa A. et al.[151];Cao Y. et al.[29],\ntopclass_wide\n]\n]\n]\n[\nMissing Modalities,\nfor tree= top_class\n[\nVAE-based=fill=green!45,pretraining,\npretraining_wide\n[\nTotalVI[62];UniPort[28];POE[206];MOE[166];CGVAE[123],\npretraining_work\n]\n]\n[\nBenchmark=fill=gray!45,top_class\ntopclass_wide\n[\nHu Y. et al.[86];Makrodimitris S. et al.[137],\ntopclass_wide\n]\n]\n]\n]\n\nSingle cell data has reached sequencing scales of hundreds of millions. Due to the significant labor and time required in laboratory settings, existing datasets often present challenges in obtaining large-scale biological annotations. In addition, single-cell data contains many complex biological factors, making it difficult to obtain a reasonable and reliable ground truth. For instance, benchmark datasets for experimental analysis of single-cell population evolution require follow-up samples with known evolutionary trajectories and developmental timelines, which are difficult to obtain under experimental conditions[106]. Moreover, the reliability of model evaluation often depends on high-quality data annotations. For example, since regulatory interactions in databases are aggregated from broad datasets and lack specificity to particular biological systems, it is unreliable to evaluate the performance of gene regulatory network (GRN) inference algorithms. A key technical solution to this problem is the construction of simulation datasets.\n\nIt has been extensively employed to evaluate and compare computational methods, concentrate on specific biological features, and establish more precise benchmarks[151]. Cao et al.[29]conducted a comprehensive review of various simulation approaches and proposed a framework for systematic benchmarking, highlighting their ability to capture biological signals and higher-order interactions, such as the mean-variance relationship among genes. However, most existing methods generate simulated data tailored to specific evaluation objectives, such as clustering or differential gene expression analysis. There are few tools specifically designed to create datasets that are applicable across diverse scenarios.\n\nscDesign3[168]employs statistical modeling methods to generate single-cell multi-omics data and spatial transcriptomics data with known cell proportions. It standardizes generative modeling approaches across various data modalities, rather than focusing on only one modality. Given a cell state covariate(factors such as cell type, cell pseudotime, and cell spatial locations) and experimental design covariates(such as batch effects and experimental conditions), the measurement valuesare modeled according to a specific distribution. This is formulated as a generalized additive model for location scale and shape (GAMLSS) , characterized by its position, proportion, and shape parameters.\n\nThe model is parametrically represented, incorporating specific link functions for each feature(distribution functions, such as Gaussian (Normal), Bernoulli, Poisson, ZINB) These link functions correspond to the mean parameter, the scale parameter(for example, standard deviation or dispersion), and zero-inflation proportion parameter. For instance, the specific link functionsfor featuresmaps the mean parameterto the model\u2019s linear predictor. This mapping depends on the chosen distribution function and consists of four key components:\n\nThe specific interceptfor feature, represents the mean of featurein the absence of other influencing factors. The batch effect, captures the influence of batchon feature. The conditional effectdenotes the impact of conditionon feature. The cell state covariates, such as the effects associated with different cell types on feature, are also considered. In scDesign3, the joint distribution of cellular features is constructed using a marginal cumulative distribution function and a copula model with parameters estimated by a maximum likelihood method. These parameters can be adjusted to generate synthetic data reflecting varying sequencing depths and cell states. Furthermore, scDesign3 is capable of producing spatial transcriptomic data based on cell type proportions derived from single-cell sequencing, simulating realistic ATAC-seq datasets at both count and read levels, and generating multi-omics datasets by integrating separate omics datasets like RNA expression or DNA methylation.\n\nIn summary, statistical modeling provides a highly interpretable framework for data generation and analysis, and its integration with DL is emerging as a significant trend.\n\nGlouNdGAN[229]is a causal model-based data generation method that allows the generation of realistic simulated data aligned with the underlying principles of GRN. The architecture of GlouNdGAN consists of five sub-networks: Causal Controller, Target Generator, Critic, Labeler, and Anti-labeler. The Causal Controller generates expression values for transcription factors (TFs), while the Target Generators produce expression values for target genes based on the causal GRN framework. The Critic estimates the Wasserstein distance between the generated data and the real data to ensure that the target gene expression is causally related to TF expression. The Labeler predicts TF expression based on generated and real target gene expression, while the Anti-labeler estimates TF expression solely from generated target gene expression. This model pre-trains the TF expression generation module and subsequently generates expression values for other genes via the Target Generator. GlouNdGAN allows researchers to simulate interference by manipulating TF expression values during the inference phase, enabling an accurate comparison of gene expression before and after interference while maintaining constant parameters. Additionally, by performing mutation experiments on TF expression for certain cell types, the researchers observed alterations in the characteristics of the generated cells, thus validating the function of TFs and their relationship to phenotypic labels. This capability positions GlouNdGAN as an ideal tool for in-situ interference experiments.\n\nAs discussed above, simulation data generation serves as a valuable tool for elucidating biological mechanisms in contexts where high-quality data is lacking. It allows the creation of diverse datasets with controllable parameters and facilitates the evaluation of model performance.\n\nAlthough genetic information is transferred from DNA to RNA and then to proteins, each modality captures distinct biological information, making it impossible for one modality to substitute for another. It has been demonstrated that multimodal analysis enhances the overall understanding of cellular heterogeneity. However, multi-channel sequencing typically incurs higher costs compared to single-channel sequencing. DL-based solutions commonly rely on VAE architectures that use either single-modality or multi-modality joint embeddings for shared latent space modeling[137]. The difference lies in how the latent variables are modeled (Fig.10). TotalVI[62]is trained on the joint embeddings of the two modalities with separate reconstruction. UniPort[28]trains a single-modality embedding to reconstruct two different modalities, compelling the encoder to learn features that are predictive of both. In the Product of Experts (POE) model[206], the joint latent variable is derived as a product of each modality\u2019s. Unlike POE, Mixture of Experts (MOE)[166]employs the sum of the joint latent variables for data reconstruction. Constrained Graph Variational Autoencoders (CGVAE)[123]learns feature embeddings for each modality individually while applying constraints to ensure that each modality can reconstruct both itself and the other modalities. Based on the benchmark results, Makrodimitris, S. et al.[137]concluded that different joint embeddings can be used for different downstream tasks.\n\nScenarios of modality completion are often related to data sparsity. Monae[181]employs data imputation to perform data denoising and modality completion simultaneously, constrained by a cross-modal prediction loss. In the feature extraction phase, a graph encoder-decoder reconstruction process extracts embedding features from multiple modalities, and contrastive learning is applied to minimize the spatial distance between embeddings of the same cell type. Therefore, when discriminative information from one modality is lacking, the latent space embeddings of other modalities can be leveraged for reconstruction. UnitedNet[180]combines multimodal ensemble with cross-modal prediction in a multi-task learning framework, trained with cross-modal prediction loss alongside generator and discriminator losses.\n\nWe have collected 17 methods, including BABEL[205], CMAE[213], LIGER[204], Seurat[173], cTP-net[227], scArches[129], scMoGNN[108], scVAEIT[47], sciPENN[107], totalVI[62], Generalized Linear Model (GLM), MCIA[142], MOFA[9], CGVAE[123], ccVAE[176], PoE[206], MoE[166], to evaluate their modality prediction performance on four benchmark datasets[86,137]. Among all the methods, totalVI shows highest cell-cell PCC on predicted modalities and PoE shows better performance than other VAE-based models (Fig.11).\n\nSECTION: 3.4Data correlation\n\nUnderstanding the relationship between biological systems and external factors is crucial to gain deeper insights into cellular dynamics and the interactions between cells and their environment. Modeling data correlation involves capturing these complex interactions and dependencies, which are affected by both spatial and temporal variations, as well as biological prior knowledge. The overall structure of this section is shown in Fig.12.\n\nfor tree=\nforked edges,\ngrow\u2019=0,\ndraw,\nrounded corners,\nnode options=align=center,,\ntext width=2.7cm,\ns sep=6pt,\ncalign=edge midpoint,\ntext=black,\n,\n[\nData Correlation,\n[\nModelingspatiotemporaldependencies, for tree=top_class\n[\nRegression-based=fill=green!45,gpa,\ngpa_wide\n[\nWalter F. C. et al.[192];Townes F. W. et al.[186];\u00c4ij\u00f6 T. et al.[3];MEFISTO[189],\ngpa_work\n]\n]\n[\nGraph-based,\nfor tree=fill=green!45,gpa,\n[\nMRF-based,\ngpa_wide\n[\nZhu Q. et al.[228];Giotto[46],\ngpa_work\n]\n]\n[\nGNN-based,\ngpa_wide\n[\nDeepLinc[117];NCEM[57],\ngpa_work\n]\n]\n[\nCNN-based,\ngpa_wide\n[\nTan X. et al.[179],\ngpa_work\n]\n]\n[\nRNN-based,\ngpa_wide\n[\nAlmagro A. et al[5],\ngpa_work\n]\n]\n]\n]\n[\nModeling prior knowledge,\nfor tree= top_class\n[\nNeural network-based,\nfor tree=fill=green!45,pretraining,\n[\nPathway information,\npretraining_wide\n[\nYan H. et al.[211],\npretraining_work\n]\n]\n[\nRegulatory networks,\npretraining_wide\n[\nGLUE[30];Yan H. et al.[211];stImpute[220];GRNInfer[115],\npretraining_work\n]\n]\n]\n[\nBenchmark=fill=gray!45,top_class\ntopclass_wide\n[\nXie Z. et al.[208];CITEdb[164];DeepCCI[214];Liu Z. et al.[125],\ntopclass_wide\n]\n]\n]\n]\n\nModeling temporal and spatial dependencies is crucial for the analysis of single-cell and spatial transcriptomics data, as numerous biological processes exhibit dynamic spatiotemporal correlations. Examples include cell differentiation during development[16], the spatial organization of cells within tissues[109], disease progression pathways[163], and variations in immune responses over time and space. Capturing these features can reveal dynamical shifts in cell states, cell-cell interactions, and complex tissue or disease structures. Spatiotemporal omics data encompass longitudinal molecular profiles from patients, molecular profiles across developmental stages, and continuous spatiotemporal omics maps. However, making comparisons across varying scales, biological samples, or conditions remains a challenging task. For example, establishing statistical correlations between samples requires the alignment of temporal and spatial coordinates across individuals or biological scales. Current approaches mainly rely on regression-based and graph-based models[190].\n\nAmong regression-based models, Gaussian process-based probabilistic models are widely used[192,186,3]. These models are effective in capturing trends of continuous variability for both time series and spatially distributed data. MEFISTO[189]leverages the Gaussian process to model latent factors by incorporating temporal and spatial information, as well as grouped data, into the covariates within the Gaussian kernel. The covariance function consists of two components: one that describes relationships across different groups (e.g., sample sets or experimental conditions), and another that captures smooth variations such as spatial locations or time points. This dual structure allows Gaussian processes (GP) to account for both inter-group heterogeneity and intra-group covariate variation. By ensuring that samples located closer together in the covariate space share more similar latent factors, the Gaussian process effectively models the continuity of relations between data points.\n\nAnother approach for jointly modeling omics latent featuresinvolves the use of graph models. Markov random fields (MRF) are undirected graphical models that assume the distribution of each node depends only on the labels of its neighboring nodes. Compared to non-parametric regression models like GP, MRF offer greater computational efficiency, as they do not require inference of the complete covariance structure across all samples. Qian Zhu et al.[228]proposed a Markovian property for spatial patterns, which constrains the correlations between neighboring nodes. By assuming that labels of neighboring cells, including gene expression states or cell types, exhibit a degree of similarity, the joint probability distribution over the spatial domain can be factorized into a product of local neighborhood probability distributions. The probability distribution for the label associated with the current nodeis jointly modeled using both its neighboring nodes\u2019 labelsand its own gene expression:\n\nGiotto[46]utilizes MRF with conditional probability distributions, such as Gaussian or Poisson, to enhance spatial clustering. This approach effectively captures smooth and continuous expression changes, thereby facilitating the identification of spatially structured cell populations.\n\nDL-based graph frameworks are increasingly employed to explore cell-cell interactions, including recurrent neural network (RNN), CNN, and GNN.\nIn GNNs, cells are represented as nodes, with edges denoting potential interactions, such as those between ligand-receptor pairs. This approach effectively integrates spatial data and gene expression profiles to reveal interaction patterns. For instance, DeepLinc[117]posits that neighboring cells are more likely to interact than cells further apart (Fig.13(a)). It constructs a cell adjacency graph based on the physical distance between cells and learns embedding features that reflect the likelihood of interactions by aggregating information from each cell along with its neighbors. Using variational graph autoencoders (VGAEs) and adversarial networks, DeepLinc employs unsupervised learning techniques to uncover intrinsic associations between the cell adjacency graph and gene expression profiles, thereby reconstructing interaction networks. NCEM[57]utilizes GNN to reconstruct gene expression vectors from cell type labels and niche composition, which are represented through graph-level predictors and adjacency matrices derived from spatial proximity. While NCEM incorporates a linear model for mathematical interpretability, experimental results demonstrate that its nonlinear variant significantly outperforms the linear one. Different from GNNs, CNNs[179]aggregate features from neighbouring regions in images through convolution operations. RNNs[5], on the other hand, propagate information from adjacent spatial points using recurrent connections.\n\nOverall, DL frameworks exhibit considerable flexibility in analyzing spatiotemporal omics data.\n\nSingle-cell data is characterized by sparse features, multi-source heterogeneity, and lack of high-quality labels, making it unreliable to draw experimental conclusions solely from the observed data. However, the incorporation of prior biological knowledge such as gene regulatory networks, cell type characteristics, and developmental trajectories, can significantly enhance the accuracy and interpretability of the analysis. Nonetheless, effectively integrating prior knowledge while avoiding potential biases and overfitting remains a challenging task.\n\nPrior knowledge mainly refers to pathway information and regulatory networks[211]obtained from databases. Pathway information concerns to molecular interactions and biochemical reactions that drive specific biological processes, such as signal transduction, metabolism, and cellular activity. This information aids in elucidating how cells respond to external stimuli or internal changes. In the context of single-cell and multi-omics analyses, pathway information is used to infer gene-gene interactions, characterize cell types, and determine cell states. It is typically sourced from well-established databases such as KEGG[96], Reactome[52], and WikiPathways[2]. Regulatory networks involve the interactions among genes, transcription factors, proteins, and other biomolecules that regulate gene expression and cellular function. These networks are commonly represented as graph where nodes denote biomolecules (e.g., genes or proteins) while edges indicate regulatory relationships (e.g., activation or inhibition). Databases such as STRING[177]and GeneMANIA[202]provide valuable insights into protein-protein interactions and gene-gene interactions, respectively. Regulatory networks facilitate the understanding of the mechanisms governing gene expression regulation, the identification of key regulatory factors, and the revelation of cell-specific patterns in gene expression.\n\nGLUE[30]integrates multi-omics data through a guidance graph, where nodes represent features from various modalities, such as genes in scRNA data and accessible chromatin regions in ATAC-seq data(Fig.13(b)). Graphs establish connections between ATAC peaks and RNA genes based on overlapping gene bodies or promoter regions. A variational posterior is employed to reconstruct the guidance map and its latent space is used as a prior for multi-omics data reconstruction. The decoder computes an inner product of feature and cell embeddings to ensure consistent embedding directions across different modalities. Hongxi Yan et al.[211]aggregate gene features within the same biological pathway to obtain pathway-level features for predictive modeling. They utilize the KEGG database together with an ensemble gradient method to identify key pathways, which significantly enhances model interpretability.\n\nSome methods use prior knowledge from existing databases to initialise edge features in gene-gene interaction networks. For instance, DeepCCI[214]uses the constructed LRIDB database to define receptors and establishes interaction networks between cell clusters based on known ligand-receptor (L\u2013R) pairs, predicting interactions by a combination of ResNet and graph convolutional network (GCN) outputs (Fig.13(c)). stImpute[220]leverages the ESM-2 protein language model to embed proteins and constructs a network of gene relationships using cosine similarity. GRNInfer[115]incorporates gene regulatory relationships from RegNetwork[126]as prior information to construct a gene graph network.\n\nFurthermore, we have collected 10 methods, including CellChat[93], CellPhoneDB[48], iTALK[201], LIANA[43], NATMI[83], scMLnet[36], SingleCellSignalR[24], Connectome[155], CytoTalk[87], and CellCall[222]. These methods leverage existing L-R pair knowledge to infer cell-cell communication, and we evaluate their cell-cell interaction prediction performance on five benchmark datasets[208,164,214,125]. Among all the methods, CellPhoneDB ranks among the top across all benchmark datasets, demonstrating the robustness of extracting cellular context (Fig.14).\n\nSECTION: 4Conclusion and Future Perspectives\n\nSECTION: 4.1Innovative AI Method for Single-Cell and spatial transcriptomics data analysis\n\nThe rapid expansion in the size, depth, and complexity of single-cell and spatial transcriptomics data requires the development of algorithms capable of effectively capturing complex gene expression patterns and spatial distributions.\n\nRecently, foundational models have emerged as a focus of single-cell omics. By leveraging self-supervised pre-training on large unlabeled scRNA-seq datasets, these models capture complex features and patterns, producing unified representations that can be fine-tuned for specific downstream tasks[212,185,77,40]. For instance, SCimilarity[79]enables rapid querying of cell states for cell type annotation. scMulan[20]converts single-cell transcriptomic data along with rich metadata (e.g., cell type, spatial context, and temporal aspects) into \"cell sentences\" (c-sentences), achieving superior performance in tasks like zero-shot cell annotation and batch correction. scGPT[40], which is based on the GPT architecture, employs self-supervised pretraining with condition tokens to model gene interactions within cells, incorporating cell type labels for tasks such as cell type prediction, and applies a \"binning\" technique to ensure semantic alignment across diverse datasets.\n\nFuture advancements in large language models, such as OpenAI\u2019s O1111https://openai.com/o1/, and agent-based methods, are expected to further enhance single-cell and spatial transcriptomics analysis. O1 leverages large-scale reinforcement learning algorithms to achieve chain-of-thought (COT) reasoning, thereby improving inference accuracy. Agent-based approaches, such as ReAct[216], integrate real-time observations to guide decision-making, facilitating more efficient and proactive error correction. These models offer considerable potential for providing interpretable inferences across a variety of downstream tasks in single-cell analysis.\n\nHowever, in contrast to parametric modeling approaches, end-to-end networks often operate as \"black boxes,\" providing limited interpretability of the inference processes. Moreover, these networks are typically trained on specific datasets and lack dynamic updates, which constrains their generalizability and robustness on unseen or uncertain data. For scientific scenarios, it is crucial to strike a balance between interpretability and accuracy. As indicated by previous studies, it may be feasible to enhance interpretability by establishing connections between prior models and observed outcomes through interpretable parametric rules.\n\nSECTION: 4.2Benchmark Datasets and Evaluation Metrics\n\nRelevant benchmarks have been established for various stages of single-cell sequencing data analysis workflows, including imputation[41], cell identification[1], clustering[193], gene regulatory networks[13], cell-cell interactions[195], and multi-omics data integration[12]. However, several studies have mentioned that the datasets and evaluation metrics employed in these benchmarks do not accurately reflect the strengths and weaknesses of contemporary algorithms[151]. Moreover, as large-scale sequencing data continues to evolve, algorithms that previously demonstrated strong performance may no longer be applicable in different application contexts[99]. These datasets may exhibit increased heterogeneity due to samples derived from diverse sequencing platforms or continuous-time and continuous-space settings.\n\nCurrent evaluation metrics mainly emphasize performance metrics such as accuracy, AUC, and RMSE, with a limited focus on biological relevance. To ensure the biological validity of model predictions, systematic validation through in vitro experiments is essential. For example, trends in gene expression, molecular properties, or cellular behavior can be compared to experimental results to confirm biological significance. Therefore, it is important to establish benchmarks that provide a more comprehensive and objective assessment of the generalizability and applicability of algorithms. Data simulations that generate benchmark datasets based on well-defined rules can provide a diverse array of labeled data for evaluation. This approach has already been applied to tasks such as cell identity recognition and modelling of gene regulatory networks[168,229], highlighting its potential as a robust tool for evaluating model performance.\n\nSECTION: 4.3Application of DL in practical scenarios\n\nHere, we summarize the applications of single-cell and spatial transcriptomics in biology, medicine, and clinical practice, providing an overview of the background for DL applications in these fields.\n\nIn biology, single-cell and spatial transcriptomics focus on embryonic[152], tissue[45], and organ development[21]. These techniques facilitate the identification and classification of various cell types and lineages, while providing insights into the evolution of cell populations throughout organogenesis.\n\nIn precision medicine, the analysis of single-cell transcriptomic data is critical for investigating disease heterogeneity[74], identifying distinct subclones within diseases[154], discovering critical disease biomarkers[4], characterizing interactions between normal and diseased cells[215], elucidating relevant signaling pathways[91], and predicting resistance[203]. Single-cell transcriptomics facilitates the construction of comprehensive cellular maps that significantly enhance the discovery of novel biomarkers and therapeutic targets[187]. In additon, scRNA-seq has demonstrated significant potential for improving patient outcomes and accelerating the development of personalized therapies[182,90].\n\nIn clinical applications, scRNA-seq plays a crucial role in characterizing patient-specific features[169]. It assists in the identification of biomarkers for patient stratification[149], elucidates the underlying mechanisms of drug action and resistance[153], supports the development of personalized treatment strategies, and enables monitoring of drug response and disease progression[124].\n\nSECTION: 5\n\nKey Points\u2022This review discusses four major challenges and related deep learning approaches in single-cell and spatial transcriptomics data analysis.\u2022This review curates 21 datasets from 9 benchmarks covering 58 computational methods and compares their performance on their respective modeling tasks.\u2022This review outlines three future research directions regarding data, methods, and applications for single-cell and spatial omics data analysis.\n\nSECTION: 6Competing interests\n\nNo competing interest is declared.\n\nSECTION: 7Author contributions statement\n\nShuang Ge and Zhixiang Ren collected and reviewed literature. Shuang Ge, Shuqing Sun, Qiang Cheng and Zhixiang Ren drafted the manuscript. All authors read and approved the final manuscript.\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.03614v2_content.txt"}, {"title": "Comparison of Deep Learning and Particle Smoother Expectation\n  Maximization Methods for Estimation of Myocardial Perfusion PET Kinetic\n  Parameters", "authors": ["Myungheon Chin", "Sarah J Zou", "Garry Chinn", "Craig S. Levin"], "published_date": "2024-12-06T01:39:10Z", "summary": "Background: Positron emission tomography (PET) is widely used for studying\ndynamic processes, such as myocardial perfusion, by acquiring data over time\nframes. Kinetic modeling in PET allows for the estimation of physiological\nparameters, offering insights into disease characterization. Conventional\napproaches have notable limitations; for example, graphical methods may reduce\naccuracy due to linearization, while non-linear least squares (NLLS) methods\nmay converge to local minima. Purpose: This study aims to develop and validate\ntwo novel methods for PET kinetic analysis of 82Rb: a particle smoother-based\nalgorithm within an Expectation-Maximization (EM) framework and a convolutional\nneural network (CNN) approach. Methods: The proposed methods were applied to\nsimulated 82Rb dynamic PET myocardial perfusion studies. Their performance was\ncompared to conventional NLLS methods and a Kalman filter-based\nExpectation-Maximization (KEM) algorithm. Results: The success rates for\nparameters F, k3, and k4 were 46.0%, 67.5%, and 54.0% for the particle smoother\nwith EM (PSEM) and 86.5%, 83.0%, and 79.5% for the CNN model, respectively,\noutperforming the NLLS method. Conclusions: The CNN and PSEM methods showed\npromising improvements over traditional methods in estimating kinetic\nparameters in dynamic PET studies, suggesting their potential for enhanced\naccuracy in disease characterization.", "arxiv_id": "2412.04706v1", "html_link": "https://arxiv.org/html/2412.04706v1", "search_term": "ti:\"deep learning\"", "html_content": "SECTION: I.Introduction\n\nPositron Emission Tomography (PET) is a nuclear medicine imaging technique that measures the three-dimensional distribution of radiolabeled molecules, known as tracers. Dynamic PET uses time-resolved imaging data to produce time activity curves (TAC) for image voxels or specified regions of interest (ROIs) to estimate kinetic parameters such as cardiac flow and receptor binding rate constants.[1]. This paper examines the kinetic analysis of dynamic myocardial perfusion PET studies conducted with the tracer82Rb.\n\nCardiovascular disease is one of the main causes of death globally. Accurate assessment of the risks of the disease and early detection of vulnerable patients is crucial[2]. Dynamic PET enables non-invasive imaging of temporal dynamics of molecular activities within a patient[1]. This approach provides additional information to assess risk of coronary artery disease (CAD) by enabling the evaluation of regional myocardial blood flow (MBF)[3,4,5,6,7,8]. Various tracers are commonly used when quantifying MBF with PET:15O-water,13N-ammonia,82Rb,11C-acetate, and18F-flurpiridaz[4,8]. Among these,82Rb is frequently used since it can be produced by an82Sr\u2005/82Rb generator and it has a short half-life (76.4 s), enabling multiple perfusion tests in short period of time[7,8]. However, the short-half life also results in poor signal-to-noise ratio images.\n\nQuantification of MBF via dynamic PET imaging requires appropriate kinetic modeling[9,10,11]. The radioactive isotope82Rb is typically modeled using a two-tissue compartmental model[3,5,7]. Estimating kinetic parameters in multi-tissue compartment scenarios often involves nonconvex optimization challenges[9,10,1,11], as objective functions in these problems can have multiple local minima, complicating parameter estimation. Standard approaches like non-linear least squares (NLLS) and graphical methods, such as the Patlak or Logan plots[12,13,11], have their respective limitations. NLLS methods, though computationally demanding, tend to produce more accurate results. In contrast, graphical methods offer quicker estimation by linearizing the problem, but this simplification may introduce errors. Despite the precision of NLLS methods, they can still yield suboptimal results. Challenges remain in accurately estimating kinetic parameters, as both methods balance trade-offs between computational efficiency and accuracy.\n\nIn our study, we investigated two novel methods for estimating PET kinetic parameters by simulating a two-tissue compartmental model of82Rb and assessing their performance against the trust region-based NLLS method and a Kalman-smoother EM (KEM) benchmark. The KEM, similar to what Gibson and Ninnes described[14], leverages the expectation-maximization (EM) algorithm, which iteratively alternates between estimating hidden states (E-step) using the Kalman smoother[15,16,17]and maximizing the Q function (M-step). The EM algorithm decomposes the multi-dimensional parameter solution space into a more manageable lower-dimensional optimization problem. The first novel method, the particle-smoother EM (PSEM), extends the EM algorithm by replacing the Kalman smoother with a particle smoother[18,19], enabling it to handle more complex state dynamics.\n\nThe second novel approach involved a convolutional neural network (CNN), a deep learning strategy that exploits spatial and temporal coherencies in datasets[20,21]. CNNs have shown strong performance in time-series analysis[22]and PET applications[23,24]. To our knowledge, this study represents the first application of a CNN to dynamic82Rb PET. We enhanced a traditional CNN model by incorporating a Time2Vec layer[25]to better capture temporal information. These methods were compared to assess their accuracy and efficiency in estimating PET kinetic parameters.\n\nSECTION: II.Methods\n\nSECTION: II.A.2TC Model for82Rb PET\n\nMany studies have demonstrated that regional myocardial perfusion PET studies with82Rb are sufficiently modeled by a two-compartment model[26,3,5]. Shown in Fig.1,denotes the flow rate from the plasma to the first compartment(the fast exchangeable state),represents the volume of the first compartment, andandthe flow rate into and out of the second compartment.can also be described astimes the total volume of the tissue, withbeing the ratio of the volume of the first compartment to the total volume of the tissue. The 2TC Model for82Rb PET can be described by the following differential equations:\n\nwhereis a concentration of the activity in plasma denoted as an input function in the model, andandare the amount of tracer in the first and second compartment, respectively.\n\nThese differential equations can be represented in the state-space form of a linear dynamical system with a hidden state vector, an input vector, and an output vector:\n\nwhere theandmatrices are application-dependent. The output vectortypically represents the decay-corrected observable tracer activities in a region of interest (ROI) for PET kinetic analysis.\n\nIn this work, we defined the matricesandin (5) for82Rb as below:\n\nwhererepresents the plasma fraction of tracer concentration in ROI.\n\nWe can discretize the system by:\n\nThis leads to the identification ofandin (4) as below:\n\nSince PET images are distributions of radioactive decay concentration measured by photon counts over a time frameand the observed concentration decays over time, we simulated the PET measurementand the input measurementusing the following equations:\n\nHere,is defined as follows:. The discrete state-space representation can be categorized as a hidden Markov model (HMM). HMM inherits the Markov property, indicating that the current hidden state depends solely on the previous state. In this context, the hidden states denoted asare an intuitive choice for the latent variables essential for an EM algorithm.\n\nSECTION: II.B.Data Generation\n\nTo generate simulation data, we first randomly assigned each kinetic parameter a value within a bounded range that reflects realistic measurements, similar to the values reported in the literature[5], assuming a uniform probability distribution for each kinetic parameter within those ranges. The range of random selection for each parameter is summarized in Table1. Secondly, we set the initial state as a zero vector, assuming all tracer concentrations are zero at the start of the scan. Then, using the selected parameters, we employed the 2TC model to generate the state variables.\n\nTo generate the input function, we used a similar analytical function to[5]for the simulation input function:. However, instead of using fixed values of 39,218 and 1,428 forand, respectively, we setandto be uniform random variables that vary between 90% to 110% of the values suggested in[5].\n\nTo simulate realistic PET measurements, additional noise calculated using the variance mean ratio (VMR) was introduced based on frame duration. This noise level was derived from a real dataset. For detailed derivations, see AppendixNoise estimation.\n\nOnce we had simulated the PET measurements, we interpolated and decay-corrected them to acquire the estimated observed tracer concentration. Note that, instead of using true observable activitydirectly for parameter estimation, we performed these two additional steps, convertingtoand back to, to include errors caused by the interpolation and the decay correction steps, which are also present in realistic kinetic analysis scenarios. The estimated input vectorwas also acquired by following the same process. Using simulation, we generated 80,000 data sets to train and cross-validate the neural network, and then additional 200 data sets were created to test the methods proposed in the work.\n\nSECTION: II.C.Parameter estimation algorithms\n\nWe employed a trust-region algorithm as a baseline non-linear least squares (NLLS) method and compared it with our proposed approaches. Considering a set of parameters, along with residuals fromfunctions, the NLLS loss functionis formulated as:\n\nTo optimize (8), we employed the trust-region method available in the SciPy library[27]. It should be noted that while many NLLS-based approaches enhance the algorithm with regularization strategies and manual adjustments, we opted to employ the method in its basic form. This was to ensure an equitable comparison with the methods that we are introducing.\n\nOne possible strategy for addressing the parameter estimation challenge is the utilization of the EM algorithm. In this section, we provide a concise overview of the EM algorithm, largely drawing upon the description byRoweis and Ghahramani[17].\n\nRather than directly estimating the maximum-likelihood parameters from the measurements, the algorithm infers the parametersby initially assuming the existence of underlying latent states, with the measurements, assuming that we havetimepoints. To estimatethat optimizes the likelihood essentially involves identifying thethat maximizes the following log-likelihood:\n\nwhereindicates a probability distribution given. To facilitate the optimization of the log-likelihood function, we can postulate a distribution over the state variables, termed the Q function. This allows us to derive the lower bound ofutilizing Jensen\u2019s inequality:\n\nThe initial term in (10b), referred to as the expected energy relative to the distribution, and the latter term denotes the entropy of. The EM algorithm optimizes iteratively alternating between the Expectation (E) step and the Maximization (M) step. The E-step approximates the log-likelihood by calculating, as denoted in (10), and by formulating the distribution. It can be shown that the optimaldistribution at stageis[17]. The M-step optimizes the parameter setthat augments the expected energy term in (10b), since the entropy component is invariant with respect to. The EM algorithm can be succinctly encapsulated as follows:\n\nEM Algorithm\n\nThe energy term that being optimized in (11b) can be reformulated as follows[19]:\n\nwhere\n\nBy making two assumptions that the kinetic model of our choice is linear and the noise distribution is Gaussian, we can perform the E-step by using a Kalman smoother[28]. The implementation and the derivation of the Kalman filter and Kalman-EM (KEM) can be found in literature[17,14].\n\nWe briefly describe the particle filter and particle smoother here and show that the three terms in (13) can be calculated with the particle smoother-based approach. A particle filter is also known as a sequential Monte Carlo method and can be used to find the solution of non-convex problems. It uses multiple \u201cparticles\u201d to sample the solution space at each iteration. With properly tuned sampling, the particles will converge to the maxima. Particle filters are related to genetic algorithms. Whereas genetic algorthms rely on heuristic rules, particle filters rely on rigorous statistical theory for the iterative updates. The details of the derivations can be found in[19,29]:\n\nWhen calculating each term in (13) in the E-step, the main challenge is to calculate the integrals ofand. The original form of the particle filter gives a Bayesian solution of the latent state sequence estimates by a sum of samples multiplied by certain weights:\n\nThe termdenotes the state sequence from the initial state to the state at timefor theth particle, whilerepresents the corresponding relative weight of this particle anddenotes the number of samples (particles). The sum of the weights for all particles,, is equal to one. The Dirac delta function ensures that these weights are factored into the probability distribution solely when there is an exact match between the actual state sequenceand the particle\u2019s state sequence. The weights as referenced in (14) are determined using the following equation:\n\nHere,denotes an importance density[29]. And it can be shown that the following equation holds by applying chain rule and Bayes theorem[30,29]:\n\nUsing this recursive weight update, it can be shown that the posterior can be approximated as follows[30,29]:\n\nThe next step involves designating an importance density function because the probability density, which is called a \u201dposterior\u201d in Bayesian framework, is unknown in advance. Theoretically, any function greater than zero may serve as the importance density, and intuitively the weights will balance the difference between the actual posterior distribution and the importance density. From a practical standpoint, prior sampling \u2014 which employs the conditional prior of the latent state \u2014 is typically the most effective approach for a wide range of applications[31].\n\nThis reduces the weight update equation (16) to the following expression:\n\nNow that we have derived the update equation that calculates the posterior and the weight update equation based on the state prior importance density, we summarize the entire algorithm as follows:\n\nParticle Filter Algorithm\n\nFor eachth iteration with input, perform the following steps to calculate.\n\nForto,\n\nForward particle by\n\nUpdate weightswith (19)\n\nPerform resampling if necessary.\u2217\n\n(\u2217: Resampling, which is an essential element of the particle filter, is the most common and effective way of preventing particle degeneration[29]. Detailed explanations and proofs can be found in[19,29].)\n\nThe particle smoother computes the smoothed densityrather than the filtered density, which the particle filter yields as the posterior. Without delving into further derivations, it\u2019s shown that the following relationship holds[19]:\n\nIt should be noted in (20b) that we introduce the smoothed weightas an outcome of particle smoothing. Employing these equations, the particle smoother is executed as follows[19]:\n\nParticle Smoother Algorithm\n\nCompute particlesand weightsforby implementing the particle filter.\n\nInitialize the smoothed weights atas\n\nRecursively compute the smoothed weights using (20).\n\nOmitting detailed proofs, we assert that (13) can be approximated using the outputs of the particle smoother with the following formulas[19]:\n\nThe weights in (22b) are determined as follows:\n\nThe equations in (22) are subsequently utilized to compute the Q function during the E-step of the EM algorithm. Our code was developed by augmenting scripts available in[32].\n\nThe model we utilized for parameter estimation is depicted in Fig.2. The input was a tensor of dimensions 1,0242, comprising the time-activity curves from the observable acitivity, which was interpolated from the PET measurements, and the input function. This tensor was augmented with a Time2Vec embedding to capture temporal dependencies, including both periodic and non-periodic patterns, while being invariant to time scaling[25]. The Time2Vec layer\u2019s kernel size was set to 2. The augmented vector was then fed into seven 1-D convolutional layers with filter sizes of 4, 8, 16, 32, 64, 128, and 256, respectively. Each convolutional layer had a kernel size of 10 and padding of 4 to reduce the tensor\u2019s width by exactly half at each stage. The tensor output from the final convolutional layer was condensed using a global average pooling layer, followed by three 16-unit fully connected layers, resulting in the output of three kinetic parameters:,, and. We employed the Adam optimizer with a learning rate of 0.0011 and utilized the mean absolute percentage error as the loss function. Tab.2summarizes the details of the configuration. Simple hyperparameter tuning was done with KerasTuner, and the tuning process included all parameters listed in Tab.2.\n\nSECTION: II.D.Evaluation methods\n\nEach parameter was estimated using three proposed methods: KEM, PSEM, and CNN. The estimations obtained from these methods were then compared to the ground truth values utilized in our simulations. To analyze the accuracy of the estimations, both absolute error and relative error between the estimated and ground truth values were considered. Additionally, the success ratio was computed for each kinetic parameter. This ratio is defined as the frequency with which a proposed method outperforms the reference method, NLLS, in our analyses. The calculations for the three metrics are based on the following simple equations:\n\nSECTION: III.Results\n\nSECTION: III.A.Particle smoother illustration\n\nFig.3shows an example of particle smoother performed on a sample TAC curve to illustrate the general idea of the particle smoother mechanism. The black dots represent particles, and we can see that the particle distribution can be used for effectively estimating the latent states in this sample case. Note that we assumed that the kinetic parameters are known in advance in this example case just to illustrate the effectiveness of the particle smoother. The proposed PSEM method uses particle smoother to calculate the elements of the expected energy of the Q function when running the E-step, and we can run the full EM algorithm by running E-step and M-step iteratively.\n\nSECTION: III.B.Simulation results\n\nFig.4shows four samples of simulation results. We can observe that the dynamics of the latent states, which are generated with the 2TC model, depend on the chosen kinetic parameters. The PET measurements and the measurements of the input function decays with time since the radioactive decay factor has been applied to the two measurements.\n\nSECTION: III.C.Interpolation results\n\nFig.5shows the result of estimating observable activities back from the PET measurements and the input measurements of same samples in Fig.4. Note that this step is necessary because we have assumed that we do not have prior information about the ground truth observable activity and we only have PET measurements and input measurements. We can see that the estimated observable activity and the input function, which are the result of applying decay correction and the interpolation to the PET measurement and input function measurement, respectively, match the ground truths with some noise. The estimation noise were reduced as time increases.\n\nSECTION: III.D.PSEM estimation along the iterations\n\nTo visualize the convergence of the PSEM algorithm, we show figure6, which presents PSEM estimation across iterations for a selected simulation sample. PSEM consistently converges to the ground truth value for estimating, irrespective of the initial values.\n\nSECTION: III.E.Parameter estimation results\n\nTab.3shows the result of differences calculated for each method, and the Tab.4shows the result of percentage differences for each method. CNN showed the best result among the other methods in terms of both the percentage error and the difference error.\n\nFig.7shows the scatter plots of parameters estimated by each proposed method and those estimated by NLLS, which is the reference method. In terms of the success rate, we can see that KEM underperformed compared to NLLS for estimating parameter, whereas PSEM and CNN outperformed NLLS. Forand, all three proposed methods outperformed NLLS, with CNN showing the best result of 79.5% success rate.\n\nSECTION: IV.Discussion\n\nFig.4illustrates how the kinetic parameters affects the simulated time-activity curves. First, we can notice that the activity of State 1 (fast exchangeable state) accumulates more whenis higher andis lower. Similarly, highvalues and lowvalues result in low activities in State 2 (slow exchangeable state). PET measurements and input function measurements decay over time, and we added noise estimated from real data sets to these measurements, resulting in signal degradations. Lastly, the changes in signal are reduced over time, which indicates that most of the extractable information regarding kinetics is stored in the early stage measurements.\n\nFig.5shows the estimated observable activities and input functions recovered from PET measurements and input function measurements. Again, this step was necessary because we should not assume that we know the simulated (true) observable activities and input function. We can see that the noise is reduced over time, mainly because the high level of signal changes in the early stage of the process make the interpolation step less accurate. We chose the spline fitting method to perform the interpolation because the spline fit showed better result than other fitting schemes we tested (simple polynomial, piecewise cubic hermite interpolating polynomial, B-spline).\n\nFig.6depicts the convergence of the ten PSEM estimations towards the ground truth value of parameter. All estimations converge monotonically to the ground truth value, demonstrating an indirect manifestation of Jensen\u2019s inequality within the EM algorithm. However, it is important to note that while this sample highlights the algorithm\u2019s effective performance in certain instances, the accuracy of PSEM outputs deteriorates when estimating multiple parameters and also varies significantly based on ground-truth values, as reported in SectionIII.E..\n\nTables3and4show the estimation performances of each proposed method in absolute errors and relative errors, respectively. The result in absolute errors shows that CNN performed significantly better than other three approaches for all three parameters. This indicates that the model learned the kinetics well without having prior knowledge regarding the compartmental model. The absolute errors of KEM for parameterwere less than that of NLLS, whereas those of PSEM underperformed compared to NLLS. This may indicate that the PSEM needs hand-tuning to be employed in practice. KEM and PSEM performed better than NLLS in estimating, but the differences among the results of three methods estimatinghad no statistical significance. The result shown in relative errors also highlights that CNN outperformed other three approaches. In estimating, the relative errors of KEM was similar to that of CNN, but PSEM underperformed compared to CNN. And in terms ofand, the relative errors of NLLS, KEM, and PSEM were similar.\n\nFig.7compares all estimations for each method with respect to those of NLLS, and shows the success rate of each method compared to NLLS. CNN shows80% success rate for all parameters. KEM outperformed NLLS in estimating all three parameters, whereas PSEM estimation forwas under 50%. However, the success rates of PSEM forandwere greater than 50%.\n\nIn practice, many regularization techniques and hand-tunings for kinetic parameter estimations are deployed during or after NLLS to improve the accuracy. In this work, however, we did not apply any of those to make fair comparisons between algorithms. We may be extend this work by comparing all the methods with the regularization and hand-tuning applied in the future.\n\nOn top of the best achieved accuracy, another advantage of CNN is that it offers kinetic model-agnostic solution, thereby minimizing hand-tuning for different tracers. However, it may be difficult to acquire a large number of data sets based on real patient scans. Desiging sophisticated realistic simulations may overcome this challenge effectively.\n\nIn this first study of the PSEM method for kinetic modeling, we simply used the hidden states of the state-space model as the latent variables in the E-step. We may improve the PSEM estimation result by moving some of the kinetic parameters into the latent space of the E-step and solving the resulting problem by Rao-Blackwellized PSEM, which has been shown to effectively improve the accuracy of the particle smoother[33]. The particle smoother is effective in solving non-linear problems, and solving one or more of the kinetic parameters in the E-step by the particle smoother may improve the quantitative accuracy. Also, PSEM has a unique advantage that it may be applied to non-linear kinetic models, unlike KEM.\n\nSECTION: V.Conclusions\n\nIn this work, we investigated, for the first time, the effectiveness of PSEM and CNN in estimating kinetic parameters of82Rb, and compared the results of each method to show that the CNN outperforms the other methods. PSEM showed mixed results, but CNN improved the accuracy significantly compared to all the other methods we tested. In the future, we plan to expand our work to other tracers and to construct more realistic simulations to improve the generalizability and we also aim to apply more advanced deep learning models, such as time-series transformers. Further, we will investigate different latent variable selections in the E-step of the PSEM and the use of Rao-Blackwellized particle smoothers.\n\nSECTION: VI.Acknowledgements\n\nSarah was supported by The Sun-Pan Family Fellowship Fund. The authors would like to thank Dr. Mojtaba Jafaritadi for helpful discussions on deep learning ideas.\n\nSECTION: VII.Conflict of Interest Statement\n\nThe authors declare no conflict of interest.\n\nSECTION: Appendix\n\nTo run realistic simulations, we estimated PET measurement noise distribution based on a real data set. The data set was acquired on a Discovery 690 PET/CT system (GE Healthcare, Milwaukee, WI, USA) in list mode. The82Rb cardiac PET data was from the rest portion of a cardiac rest-stress study.\n\nWe are interested in finding the relationship between time frame length and noise. We constructed images without decay correction starting for time frame lengths of 1, 2, 5, 10, 20, and 30 seconds. We then choose a 36 pixel square region of a tissue of interest. We normalized the activity values of the region to have a mean of 1 and report variance as variance mean ratio (VMR) and use (27) to calaculate standard error (SE) for time frame as\n\nwhereis the total number of pixels in an ROI. We then interpolated the time frame and SE values to get realistic noise estimation for arbitrary time frame length.\n\nVMR is calculated as follows. Let\u2019s denote a decay-corrected PET measurement atth time frame as, which can be considered as a random variable. Directly estimating the noise level ofcan be difficult, so we frame this problem as estimating standard error of sample distributions. A ROI in each PET image is a group of pixels, and thus can be seen as a \u201csample\u201d of signals. And we can calculate VMR as follows:\n\nwhereis the mean ROI signal at time frameandis the variance of the ROI signal at time frame. We assume that the noise can be modeled with a normal distribution, and we also assume that the VMR is constant across all images. Then the following holds:\n\nwhereindicates the signal atth pixel of the ROI of the image at time frame, normalized by the mean signal of the ROI. Then, using normality assumption, the ROI signal for each time frame can be modeled as follows:\n\nThe VMR value and time frame pairs from the real data set was listed in Table5, and we implemented noise distribution in simulation by using (30).\n\nSECTION: References\n\nSECTION:", "text_file": "data\\paper_texts\\2412.04706v1_content.txt"}, {"title": "Enhancing Medical Image Segmentation with Deep Learning and Diffusion\n  Models", "authors": ["Houze Liu", "Tong Zhou", "Yanlin Xiang", "Aoran Shen", "Jiacheng Hu", "Junliang Du"], "published_date": "2024-11-21T17:49:15Z", "summary": "Medical image segmentation is crucial for accurate clinical diagnoses, yet it\nfaces challenges such as low contrast between lesions and normal tissues,\nunclear boundaries, and high variability across patients. Deep learning has\nimproved segmentation accuracy and efficiency, but it still relies heavily on\nexpert annotations and struggles with the complexities of medical images. The\nsmall size of medical image datasets and the high cost of data acquisition\nfurther limit the performance of segmentation networks. Diffusion models, with\ntheir iterative denoising process, offer a promising alternative for better\ndetail capture in segmentation. However, they face difficulties in accurately\nsegmenting small targets and maintaining the precision of boundary details.\nThis article discusses the importance of medical image segmentation, the\nlimitations of current deep learning approaches, and the potential of diffusion\nmodels to address these challenges.", "arxiv_id": "2411.14353v2", "html_link": "https://arxiv.org/html/2411.14353v2", "search_term": "ti:\"deep learning\"", "html_error": "Error processing HTML: HTTP Error 404: Not Found"}, {"title": "Deep Learning application for stellar parameters determination: III-\n  Denoising Procedure", "authors": ["Marwan Gebran", "Ian Bentley", "Rose Brienza", "Fr\u00e9d\u00e9ric Paletou"], "published_date": "2024-12-05T21:53:46Z", "summary": "In this third paper in a series, we investigate the need of spectra denoising\nfor the derivation of stellar parameters. We have used two distinct datasets\nfor this work. The first one contains spectra in the range of 4450-5400 {\\AA}\nat a resolution of 42000 and the second in the range of 8400-8800 {\\AA} at a\nresolution of 11500. We constructed two denoising techniques, an autoencoder,\nand a Principal Component Analysis. Using random Gaussian noise added to\nsynthetic spectra, we have trained a Neural Network to derive the stellar\nparameters Teff, log g, ve sin i, {\\xi}t, and [M/H] of the denoised spectra. We\nfind that, independently of the denoising technique, the stellar parameters\naccuracy values do not improve once we denoise the synthetic spectra. This is\ntrue with and without applying data augmentation to the stellar parameters\nNeural Network.", "arxiv_id": "2412.04631v1", "html_link": "https://arxiv.org/html/2412.04631v1", "search_term": "ti:\"deep learning\"", "html_content": "SECTION: Deep Learning application for stellar parameters determination:III- Denoising Procedure\n\nIn this third paper in a series, we investigate the need of spectra denoising for the derivation of stellar parameters. We have used two distinct datasets for this work. The first one contains spectra in the range of 4,450-5,400 \u00c5\u00a0at a resolution of 42,000 and the second in the range of 8,400\u20138,800 \u00c5\u00a0at a resolution of 11,500. We constructed two denoising techniques, an autoencoder, and a Principal Component Analysis. Using random Gaussian noise added to synthetic spectra, we have trained a Neural Network to derive the stellar parameters,,,, andof the denoised spectra. We find that, independently of the denoising technique, the stellar parameters accuracy values do not improve once we denoise the synthetic spectra. This is true with and without applying data augmentation to the stellar parameters Neural Network.\n\nSECTION: 1Introduction\n\nObservations in astronomy have always been associated with noise. Trying to minimize the noise is one of the needs of astronomers. Several observation techniques have been suggested to reduce the noise in spectra, however, once the observation is performed, the only way to proceed is to apply mathematical algorithms that can improve the Signal-to-Noise Ratio (SNR) of the data. These techniques involve but are not limited to Gaussian smoothing(Chung,,2020), median filtering(Kumar and Sodhi,,2020), wavelet denoising(Halidou et\u00a0al.,,2023), and Principal Component Analysis(Bacchelli and Papi,,2006; Zhang et\u00a0al.,,2010; Murali et\u00a0al.,,2012; Li,,2018). More recently, and with the advancement of computational power, Deep Learning algorithms started to be used in that purpose.Gheller and Vazza, (2022)used a Convolutional Denoising autoencoder to decrease the noise of synthetic images of state-of-the-art radio telescopes like LOFAR(Offringa et\u00a0al.,,2013), MeerKAT(Jonas,,2009), and MWA(Tingay et\u00a0al.,,2013). The technique was applied on different kinds of corrupted input images. The autoencoder was able to effectively denoise images identifying and extracting faint objects at the limits of the instrumental sensitivity. The authors state that their autoencoder was capable of removing noise while preserving the properties of the regions of the sources with SNR as low as 1.Scourfield et\u00a0al., (2023)used a variational autoencoder to denoise optical SDSS spectra of galaxies (York et\u00a0al.,2000).Their main goal was to denoise the spectra while keeping the important information they can retrieve from low SNR galaxy spectra and avoiding the use of sample averaging methods (smoothing or spectral stacking). They tested the method in the context of large optical spectroscopy surveys by simulating a population of spectra with noise to mimic the ones at galaxies at a redshift of. Their results showed that the technique can recover the shape and scatter of the mass-metallicity relation in this sample.\n\nIn this work, we introduce two types of spectral denoising techniques, autoencoders(Ballard,,1987; Baldi,,2011)and Principal Component Analysis (PCA,Wold et\u00a0al.,1987; Ma\u0107kiewicz and Ratajczak,1993). We test the need of the denoising technique on the derived stellar parameters: effective temperature, surface gravity, equatorial projected rotational velocity, microturbulence velocity, and the overall metallicity. These stellar parameters are derived using the Neural Network introduced our previous work(Gebran et\u00a0al.,,2022,2023; Gebran,,2024). The paper is divided as follows: Sec.2introduces the calculation of both datasets and noisy spectra, Sec.3explains the autoencoder construction used in the denoising procedure, and Sec.4describes the denoising technique using Principal Component Analysis. Section5shows the results of the denoising technique using both procedures and the effect on the derived stellar parameter accuracy values. Finally, we conclude in Sec.6.\n\nSECTION: 2Datasets\n\nTwo datasets were used in the context of the present study. The one analyzed inGebran et\u00a0al., (2023)and the one ofGebran, (2024). The characteristics of these two datasets are described in Tab.1. The reason for selecting these diverse datasets is to check the procedure over different wavelength ranges and different resolving power.\n\nThe steps of calculating the datasets are detailed inGebran et\u00a0al., (2022,2023)andGebran, (2024). In summary, line-blanketed model atmospheres are calculated using ATLAS9(Kurucz,,1992). The models are plane parallel and in Local Thermodynamic Equilibrium (LTE). They are in hydrostatic and radiative equilibrium. We have calculated the models using the Opacity Distribution Function (ODF) ofCastelli and Kurucz, (2003). Convection was included according to Smalley\u2019s prescriptions(Smalley,,2004). Convection is included in the atmospheres of stars cooler than 8,500 K using the mixing length theory. A mixing length parameter of 0.5 was used for 7,000 KTeff8,500 K, and 1.25 for Teff7000 K.\n\nWe have used the radiative transfer code SYNSPEC(Hubeny and Lanz,,2017)to calculate the synthetic spectra. As mentioned previously, two datasets were calculated with each one containing around 200,000 spectra. In both datasets, metal abundances were scaled with respect to theGrevesse and Sauval, (1998)solar value from -1.5 dex up to +1.5 dex. The effective temperature, surface gravity, projected equatorial velocity, and microturbulence velocity were also modified according to the values displayed in Tab.1. The first dataset consists of spectra having a resolution of 42,000 and a wavelength range between 4,450 and 5,400 \u00c5. As explained inGebran et\u00a0al., (2022,2023), this wavelength range is sensitive to all stellar parameters in the spectral range of AFGK stars. The second dataset has spectra computed between 8,400 and 8,800 \u00c5\u00a0at a resolution of 11,500. This region includes the Gaia Radial Velocity Spectrometer (RVS,Cropper et\u00a0al.,2018). The RVS spectra contain lines sensitive to the stellar parameters and to the chemical abundance of many metals ( Mg, Si, Ca, Ti, Cr, Fe, Ni, and Zr, among others) at different ionization stages. The linelist used in this work is the one used inGebran et\u00a0al., (2022,2023). It contains updated values for the atomic parameters such as the wavelength of the transitions, the oscillator strengths, the damping constants, and others.\n\nIn summary, we ended up with two datasets of around 200,000 synthetic spectra each, with,,,, andrandomly chosen from Tab.1. Figure1shows a color map of a sub-sample of the datasets. The Balmer line is detected in the left color map for dataset 1 and the absorption lines of the calcium triplet (= 8,498, 8,542, 8,662 \u00c5) are also shown in the color map of dataset 2 in the bottom part of the figure.\n\nFor each dataset, a set of spectra were calculated with random Gaussian noise between 5 and 300. This SNR is used to mimic the noisy observations that we will be denoising later on as they represent the average SNR encountered in real stellar spectra. An example of a spectrum calculated with and without noise in the parameter range of dataset 2 is shown in Fig.2.\n\nSECTION: 2.1Data Augmentation\n\nWe have also tested the effect of data augmentation in this work, and for that reason, we have calculated extra dataset as suggested inGebran et\u00a0al., (2022). Data augmentation is a regularization technique that by increasing the diversity of the training data by applying different transformations to the existing one, helps in avoiding over-fitting and improves the predictions of stellar labels when applied with real observed data(Gebran et\u00a0al.,,2023). We have used the same approach ofGebran et\u00a0al., (2022)in which 5 replicas of each spectrum in the dataset were performed. These replicas consist of\n\nAdding to each spectrum a Gaussian noise with a SNR ranging randomly between 5 and 300.\n\nThe flux of each spectrum is multiplied with a scaling factor selected randomly between 0.95 and 1.05.\n\nThe flux of each spectrum is multiplied with a new random scaling factor and noise was added.\n\nThe flux of each spectrum is multiplied by a second-degree polynomial with values ranging between 0.95 and 1.05 and having its maximum randomly selected in the wavelength range of the dataset.\n\nThe flux of each spectrum is multiplied by a second-degree polynomial and Gaussian noise added to it.\n\nFor more details about data augmentation, we refer the reader toGebran et\u00a0al., (2022).\n\nSECTION: 3Auto-Encoders\n\nAutoencoders, usually used in denoising and dimensionality reduction techniques(Lecun,,1987; Fogelman Soulie et\u00a0al.,,1987; Ballard,,1987; Baldi,,2011; Schmidhuber,,2014; Einig et\u00a0al.,,2023; Scourfield et\u00a0al.,,2023), are a type of Neural Networks that work in an unsupervised way. They consist of two distinct yet similar algorithms, an encoder and a decoder. The encoder's role is to transform the spectra from a dimension offlux point to a smaller size ofinside a Latent Space. The decoder re-transform theto the original spectrum offlux point. The choice ofdepends on the characteristics of the dataset. However, using the two datasets in this work, we found that the optimal size for the Latent Space is. This is found by minimizing the difference between the output spectra and the input one during the training process. It is true that different values ofcould be used, but our choice ofwas based on the smallest value that gives a reconstruction error less than 0.5% as will be explained in the next steps.\n\nThe classical architecture of an autoencoder is shown in Fig.3where the initial spectrum is introduced having 19,000 or 4,000 data points depending on the dataset and is then reduced topoints through successive hidden layers. This first step defines the encoder part of the autoencoder. Then, thepoints are transformed to 19,000 or 4,000 data points while passing through different hidden layers. This second step defines the Decoder part of the autoencoder. The hidden layers are usually symmetrical in the encoder and decoder parts.\n\nTwo autoencoders were used in this work, one for each dataset. In both cases, the spectra are\nreduced to 10 parameters in the Latent Space. The architecture of the used autoencoders is displayed in Tab.2. We have used an Adam optimizer with a Mean Squared Error (MSE) loss function.\n\nCalculations were performed usingTensorFlow111https://www.tensorflow.org/with theKeras222https://keras.io/interface and were written inPython.\n\nThe training of the autoencoders was performed using the 2 datasets containing the synthetic spectra with no noise. The convergence is achieved when the difference between the output and the input spectra is minimized through the MSE. Convergence usually occurs after around 500 epochs. For both datasets, we achieved an R2score larger than. Meaning that the reconstruction of the spectra is performed with an error0.5%. Once the training is done, the denoising is performed when the trained autoencoders are applied to the noisy spectra.\n\nSECTION: 4Principal Component Analysis\n\nPCA is a non-parametric mathematical transformation that extracts relevant information from a dataset(Wold et\u00a0al.,,1987; Ma\u0107kiewicz and Ratajczak,,1993). Its goal is to compute the most meaningful basis to represent a noisy dataset. expressanoisydataset. The new basis usually reveals hidden structure and filters out the noise(Shlens,,2014). PCA has been used for denoising(Bacchelli and Papi,,2006; Zhang et\u00a0al.,,2010; Murali et\u00a0al.,,2012; Li,,2018)or spectral dimension reduction(Ma\u0107kiewicz and Ratajczak,,1993;Paletou et\u00a0al., 2015a,; Gebran et\u00a0al.,,2016,2022,2023). The main power of PCA is that it can reduce the dimension of the data while maintaining significant patterns and trends.\n\nThe basic idea behind the use of PCA is to derive a small number of eigenvectors and use them to recover the information in the spectra. The steps of PCA calculation are\n\nThe matrix containing the Training dataset hasflux points per spectrum, therefore the dataset can then be represented by a matrixMof sizewhererepresents the number of spectra in the dataset.\n\nThe matrixMis then averaged along the-axis and this average is stored in a vector.\n\nThe variance-covariance matrixCis calculated as\n\nwhere the superscript \"T\" stands for the transpose operator.\n\nThe eigenvectorsofCare then calculated.Chas a dimension of. The Principal Components (PC) correspond to the eigenvectors sorted in decreasing magnitude.\n\nEach spectrum ofMis then projected on these PCs in order to find its corresponding coefficientdefined as\n\nThe original \"denoised spectrum\" can be calculated using\n\nThe PCA can reduce the size of each spectrum fromto. The choice ofdepends on the many parameters, the size of the dataset, the wavelength range, and the shape of the spectra lines. We have opted for a value forthat reduces the mean reconstructed error to a value <0.5% according to the following equation:\n\nWe have opted to a value forthat reduces the mean reconstructed error to a value <0.5%. This value if found to be=50. A detailed description of all steps of the PCA can be found inPaletou et\u00a0al., 2015a;Paletou et\u00a0al., 2015b; Gebran et\u00a0al., (2016,2022,2023); Gebran, (2024). For both datasets, we achieved an R2score larger than.\n\nSECTION: 5Denoising and parameters determination\n\nThe datasets that contain the synthetic spectra without any added noise are used to train the autoencoder and to find the eigenvectors of the PCA procedure. These two techniques are then used on the set of noisy spectra that are calculated in Sec.2. The evaluation of the denoising procedure is tested in two ways. First, we checked the similarity of the denoised spectra with the original one with no noise added. Second, we checked the accuracy of the derived stellar parameters when we applied the procedures ofGebran et\u00a0al., (2022,2023)on the denoised spectra from the autoencoder and PCA.\n\nAutoencoders usually replace PCA because of their non-linear properties, however, both techniques showed a good reconstruction power as shown by the R2score in Secs.3and4. A way to visualize the denoising of spectra is shown in Fig.4. The figure is divided into two parts, the upper one displays a spectrum having the parameters of dataset 1 and the bottom one has the parameters of dataset 2. In each part, the noisy spectrum is in black, the original one without noise is in dashed blue, the denoised spectrum using the autoencoder (left panel) or PCA (right panel) technique is in red, and the difference between the denoised spectrum and the original one without noise is in dash-dot green.\n\nInGebran et\u00a0al., (2022,2023)we have introduced a technique to derive the stellar parameters of spectra using a Neural Network. We have used the same procedure to derive the accuracy of the stellar parameters once we apply the same technique to the denoised spectra. The main purpose of this step is not to evaluate if the derivation technique is accurate or not but it is to check how similar are the derived stellar parameters of the noisy spectra to the ones derived from the original spectra with no noise added.\n\nThe networks that we used are made of several fully dense layers and are trained to derive each parameters separately. The layers are described in Tab.3. The first step of the analysis is to reduce the dimension of the spectra using a PCA procedure. This PCA is not related to the one used for denoising, it is just a step for optimizing the network and making the training faster (SeeGebran et\u00a0al.,2022for more details).\n\nTwo different training are performed for each dataset. The first one is done using a dataset of only synthetic spectra with no noise added and the second one consists of applying data augmentation with spectra having a range of SNR between 3 and 300.\n\nBecause we already know the stellar parameters of the spectra, the evaluation is performed by calculating the difference between the predicted parameter and the original one using the equation\n\nwhereis the total number of noisy spectra used in the evaluation. This is done for,,,, and. Tables4and5display the accuracy values for the parameters for the two datasets when deriving the stellar labels of25,000 with no noise added (column 2), with random noise (column 3), with random noise then denoised using autoencoder of Sec.3(column 4) and using PCA of Sec.4(column 5). Each table is divided into two, one part when data augmentation is performed and one without it.\n\nA detailed analysis of Tabs.4and5show that:\n\nData augmentation is an important step to be applied if we need to derive the stellar parameters of noisy spectra. Without it, the model will only learn to derive the parameters of synthetic spectra without any noise added. A similar conclusion was also found inGebran et\u00a0al., (2023).\n\nPCA denoising is capable of recovering the line profile and the details in the spectra. This is reflected by comparing the accuracy values of the derived parameters using the denoised spectra from the autoencoders and PCA (i.e. comparing Cols. 4 and 5).\n\nThe parameters derived using the PCA denoising technique are more accurate than the ones derived using the autoencoder denoising.\n\nNo denoising technique is capable of improving the accuracy of the stellar parameters for the one directly derived from noisy spectra (displayed in Col. 3).\n\nThe stellar parameter algorithm is capable of deriving the stellar labels without the need for a denoising technique.\n\nThese tests show mainly that data augmentation is very important when Neural Networks are used to derive the stellar parameters of noisy spectra, a results already found byGebran et\u00a0al., (2022,2023). As an example, Fig.5displays the predictedwith respect to the original one for the data with noise from the augmented dataset 2 (left panel) and the denoised data using autoencoder (right panel) from the same dataset. The data are color-coded to the SNR values. The straight black line represents the best prediction line (). The left panel shows that the highly dispersed results are the ones for the low SNR spectra. Once the spectra are denoised, the dispersion appears to be present for all SNR values with no specific trend or deviation. This is true for all stellar parameters. Independently of the denoising technique, there is no improvement found in the accuracy values of the derived parameters of denoised spectra when the networks were trained on noisy spectra. Applying the networks on noisy data gives more accurate results then when it is applied on denoised data.\n\nSECTION: 6Conclusion\n\nIn this work, we have applied two different denoising techniques, an autoencoder, and a PCA, on spectra with random Gaussian noise added to derive the stellar parameters using Neural Networks ofGebran et\u00a0al., (2022,2023). The method was applied to two different spectra ranges, one in 4,450\u20135,400 \u00c5\u00a0and one in the Gaia RVS range from 8,400\u20138,800 \u00c5. In this study, we do not constrain the stellar parameter derivation technique, this was done previously inGebran et\u00a0al., (2022,2023).\nInterestingly, when applying the model to denoised spectra, there was no noticeable improvement in the accuracy of the derived fundamental parameters, such as,,,, and. This outcome was unexpected, as denoising is typically thought to enhance the precision of predictions. However, the results indicate that data augmentation plays a more crucial role. When the model is trained on datasets that include noise, the accuracy of predictions for noisy spectra improves significantly, suggesting that the network becomes better equipped to handle real observed spectra. This highlights the importance of incorporating noisy data into training rather than relying on post-processing techniques like denoising to improve accuracy.\nTo further validate these findings, it would be valuable to explore other denoising techniques and assess their impact on prediction accuracy. Techniques such as those presented inAlsberg et\u00a0al., (1997),Koziol et\u00a0al., (2018), andZhao et\u00a0al., (2021)could be tested to see if they yield better results in reducing noise while maintaining or enhancing the precision of derived parameters. These additional experiments would help solidify the conclusion that data augmentation is more effective than denoising in improving the accuracy of noisy spectra predictions, offering deeper insights into how best to model real observational spectra.\n\nAcknowledgment: The authors acknowledge Saint Mary's College for providing the high-power computing cluster used in this work. The authors are grateful for the reviewer\u2019s valuable comments that improved the manuscript.\n\nFunding information: Authors state no funding involved.\n\nAuthor contributions: All authors have accepted responsibility for the entire content of this manuscript and consented to its submission to the journal, reviewed all the results, and approved the final version of the manuscript. MG and RB designed the code and carried out the calculations. MG prepared the manuscript with contributions from all co-authors.\n\nConflict of interest: The authors state no conflict of interest.\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.04631v1_content.txt"}, {"title": "WinTSR: A Windowed Temporal Saliency Rescaling Method for Interpreting\n  Time Series Deep Learning Models", "authors": ["Md. Khairul Islam", "Judy Fox"], "published_date": "2024-12-05T17:15:07Z", "summary": "Interpreting complex time series forecasting models is challenging due to the\ntemporal dependencies between time steps and the dynamic relevance of input\nfeatures over time. Existing interpretation methods are limited by focusing\nmostly on classification tasks, evaluating using custom baseline models instead\nof the latest time series models, using simple synthetic datasets, and\nrequiring training another model. We introduce a novel interpretation method\ncalled Windowed Temporal Saliency Rescaling (WinTSR) addressing these\nlimitations. WinTSR explicitly captures temporal dependencies among the past\ntime steps and efficiently scales the feature importance with this time\nimportance. We benchmark WinTSR against 10 recent interpretation techniques\nwith 5 state-of-the-art deep-learning models of different architectures,\nincluding a time series foundation model. We use 3 real-world datasets for both\ntime-series classification and regression. Our comprehensive analysis shows\nthat WinTSR significantly outranks the other local interpretation methods in\noverall performance. Finally, we provide a novel and open-source framework to\ninterpret the latest time series transformers and foundation models.", "arxiv_id": "2412.04532v1", "html_link": "https://arxiv.org/html/2412.04532v1", "search_term": "ti:\"deep learning\"", "html_content": "SECTION: WinTSR: A Windowed Temporal Saliency Rescaling Method for InterpretingTime Series Deep Learning Models\n\nInterpreting complex time series forecasting models is challenging due to the temporal dependencies between time steps and the dynamic relevance of input features over time. Existing interpretation methods are limited by focusing mostly on classification tasks, evaluating using custom baseline models instead of the latest time series models, using simple synthetic datasets, and requiring training another model. We introduce a novel interpretation method calledWindowed Temporal Saliency Rescaling (WinTSR)addressing these limitations. WinTSR explicitly captures temporal dependencies among the past time steps and efficiently scales the feature importance with this time importance. We benchmark WinTSR against 10 recent interpretation techniques with 5 state-of-the-art deep-learning models of different architectures, including a time series foundation model. We use 3 real-world datasets for both time-series classification and regression. Our comprehensive analysis shows that WinTSR significantly outranks the other local interpretation methods in overall performance. Finally, we provide a novel and open-source framework to interpret the latest time series transformers and foundation models.\n\nSECTION: 1Introduction\n\nTime-series deep learning models have achieved unprecedented performance in recent years. However, the lack of explainability remains one of the key challenges for their widespread use. Explanations provide the necessary transparency to make reliable decisions, especially in sensitive data such as healthcare, finance, energy, traffic, weather, stocks, and many other science domains(Benidis et\u00a0al.2022). These explanations can be eitherGlobal, the logic and reasoning of the entire model, orLocal, the model\u2019s specific decision on an instance.Post-hocinterpretation methods are generally applied after the model has already been trained, whileIn-hocmethods work during the model training time.Model-agnosticmethods work on black-box models and do not require specific model architecture to work. Our proposed interpretation method is local, post-hoc, and model-agnostic.\n\nUnlike image and text data, the interpretability of multivariate time series models has been relatively under-explored and difficult to visualize. By explaining time series models, one can highlight the importance of input features to the prediction of the model(Rojat et\u00a0al.2021), find intuitive patterns(Lim et\u00a0al.2021), and visualize saliency maps(Leung et\u00a0al.2023)without relying on the model architecture. This work focuses on the local interpretation techniques for deeper insights into the importance of temporal features. Interpretation methods commonly estimate how relevant each input feature is to the model\u2019s output. However, existing works are limited by: (1) benchmarking using simple baseline models (e.g. LSTM, GRU), not recent SOTA time series models that are used in practice (2) focusing mostly on classification tasks, which makes generalization difficult (3) not efficiently capturing temporal dependency (4) train another model to interpret one model.\n\nWe propose theWindowed Temporal Saliency Rescaling (WinTSR)method to address these challenges. It is benchmarked using the latest time series models of different architectures (including an LLM-based foundation model), tested on both classification and regression tasks, efficiently considers the temporal importance when interpreting, and does not require training a separate model. Our overall framework is summarized in Figure1. In short, our contributions are,\n\nA novel local interpretation method namedWinTSRthat calculates the time importance along the look-back window and rescales the feature importance based on it. This significantly improves the performance by capturing the delayed impact between input and output, while still being relatively fast.\n\nExtensive analysis with 3 real-world datasets for classification and regression tasks, and interpretation evaluation using robust metrics.\n\nBenchmarkWinTSRwith 5 state-of-the-art time series models (DLinear, MICN, SegRNN, iTransformer) including a foundation model (CALF) to demonstrate that WinTSR is generalizable and consistently outperforms in different model architectures.\n\nA unified open-source framework that includes 20+ recent time series models (including 3 foundation models, AppendixE) with 10+ popular interpretation methods. Also, visualize and compare the multivariate temporal trends interpreted by different methods.Code availableas open source in the Github111https://anonymous.4open.science/r/WinTSR\n\nSECTION: 2Related Works\n\nTime series interpretation methods cover a wide range of tasks and datasets(Rojat et\u00a0al.2021; Turb\u00e9 et\u00a0al.2023). Table1summarizes the comparisons of our work with the related methods.Gradient basedmethods, such as Integrated Gradients(Sundararajan, Taly, and Yan2017), and GradientSHAP(Erion et\u00a0al.2019)use the gradient of the model predictions to input features to generate importance scores.Perturbation basedmethods, such as Feature Ablation(Suresh et\u00a0al.2017), and Augmented Feature Occlusion(Tonekaboni et\u00a0al.2020)replace a feature or a set of features from the input using some baselines or generated masks and measure the importance based on the model output change. These methods are mainly proposed for image or language models anddo not consider the temporal dependencies in the time series data.\n\nRecent works like Dyna Mask(Crabbe and van\u00a0der Schaar2021), and Extremal Mask(Enguehard2023a)focus onlearning the masksto better perturb the input features. Model-based saliency methods, such as(Kaji et\u00a0al.2019; Lim et\u00a0al.2021; Islam et\u00a0al.2023), use the model architecture e.g. attention layers, to generate importance scores. ContraLSP(Liu et\u00a0al.2024d), proposed a contrastive learning method to learn locally sparsed perturbation. TIMEX(Queen et\u00a0al.2024)trained interpretable surrogates to learn stable explanations from latent space. However, TIMEX is limited to classification and assumes access to latent pretrained space. These methods overall have performed very well on synthetic data or real-world classification tasks. However, theyrequire training another model to interpret the target model, which adds additional complexity. These arenot benchmarked for regression tasksand often include algorithm design exclusively for classification(Queen et\u00a0al.2024). Also,heavily uses simple RNN or LSTM baselines which are not state-of-the-art time series models.\n\nTSR(Ismail et\u00a0al.2020)improved interpretation by considering the temporal dimension separately. However, this comes with a heavy computational cost and was not benchmarked on real-world time series data. Feature Importance in Time, FIT(Tonekaboni et\u00a0al.2020)conditioned on the last input time step to calculate the distribution shift using KL-divergence of the predicted probability, but only supports classification tasks. WinIT(Leung et\u00a0al.2023)proposed a sliding window-based approach to calculate delayed feature importance for classification tasks.\n\nSo we need an interpretation method that is generalized for classification and regression, considers the time dependency of input data, and does not require training another model. In this work, we achieve this by, efficiently considering the time dependency, using a simple masking technique, and developing a novel framework that allows comparing these methods with SOTA time series models.\n\nSECTION: 3Windowed Temporal Saliency Rescaling\n\nSECTION: 3.1Problem Statement\n\nWe consider a multivariate multi-horizon time series setting with length, the number of input features, target outputs, and totalinstances.is the input featureat time. Past information within a fixed look-back windowis used to forecast for the nexttime steps or the target class. The target output at timeis. The black-box modelis defined as,\n\nwhereis the predicted class or the forecast attime steps in the future.is the input slice at timeof length.\nAn input feature at positionin the full input matrix at time stepis denoted as.\n\nThe interpretation goal is to calculate the importance matrix,for each outputor prediction horizon. This is a matrix of sizefor classification andfor regression. We find the relevance of the featureby masking it in the input matrixand measuring the model output change,\n\nwhereis the input after masking/perturbing feature. Thedistance_scorecan vary based on interpretation methods and prediction tasks. For example, l1-norm for regression and l1-norm or KL-divergence for classification.\n\nSECTION: 3.2Our Approach\n\nWe proposeWindowed Temporal Saliency Rescaling (WinTSR)across the input time window to calculate the importance score matrixat a time. Our method differs from previous approaches by accounting for the importance of a feature observation in a multi-horizon setting over multiple windows containing the target time step. The details are in the following Algorithm1. The method returns an importance matrixwhich is later used to evaluate the interpretation performance.\n\nFor a distance score, we calculate the simple \u2018L1 distance\u2018 between the original and perturbed prediction for classification and regression. Unlike TSR, which uses the \u2018L1 distance\u2018 between the original and perturbed importance matrix returned from another interpretation method. This significantly improves our run time compared to TSR and removes the dependency on a second interpretation method. WinIT perturbs all values in a sliding window with and without the target feature, then finds the feature importance by subtracting them. Since we perturb only the individual feature, this reduces the computation overhead of perturbing a range of features and removes the dependency of choosing a best-fit sliding window.\n\nThe time relevance score enables us to skip less important time steps to speed up the computation similar to(Ismail et\u00a0al.2020). TSR uses the first feature value for masking, while WinIT uses feature augmentation (generated from past features). We generated random values from a normal distribution for masking and the input features are already normalized during the data pre-processing period.\n\nSECTION: 4Experimental Setup\n\nWe compare WinTSR to ten recent local and post-hoc interpretation methods. We evaluate them with five state-of-the-art deep learning time series models across three datasets for classification and regression tasks.\n\nSECTION: 4.1Datasets\n\nWe use the datasets shown in Table2. Electricity and Traffic datasets contain the electricity consumption rate and traffic occupancy over the past hours respectively. The task is to forecast those values for the next 24 hours based on past observations and time-encoded features from the last 96 hours. The MIMIC-III dataset contains patient info and lab reports from a hospital. The goal is to predict whether the patient will die during their hospital stay, based on the patient demographic info and lab reports over the last 48 hours. This is a private dataset but easy to apply for access and popularly used in related works(Leung et\u00a0al.2023; Enguehard2023a). Details on the datasets and features are in AppendixA. The input values are standard normalized. The datasets are split into train validation test sets using the 8:1:1 ratio. The best model by validation loss is used in testing and for the rest of the experiments.\n\nDatasetFeatureSizeWindowOutputTaskElectricity526.2k9624RegressionTraffic520.7k9624RegressionMIMIC-III3222.9k482Classification\n\nSECTION: 4.2Models\n\nWe use five neural network architecture groups (Linear, CNN, RNN, Transformer, and LLM) for our experiment. Multiple models are chosen to generalize the proposed method across different network architectures. We show how the same interpretation method impacts different models. A complete list of available models in our framework is given in AppendixE. We selected these five models based on their state-of-the-art performance in their respective architecture. These models are : (1)DLinear(Zeng et\u00a0al.2023)- Linear, (2)SegRNN(Lin et\u00a0al.2023)- Recurrent Neural Network (RNN), (3)MICN(Wang et\u00a0al.2023)- Convolutional Neural Network (CNN), and (4)iTransformer(Liu et\u00a0al.2024b)- Transformer, and (5)CALF(Liu et\u00a0al.2024a)- A recent pretrained LLM model for generalized time series forecasting using cross-modal fine-tuning.\n\nWe follow the Time-Series-Library(Wu et\u00a0al.2023)222https://github.com/thuml/Time-Series-Libraryimplementation of these models. This ensures we follow the latest benchmarks. Table3shows the iTransformer model performs best across all cases. Details of the hyperparameters are listed in AppendixB.\n\nSECTION: 4.3Interpretation Methods\n\nWe use the following post-hoc interpretation analysis methods for comparison in this work: (1) Feature Ablation (FA,Suresh et\u00a0al. (2017)) (2) Augmented Feature Occlusion (AFO,Tonekaboni et\u00a0al. (2020)) (3) Feature Permutation (FP,Molnar (2020)) (4) Integrated Gradients (IG,(Sundararajan, Taly, and Yan2017)) (5) Gradient Shap (GS,Lundberg and Lee (2017)) (6) Dyna Mask (DM,Crabbe and van\u00a0der Schaar (2021)) (7) Extremal MaskEnguehard (2023a)(8) Windowed Feature Importance in Time (WinIT,Leung et\u00a0al. (2023)) (9) Temporal Saliency Rescaling (TSR,Ismail et\u00a0al. (2020)), and (10) Contrastive and Locally Sparse Perturbations (ContraLSP,Liu et\u00a0al. (2024d)).\n\nWe choose them based on versatility across different model architectures and tasks. Captum(Kokhlikyan et\u00a0al.2020)333https://captum.ai/and Time Interpret(Enguehard2023b)444https://josephenguehard.github.io/time_interpretlibraries were used to implement the interpretation methods. Unlike(Enguehard2023b), which runs the methods on the CPU, we implemented our framework to run all methods with GPU, thus increasing the interpretation speed. The baselines to mask inputs were randomly generated from the normal distribution, the raw inputs were also normalized. We excluded the methods which are classification only (e.g. FIT) or no public implementation is not available (e.g. CGS-Mask). For TSR, we used the best combination in their work (TSR with Integrated Gradients and).\n\nSECTION: 4.4Evaluating Interpretation\n\nWe follow(Ozyegen, Ilic, and Cevik2022; Turb\u00e9 et\u00a0al.2023)to evaluate interpretation when no interpretation ground truth is present. Figure2(b) briefly illustrates the evaluation framework. The steps are:\n\nSort relevance scoresso thatis theelement in the ordered rank set. Hereis the look-back window andis the number of features.\n\nFind topfeatures in this set, where. Mask these top features or every other feature in the input.\n\nCalculate the change in the model\u2019s output to the original output using different metrics. We use the AUC drop for classification(Leung et\u00a0al.2023)and Mean Absolute Error (MAE) for regression.\n\nAbbreviations:AOPC: Area over the perturbation curve for classification,AOPCR: Area over the perturbation curve for regression,FA: Feature Ablation,AFO: Augmented Feature Occlusion,FP: Feature Permutation,IG: Integrated Gradients,GS: Gradient Shap,DM: Dyna Mask,EM: Extremal Mask,WinIT: Windowed Feature Importance in Time,TSR: Temporal Saliency Scaling with Integrated Gradients,ContraLSP: Contrastive and Locally\nSparse Perturbation,WinTSR: Windowed Temporal Saliency Rescaling.\n\nDeYoung et\u00a0al. (2019)proposed to measure thecomprehensivenessandsufficiencyto ensure the faithfulness of the explained rationales. Which are similar to the precision and recall fromIsmail et\u00a0al. (2020). (1)Comprehensiveness: Were all features needed to make a prediction selected? Once important features are masked, the model should be less confident in its prediction. (2)Sufficiency:Are the top feature sufficient to make the prediction? This is achieved by masking all features except the top. In summary,the higher the comprehensiveness loss and the lower the sufficiency loss the better. We define the set of toprelevant features selected by the interpretation method for the-th inputas, the input after removing those features as. Then these two terms are calculated as:\n\nForbins of topfeatures (we use top 5%, 7.5%, 10%, and 15% features, hence.), the aggregated comprehensiveness score(DeYoung et\u00a0al.2019)for the classification task is called the \u201dArea Over the Perturbation Curve\u201d (AOPC, Equation4). For AUC drop, this will calculate the drop for each output classafter masking topfeatures for each, then calculate the average drop.\n\nSimilarly for regression,(Ozyegen, Ilic, and Cevik2022)defined the \u201dArea Over the Perturbation Curve for Regression\u201d (AOPCR, Equation5). For MAE, it calculates the change in prediction for each outputand prediction horizonby masking topfeatures for eachthen takes the average. AOPC and AOPCR for sufficiency are calculated similarly after replacingwith.\n\nSECTION: 5Results\n\nThis section shows the interpretation results and visualizations. Then discuss our time complexity and the effect of changing the lookback window.\n\nSECTION: 5.1Benchmark Evaluation\n\nTable4shows the overall results. Our method performs the best or second best in most cases. This is consistent across different datasets and models. We ranked the methods for each dataset and model in terms of overall comprehensiveness and Sufficiency. Then we averaged the ranks in the rightmost columns and used for the final rank.WinTSR achieves the best average rank in each dataset, 1(1.40.5), 1(1.40.05), and 1(2.41.5) in the Electricity, Traffic, and MIMIC-III respectively.\n\nIntegrated Gradient achieves the best results in a few cases for comprehensiveness in regression but fails in others. TSR performs significantly better for comprehensiveness in the MIMIC-III dataset, but its high sufficiency in the same dataset shows the top features it selects are not sufficient. Feature Ablation method also consistently performed well and achieved 2nd rank overall. We also see the mask learner methods, in practice do not interpret the SOTA models well.\n\nSECTION: 5.2Visualizing Interpretation\n\nVisualizing the interpretations helps to understand their meaning. However, unlike images and texts, time series interpretations are harder to visualize and to verify intuitively. Here we 1) visualize persistent temporal patterns (trends present across the dataset) and 2) highlight the top features across time. Figure3shows the raw input feature (left) and the interpretation of these features (using normalized relevance/importance score). The MIMIC-III dataset is visualized with a heatmap due to many features (31), the other two datasets are shown with line plots. The interpretation results of the four selected methods are presented for comparison using the best-performing iTransformer model of 1st iteration.\n\nThe relevance scores shown here are for forecasting the target for the next hour () or predicting mortality class () for simplicity. Electricity and traffic features show a daily pattern, where the importance is highest at the most recent time step () and the same time the previous day (). Sometimes at the last peak. This means, to predict the electricity consumption or traffic occupancy, past observations from recent times or the same daytime or last peak hour are important. For MIMIC-III the goal is to interpret which factors at which time were important in predicting the patient\u2019s death. Figure3(c) shows the top three points interpreted by the methods, where WinIT and TSR display the features important in the last 12 hours, whereas WinTSR and FA identify these features much earlier, within the first 12 hours, and then again around the last 12 hours. Temporal change of the important features is visible in WinTSR, WinIT, and TSR as they all consider temporal dependency.\n\nSECTION: 5.3Time Complexity\n\nWe summarize the run time information in Table5for some selected methods, Appendix11includes the complete results. Our WinTSR method\u2019s time complexity is, whereis the lookback window andis the number of features. The perturbation-based methods (FA, AFO, FP) have similar run-time efficiencysince they also perturb each feature at each time point. WinIT has time complexity, but since it needs to perturb a sliding window of feature each time, it is slower in practice. Gradient-based methods (IG, GS, DM) run the fastest. TheTSRmethod is the slowest since it repeatedly applies theIGmethod across each time and feature column, then along the time axis to calculate the time relevance score. The time complexity iswhereis the time complexity of the Integrated Gradient (IG) method. In practice,WinTSR is around 32 to 367 times faster than TSR.\n\nSECTION: 5.4Varying Lookback Window\n\nSince the lookback window size is an integral part of capturing temporal dependency, it is important to analyze the effect of changing the window size. By design, the WinIT method supports variable window length, where TSR and WinTSR compute over the whole training window size. We retrained the best-performing iTransformer model for different lookback windows and interpreted it by comparing the 3 window-based methods (WinIT, TSR, WinTSR), specifically on temporal dependency. The results are shown in Table6. We reduced the lookback to 24-hour and 48-hour for Electricity and Traffic (original data have 96-hour lookback). For MIMIC-III, we varied the lookback to 24-hour and 36-hour since the original data had a 48-hour lookback. WinTSR performs best or 2nd best in most cases, showing its robustness across different input window sizes.\n\nSECTION: 6Conclusion and Future Work\n\nIn this paper, we present a novel local interpretation method \u201dWindowed Temporal Saliency Rescaling\u201d that explicitly accounts for the dynamic temporal nature of input data and explains their features\u2019 importance. Through extensive experiments and metric comparisons, our analysis 1)shows WinTSR provides a more accurate interpretation of temporal dependencies among features; 2) benchmarks different neural network models: DLinear (Linear), SegRNN (RNN), MICN (CNN), iTransformer (Transformer), and CALF (LLM). 3) compares with ten widely used interpretation methods; 4) presents an easy-to-use framework by combining a popular time series library with interpretation libraries. This framework enables the quantitative measurement of time series interpretation across many recent models and methods.\n\nFor future work, we will identify higher-level patterns and trends in time series models by explicitly incorporating both spatial and temporal domains, to enhance the effectiveness and efficiency of AI interpretability in time series datasets. We will explore using the pre-trained foundation models to explain features in the time series domain.\n\nSECTION: References\n\nSECTION: Appendix ADataset and Features\n\nSECTION: A.1Electricity\n\nThe UCI Electricity dataset(Trindade2015)contains the consumption of 321 customers from 2012 to 2014. We aggregated it on an hourly level. This data has been used as a benchmark in many time series forecasting models(Wu et\u00a0al.2023; Zeng et\u00a0al.2023; Ozyegen, Ilic, and Cevik2022). Following(Wu et\u00a0al.2021)we use the past 96 hours to forecast over the next 24 hours. And we added four time-encoded features: month, day, hour, and day of the week.\n\nSECTION: A.2Traffic\n\nThe UCI PEM-SF Traffic Dataset describes the occupancy rate (with) of 440 SF Bay Area freeways from 2015 to 2016. It is also aggregated on an hourly level. Following(Wu et\u00a0al.2021)we used a look-back window of 96 hours, a forecast horizon of 24 hours, and the 821st user as the target variable. We added four time-encoded features: month, day, hour, and day of the week.\n\nSECTION: A.3MIMIC-III Mortality\n\nA multivariate real-world clinical time series dataset with a range of vital and lab measurements taken over time for over 40,000 patients(Johnson et\u00a0al.2016). It is widely used in healthcare and medical AI-related research, and also in time series interpretation(Tonekaboni et\u00a0al.2020; Leung et\u00a0al.2023). We follow the pre-processing procedure by(Leung et\u00a0al.2023)to drop patients with missing information and then aggregate. Among the 22988 patient data left in the dataset, 2290 died during their hospital stay. We use the measurements hourly over 48 hours to predict patient mortality (whether the patient died). Table7lists the clinical features used from the MIMIC-III patient dataset used in our experiments. There are four static features, twenty lab measurements, and eight vitality indicators.\n\nSECTION: Appendix BParameters and Notations\n\nThe model and training parameters are chosen following(Wu et\u00a0al.2023)for a consistent comparison with the state-of-the-art. Table8and9list the training parameters and the model hyperparameters used during our experiments. Table10summarizes the notations mainly defined during the problem statement and interpretation evaluation framework.\n\nSECTION: Appendix CInterpretation methods\n\nThe following describes the interpretation methods we have compared within this paper.\n\nFeature Ablation (FA):The difference in output after replacing each feature with a baseline.(Suresh et\u00a0al.2017).\n\nAugmented Feature Occlusion (AFO):Tonekaboni et\u00a0al. (2020)ablated the input features by sampling counterfactuals from the bootstrapped distribution.\n\nFeature Permutation (FP):Permutes the input feature values within a batch and computes the difference between original and shuffled outputs(Molnar2020).\n\nIntegrated Gradients (IG):Sundararajan, Taly, and Yan (2017)assigned an importance score to each input feature by approximating the integral of gradients of the model\u2019s output to the inputs.\n\nGradient Shap (GS):Lundberg and Lee (2017)approximated SHAP values by computing the expectations of gradients by randomly sampling from the distribution of baselines/references.\n\nDyna Mask (DM):Crabbe and van\u00a0der Schaar (2021)learned masks representing feature importance.\n\nExtremal Mask (EM):Enguehard (2023a)improved the static perturbation from Dyna Mask by learning not only masks but also associated perturbations.\n\nWindowed Feature Importance in Time (WinIT):Leung et\u00a0al. (2023)explicitly accounted for the temporal dependence among observations of the same feature by summarizing its importance over a lookback window.\n\nTemporal Saliency Rescaling (TSR):Ismail et\u00a0al. (2020)proposed to separate the temporal dimension when calculating feature importance and rescaling it.\n\nContraLSP:Liu et\u00a0al. (2024d)designed a contrastive learning-based masking method to learn locally sparse perturbations for better explaining feature relevance with and without top important features.\n\nSECTION: Appendix DTime Complexity\n\nTable11shows the full run time comparison between the interpretation methods.\n\nSECTION: Appendix EAvailable Models\n\nOur framework currently includes the following time series foundation models:\n\nCALF: Aligns LLMs for time series forecasting with cross-modal fine-tuning(Liu et\u00a0al.2024a).\n\nTimeLLM: Reprograms LLMs and its tokenization for better forecasting(Jin et\u00a0al.2024).\n\nGPT4TS: Generalizes pretrained LLMs (GPT-2, Bert) for time series.\n\nWe include the following transformer-based and other recent time series models in our proposed framework:\n\nTimeMixer,Wang et\u00a0al. (2024)\n\nTSMixer,Chen et\u00a0al. (2023)\n\niTransformer,Liu et\u00a0al. (2024b)\n\nTimesNet,Wu et\u00a0al. (2022)\n\nDLinear,Zeng et\u00a0al. (2023)\n\nPatchTST,Nie et\u00a0al. (2022)\n\nMICN,Wang et\u00a0al. (2023)\n\nCrossformer,Zhang and Yan (2023)\n\nSegRNN,Lin et\u00a0al. (2023)\n\nKoopa,Liu et\u00a0al. (2024c)\n\nFreTS,Yi et\u00a0al. (2024)\n\nTiDE,Das et\u00a0al. (2023)\n\nLightTS,Zhang et\u00a0al. (2022)\n\nETSformer,Woo et\u00a0al. (2022)\n\nNon-stationary Transformer,Liu et\u00a0al. (2022b)\n\nFEDformer,Zhou et\u00a0al. (2022b)\n\nPyraformer,Liu et\u00a0al. (2022a)\n\nFiLM,Zhou et\u00a0al. (2022a)\n\nAutoformer,Wu et\u00a0al. (2021)\n\nInformer,Zhou et\u00a0al. (2021)\n\nReformer,Kitaev, Kaiser, and Levskaya (2020)\n\nTransformer,(Vaswani et\u00a0al.2017)\n\nSECTION: Appendix FReproducibility Statement\n\nOur source code and documentation are already publicly available on GitHub. The public link will be shared upon acceptance. The documentation includes detailed settings and instructions for reproducing the experiments. The Electricity and Traffic datasets are publicly available. The private MIIMIC-III dataset can be accessed by following the steps at https://mimic.mit.edu/docs/gettingstarted/. In addition, we follow the procedures outlined in previous studies to preprocess the datasets and also include the code in our project. Our GitHub repository has singularity and docker definitions to ensure a reproducible container environment. The experiments are run on a single-node Linux server with 16 GB RAM and NVIDIA RTX GPU. We use a Python 3.12 environment with Pytorch 2.3.1 and Cuda 11.8. Version details of other libraries are given in our repository. The random processes are seeded to ensure reproducibility.", "text_file": "data\\paper_texts\\2412.04532v1_content.txt"}, {"title": "Reachable Polyhedral Marching (RPM): An Exact Analysis Tool for\n  Deep-Learned Control Systems", "authors": ["Joseph A. Vincent", "Mac Schwager"], "published_date": "2022-10-15T17:15:53Z", "summary": "Neural networks are increasingly used in robotics as policies, state\ntransition models, state estimation models, or all of the above. With these\ncomponents being learned from data, it is important to be able to analyze what\nbehaviors were learned and how this affects closed-loop performance. In this\npaper we take steps toward this goal by developing methods for computing\ncontrol invariant sets and regions of attraction (ROAs) of dynamical systems\nrepresented as neural networks. We focus our attention on feedforward neural\nnetworks with the rectified linear unit (ReLU) activation, which are known to\nimplement continuous piecewise-affine (PWA) functions. We describe the\nReachable Polyhedral Marching (RPM) algorithm for enumerating the affine pieces\nof a neural network through an incremental connected walk. We then use this\nalgorithm to compute exact forward and backward reachable sets, from which we\nprovide methods for computing control invariant sets and ROAs. Our approach is\nunique in that we find these sets incrementally, without Lyapunov-based tools.\nIn our examples we demonstrate the ability of our approach to find non-convex\ncontrol invariant sets and ROAs on tasks with learned van der Pol oscillator\nand pendulum models. Further, we provide an accelerated algorithm for computing\nROAs that leverages the incremental and connected enumeration of affine regions\nthat RPM provides. We show this acceleration to lead to a 15x speedup in our\nexamples. Finally, we apply our methods to find a set of states that are\nstabilized by an image-based controller for an aircraft runway control problem.", "arxiv_id": "2210.08339v3", "html_link": "https://arxiv.org/html/2210.08339v3", "search_term": "ti:\"deep learning\"", "html_content": "SECTION: Reachable Polyhedral Marching (RPM): An Exact Analysis Tool for Deep-Learned Control Systems\n\nNeural networks are increasingly used in robotics as policies, state transition models, state estimation models, or all of the above.\nWith these components being learned from data, it is important to be able to analyze what behaviors were learned and how this affects closed-loop performance.\nIn this paper we take steps toward this goal by developing methods for computing control invariant sets and regions of attraction (ROAs) of dynamical systems represented as neural networks.\nWe focus our attention on feedforward neural networks with the rectified linear unit (ReLU) activation, which are known to implement continuous piecewise-affine (PWA) functions.\nWe describe the Reachable Polyhedral Marching (RPM) algorithm for enumerating the affine pieces of a neural network through an incremental connected walk.\nWe then use this algorithm to compute exact forward and backward reachable sets, from which we provide methods for computing control invariant sets and ROAs.\nOur approach is unique in that we find these sets incrementally, without Lyapunov-based tools.\nIn our examples we demonstrate the ability of our approach to find non-convex control invariant sets and ROAs on tasks with learned van der Pol oscillator and pendulum models.\nFurther, we provide an accelerated algorithm for computing ROAs that leverages the incremental and connected enumeration of affine regions that RPM provides.\nWe show this acceleration to lead to a 15x speedup in our examples.\nFinally, we apply our methods to find a set of states that are stabilized by an image-based controller for an aircraft runway control problem.\n\nSECTION: IIntroduction\n\nSECTION: I-AOverview\n\nIn this paper we describe the Reachable Polyhedral Marching (RPM) algorithm for enumerating the affine regions of neural networks with rectified linear unit (ReLU) activation.\nThis algorithm can then be leveraged to compute forward and backward reachable sets of neural networks.Our algorithm provides a building block for proving safety properties for autonomous systems with learned perception, dynamics, or control components in the loop.\nSpecifically, given a set in the input space, RPM computes the set of all corresponding outputs.\nSimilarly, given a set of outputs, RPM computes the set of all corresponding inputs under the ReLU network.\nWe use these capabilities to compute control invariant sets and regions of attraction (ROAs) for dynamical systems represented as neural networks.Computing these sets helps roboticists (i) verify whether safety specifications on the robot state are met, (ii) identify states that will converge to some desirable equilibria, and (iii) identify regions in the state space for which a system can be controlled.\nIdentification of these sets is an important part of the control design process, as directly synthesizing control policies that meet invariance constraints by construction is challenging[1].\nIf a constraint is not met, the reachable or invariant sets computed give insight into which states lead to violation of the constraints, informing targeted policy improvements.\n\nIt is well known that ReLU networks implement continuous piecewise-affine (PWA) functions, that is, the input space for a ReLU network can be tessellated into polyhedra, and over each polyhedron the neural network is affine.\nThe RPM algorithms explicitly finds this equivalent PWA representation for a given ReLU network.\nFigure1illustrates how RPM incrementally solves for this representation.\nThe algorithm incrementally enumerates the polyhedra and affine pieces of the PWA function by solving a series of Linear Programs (LPs), and following analytical edge flipping rules to determine neighboring polyhedra.\nThe algorithm starts with an initial polyhedron, then solves for its neighboring polyhedra, then neighbors of neighbors, etc., until the desired input set is tessellated.\nIn this way, our method is geometrically similar to fast marching methods in optimal control[2], path planning[3,4], and graphics[5,6].\n\nWe then use computational methods for PWA reachability to compute forward and backward reachable sets for each polyhedron and associated affine function.The ability to compute forward and backward reachable sets allows us to search for and construct control invariant sets and ROAs without Lyapunov tools.\nFinally, we also propose an accelerated backward reachability procedure that leverages the connected enumeration of the RPM algorithm to restrict the number of polyhedra the algorithm searches over.\nWe show that this accelerated procedure is guaranteed to enumerate the entire backward reachable set in the case that the deep network is a homeomorphism (a continuous bijection with continuous inverse).\nChecking this condition is simply done, and represents a novel general procedure for examining the invertibility of neural networks whose hidden layers are not bijections themselves.\n\nSECTION: I-BExisting Work\n\nMost existing algorithms that compute exact reachable sets of neural networks iterate through the network layer-by-layer[7,8,9].\nThe layer-by-layer approaches obtain the entire reachable set at once at the end of the computation, rather than revealing the reachable set piece by piece throughout the computation.\nConsequently, if the computation must end early due to memory constraints or a computer fault, no usable result is obtained.\nIn contrast, RPM builds the reachable set one polyhedron at a time, leading to a partial, but still potentially useful result if computation is halted before the algorithm runs to completion.\n\nThe nnenum tool uses an exact reachability algorithm that is also incremental[10].\nThe difference between this tool and our RPM algorithm is that RPM enumerates affine regions of the neural network in a connected walk; each new region that is revealed is connected to a previous region.\nIn contrast, the algorithm used by nnenum does not have this property; whether a new region is connected to a previous one is unknown by the algorithm.\nIn SectionVIwe show how the connected enumeration property leads to an accelerated algorithm for computing ROAs.\nAnother way to conceptualize this difference between RPM and nnenum is that RPM can also return the graph that describes which affine regions neighbor one another.\n\nSECTION: I-CContributions and Organization\n\nIn this paper, we build off the RPM algorithm to construct control invariant sets and ROAs for dynamical systems represented as neural networks.\nBy taking this approach, we are able to compute intricate invariant sets that may be non-convex and even disconnected.\nIn numerical examples, we compute ROAs for a learned van der Pol oscillator, and show this computation enjoys ax speedup because the learned dynamics are homeomorphic.\nWe then pair RPM with the MPT3[11]Matlab toolbox to find an a control invariant set for a learned torque-controlled pendulum.\nFinally, we apply our algorithm to find a set of states stabilized by an image-based airplane runway taxiing system, TaxiNet[12].\nThe closed-loop system is PWA withregions, orders of magnitude larger than PWA systems for which other nonlinear stability analysis approaches have been demonstrated.\n\nThe contributions of this paper are\n\nFor ReLU neural networks, a theorem to analytically determine an input space polyhedron from its neighbor by flipping neuron activations in the network.\n\nApplication of PWA analysis tools to compute control invariant sets and regions of attraction for dynamical systems represented as ReLU networks.\n\nAn accelerated exact backward reachability algorithm for ReLU networks that leverages a novel method for checking the invertibility of the network.\n\nThis paper builds upon an earlier conference version[13], that first introduced the RPM algorithm to convert a ReLU network into a PWA representation.\nThis paper improves upon[13]by (i) adding a formal proof of correctness for the main RPM algorithm, (ii) extending the RPM framework to compute control invariant sets and ROAs, and (iii) introducing an accelerated method for computing ROAs.\n\nThe paper is organized as follows.\nWe give related work in SectionIIand give background and state the problem in SectionIII.\nSectionIVdescribes the RPM algorithm and explains its derivation.\nIn SectionVwe describe how RPM is used to perform forward and backward reachability computations for ReLU networks over multiple time steps.\nIn SectionVIwe describe how RPM is leveraged to compute control invariant sets and ROAs.\nFinally, SectionVIIpresents numerical results for computing control invariant sets and ROAs for learned dynamical systems and we offer conclusions in SectionVIII.\n\nSECTION: IIRelated Work\n\nThough the analysis of neural networks is a young field, a broad literature has emerged to address varied questions related to interpretability, trustworthiness, and safety verification. Much work has been dedicated to characterizing the expressive potential of ReLU networks by studying how the number of affine regions scales with network depth and width[14,15,16]. Other research includes encoding piecewise-affine (PWA) functions as ReLU networks[17,18], learning deep signed distance functions and extracting level sets[6], and learning neural network dynamics or controllers that satisfy stability conditions[19,20], which may be more broadly grouped with correct-by-construction training approaches[21,22].\n\nSpurred by pioneering methods such as Reluplex[23], the field of neural network verification has emerged to address the problem of analyzing properties of neural networks over continuous input sets. A survey of the neural network verification literature is given by[24]. Reachability approaches are a subset of this literature and are especially useful for analysis of learned dynamical systems.\n\nReachability methods can be categorized into overapproximate and exact methods.The reachable sets computed by overapproximate methods are guaranteed to capture all reachable states, but may also contain states that are unreachable.Overapproximate methods often compute neuron-wise bounds either from interval arithmetic or symbolically[25,26,27,28,29,30]. Optimization based approaches are also used to solve for bounds on a reachable output set in[31,32,33,34]. Other approaches include modeling the network as a hybrid system[35], abstracting the domain[36], and performing layer-by-layer operations on zonotopes[37].Closed-loop forward reachability using overapproximate methods is investigated in[38,39,40,41,42,43]Closed-loop backward reachability using overapproximate methods is investigated in[44,45].\n\nExact reachability methods have also been proposed, although to a lesser degree.\nThese methods either iteratively refine the reachable set by applying layer-by-layer transformations[7,8,9], or they solve for the explicit PWA representation and compute reachable sets from this[10,13].\nSimilar layer-by-layer approaches have also been proposed to solve for the explicit PWA representation of a ReLU network[46,47,48].\n\nOur RPM algorithm is an exact method, meaning that there is no conservatism in the reachable sets computed by our algorithm.\nOur algorithm inherits all the advantages of exact methods, but is unique in that it enumerates the reachable set in an incremental and connected walk.This makes our high-level procedure more similar to explicit model predictive control methods[49]than to other neural network reachability methods.\nFinally, all intermediate polyhedra and affine map matrices of layer-by-layer methods must be stored in memory until the algorithm terminates, whereas with incremental methods, once a polyhedron-affine map pair is computed it can be sent to external memory and only a binary vector (the neuron activations) needs to be stored to continue the algorithm.\nThis is especially useful because no method has been shown to accurately estimate the number of regions without explicit enumeration.Unlike other incremental approaches, ours explores affine regions in a connected walk. As a point of distinction, this allows our algorithm to not only output the PWA representation, but for each affine region also output the neighboring affine regions.\n\nIn this paper we demonstrate how RPM can be used not only for finite-time reachability, but also for computingcontrol invariant sets and ROAs.Computing these sets tends to be much harder than computing finite-time reachable sets, but the associated guarantees hold for all time, unlike with finite-time reachable sets.\nFew methods exist for computing invariant sets or ROAs for dynamical systems represented as neural networks, and most that do rely on an optimization-based search of a Lyapunov function.\nIn[50], the authors learn a quadratic Lyapunov function using semidefinite programming that results in an ellipsoidal ROA, a drawback of which is that ellipsoids are not very expressive sets.\nIn contrast, nonconvex ROAs can be computed by[51,52,53].\nThese methods learn Lyapunov functions for a given autonomous dynamical system and verify the Lyapunov conditions either using many samples and Lipschitz continuity or mixed integer programming.\nIn[54], an optimization and sampling-based approach for synthesizing control invariant sets is given, but as with searching for Lyapunov functions, it may fail in finding a control invariant set when one exists.\nAll of these methods inherit the drawbacks of Lyapunov-style approaches wherein a valid Lyapunov function may not be found when one exists, and certifying the Lyapunov conditions is challenging.In contrast to these methods, we explicitly solve for the PWA form of the dynamics, then find invariant sets using reachability methods rather than Lyapunov theory.\nA reachability-based approach like ours can be more constructive and reliable than Lyapunov approaches, as we show in our experiments.Lastly, a non-Lyapunov method is given in[55], where the focus is on continuous-time dynamical systems, whereas our focus is on discrete-time.\n\nFor PWA dynamics, there are specialized algorithms for computing ROAs based on convex optimization approaches for computing Lyapunov functions[56,57,58,59], or reachability approaches[11,60].\nA drawback to the Lyapunov approaches for PWA dynamics is they can be too conservative and the standard approaches require the user to provide an invariant domain, which itself can be very challenging to find.\nFurthermore, in Section 7.2 of[56], motivating examples are given where a variety of Lyapunov approaches fail to find valid Lyapunov functions for very small PWA systems that are known to be stable.\n\nIn contrast to the Lyapunov approaches, the reachability approach requires the user to supply an initial \u2018seed\u2019 set of states that is a ROA and uses backward reachability to grow the size of the ROA.\nFinding a seed ROA can be difficult for general nonlinear systems, but for PWA systems can be easily found due to the dynamics being locally linear.Although reachability-based computation of invariant sets has been explored for PWA dynamical systems before[60], a novelty in our work is pairing these methods with our RPM algorithm so that they can apply to dynamical systems represented as neural networks.Our proposed method for finding ROAs of neural networks follows the backward reachability approach from the literature.In addition, we show that our backward reachability algorithm may be sped up considerably when restricting RPM to enumerate a connected backward reachable set (a feature enabled by the connected walk of the algorithm).\n\nSECTION: IIIBackground and Problem Statement\n\nWe begin by formalizing a model of a ReLU network, and defining various concepts related to polyhedra, PWA functions, and dynamical systems.We then state the main problems we seek to address in this paper.\n\nSECTION: III-AReLU Networks\n\nAn-layer feedforward neural network implements a function, with the mapdefined recursively by the iteration\n\nwhereis the input,is the output, andis the hidden layer output of the network at layer. The functionis the activation function at layer, andandare the weights and biases, respectively, wheredenotes the number of neurons in layer. We assumeis an identity map and all hidden layer activations are ReLU,\n\nwhereis the pre-activation hidden state at layer. We can rewrite the iteration in (1) in a homogeneous form\n\nwhere\n\nWe defineas the tuple containing all the parameters that define the function.\n\nGiven an inputto a ReLU network, every hidden neuron has an associated binaryneuron activationof zero or one, corresponding to whether the preactivation value is nonpositive or positive, respectively.111This choice is arbitrary. Some works use the opposite convention.Specifically, the activation for neuronin layeris given by\n\nWe define the activation pattern at layeras the binary vectorand the activation pattern of the whole network as the tuple. For a network withneurons there arepossible combinations of neuron activations, however not all are realizable by the network due to inter-dependencies between neurons from layer to layer, as we will see later in the paper. Empirically, the number of activation patterns achievable by the network is better approximated byfor input space with dimension[61].\n\nWe can write equivalent expressions for the neural network function in terms of the activation pattern. Define the diagonal activation matrix for layeras, where ais appended at the end222Note that the homogeneous form adds a dummy neuron at each layer that is always active, since the last element of the hidden state isin homogeneous form, and thus always positive.to match the dimensions of the homogeneous form in (3). Using the activation matrix, we can write the iteration defining the ReLU network from (3) as\n\nFinally, the map from inputto the hidden layer preactivationcan be written explicitly from the iteration (6) as\n\nwhereis therow of the matrix. Similarly, the whole neural network functioncan be written explicitly as a map fromtoas\n\nIt is well known that in (8), the outputis a continuous and PWA function of the input[14,15,16]. Next, we mathematically characterize PWA functions and give useful definitions.\n\nSECTION: III-BPolyhedra and PWA Functions\n\nA convex polyhedron is a closed convex setdefined by a finite collection ofhalfspace constraints,\n\nwhere,, and eachpair defines a halfspace constraint. Defining matrixand vector, we can equivalently write\n\nWe henceforth refer to convex polyhedra as just polyhedra.\n\nThe above representation of a polyhedron is known as the halfspace representation, or H-representation.\nNote:\n\nA polyhedron can be bounded or unbounded.\n\nA polyhedron can occupy a dimension less than the ambient dimension (iffor some pairsand some positive scalars).\n\nA polyhedron can be empty (such that).\n\nWithout loss of generality, the halfspace constraints for a polyhedron can be normalized so that(by dividing the unnormalized parametersandby), whereis thenorm.represents a degenerate constraint that is trivially satisfied if.\n\nAn H-representation of a polyhedron may have an arbitrary number of redundant constraints, which are constraints that are implied by other constraints.\n\nA polyhedral tessellationis a finite set of polyhedra that tessellate a set. That is,and, for all, wheredenotes the-dimensional Euclidean volume of a set (the integral over the set with respect to the Lebesgue measure of dimension).\n\nIntuitively, the polyhedra in a tessellation together cover the set, and can only intersect with each other at shared faces, edges, vertices, etc.In the remainder of the paper when we refer to shared faces between neighboring polyhedra we mean thedimensional polyhedron given by the set intersection.\n\nTwo polyhedra,and, are neighbors if their intersection has non-zero Euclidean volume indimensions, that is,.\n\nIntuitively, neighboring polyhedra are adjacent to one another in the tessellation, and share a common face. A polyhedral tessellation naturally induces a graph in which each node is a polyhedron and edges exist between neighboring polyhedra.\n\nA PWA function is a functionthat is affine over each polyhdron in a polyhedral tessellationof. Specifically,is defined as\n\nwhere,.\n\nNote that this requires a collection of tuples, wheredefine the polyhedron, anddefine the affine function over that polyhedron.\nWe refer to the PWA representation in (9) as theexplicitPWA representation, as each affine map and polyhedron is written explicitly without specifying the relationship between them.\nThere also exist other representations, such as the lattice representation[62,63]and the, so called, canonical representation[64,65].\n\nSECTION: III-CDynamical Systems\n\nIn this paper we are concerned with analyzing dynamical systems represented by ReLU networks.\nSpecifically, we consider the situations when the ReLU network represents the state transition function for a controlled (10) or autonomous (11) dynamical system,\n\nFor notational convenience, in the remainder of the paper we drop thesubscript from.\nWe next define key concepts and state the overall problems that we seek to solve.\n\nGiven an initial set of statesand a set of admissible control inputs, a-step forward reachable set can be defined recursively,\n\nGiven a final set of statesand a set of admissible control inputs, a-step backward reachable set can be defined recursively,\n\nThe ability to compute-step forward and backward reachable sets will later enable us to compute control invariant sets and regions of attraction.\n\nGiven a set of admissible control inputs, a control invariant set is a set of states for which there exists a sequence of control inputs such that the system state remains in the set for all time.\nIfis a control invariant set, then\n\nwhereandis the natural numbers.\n\nGiven an autonomous dynamical system, an invariant region of attraction (ROA) is a set of states that asymptotically converges to an equilibrium state () and for which sequences of states remain for all time.\nIfis an ROA, then\n\nwhere.\n\nIn this paper we seek to solve the following problems.\n\nGiven a ReLU networkand a polyhedral input set, compute the forward reachable set. Similarly, given a polyhedral output setcompute the backward reachable set.\n\nGiven a ReLU network that implements a discrete-time dynamical systemand state domain, identify the existence of stable fixed-point equilibriaand compute associated ROAs.\n\nGiven a ReLU network that implements a discrete-time dynamical system, a state domain, and set of admissible inputs, compute a control invariant set (if one exists).\n\nGiven a ReLU network that implements a function, determine whether the network is a homeomorphism (bijection with continuous inverse).\n\nIn the next section we describe the RPM algorithm for transcribing the ReLU network functioninto its equivalent explicit PWA representation. We then address the solution of the above problems in the subsequent sections.\n\nSECTION: IVFrom ReLU Network to PWA Function\n\nFirst, we seek to construct the explicit PWA representation of a ReLU network. Our method enumerates each polyhedral region and its associated affine map directly from the ReLU network. In Sec.IV-Awe show how polyhedra and affine maps are computed from the activation pattern of a ReLU network. In Sec.IV-Bwe show how polyhedral representations are reduced to a minimal form, which is used in Sec.IV-Cto determine neighboring polyhedra given a current polyhedron. This leads to a recursive procedure, that we call Reachable Polyhedral Marching (RPM), in which we explore an expanding front of polyhedra, ultimately giving the explicit PWA representation, as explained in Sec.IV-D.\n\nSECTION: IV-ADetermining Polyhedral Regions from Activation Patterns\n\nWe show here that the network activation patternfrom (5) has a one-to-one correspondence with the regions of the polyhedral tessellation underlying the ReLU network. We show how to explicitly extract the half-space constraints defining the polyhedron from the activation pattern.\n\nConsider the expression for the preactivation valuein (7). We can re-write this equation as\n\nwhere\n\nThe activationis decided by the test, which we can write as, defining a halfspace constraint in the input space. Specifically, we have the following cases,\n\nThis defines a halfspace constraint, that may or may not be open (due to the strict \u2018\u2019 inequality). However, on the boundary, we havefrom (12), leading to the post activation hidden state. We see that the value of the activationis irrelevant on the boundary, as the post activation state evaluates to zero regardless. We can therefore replace the \u2018\u2019 with \u2018\u2019 in (14) without loss of generality.\n\nFinally, to obtain the standard normalized form for a halfspace constraint (), we define\n\nwhere, in the degenerate case when, we defineand. Hence, we obtain one halfspace constraintfor each neuron activation state.\n\nGiven a specific input, we can then take the resulting activation pattern of the network, and directly extract the halfspace constraints that apply at that activation state from (14). In fact, (13) shows thatare actually functions of the activation patterns at earlier layers in the network. Indeed, consider perturbingas. The halfspace constraintswill remain fixed under this perturbation untilis large enough to change the activation pattern of an earlier layer,. Consider the set of all such perturbed input valuesthat do not result in a change in any neuron activation. We have\n\nwhich is a polyhedron. We see that for each activation pattern, there exists an associated polyhedronin the input space over which that activation pattern remains constant. The procedure for determining the polyhedron associated with an activation pattern is formalized in Algorithm1. A unique activation patterncan then be defined for each affine region. To be clear, we refer to the activation pattern for regionas, and use subscripts in parentheses to refer to the specific layer and neuron activation values. Lastly, for a fixed activation pattern, from (8) the ReLU network simplifies to the affine mapwhere\n\nSECTION: IV-BFinding Essential Constraints\n\nAs mentioned previously, a polyhedron may have redundant constraints. We find that many of the constraints for the polyhedrongenerated byare either duplicates or redundant, and can thus be removed. We define more formally the concepts of duplicate and redundant constraints.\n\nA constraintis duplicate if there exists a scalarand a constraint,, such that.\n\nA constraint is redundant if the feasible set does not change upon its removal. An essential constraint is not redundant.\n\nWe next describe how to remove the redundant constraints in a H-representation, leaving only the essential halfspace constraints. We first normalize all constraints, remove any duplicate constraints, and consider the resulting H-representation. To determine if the remainingconstraint is essential or redundant, we define a new set of constraints with theconstraint removed,\n\nand solve the linear program\n\nIf the optimal objective value is less than or equal to, constraintis redundant.\nNote, it is critical that any duplicate constraints are removed before this procedure. We formalize this procedure in Algorithm2.\n\nIn the worst case, a single LP must be solved for each constraint to determine whether it is essential or redundant. However, heuristics exist to practically avoid this worst case complexity. For computational efficiency, Algorithm2can be modified to first identify and remove a subset of redundant constraints using the bounding box heuristic[66]. We observe that this can result in identifying as many asof the redundant constraints. We find that other heuristics such as ray-shooting do not improve performance in our tests.\n\nSECTION: IV-CDetermining Neighboring Activation Patterns\n\nConsider two neighboring polyhedraandsuch that their shared face is identified byandis an essential constraint for, whileis an essential constraint for. We callthe neighbor constraint. Given the neuron activation patternfor, we next describe a procedure to find the activation patternfor, from which we can find the essential halfspace constraints from the procedure described in the previous section. This allows us to systematically generate all the activation patterns that are realizable by the network, and to obtain the polyhedra and affine maps associated with each of those activation patterns.\n\nIntuitively, to generateone ought to simply flip the activation of the neurons defining, and compute all the resulting new halfspace constraints from later layers in the network from (13). However, this intuitive procedure is incomplete because it does not correctly account for the influence that one neuron may have on other neurons in later layers. Flipping one neuron may lead to other neurons in later layers being flipped as well. To correctly determine the downstream effects of flipping a neuron, we provide a theorem for when neuron activationsmustbe flipped.\n\nFor a given region, each neuron defines a hyperplane, as given by (15). Suppose regionsandneighbor each other and are separated bywhere. Then,\n\nif", "text_file": "data\\paper_texts\\2210.08339v3_content.txt"}, {"title": "On Deep-Learning-Based Closures for Algebraic Surrogate Models of\n  Turbulent Flows", "authors": ["Benet Eiximeno", "Marcial Sanch\u00eds-Agudo", "Arnau Mir\u00f3", "Ivette Rodr\u00edguez", "Ricardo Vinuesa", "Oriol Lehmkuhl"], "published_date": "2024-12-05T15:21:10Z", "summary": "A deep-learning-based closure model to address energy loss in low-dimensional\nsurrogate models based on proper-orthogonal-decomposition (POD) modes is\nintroduced. Using a transformer-encoder block with easy-attention mechanism,\nthe model predicts the spatial probability density function of fluctuations not\ncaptured by the truncated POD modes. The methodology is demonstrated on the\nwake of the Windsor body at yaw angles of [2.5,5,7.5,10,12.5], with 7.5 as a\ntest case. Key coherent modes are identified by clustering them based on\ndominant frequency dynamics using Hotelling T2 on the spectral properties of\ntemporal coefficients. These coherent modes account for nearly 60% of the total\nenergy while comprising less than 10% of all modes. A common POD basis is\ncreated by concatenating coherent modes from training angles and\northonormalizing the set, reducing the basis vectors from 142 to 90 without\nlosing information. Transformers with different size on the attention layer,\n(64, 128 and 256), are trained to model the missing fluctuations. Larger\nattention sizes always improve predictions for the training set, but the\ntransformer with an attention layer of size 256 overshoots the fluctuations\npredictions in the test set because they have lower intensity than in the\ntraining cases. Adding the predicted fluctuations closes the energy gap between\nthe reconstruction and the original flow field, improving predictions for\nenergy, root-mean-square velocity fluctuations, and instantaneous flow fields.\nThe deepest architecture reduces mean energy error from 37% to 12% and\ndecreases the Kullback--Leibler divergence of velocity distributions from\nKL=0.2 to below KL=0.026.", "arxiv_id": "2412.04239v1", "html_link": "https://arxiv.org/html/2412.04239v1", "search_term": "ti:\"deep learning\"", "html_content": "SECTION: On Deep-Learning-Based Closures for Algebraic Surrogate Models of Turbulent Flows\n\nA deep-learning-based closure model to address energy loss in low-dimensional surrogate models based on proper-orthogonal-decomposition (POD) modes is introduced. Using a transformer-encoder block with easy-attention mechanism, the model predicts the spatial probability density function of fluctuations not captured by the truncated POD modes. The methodology is demonstrated on the wake of the Windsor body at yaw angles of, withas a test case. Key coherent modes are identified by clustering them based on dominant frequency dynamics using Hotelling\u2019son the spectral properties of temporal coefficients. These coherent modes account for nearly 60% of the total energy while comprising less than 10% of all modes. A common POD basis is created by concatenating coherent modes from training angles and orthonormalizing the set, reducing the basis vectors from 142 to 90 without losing information. Transformers with different size on the attention layer, (64, 128 and 256), are trained to model the missing fluctuations. Larger attention sizes always improve predictions for the training set, but the transformer with an attention layer of size 256 overshoots the fluctuations predictions in the test set because they have lower intensity than in the training cases. Adding the predicted fluctuations closes the energy gap between the reconstruction and the original flow field, improving predictions for energy, root-mean-square velocity fluctuations, and instantaneous flow fields. The deepest architecture reduces mean energy error from 37% to 12% and decreases the Kullback\u2013Leibler divergence of velocity distributions fromto below.\n\nSECTION: 1Introduction\n\nSurrogate models are data-driven computational techniques used in various scientific and engineering fields to approximate complex systems or functions. These models serve as simpler substitutes for both experiments and computationally expensive simulations, thus providing quicker, yet sufficiently accurate results(Sun & Wang,2019). Surrogate models are mainly utilized to estimate the optimum-product solution or as instrumental tools to evaluate the performance in the initial stages of the vehicle development because they reduce the resource requirements for design exploration(Kuyaet\u00a0al.,2011; Yondoet\u00a0al.,2018).\n\nIn the particular case of fluid-dynamics applications, surrogates are tipically built on a reduced space due to the complexity and high dimensionality of the original phenomenon(Yondoet\u00a0al.,2018). The dimensionality reduction can be done either with algebraic methods,e.g.the proper-orthogonal decomposition (POD)(Lumley,1981), or employing deep-learning-based techniques. POD was first introduced in fluid dynamics byLumley (1981)to express the chaotic turbulent motions into modes representing some portion of the total fluctuating energy of the flow.Sirovich (1987)explored the relationship between POD and the dominant features of the flow, and showed that POD is a relevant tool for the study of vortex dynamics in all types of fluid flows. Recently, other modal decompositions have been introduced in order to obtain modes that are associated with a single frequency instead of the range of frequencies present in the time series of the temporal coefficients in POD. Among these new techniques, the most popular are dynamic-mode decomposition (DMD)(Schmid,2010)and spectral proper-orthogonal decomposition (SPOD)(Towneet\u00a0al.,2018). Note that while POD and SPOD rank the modes in terms of their contribution to the reconstruction of the original flow, DMD obtains modes classified in terms of their dynamical importance to minimize errors in the reconstruction.\n\nAlternatively, deep-learning methods for dimensionality reduction are based on unsupervised-learning methodologies such as autoencoders. There are application examples of several autoencoder architectures for dimensionality reduction in fluid dynamics, including vanilla(Eivaziet\u00a0al.,2020; Murataet\u00a0al.,2019), hierachical(Fukamiet\u00a0al.,2020), physics-assimilated(Zhang,2023)and variational autoencoders(Eivaziet\u00a0al.,2022; Wanget\u00a0al.,2024; Solera-Ricoet\u00a0al.,2024; Akkariet\u00a0al.,2022). All of them are able to capture the non-linear behaviour of dynamical systems with a higher compression capacity than any POD-based methodology thanks to the excellent capabilities of spatial convolutions for non-linear feature extraction(Bruntonet\u00a0al.,2020; Vinuesa & Brunton,2022).\n\nIt is particularly relevant to mention that-variational autoencoders based on convolutional neural networks (CNN-VAEs) have been used successfully to obtain a disentangled latent representation of turbulent fluid flows. For instance,Eivaziet\u00a0al.(2022)compressed the turbulent flow around a simplified urban environment into 5 orthogonal latent variables containing more than the 85% of the flow energy. However, the need of convolutional layers restricts the usage of this technique to geometries that can be represented on a regular grid. On the other hand, algebraic decompositions can be used on unstructured grids at the cost of losing a significant amount of the energy of the system. A good illustration of this is the aforementioned study fromEivaziet\u00a0al.(2022), where 5 POD modes barely recover 30% of the flow energy. Accurately capturing all the fluctuations in a turbulent flow would require selecting nearly all the modes of the system.\n\nCoupletet\u00a0al.(2003)proved that large-index POD modes drain energy from the more significant modes, yielding an energy-cascade structure. Such a modal-energy redistribution suggests that reduced-order models (ROMs) can be built on a small number of significant modes that represent the majority of flow features and the contribution of the rest of modes can be modelled as an additional term to the ROM. This conclusion has led to an intense research on closures for reduced-order models based on Galerkin and Petrov\u2013Galerkin projections of the Navier\u2013Stokes equations. These models constitute a fundamental pillar for the stability of the projection(Stabile & Rozza,2018; Kaptanogluet\u00a0al.,2021)and have been traditionally inspired by sub-grid scale models such as the ones used in large-eddy simulations (LESs)(Wanget\u00a0al.,2012; Hijaziet\u00a0al.,2020; Imtiaz & Akhtar,2020). More recently, such closures have been modelled with data-driven techniques such as probabilistic neural networks(Mauliket\u00a0al.,2020). A recent review and comparison of data-driven methods for ROM closures can be found inPrakash & Zhang (2024).\n\nThe main goal of this manuscript is to present a new data-driven model capable of recovering the energy loss due to modal truncation in POD. Instead of working in the reduced space as the aforementioned closures, this work is focused on learning the spatial probability density function of the difference between the original field and the POD reconstruction using only the most significant modes with a transformer model(Vaswaniet\u00a0al.,2017). A transformer is a deep-neural-network architecture initially developed in the field of natural-language processing (NLP). Since then, it has revolutionized many areas of machine learning thanks to its attention mechanism, which enables identifying long-range dependencies in the data more effectively than traditional models(Yousifet\u00a0al.,2023). The rationale behind the approach proposed is to build a reduced-order model capable of predicting the most significant features of the flow, which are fully dependent on the geometry and initial conditions, and then add a separate correction for the smaller turbulent scales. The methodology is tested on the turbulent wake of the flow past the Windsor body(Littlewood & Passmore,2010), which is a simplified square-back vehicle. The model is designed to produce a closure valid for any free-stream-velocity direction in a yaw-angle range. The objective of the closure is to improve the POD reconstruction of the root-mean-square values of the stream-wise velocity fluctuations. This test case is highly relevant for the automotive industry because in any road vehicle the drag force increases linearly for yaw angles in the range of(Howell,2015). This drag increase is completely independent of the zero-yaw drag, thereby making it impossible to extrapolate the performance in cross-flow conditions from the parallel-flow case(Howell,2015). Hence, car manufacturers need to evaluate the aerodynamic performance under yawed flows in the development loop of a new vehicle(D\u2019Hoogeet\u00a0al.,2014). The development could be massively accelerated by using a surrogate model instead of re-running the simulations and wind-tunnel tests that are needed to characterize the aerodynamic performance of a road vehicle(Zhanget\u00a0al.,2006)at every angle of interest.\n\nThere are a number of studies in the literature that propose models able to evaluate dependence of the forces and moments on the yaw angle. For instance,Gonget\u00a0al.(2012)built a surrogate model based on the Kriging interpolation technique, which obtains the optimal wind-deflector geometry in a tractor trailer to reduce drag in crosswind situations. Similarly,Ghoreyshi & Cummings (2014)developed a model able to predict the dependence of the forces and moments on the Mach number, the angle of attack and the side-slip angle of an aircraft, whileZhanget\u00a0al.(2021)presented a model to predict the derailment coefficient of a train in crossflows. Lately,Eiximenoet\u00a0al.(2024c)developed a model to interpolate the mean base pressure in the Windsor body. To the authors\u2019 knowledge, there are no models that can evaluate changes in high order statistics on unseen flow conditions, hence, there are no models able to predict the velocity fluctuations with the yaw angle.\n\nThe rest of this manuscript is organized as follows:section\u00a02describes how the closure is formulated, how the significant POD modes are selected and how the model is extended to multiple flow conditions; then,section\u00a03shows the accuracy of the closure in the wake behind the Windsor body; and finally,section\u00a04summarizes the main findings of the manuscript.\n\nSECTION: 2Methodology\n\nThis section describes the methods used in the manuscript, including the Windsor-body dataset employed to test the methodology, a mathematical definition of POD and the selection of the most significant modes, together with an explanation of the model used to add the energy from the truncated modes.\n\nSECTION: 2.1Dataset description\n\nThe test dataset is the turbulent wake behind the Windsor body, the simplified square-back vehicle depicted inFigure\u00a01, at a Reynolds number of, whereis the magnitude of the free-stream velocity,is the length of the model andis the kinematic viscosity of the fluid. The data was generated by means of wall-modeled large-eddy simulations at yaw angles of. For the simulations, the spatially filtered incompressible Navier\u2013Stokes equations,\n\nwere numerically integrated using SOD2D (Spectral high-Order coDe 2 solve partial Differential equations)(Gasparinoet\u00a0al.,2024b), a low-dissipation spectral-element-method (SEM) code(Gasparinoet\u00a0al.,2024a).\n\nIn the equations aboveare the spatial coordinates (or,, and),(or,, and) stands for the velocity components andis the pressure. Note thatis the density of the fluid. The filtered variables are represented by.\nThe right-hand-side term inEquation\u00a02represents the sub-grid stresses, and its anisotropic part is expressed as,\n\nwhere the large-scale rate-of-strain tensoris evaluated as, withandbeing the Kronecker delta. Here, the unresolved scales are modelled using the local formulation of the integral length-scale approximation (ILSA)(Lehmkuhlet\u00a0al.,2019). The near wall region was modelled using the Reichardt wall-law(Reichardt,1951)with an exchange location in the 5th node(Lehmkuhlet\u00a0al.,2018).\n\nAfter the initial transients had been washed out, all simulations were run for 60 additional convective time units,, to collect 660 snapshots. The data for the model assessment was interpolated into the plane represented inFigure\u00a01. This plane is perpendicular to the vertical axis, therefore it contains the dynamics of both the leeward and windward sides of the wake. It is located at, which is half of the vehicle height when measured from the bottom of the body.\n\nIn the present work only a brief comparison of the fluid flow at the different yaw angles is shown to illustrate the different conditions in which the closure needs to be valid. For more details on the numerical model, grid and simulations accuracy, the reader is referred to the previous work byEiximenoet\u00a0al.(2024c)on the development of a surrogate model for the base pressure of the Windsor body.\n\nIn terms of the averaged flow, the wake of square-back bluff bodies in a yawed free stream flow is dominated by two vortices: one on the leeward side () and one on the windward side (), as it was shown byBooysenet\u00a0al.(2022). InFigure\u00a02, the flow streamlines are plotted forand. As reported byBooysenet\u00a0al.(2022), the vortex on the leeward side dominates the recirculation and gains intensity over the windward vortex as the yaw angle increases. This effect moves the vortex centers and the saddle point to the leeward side of the vehicle and closer to the body.\n\nThe changes in the vortex intensity have an effect on the recirculation length.Lorite-D\u00edezet\u00a0al.(2020)identified, in a square-back Ahmed body, that this vortex interaction leads to a decrease on the recirculation length and, to a deflection of the recirculation bubble towards the leeward side of the wake. Similar to the square-back Ahmed body, in the Windsor body, this trend is also observed. This can be seen inFigure\u00a03, where the mean streamwise velocityfor the cases atandis plotted. Here the recirculation length varies from 0.41L to 0.28L.\n\nInFigure\u00a04changes in the velocity fluctuations brought about with the yaw angle are illustrated by comparing the root-mean-square of the streamwise velocity fluctuations,at bothand.Figure\u00a04shows that a larger yaw angle increases the entrainment of irrotational free-stream into the near wake, resulting in larger fluctuation intensity and a steeper shear layer angle on both sides of the vehicle. The latter leads to a narrower wake. This is in agreement with the findings fromLiet\u00a0al.(2019)on a square-back Ahmed body.\n\nThe above changes in the mean flow with the yaw angle can also be observed when the mean streamwise velocity and its fluctuations are plotted along a streamwise line atand over a cross-stream line at, respectively (seeFigure\u00a05). For the objectives of the current work, it is relevant to remark that neither the fluctuations maxima nor their positions in the domain have a linear evolution with the yaw angle. Thus, it is not possible to derive a linear model to predict them.\n\nSECTION: 2.2Proper-orthogonal decomposition (POD)\n\nPOD is used in this work as a dimensionality-reduction technique. It is an efficient way to capture an infinite-dimensional process with a reduced number of modes(Holmeset\u00a0al.,1997). This method is based on finding a set of deterministic functions that characterize the dominant features of the system given by the field. This decomposition can be written as,\n\nwhereis the number of functions to decompose the field into. POD requires that the basis for the spatial modes is orthonormal, i.e.,\n\nand optimal, so that the the firstvectors are the ones that reconstruct the database with the minimum possible error.\n\nIn this work, the chosen method to perform POD is the singular-value decomposition (SVD). The SVD decomposes the initial snapshot matrix,, into the left singular vectors,, the singular values,, and the right singular vectors,,\n\nEach column ofcontains a spatial mode,and each column ofgives the evolution of the time coefficient,, of the corresponding mode. The singular values are given in a diagonal matrix and are associated with the energy contribution of each mode in descending order. The higher the singular value, the more energy is contained in the mode. The POD analysis has been performed using pyLOM(Eiximenoet\u00a0al.,2024a), a high-performance-computing reduced-order-modelling code that has a parallel and scalable algorithm for the singular-value decomposition(Eiximenoet\u00a0al.,2024b).\n\nSECTION: 2.3On the significance of POD modes\n\nTurbulent flows are characterized by a flat-tail of singular values, making it difficult to set an energy threshold to select the modes onto which the data has to be projected. This threshold is set arbitrarily and is decided based on a trade-off between accuracy of the model and evaluation cost. To overcome this issue, in this work the selection of the relevant modes is based on their frequency content. The objective is to select only the modes that contain relevant information on the frequency of the coherent structures of the flow. This is achieved by identifying outlier modes in the power-spectral density (PSD) matrix of the temporal coefficients,. In other words, the selected modes are those that exhibit a frequency spectrum significantly different from the rest.\n\nThe PSD matrix ofis computed by performing the Lomb-Scargle periodogram to the temporal coefficient of each mode of the system. Then, the outlier modes are identified with principal-component analysis, PCA. PCA is analogous to POD once the data has been normalized with its variance and centered to its mean. Since the PCA model may contain numerous components, its information is summarized using Hotelling\u2019s:\n\nwhereis the projection of the PSD of modeinto the PCA componentandis the covariance of that component. Note thatcan be seen as the distance from the center of the hyperplane formed by the components to the projection of the observation onto the hyperplane. The larger thevalue is, the more relevant frequency content the mode will have. Hence, the modes now can be selected with athreshold value that contains all the outliers.\n\nSECTION: 2.4POD projection and reconstruction\n\nThe POD basis for data projection is built using the spatial correlations of themodes corresponding to the frequency outliers. When working withdifferent inlet conditions, one can find an optimal POD basis among them by concatenating the spatial correlations of the outlier modes from each case to create the following matrix:\n\nThen, POD is applied to matrixto find an orthonormal basis that contains the information of the selected modes for each of the inlet conditions:\n\nThe resulting basis can be truncated as long as there are no information losses, i.e.\u00a0the selected modes are able to recover more than 99% of the energy.\n\nThe data matrixcan be projected now ontoas follows:\n\nwith the assurance that all coherent modes inside the inlet-conditions range are included in the reduced-order model. This operation reduces the dimensionality of the numerical data and sets a latent space for any surrogate-modelling applications. Such a surrogate model can be used to perform temporal predictions of the system or to evaluate its response to any condition in the evaluated range.\n\nWhen a prediction,, is reprojected back into the full-order space:\n\nthe main behavior of the system is captured, however, the model lacks the energy from the modes that were discarded during its construction.\n\nSECTION: 2.5Closure model\n\nThe missing energy of the prediction arises from the error between the original data and the reconstruction from the truncated POD modes:\n\nTo build a closure for the missing scales in the POD projection and reconstruction process, it is essential to understand the spatial and temporal distribution of this error. In other words, it is necessary to determine where and when this error is more likely to occur. The strategy followed in this work involves learning the evolution of the error as a function of the recovered fluctuations,, since this field contains all relevant information about the system\u2019s state at all points in the domain for the studied timestep. To achieve this, a transformer(Vaswaniet\u00a0al.,2017)encoder block is trained to minimize the difference between the actual error field,, and the predicted one, using the temporal series ofacross all points in the domain. The training process employs a mean-squared-error loss function. Thus, ifis known for a given timestep, the transformer can predict the corresponding error field,. From now on, the error predicted by the transformer is represented asand the error of the model after considering the closure is defined as:\n\nThe choice of using a transformer-based model is motivated by their ability to identify and predict the temporal dynamics of chaotic systems by capturing long-term dependencies in the data(Wuet\u00a0al.,2022; Geneva & Zabaras,2022; Sanchis-Agudoet\u00a0al.,2023). Additionally, transformers are well-suited for forecasting time series based on other spatial variables(Wang,2023)through their variant known as visual transformers (ViT). Transformers can be seen as universal approximators to probability density functions(Furuyaet\u00a0al.,2024). Hence, the proposed model actually learns the joint probability density function (PDF) ofgiven,. Furthermore, there exists an attention-only, Transformerwith attention normalizationsuch that, for any auto-regressive sequenceconverges exponentially fast asgoes to infinity, whereis the number of attention layers. Denoting\n\none has\n\nFor a more detailed study of the transformer\u2019s universality and the analytic intrinsics when approximating the theoretical measure the reader is referred toGeshkovskiet\u00a0al.(2024); Sander &\nPeyr\u00e9 (2024).\n\nThe latter definition ensures that the system modeled by the transformer is statistically equivalent to the original one and that the closure will be generalizable as long as the joint PDFfor a new set of data is similar to the original one. Such similarity is quantified using the Kullback\u2013Leibler divergence,:\n\nwhererepresents the joint PDF of the error for a single snapshot, whileis the joint PDF for all snapshots included in the training.\n\nIn this study, the input signal has a time-delay dimension of 48 steps, which means that the input to the transformer is a sequence of 48 consecutive time steps of the POD reconstruction. The output is the prediction of the error of the first time instant of the input series. A time-space embedding module is added to each point time signal to incorporate temporal and spatial information before passing it to the transformer blocks, allowing the model to distinguish between the evolution of the velocity in different points at different time steps. An average pooling and a max-pooling layer are added to the time-space embedding. Both of them are one dimensional and have a stride of two steps.\n\nThree different architectures (Table\u00a01) are tested to see the effect of the number of parameters on the closure accuracy. All of them are based on a single transformer encoder block (Figure\u00a06) with eight attention heads followed by a feed-forward layer. The only change between the three architectures is the size of the attention layers. The shallowest architecture has an attention size of 64 and then increases to 128 and 256. Those layers are in charge of measuring the importance of different parts of the input sequence when making predictions(Bahdanauet\u00a0al.,2014). Note that, the dimension of the feed-forward layer is set to 128 in all three cases. This layer learns complex non-linear relationships between the input and output sequences.\n\nThe choice of using multi-head attention is based on its oustanding performance over scaled dot product attention as it allows the model to jointly attend to information from different representation subspaces at different positions(Vaswaniet\u00a0al.,2017). In particular, the current architecture employs the easy-attention mechanism(Sanchis-Agudoet\u00a0al.,2023), which has demonstrated promising performance in predicting the temporal dynamics of chaotic systems, significantly outperforming the self-attention transformer(Solera-Ricoet\u00a0al.,2024). The easy-attention mechanism originally presented bySanchis-Agudoet\u00a0al.(2023)is defined by the mapping, given by the equation, where both the pseudo-input, input after embedding, and output matrices have the same dimensions, the attention size (Table\u00a01). In this formulation,andare matrices of trainable parameters, withrepresenting the temporal feature dimension andthe spatial feature dimension. Following the standard notation used in transformer architectures,denotes the values, while the matrixrepresents the attention weights. This mechanism, expressed as a kernel operation, can be formulated as:\n\nTo extend the easy-attention mechanism to the multi-head attention strategy, we consider multiple attention heads so that each of them focus on different parts of the input space. In this case, the inputis projected into multiple subspaces, allowing the model to attend to different sources of information simultaneously. For each attention head, we perform the same kernel operation as defined in the original mechanism, but with distinct sets of trainable parameters for the attention weights and value projections.\n\nThe multi-head version of the kernel operation can be written as:\n\nwheredenotes the index of the attention head,is the number of attention heads, and eachandare distinct trainable parameters for the-th attention head.\nThe final output of the multi-head attention is obtained by concatenating the outputs of each attention head:\n\nAfter the transformer block, a one-dimensional convolutional network of the same size as the attention layer and a fully connected layer of size of the number of points are added to decode the transformer output and form the final spatial prediction of the POD reconstruction error.\n\nThe training of each architecture was conducted along 3500 epochs, which required up to 8 hours and 15 minutes using an NVIDIA H100 GPU from the accelerated partition of the supercomputer MareNostrum 5(Barcelona Supercomputing Center,2024). An extensive discussion on the accuracy of each architecture is presented next in the results section.\n\nSECTION: 3Results\n\nThis section presents the performance of the designed closure model for the POD reconstruction of the turbulent wake behind the Windsor body. The dataset described insection\u00a02is split between the training,, and test,, sets. The training set is used to build a common POD basis along the yaw-angle range and to train the transformer which predicts the reconstruction error. Then, the high-fidelity results atare projected into that POD basis and reconstructed with the additional closure term from the transformer to assess its performance on unseen data. All results are obtained for the streamwise velocity fluctuations, therefore,inEquation\u00a06is equivalent to.\n\nSECTION: 3.1POD common basis\n\nThe first step to build the common basis is to perform the POD of each of the training angles individually. After that, the PSD-based mode-selection process described insection\u00a02is applied.Figure\u00a07shows theclustering results for each of the angles. The threshold for the coherent-mode selection is set to. Table2shows the number of selected modes in each case and the amount of energy recovered. It can be seen that the tendency is to have between 30 and 40 modes per angle containing coherent structures that represent a somewhat larger amount than half of the total energy. The case atis the one in which the coherent modes account for the smallest energy percentage, 52.0%, while forthey account for up to 58.1%. It is important to note that the rest of the energy is shared among the remaining 600 non-coherent modes, therefore, each of them has a small individual contribution to the total energy of the system.\n\nTwo modes of the case atare used to illustrate the clustering process.Figure\u00a08compares the spatial correlation of a coherent mode with the one of a non-coherent mode. Note that the chosen coherent mode is the fifth most energetic one and the non-coherent mode is the 450th most energetic one. The coherent mode is clearly dominated by four large correlated regions linked to the vortex shed from the windward side of the vehicle, whereas the non-coherent mode depicts multiple small scales.\n\nFigure\u00a09compares the temporal coefficient and its spectrum for both modes. The spectrum of the coherent mode (9(a)right) exhibits a peak at the non-dimensional frequency of. This peak corresponds to the windward vortex-shedding frequency(Eiximenoet\u00a0al.,2024c; Booysenet\u00a0al.,2022). Note that no dominant frequencies can be observed in the spectrum of the non-coherent mode (9(b)right), which is completely flat as those from pure white noise signals. These modes are seen as noise in the reduced system as their temporal coefficients are completely uncorrelated. The temporal coefficients of the non-coherent modes (9(b)left) suggest that the lack of correlation might come from an inadequate sampling frequency, this one being lower than the dominant frequency of these modes. The noisy and random evolution of the non-coherent modes, together with their small individual energy contribution, would increase the cost of a surrogate model if they were included in the reduced system. However, they cannot be discarded without an efficient closure that accounts for the large energy percentage that they contain as a group.\n\nThe spatial correlations of the selected modes are then concatenated to create the matrixas inEquation\u00a08. As some coherent modes might be repeated in the yaw angle range, POD is applied to matrixto find the optimal and orthonormal basis that contains the information of the selected modes for the four angles.Figure\u00a010shows the cumulative singular values to prove that instead of using all the 142 coherent modes, 90 vectors are enough to represent the information of all the coherent modes in the yaw-angle range under study.\n\n11(a)presents the kernel density estimate of all the training snapshots for the original field,, its reconstruction after being projected into the common POD basis,, and the error between both of them,. The most likely situation is to have fluctuations close to zero in the original and reconstructed fields. This is explained by the large unperturbed area in the leeward () side of the domain. The source of error is then the filtering performed by the POD reconstruction of the high-amplitude fluctuations. Such filtering yields a field that is more likely to have points with velocity fluctuations close to zero than in the original case.11(a)also confirms that this is holds true for the test case at, bringing evidence that the common basis is valid for any angle in the studied range.11(b)compares the joint probability density function of the error given the reconstruction from the common basis,, for the training and tests fields. As stated insection\u00a02, this is the probability density function learnt by the closure model as it ensures that the predicted error yields a statistically equivalent system to the original one. Consistently with the results shown in11(a), the most likely case in both the training and test datasets is to have a state with the velocity reconstruction and its error with the original field being close to zero. The most probable values for the test set match the ones of the training set, however, the limits offor the training set are wider than those at.\n\nSECTION: 3.2Statistical closure accuracy\n\nThe three different architectures described inTable\u00a01are tested in order to assess the correct size of the attention layer. InFigure\u00a012, the probability density function,, given by the transformer output with the original one, represented in11(b), for both the training and test datasets are compared. Architecture 1, with an attention layer of size, performs poorly in learning both the center and the limits of the distribution. The Kullback\u2013Leibler divergence between the transformer prediction and the original probability for the training data is ofand for the test data is. Both values are the highest ones obtained during the architecture refinement process. In this case, the main source of error is that the PDF learnt by the transformer is much narrower than the original one, meaning that the model fails to recover the fluctuations with larger amplitude.\n\nIncreasing the attention layer to, with its subsequent duplication of the number of parameters, allows the transformer to learn a wider area of. This reduces the KL divergence with the original data tofor the training snapshots andfor the test snapshots. It is relevant to mention that this architecture nearly matches the output distribution for the test set as the limits offorare narrower than the ones found in the training set.\n\nDuplicating the attention size toleads to the best match of the training dataset of the three architectures. The learnt PDF expands for a wider area of fluctuations and the KL divergence is reduced to. Now the KL divergence on the test set is. The negative sign accounts for the larger fluctuations from the training set that are not present in the case ofand are already learnt by the transformer. Moreover, in this case the absolute value ofis slightly larger than the one found with architecture 2. This is the last step of architecture refinement because the evaluation of the test set has already crossed the ideal prediction in which the KL divergence would be null as the model shows the firsts signs of overfitting.\n\nThe wider area oflearnt by the architectures with larger attention size can be linked to the amount of turbulent kinetic energy (TKE),\n\nrecovered by the closure model. The TKE recovered in each case is quantified with the kernel density estimate among all the snapshots of the training and test sets separately.Figure\u00a013effectively showcases that the most likely energy value,, after the reconstruction from the POD common basis is significantly lower than the one of the original flow. For the training snapshots, it is reduced fromtoand for the test dataset it decreases fromto.\n\nFigure\u00a013also shows that the most likely energy value when adding the closure term increases with the attention layer size of the transformer used to model the missing fluctuations. In the training angles,increases fromtowhen the attention sizes changes fromto. It finally reaches the value ofwith the largest architecture of. A similar behavior is observed with the case at, for the architectures withandasgoes up toand, respectively. However, for the architecture withthe most likely energy value,, is slightly higher than in the original flow. This can be explained by the fact that the probability density function of the fluctuations predicted by the transformer is wider than the ones of the real case (Figure\u00a012).\n\nThis analysis brings evidence that the closure model actually reduces the offset between the energy of the POD reconstruction and the one of the original system.\nIt is important to note that the accuracy on the energy prediction is directly linked with the KL divergence between the predictedby the transformer and the ground truth. When the KL divergence is positive, the energy added by the closure is still smaller than the gap between the original flow and the POD reconstruction. Zero KL divergence would mean a perfect match between the model and the ground truth with no energy deviation. On the last scenario, a negative KL divergence indicates that the model is overshooting the predicted fluctuations, and with it the turbulent kinetic energy. Either with a positive or negative KL divergence, a larger absolute value indicates a larger deviation in the additional turbulent kinetic energy.\n\nSECTION: 3.3Spatial field reconstruction\n\nUp to this point of the discussion, it has been proven that adding a field of fluctuations based on thelearnt by the proposed transformer architectures is enough to close the energy gap of a POD reconstruction and the original flow field. However, it remains to be proven that the closure model can distribute these fluctuations adequately across the spatial and temporal domains.\n\nFigure\u00a014shows the root mean square (rms) of the velocity fluctuations in the spatial domain for the reconstruction from the common POD basis, the closure term trained withandtogether with the ones of the original field. The rms of velocity fluctuations is closely related with the local contribution to the total turbulent kinetic energy of the flow, hence, a field with the closure term matching the rms fluctuations of the original case could be considered accurate in space and statistically equivalent in time. In the figure, the case atis used to illustrate the performance on the various training angles. The results atare also plotted to show the performance of the model on unseen data. Moreover, the four mentioned reconstructions together with the one atare evaluated along the line atinFigure\u00a015. The reader is referred toAppendix\u00a0Afor the equivalent figures toFigure\u00a014andFigure\u00a015corresponding to the training cases at.\n\nBoth figures illustrate that the common basis captures the positions of the fluctuation maxima and their correct distribution along the domain, ensuring that the main flow structures are preserved throughout the projection and reconstruction processes (Equation\u00a010andEquation\u00a011). This is also valid for the case at, despite its features were not explicitly included in the basis.\n\nIn all analyzed angles, the reconstruction from the common basis misses the actual value by an offset associated with the filtered fluctuations.Figure\u00a014andFigure\u00a015show that increasing the attention size helps to close the gap in the rms fluctuations in all areas of the domain. The larger range of fluctuations learnt by the deeper architecture () and its additional kinetic energy added to the flow, translates to a nearly perfect match of the rms of the velocity fluctuations. It is worth mentioning that in the training cases most of the differences between the original field and the closure prediction arise from the model underestimating the fluctuations, however, atall the error of the closure is attributed to a slight overprediction.\n\nAs the offset between the POD reconstruction and the original field is not constant throughout the whole domain,Figure\u00a014andFigure\u00a015are also the evidence that the closure learns how much energy the POD reconstruction missed depending on the domain region. This proves that the closure model does not only close the energy gap statistically over all the points of all snapshots, but also that it can give accurate predictions of what happens in every point in the domain.\n\nSECTION: 3.4Instantaneous-field reconstruction\n\nAfter discussing how the closure model can emulate a field which is statistically equivalent to the original one in all points of the domain, it is time to discuss its impact on the reconstruction of the instantaneous fields. In this case, all comparisons are done with architecture 3 () as it is the only one able to close all the energy gap between the reconstruction and the original flow.\n\nThe first step is proving that the closure learns the amount of energy missing in each timestep. To do so,Figure\u00a016shows the temporal evolution of the total turbulent kinetic energy together with its POD reconstruction and the reconstruction corrected with the closure model for the case at. This case is the evidence that common POD basis successfully captures the instants of all energy maxima and minima of the original field. Once again, this is still valid even if the flow condition was not included in the database.\n\nFigure\u00a016also shows that the closure model represents the energy missing in each snapshot instead of adding the same energy to all of them. However, in the particular case of, the actual energy predicted by the closure is consistently higher than the one of the original flow in each snapshot, as have been discussed in the previous paragraphs. Table4links the better prediction of the energy temporal evolution with the mean relative error regarding the energy of the original field. In all angles this error has been reduced from overto a margin betweenand.\n\nClosing the energy gap appropiately in each snapshot also comes with a better prediction of the instantaneous fluctuations. To exemplify this,Figure\u00a017compares the original instantaneous field and its reconstruction for a handpicked snapshot at, effectively showcasing that the POD reconstruction exhibits large deviations from the original data. In fact,18(a)illustrates that the reconstruction from the standard POD basis leads to a relative error larger thanin 57.8% of the points in the domain. After adding the closure term, the accuracy of the reconstruction is increased so that only 13.7% of the points have a relative error higher than.\n\nThe comparison between the probability density functions (PDF) of the fields,18(b), agrees with11(a)on showing that the POD reconstruction filters the high-amplitude fluctuations by increasing the points with fluctuations close to zero. When adding the closure term, the probability density function of the velocity fluctuations is nearly identical to the one of the original field. In fact, the Kullback\u2013Leibler (KL) divergence between the reconstructed PDFs and the original one is reduced fromtoafter adding the term predicted by the transformer. Table5shows the meanover all snapshots to show that adding the closure term reduces the KL divergence with the original field of instantaneous velocity fluctuations regardless of the yaw angle.\n\nSECTION: 4Conclusions\n\nThis manuscript presents a deep-learning-based closure model for truncated POD modes. The main objective is to provide a methodology to recover the energy lost when building a surrogate model on a low dimensional space. To do so, a transformer model is used to learn the spatial probability density function of the difference between the original flow field and the POD reconstruction from the modes that would be included in a surrogate. The methodology is tested for the streamwise velocity fluctuations on a slice in the wake of the Windsor body at the yaw angles of. As the model has to be generelizable for unseen data, the case atis used as a test dataset and the rest of angles are used for training.\n\nBefore working on the transformer model, a set of POD modes at each of the training angles is selected. Those modes have to be the most meaningful ones in the system as they would constitute the core of a reduced-order model. The selection process is based on performing principal-component analysis on the power-spectral density of the temporal coefficients. Then the modes with an outstanding frequency behavior are clustered with Hotelling\u2019s. Those modes are named as coherent modes and this selection process ensures that they are the only ones that present relevant frequency dynamics.\n\nIn the particular case of the Windsor body, less than ten percent of the modes are coherent, however, they account for nearly 60% of the energy. The remaining 40% is distributed along the more than 600 non-coherent modes and is the one that needs to be modelled by the closure. The clustered modes from each training angle are concatenated to form a common basis that preserves the coherent structures inside the studied yaw angle range. POD is applied to the concatenated modes to ensure that all vectors from the basis are orthonormal between themselves and that they are the optimal representation of the coherent modes in that range. This operation reduced the number of basis vectors from 142 to 90 without any additional information loss.\n\nProjecting any snapshot (regardless of whether it was included in the training set or not) into the common POD basis, filters the high-amplitude fluctuations. A transformer-encoder block with an easy-attention mechanism is used to learn the probability density function of the missing fluctuations depending on the reconstructed value from the POD common basis. Three different transformer architectures are trained in order to assess its effect on the recovered fluctuations. The main difference between the architectures is the change on the attention size. In the shallowest architecture it takes the value of. Then it is doubled twice to get an attention size ofand.\n\nThe larger the attention size, the more fluctuations from the training set are recovered. The accuracy of the prediction is quantified with the KL divergence between the transformer output and the original field. For the training set it reduces fromtowhen the attention size changes fromto. In the case of the test set, there is also an accuracy improvement when doubling the attention size up to, but then, the first signs of overfitting to the training data are seen with the deepest architecture. The evaluation of the closure atfor the architecture withis the only case in which the KL divergence has a negative value,. Note that a negative KL implies that the transformer has learnt larger fluctuation amplitudes from the training set that are not present in the test set.\n\nAdding the fluctuations field predicted by the transformer reduces the energy gap between the POD reconstruction and the original field. The architectures with larger attention size recover more energy than the shallower transformers. For instance, in the training set, the most likely energy value after adding the closure withis of, and it rises tofor. In this case, since the fluctuations from the training set are larger than the ones of the test set, the overfitting observed when comparing the probability density function leads to an overshoot in the energy prediction. The evaluation of the test set withyieldsbut the most likely energy value in the original flow is.\n\nAdding these fluctuations also leads to an improvement on the prediction of the root mean square value of the velocity fluctuations. The reconstruction from the common POD basis is able to capture the distribution of all local maxima and minima, but it falls short when matching the correct value. Then, the closure model helps to recover the missing fluctuations in the correct part of the domain. Once again, an increase on the attention size leads to a better closure of the offset. The evaluation of the deepest architecture atis the only case in which the rms prediction is larger than the original flow value. This is related to the energy overshoot discussed in the previous paragraph.\n\nFinally, this manuscript proves that the energy added via the predicted fluctuations also reduces the error in the instantaneous flow field prediction. This is particularly true for the architecture with. The temporal mean of the energy prediction error is reduced on all angles from more than theto less than the. Moreover, the KL divergence between the velocity distribution reconstructed by POD and the one of the original field is consistently larger than, but the closure reduces it to less than.\n\n[Acknowledgements]The authors acknowledge the Barcelona Supercomputing Center for the usage of MareNostrum 5 during the development of this manuscript. The authors also acknowledge the insightful conversations with Ferm\u00edn Mallor during the development of the initial ideas giving place to this work.\n\n[Funding]The research leading to this work has been partially funded by the project TIFON with reference PLEC2023-010251/ AEI/10.13039/501100011033. B. Eiximeno\u2019s work was funded by a contract from the Subprograma de Ayudas Predoctorales given by the Ministerio de Ciencia e Innovaci\u00f3n (PRE2021-096927). Oriol Lehmkuhl has been partially supported by a Ramon y Cajal postdoctoral contract (Ref: RYC2018-025949-I). The authors acknowledge the support of Departament de Recerca i Universitats de la Generalitat de Catalunya to the Research Group Large-scale Computational Fluid Dynamics (Code: 2021 SGR 00902) and the Turbulence and Aerodynamics Research Group (Code: 2021 SGR 01051). We also acknowledge the Barcelona Supercomputing Center for awarding us access to the MareNostrum IV machine based in Barcelona, Spain.\n\nMarcial Sanchis-Agudo and Ricardo Vinuesa would like to acknowledge the support from Marie Sklodowska-Curie Actions project MODELAIR, funded by the European Commission under the Horizon Europe program through grant agreement number 101072559.\n\n[Declaration of interests]The authors report no conflict of interest.\n\n[Data availability statement]The data that support the findings of this study is available upon request. See JFM\u2019sresearch transparency policyfor more information\n\n[Author ORCIDs]B. Eiximeno, https://orcid.org/0000-0001-7018-6371; M. Sanchis-Agudo, https://orcid.org/0009-0000-1194-7900; A. Mir\u00f3, https://orcid.org/0000-0002-2772-6050; I. Rodriguez, https://orcid.org/0000-0002-3749-277X; R. Vinuesa, https://orcid.org/0000-0001-6570-5499; O. Lehmkuhl, https://orcid.org/0000-0002-2670-1871\n\n[Author contributions]B. E.:Writing \u2013 review & editing, Writing \u2013 original draft, Visualization, Validation, Software, Methodology, Investigation, Formal analysis, Data curation, Conceptualization.M. S-A.:Writing \u2013 review & editing, Writing \u2013 original draft, Software, Methodology, Investigation, Conceptualization.A. M.:Writing \u2013 review & editing, Supervision, Software.I. R.:Writing \u2013 review & editing, Supervision, Methodology.R. V:Writing \u2013 review & editing, Supervision, Resources, Project administration, Funding acquisition.O. L.:Writing \u2013 review & editing, Validation, Supervision, Software, Resources, Project administration, Methodology, Investigation, Funding acquisition.\n\nSECTION: Appendix AAccuracy of the closure on the training angles\n\nThis appendix presents the comparison of the root mean square value of the streamwise velocity fluctuations for the angles of. Those angles were also included in the common basis as the case of. Moreover, the error of their reconstruction was also included in the dataset used for the training of the transformer.Figure\u00a019complementsFigure\u00a014andFigure\u00a020complementsFigure\u00a015as in those figures only the case atwas used to illustrate the effect of the closure on the cases used during training.\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.04239v1_content.txt"}, {"title": "Physics-informed Deep Learning for Muscle Force Prediction with\n  Unlabeled sEMG Signals", "authors": ["Shuhao Ma", "Jie Zhang", "Chaoyang Shi", "Pei Di", "Ian D. Robertson", "Zhi-Qiang Zhang"], "published_date": "2024-12-05T14:47:38Z", "summary": "Computational biomechanical analysis plays a pivotal role in understanding\nand improving human movements and physical functions. Although physics-based\nmodeling methods can interpret the dynamic interaction between the neural drive\nto muscle dynamics and joint kinematics, they suffer from high computational\nlatency. In recent years, data-driven methods have emerged as a promising\nalternative due to their fast execution speed, but label information is still\nrequired during training, which is not easy to acquire in practice. To tackle\nthese issues, this paper presents a novel physics-informed deep learning method\nto predict muscle forces without any label information during model training.\nIn addition, the proposed method could also identify personalized muscle-tendon\nparameters. To achieve this, the Hill muscle model-based forward dynamics is\nembedded into the deep neural network as the additional loss to further\nregulate the behavior of the deep neural network. Experimental validations on\nthe wrist joint from six healthy subjects are performed, and a fully connected\nneural network (FNN) is selected to implement the proposed method. The\npredicted results of muscle forces show comparable or even lower root mean\nsquare error (RMSE) and higher coefficient of determination compared with\nbaseline methods, which have to use the labeled surface electromyography (sEMG)\nsignals, and it can also identify muscle-tendon parameters accurately,\ndemonstrating the effectiveness of the proposed physics-informed deep learning\nmethod.", "arxiv_id": "2412.04213v1", "html_link": "https://arxiv.org/html/2412.04213v1", "search_term": "ti:\"deep learning\"", "html_content": "SECTION: Physics-informed Deep Learning for Muscle Force Prediction with Unlabeled sEMG Signals\u2020\u2020thanks:This work was supported in part by the UKRI Horizon Europe Guarantee under Grant EP/Y027930/1, in part by the Royal Society under Grant IEC/NSF/211360, in part by the EU Marie Curie Individual Fellowship under Grant 101023097, and in part by the China Scholarship Council (CSC) under Grant 202208320117. (Corresponding author: Zhi-Qiang Zhang)\u2020\u2020thanks:The first two authors contributed equally to this work.\u2020\u2020thanks:Shuhao Ma, Ian D. Robertson, and Zhi-Qiang Zhang are with the School of Electronic and Electrical Engineering, University of Leeds, Leeds LS2 9JT, U.K. (e-mail: elsma@leeds.ac.uk; i.d.robertson@leeds.ac.uk; z.zhang3@leeds.ac.uk).\u2020\u2020thanks:Jie Zhang is with the Centre for Wireless Innovation, Queen\u2019s University Belfast, Belfast BT3 9DT, U.K. (e-mail: jie.zhang@qub.ac.uk).\u2020\u2020thanks:Chaoyang Shi is with the Key Laboratory of Mechanism Theory and Equipment Design of Ministry of Education, School of Mechanical Engineering, Tianjin University, Tianjin 300072, China, and also with the International Institute for Innovative Design and Intelligent Manufacturing of Tianjin University in Zhejiang, Shaoxing 312000, China (e-mail: chaoyang.shi@tju.edu.cn).\u2020\u2020thanks:Pei Di is with Rex Bionics Ltd, PO Box 316-063\nAuckland 0760, New Zealand (e-mail: pei.di@imaxhealth.com).\n\nComputational biomechanical analysis plays a pivotal role in understanding and improving human movements and physical functions. Although physics-based modeling methods can interpret the dynamic interaction between the neural drive to muscle dynamics and joint kinematics, they suffer from high computational latency. In recent years, data-driven methods have emerged as a promising alternative due to their fast execution speed, but label information is still required during training, which is not easy to acquire in practice. To tackle these issues, this paper presents a novel physics-informed deep learning method to predict muscle forces without any label information during model training. In addition, the proposed method could also identify personalized muscle-tendon parameters. To achieve this, the Hill muscle model-based forward dynamics is embedded into the deep neural network as the additional loss to further regulate the behavior of the deep neural network. Experimental validations on the wrist joint from six healthy subjects are performed, and a fully connected neural network (FNN) is selected to implement the proposed method. The predicted results of muscle forces show comparable or even lower root mean square error (RMSE) and higher coefficient of determination compared with baseline methods, which have to use the labeled surface electromyography (sEMG) signals, and it can also identify muscle-tendon parameters accurately, demonstrating the effectiveness of the proposed physics-informed deep learning method.\n\nSECTION: IIntroduction\n\nHuman movements need the coordinated actions of various muscle elements, thus accurate muscle force estimation could support promising applications in diverse domains, ranging from efficacious rehabilitation protocol design[1], optimizing motion control[2,3], to enhancing clinical decision-making[4,5,6]and the performance of athletes[7,8]. The majority of muscle force estimation methods are based on physics-based modeling techniques. For instance, inverse dynamics techniques have been validated to generate reasonable estimations of muscle forces and muscular activation patterns usually based on static optimization[9,10,11,12,13]. The static optimization could find the set of muscle forces by minimizing the physiological criterion, such as muscle activation, volume-scaled activation, forces, stresses, metabolic energy or joint contact forces. However, it is challenging to provide the biologically consistent rationale for the selection of any objective function[14,15], due to the lack of knowledge about the method used by the central nervous system[16]. Furthermore, physics-based modeling methods also suffer from high computational latency, especially in complex modeling scenarios[17,18].\n\nTo address the time-consuming issue of physics-based methods, data-driven methods have been investigated to establish relationships between the movement variables and neuromuscular status, such as from electromyography (EMG) signals to muscle forces, in the past few years[19,20,21,22]. Although the training of deep neural networks may be lengthy, as the inference only involves a relatively simple forward pass through the network, it is computationally inexpensive and thus very quick.\nFor instance, Hua et al.[23]proposed a linear regression (LR) and long short-term memory (LSTM)-integrated method (LR-LSTM) to predict the muscle force under the isometric contraction state. Tang et al.[24]developed a modified framework to accurately predict muscle forces based on encoder-decoder networks.\nMoreover, Lu et al.[25]designed an integrated deep learning framework that combined a convolutional neural network (CNN) and a bidirectional LSTM (BiLSTM), complemented by an attention mechanism, for elbow flexion force estimation.\nHowever, all these models are established without explicit physical modeling of the underlying neuromechanical processes, and these conventional \u201cblack-box\u201d tools do not consider the physical significance underlying the modeling process[26,27].\n\nIn recent years, the integration of physics-based modeling and data-driven modeling has emerged as an effective strategy to overcome the limitations of these two methods, such as deep energy method-based deep neural network[28], deep Ritz method[29], physics-informed deep neural operator networks[30], and thermodynamics-informed neural network[31], etc.\nIn musculoskeletal (MSK) modeling, some existing works also investigate the integration of physics domain knowledge and data-driven modeling. Specifically, Zhang et al.[32]proposed a physics-informed deep learning framework for muscle forces and joint kinematics prediction, in which the equation of motion was embedded into the loss function as the soft constraints to penalize and regularize the deep neural network training. They also designed a physics-informed deep transfer learning framework to strengthen the performance of the personalized MSK modeling[33]. Taneja et al.[34]designed a novel physics-informed parameter identification neural network for simultaneously predicting motion and identifying parameters of MSK systems. They also developed a multi-resolution physics-informed recurrent neural network to further enhance motion prediction and parameter identification[35]. Shi et al.[36]developed a physics-informed low-shot learning approach based on generative adversarial network for muscle forces and joint kinematics prediction, which first integrated the Lagrange\u2019s equation of motion into the generative model to restrain the structured decoding of discriminative features, and a physics-informed policy gradient was then proposed to enhance the adversarial learning efficiency by rewarding the consistent physical representation of extrapolated estimations and physical references. Although the aforementioned physics-informed data-driven methods have achieved great progress for MSK modeling enhancement, there are still two main challenging issues: 1) Labeled data are required for model training[32,33,36], 2) For muscle force prediction,[34]and[35]need to reprocess the network\u2019s output in conjunction with the MSK dynamics, making the running latency far over the maximum 75 ms considered optimal real-time biofeedback. Therefore, it is urgent to design a novel physics-informed neural network framework, that does not need to acquire a large amount and sufficient labeled data for deep neural network training, and can still work well in real-time application scenarios.\n\nIn this paper, a novel physics-informed deep learning method is presented to predict muscle forces using unlabeled surface EMG (sEMG) data. Additionally, the proposed method could also identify muscle-tendon parameters of the Hill muscle model.\nIn the proposed method, a fully connected neural network (FNN) is utilized to implement the designed physics-informed deep learning framework, and the Hill muscle model is embedded into FNN as the additional loss component to further penalize and regularize the behavior of FNN.\nTo validate the proposed method, a self-collected dataset consisting of six healthy subjects performing wrist flexion/extension motion is used in the experiments. According to the experimental results, the proposed method with unlabeled sEMG data shows comparable and even better performance compared with selected machine learning and deep learning methods, which have to use labeled sEMG data.\n\nThe remainder of this paper is organized as follows: The proposed physics-informed deep learning method is detailed in SectionII, including the main framework, the network architecture and training strategy, the loss function, and the incorporation of Hill-muscle-based forward dynamics. Dataset and experimental settings are described in SectionIII. Experimental results are reported in SectionIV, and discussions are presented in SectionV. Finally, conclusions are given in SectionVI.\n\nSECTION: IIMethods\n\nIn this section, we first describe the details of the proposed method, in the context of muscle force prediction and muscle-tendon parameters identification from\nunlabeled sEMG signals, including the main framework, the network architecture and training, the loss function as well as the incorporation of Hill-muscle-based forward dynamics.\n\nSECTION: II-AMain Framework\n\nFig.1shows the main framework of the proposed method, in the context of muscle forces prediction and muscle-related physiological parameters identification from unlabeled sEMG signals. Specifically, in the neural network surrogate, inputs to the-parameterized deep neural network are sEMG measurements and the corresponding time, while outputs are the joint movementand muscle forces, whereis the total number of muscles at the joint of interest. A FNN is utilized to extract more discriminative features and build the relationship between the inputs and outputs. Different from conventional loss functions, the novel total loss consists of the data-based loss and physics-informed losses. The data-based loss is based on mean squared error (MSE), while the physics-informed losses are based on the-parameterized underlying Hill-muscle-based forward dynamics, whereandis the EMG-to-activation coefficient.\n\nSECTION: II-BFNN Architecture and Training\n\nWithout loss of generality, a FNN is utilized as the deep neural network to implement the proposed method, and it is composed of four fully connected (FC) blocks and one regression block. To be specific, each FC block has one linear layer, one ReLU layer and one dropout layer. The regression block consists of one ReLU layer and one dropout layer. The trainable parameters of FNN are obtained by minimizing the loss function (more details about the loss function refer to Section II-C). The training is performed using the Adam algorithm with an initial learning rate of 0.001, the batch size is 1, the maximum iteration is 1000, and the dropout rate is 0.3.\n\nSECTION: II-CLoss Function Design\n\nThe designed loss function of the proposed method includes the data-based loss, and physics-informed lossesand, which can be represented as\n\nwhereis the MSE of the actual joint angles and predicted joint angles,represents the Hill-muscle-based forward dynamics constraint,is an implicit relationship between muscle forces predicted by the neural network and calculated by the embedded Hill muscle model.\n\nThe MSE of ground truths of the joint angle and the joint angle predicted by FNN is\n\nwhereis as the ground truth of the joint angle andis the predicted joint angle of FNN with the trainable parametersat time.\n\nreflects underlying relationships among the muscle force and kinematics in human motion, which can be written as\n\nwhereandare the mass matrix, the Centrifugal and Coriolis force, and the gravity.andare the predicted joint angular velocity and joint angular acceleration.represents the joint torque, which is calculated by the summation of the product of the moment arm and muscle-tendon force:\n\nwhereis the number of muscles involved,is the moment arm of theth muscle which can be calculated using the polynomial equation and the scale coefficient against joint angle[37],is the estimated muscle force by the Hill muscle model with muscle-tendon parameters(Additional details about the calculation of the muscle forceare located in SectionII-D).\n\nThere is also an implicit relationship between the muscle forcespredicted by FNN and the muscle forcecalculated by the Hill muscle model. Thus,is designed for estimating muscle forces by minimizing the difference betweenand, which can be written as\n\nTherefore, the optimal neural network parametersand the subject-specific physiological parameterscan be obtained by minimizing the composite loss function:\n\nSECTION: II-DHill Muscle Force Estimation\n\nFor theth muscle-tendon unit, its muscle-tendon parametersinclude the isometric muscle force, the optimal muscle length, the maximum contraction velocity, the tendon slack lengthand the optimal pennation angle,, and the EMG-to-activation coefficient.\n\nThe Hill-muscle-based forward dynamics model includes activation dynamics and contraction dynamics.\nActivation dynamics refer to the process of transforming pre-processed sEMG signalsinto muscle activation signals, which can be estimated by[38]\n\nMuscle forces will be determined, once muscle activation signalshave been obtained. Contraction dynamics used in this study are described by the rigid musculotendon model[39], in which the pennated muscle element, comprising a contractile element in parallel with a passive elastic element, is connected to an inextensible tendon element. Therefore, the muscle-tendon force can be calculated[40]:\n\nwhereandare the active force generated by the muscle contraction and the passive force generated by the muscle stretch, respectively.\nThe pennation angleis the angle between the orientation of the muscle fiber and tendon, and the pennation angle at the current muscle fiber lengthcan be calculated through Eq. (9).\nTo update the muscle length, the muscle\u2013tendon lengthis approximated by the higher-order polynomial with respect to the predicted joint angle, which is exported from OpenSim[41].is the tendon length, andis the contraction velocity which is defined as the time derivative of muscle fiber length.,andinterpret the force-length-velocity characteristics relating toand normalized muscle length.\n\nBefore the model training, all the physiological parameters included inneed to be initialized by linear scaling based on the initial values of the generic model from OpenSim. These parameters will be continuously updated in each iteration during the model training process.\n\nSECTION: IIIDataset and Experimental Settings\n\nIn this section, data collection and preprocessing are first detailed, physiological parameters used in this study, evaluation criteria and baseline methods are then presented, respectively.\n\nSECTION: III-AData Collection and Preprocessing\n\nAs approved by the MaPS and Engineering Joint Faculty Research Ethics Committee of the University of Leeds (MEEC18-002), this study involves the participation of six subjects who have all provided signed consent forms. We collected data on the subjects\u2019 weight and the length of their hands to calculate the moment of inertia of their hands.\n\nDuring the data collection process, participants were instructed to maintain a straight torso with their shoulder abducted at aangle and their elbow joints flexed at aangle. The continuous wrist flexion/extension motion was recorded using the VICON motion capture system, which tracked joint angles at a rate of 250 Hz using 16 reflective markers on the upper limb. In the meantime, sEMG signals were recorded by Avanti Sensors at a rate of 2000 Hz from the primary wrist muscles, including the Flexor Carpi Radialis (FCR), Flexor Carpi Ulnaris (FCU), Extensor Carpi Radialis Longus (ECRL), Extensor Carpi Radialis Brevis (ECRB), and Extensor Carpi Ulnaris (ECU). The sEMG signals and motion data were synchronized and resampled at a rate of 1000 Hz.\nEach participant completed two repetitive trials at different speeds with a three-minute break between the speed changes to prevent muscle fatigue[42].\n\nThe collected sEMG signals underwent a series of processing steps, which included band-pass filtering (20 Hz to 450 Hz), full-wave rectification, and low-pass filtering (6 Hz). Subsequently, these signals were normalized based on the maximum voluntary contraction recorded prior to the experiments, resulting in enveloped sEMG signals. Each trial involving wrist movement included data on time, sEMG signals, and wrist joint angles. The muscle forces calculated by the computed muscle control (CMC) tool from OpenSim were used as ground truths in the experiments.\n\nSECTION: III-BInitialization of Physiological Parameters\n\nAmong the physiological parameters of the muscle-tendon units involved, we choose the maximum isometric muscle forceand the optimal muscle fiber lengthfor the identification. The nonlinear shape factorin the activation dynamics also needs to be identified.\nOther physiological parameters are obtained by linear scaling based on the initial values of the generic model from OpenSim. TableIshows the details of the initialization of all the physiological parameters of a specific subject as an example. Since there may be differences in terms of magnitude and scale between each parameter due to their different physiological natures, it is necessary to normalize them before training.\n\nSECTION: III-CEvaluation Criteria\n\nIn the experiments, root mean square error (RMSE) and coefficient of determinationare considered as the evaluation criteria to quantify the performance of the proposed method. RMSE is\n\nwhereis the number of samples,andindicate the ground truth and the predicted value at time, respectively.\n\ncould be calculated by\n\nwheredenotes the mean value of all the samples.\n\nSECTION: III-DBaseline Methods\n\nTo verify the effectiveness of the proposed method, we select LSTM, gated recurrent unit (GRU), CNN, FNN, support vector regression (SVR) and extreme learning machine (ELM) as baseline methods in the experiments. Specifically, the hidden dimensional of LSTM and GRU is 64, and the number of layers is 2, and the batch size of them is 8. CNN has convolutional layers and one FC layer. For each convolutional layer, the kernel size, stride, and padding number are 3, 1 and 3, respectively. The Adam optimizer is employed for CNN training, the batch size is set as 8. FNN has four FC blocks and two regression blocks but without the physics-informed component. Adam optimizer is employed for FNN training, the batch size is set as 1, and the maximum iteration is set as 1000. The radial basis function (RBF) is selected as the kernel function of SVR, and the parameter, which controls the tolerance of the training samples, is set as 100, and the kernel function parameters, which controls the range of the kernel function influence, is set as 1. ELM is a kind of single hidden layer feed-forward neural network with randomly generated hidden layer parameters, its hidden node number is 64 and the Sigmoid function is utilized as the activation function.\n\nSECTION: IVResults\n\nIn this section, we evaluate the performance of the proposed method using the self-collected dataset.\nThe convergence of loss terms is first illustrated, and the parameter identification is then demonstrated. Next, the overall comparisons depict the outcomes of both the proposed method and baseline methods. The robustness and generalization of the proposed method are also investigated, including the performance in the intrasession scenario, effects of network architectures and parameters, and training data number.\nThe proposed method and all the baseline methods are carried out under the framework of PyTorch, they are implemented on a laptop with a GeForce RTX 3070 Ti graphics card and 32 GB RAM.\n\nSECTION: IV-ADemonstration of Loss Function Convergence\n\nFig.2shows the convergence of different loss terms.\nAccording to Fig.2, we can observe that despite the differences in the final convergence values, these four loss terms demonstrate remarkably consistent convergence trends throughout the entirety of the training process. Specifically, all these loss terms could converge after about 800 iterations and finally converge with fast speeds, indicating the effectiveness of the proposed loss function.\nFurthermore, the total loss, as well asand, exhibit a smooth and stable convergence pattern throughout the training period.\nIn contrast, the MSE lossshows rapid convergence within the initial 200 epochs, followed by slight oscillations. This oscillation could be attributed to the relatively small absolute magnitude of.\n\nSECTION: IV-BEvaluation of Physiological Parameter Identification\n\nThe subject-specific physiological parameters are identified during the training of the proposed method. TableIIpresents the estimation and physiological range of the parameters of a specific subject as an example.\nPhysiological ranges of the parameters are chosen according to[43]. The ranges of the maximum isometric forceare set asof the initial guess, while the ranges of the optimal muscle fiber lengthare set asof the initial guess (Details of the initial guesses of these physiological parameters refer to TableI). The identified physiological parameters by the proposed method are all within the physiological range and possess physiological consistency. The identified muscle activation dynamics parameter A is -2.29, which is physiologically acceptable in the range of -3 to 0.01.\n\nFig.3demonstrates the evolution of the identified physiological parameters during the training of the proposed method. In Fig.3, the blue solid line illustrates the variation process of the parameters and the black dashed line indicates the estimated value by the proposed method which is the final convergent value of evolution. According to TableIIand Fig.3, the identified physiological parameters are within the physiologically acceptable range, indicating that the muscle forces calculated by the personalized Hill muscle model embedded in the proposed method are reasonable, which would directly benefit the guidance of the muscle force prediction.\n\nSECTION: IV-COverall Comparison\n\nFor the prediction of muscle forces, the proposed method uses the unlabeled sEMG data in the training phase, while the baseline methods use the labeled sEMG data. Fig.4shows the representative results of the proposed method for the prediction of muscle forces FCR, FCU, ECRL, ECRB, and ECU. According to Fig.4, we can find the proposed method could predict the muscle forces well.\n\nDetailed comparisons of all the subjects between the proposed method and baseline methods are presented in Table III and Table IV. In the experiment, we use the data with the same flexion speed to train and test the proposed method and baseline methods. We randomly select 70% of the data for training, while the rest 30% for testing. The number of training data is 10500, and the number of testing data is 4500. According to Table III and Table IV, deep learning-based methods, including the proposed method, LSTM, GRU, CNN and FNN, achieve better-predicted performance than machine learning-based methods, i.e., SVR and ELM, as evidenced by smaller RMSEs and higherin most cases. Because these deep learning-based methods could automatically extract high-level features from the collected data. Furthermore, the proposed method could achieve comparable performance with LSTM and GRU in some situations with unlabeled data, and the performance of the proposed method is better than that of FNN, which indicates the effectiveness of the designed loss function.\n\nFig.5shows the average RMSEs of muscle forces prediction of the proposed method and baseline methods. The proposed method achieves an overall performance similar to that of LSTM, GRU, CNN and FNN without direct reliance on actual muscle force labels.\nIn the training process, FNN used in the proposed method is not only trained based on the MSE loss but also enhanced by the physics-informed losses. The embedded physics laws provide the potential relationships between the output variables as learnable features for the training.\n\nTableVdetails the training time of deep learning-based methods, including GRU, LSTM, CNN, FNN and the proposed method. Accordingly, for all the methods, the training time is less with the increase of the batch size, and the proposed method has the longest training time compared to other baseline methods. This is because the proposed method is developed under the PINN framework, it not only involves the minimization of the MSE of FNN but also the regularization of physics-derived terms.\n\nSECTION: IV-DEvaluation of Intrasession Scenario\n\nThe performance of the proposed method in the intrasession scenario is also demonstrated to validate its robustness.\nFor each subject, we train the proposed method and baseline methods with the data of one flexion speed and then test them using the data of another flexion speed. In the experiment, we only demonstrate the comparison results of the proposed method and deep learning methods in Fig.6to make these results clearer.\nAccording to Fig.6, the proposed method demonstrates exceptional performance in datasets with different distributions, but the predicted results of some baseline methods are degraded. In particular, concerning the predicted results of muscle forces of ECRL and ECU, the predicted results yielded by the proposed method demonstrate a notably enhanced congruence with the underlying ground truth.\nLSTM, GRU, and CNN demonstrate the ability of motion pattern recognition since their muscle force prediction curves are generally consistent with the trend of ground truth.\nAdditionally, these methods exhibit the proficiency of dynamical tracking in part of the predicted results but the error remains in other predicted results, especially when it comes to capturing peak and trough values, noticeable discrepancies can be observed in the predicted values, which reflects the limitation of the stability.\nSpecifically, it demonstrates strong performance in the prediction of FCR, FCU, and ECRL, while it still exhibits significant discrepancies in the prediction of the ECRB and ECU.\nThe proposed method manifests a discernible capability to predict muscle forces on data characterized by the diverse distribution without label information.\n\nSECTION: IV-EEffects of Network Architectures\n\nTo investigate the effects of network architectures on performance, we implement the proposed method with different numbers of FC blocks. TableVIlists the detailed comparison results, we can find the proposed method could achieve the best performance with four FC blocks. Although the increase in the number of FC blocks would help extract more representative features, the proposed method may be overfitting when we continue to add FC blocks, which degrades its performance.\n\nSECTION: IV-FEffects of Network Parameters\n\nWe also consider the effects of network parameters on the performance, including batch size, learning rate and type of activation function. TableVIIshows R2of the proposed method with different learning rates, it seems that its R2is without obvious fluctuations. TableVIIIlists R2of the proposed method with different types of activation functions, we can find when ReLU is selected as the activation function, the proposed method has the best performance. TableIXshows the effects of different batch sizes. When the batch size is 1, the proposed method achieves better performance, because the network could learn the representations better.\n\nSECTION: IV-GEffects of Training Data Number\n\nTableXshows the experimental results of the proposed method with a different number of training data. When the number of training data is more than 10500, the proposed method achieves satisfactory performance with little fluctuations.\nIncreasing the training data beyond 10500 samples does not significantly enhance the performance of the proposed method, as evidenced by the minimal improvements seen with 14500 and 17000 samples. Such findings highlight the importance of a balanced approach to data collection and model training, emphasizing data quality and representativeness over sheer quantity.\n\nSECTION: VDiscussion\n\nIn this section, we discuss the generalization of the proposed method, and potential ways to further enhance its performance from various aspects.\n\nIn this paper, we only use muscle forces prediction of wrist flexion/extension as an example to demonstrate the feasibility and effectiveness of the proposed method. Actually, the proposed method can also be generalized to other joints. TableXI, TableXII, Fig.7and Fig.8show the details of physiological parameter identification and muscle forces prediction (including biceps femoris short head (BFS) and rectus femoris (RF)) of knee flexion/extension. To be specific, TableXIand Fig.7show the results of the identified physiological parameters, we can find all these physiological parameters are within the physiologically acceptable range. Additionally, TableXIIand Fig.8detail the predicted results of BFS and RF. Accordingly, the proposed method can fit the ground truth curve well and obtain comparable predictions compared with FNN even without any label information.\n\nDuring the implementation of the proposed method, we partially simplify the MSK forward dynamics model by reducing the number of individualized physiological parameters. Only the maximum isometric muscle force and the optimal fiber length are considered to be identified, and all the other physiological parameters are directly derived from the scaled wrist model. Moreover, five primary muscles have been selected as the key actuators for wrist flexion/extension, but these muscle-tendon units may also affect other degrees of freedom in wrist movements. In the future, we will try to relax these simplifications and assumptions by considering more physiological parameters and physics laws to obtain a more physiologically accurate representation of muscle tissues with connective tissues and muscle fibers, making it more feasible in practical and clinical applications. The computational time of the proposed method is longer than baseline methods because it is developed under the physics-informed neural network framework, which not only involves the minimization of the MSE of FNN but also the regularization of physics-informed terms during the network training. In the future, we will design a distributed framework for the proposed method to accelerate its training, and also consider pre-training an initial model with subject-specific data and then updating the model with other subjects\u2019 data, which can simultaneously reduce the training time and enhance the generalization.\n\nSECTION: VIConclusion\n\nThis paper presents a novel physics-informed deep-learning method mainly for muscle forces estimation with unlabeled sEMG data, and the proposed method could simultaneously identify parameters of the Hill muscle model. Specifically, the proposed method uses the MSK forward dynamics as the residual loss for the identification of personalized physiological parameters and another residual constraint based on the muscle contraction dynamics for the estimation of muscle forces without data labels. Comprehensive experiments indicate the feasibility of the proposed method.\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.04213v1_content.txt"}, {"title": "Fixed-Mean Gaussian Processes for Post-hoc Bayesian Deep Learning", "authors": ["Luis A. Ortega", "Sim\u00f3n Rodr\u00edguez-Santana", "Daniel Hern\u00e1ndez-Lobato"], "published_date": "2024-12-05T14:17:16Z", "summary": "Recently, there has been an increasing interest in performing post-hoc\nuncertainty estimation about the predictions of pre-trained deep neural\nnetworks (DNNs). Given a pre-trained DNN via back-propagation, these methods\nenhance the original network by adding output confidence measures, such as\nerror bars, without compromising its initial accuracy. In this context, we\nintroduce a novel family of sparse variational Gaussian processes (GPs), where\nthe posterior mean is fixed to any continuous function when using a universal\nkernel. Specifically, we fix the mean of this GP to the output of the\npre-trained DNN, allowing our approach to effectively fit the GP's predictive\nvariances to estimate the DNN prediction uncertainty. Our approach leverages\nvariational inference (VI) for efficient stochastic optimization, with training\ncosts that remain independent of the number of training points, scaling\nefficiently to large datasets such as ImageNet. The proposed method, called\nfixed mean GP (FMGP), is architecture-agnostic, relying solely on the\npre-trained model's outputs to adjust the predictive variances. Experimental\nresults demonstrate that FMGP improves both uncertainty estimation and\ncomputational efficiency when compared to state-of-the-art methods.", "arxiv_id": "2412.04177v1", "html_link": "https://arxiv.org/html/2412.04177v1", "search_term": "ti:\"deep learning\"", "html_content": "SECTION: Fixed-Mean Gaussian Processes forPost-hocBayesian Deep Learning\n\nRecently, there has been an increasing interest in performingpost-hocuncertainty estimation about the predictions of pre-trained deep neural networks (DNNs).\nGiven a pre-trained DNN via back-propagation, these methods enhance the original network by adding output confidence measures, such as error bars, without compromising its initial accuracy.\nIn this context, we introduce a novel family of sparse variational Gaussian processes (GPs), where the posterior mean is fixed to any continuous function when using a universal kernel.\nSpecifically, we fix the mean of this GP to the output of the pre-trained DNN, allowing our approach to effectively fit the GP\u2019s predictive variances to estimate the\nDNN prediction uncertainty. Our approach leverages variational inference (VI) for efficient stochastic optimization, with training costs that remain independent of the number of training points, scaling efficiently to large datasets such as ImageNet.\nThe proposed method, called fixed mean GP (FMGP), is architecture-agnostic, relying solely on the pre-trained model\u2019s outputs to adjust the predictive variances.\nExperimental results demonstrate that FMGP improves both uncertainty estimation and computational efficiency when compared to state-of-the-art methods.\n\nSECTION: IIntroduction\n\nOver the last years, deep neural networks (DNNs) have become thede-factosolution for a range of pattern recognition problems due to their ability to model deterministic connections and obtain state-of-the-art generalization performance[18]. However, DNNs suffer from significant disadvantages such as poorly calibrated probabilistic forecasts[17]and poor reasoning ability in scenarios demanding model uncertainty[5]. These issues are critical in risk-sensitive situations, s.a. autonomous driving[23]or healthcare[28].\n\nBayesian neural networks (BNNs) have successfully addressed the aforementioned issues in small-scale problems[34,39,16]. However, employing these models in practical scenarios remains challenging, as they typically involve high-dimensional, multi-modal posterior distributions over the space of neural network parameters. Moreover, due to the intractability of the calculations required, the exact posterior in large BNNs is generally approximated through diverse inference techniques, including variational inference (VI)[5], Markov chain Monte Carlo (MCMC)[7]and the Laplace approximation (LA)[35,43], among others. Nevertheless, empirical results often reveal a loss in predictive performance compared to simple DNNs trained via back-propagation[52].\n\nRecently, approaches based on the Linearized Laplace Approximation (LLA)[21],\nwhich applies the Laplace Approximation to a linearized version of the DNN w.r.t. its parameters, have gained significant popularity. Theirpost-hocnature ensures that model performance is preserved. Specifically, LLA methods enhance the output of the DNN with the corresponding error bars that quantify prediction uncertainty. Notwithstanding, they demand computing the Jacobian of the DNN w.r.t. the parameters for each input of the training dataset, which is computationally expensive. Consequently, LLA methods often lack the scalability needed to be applied to large models and/or datasets.\n\nIn this work, we introduce a new family of sparse Gaussian processes (GPs), calledfixed-mean Gaussian processes(FMGPs). This approach leverages the dual representation of GPs in the Reproducing Kernel Hilbert Space (RKHS) and the concept ofdecoupledinducing points for sparse GPs[8]. Specifically, by employing an universal kernel, our method enables fixing the posterior mean to any given continuous function. Then, it learns the corresponding posterior covariances of the model. As a result, the posterior mean can be set equal to the output of a high-performing DNN pre-trained via back-propagation. VI is then used to stochastically optimize the GP\u2019s predictive variances, providing useful error bars around the DNN\u2019s predictions. The proposed method, FMGP, effectivelyconverts any pre-trained DNN into a Bayesian DNNthrough function-space inference. The two main advantages of this approach are: (i) it is scalable to large neural networks, as it avoids requiring DNN Jacobians and is less affected by the number of DNN parameters, and (ii) it leverages function-space inference for improved uncertainty estimation.\n\nThe main benefit of using FMGP is itspost-hocnature, where the pre-trained model predictions are preserved as the posterior mean of the GP, ensuring high performance. Additionally, compared to otherpost-hocapproaches, the key advantage of FMGP is its architecture-agnostic design, as it relies solely on the DNN\u2019s outputs to accurately learn the predictive variances. This contrasts with methods such as LLA and its variants[11,42], or mean-field approaches based on VI and fine-tuning[12], which become computationally prohibitive for very large models due to requiring (i) high-dimensional DNN Jacobians or (ii) direct interaction with the parameters of the DNN. Further details on these methods and their differences with respect to FMGP are provided in SectionIV.\n\nFig.1illustrates the predictive distributions obtained by different methods on a toy 1-dimensional regression problem. We observe that the predictive distribution of FMGP closely resembles that of Hamiltonian Monte Carlo (HMC), which is considered the gold standard for this simple problem (note that HMC does not scale to large problems). In contrast, the predictive distributions of other methods from the literature either significantly underestimate or overestimate the predictive variance in some regions of the input space. Further details about this experiment are provided in SectionV-A.\n\nSECTION: Contributions\n\nWe (i) define FMGP as a family of GPs that can be used to perform uncertainty estimation in\npre-trained DNN without losing prediction performance; and (ii) show how VI can be used to stochastically fit predictive variances and optimize hyper-parameters. This results in apost-hocmethod that is independent of the network structure or architecture (e.g., it does not require computing DNN Jacobians). Furthermore, (iii) we show the scalability of FMGP at training and test time across multiple regression and classification problems, including ResNet[18]models, with millions of parameters, and the ImageNet dataset, featuring thousands of class labels and millions of data instances[46]. Finally, (iv) we illustrate the utility of FMGP in apractical datasetscenario using QM9[45]for feature prediction in the context of molecules.\n\nSECTION: IIBackground\n\nWe aim to infer an unknown function, based on noisy\nobservationsat known locations. Deep learning (DL) tackles this by defining a neural network architecture that corresponds to a set of functions, specified by a set of parameters. The underlying assumption is that, ifis large enough, some element inwill closely approximate. That is,s.t.. DL optimizesvia back-propagation, using the observed data to find. Nevertheless, despite its success in various tasks[49], DL methods generally lack proper output uncertainty estimation, often resulting in over-confident predictions in regions without training data, where the uncertainty aroundis expected to be larger.\n\nIn Bayesian inference, the observationsare related to the target function evaluationsthrough a likelihood function. In regression settings, where, the likelihood is often a homoscedastic Gaussian with variance. In classification, where, the likelihood is categorical with class probabilities given bye.g.a softmax activation function, meaningrepresents a set of multi-output functions, one per class label.\n\nBayesian neural networks (BNNs) follow a probabilistic framework[35], placing a prior over the network parametersand computing the Bayesian posteriorfor predictions. Due to the non-linear nature of DNNs, calculating the posterior analytically is intractable. Therefore, most methods rely on an approximate posterior, used to estimate predictions via Monte Carlo sampling, whereandrepresents the number of Monte Carlo samples. This allows effectively capturing the uncertainty in the model\u2019s predictions[4].\n\nIn this work, rather than following the Bayesian approach in the space of parameters, we take on afunction-spaceperspective. This involves placing a prior directly over the space of functionsand constructing an approximate posterior for the target functionto make predictions. These predictions can be computed exactly in regression settings using GP posteriors. In the case of classification problems, given an approximate GP posterior, they have to be approximated via Monte Carlo sampling.\n\nSECTION: II-AGaussian Processes\n\nGaussian processes (GPs) are statistically defined as an infinite collection of random variables such that any finite subset is jointly Gaussian. They are fully specified via mean and covariance functions. From this definition, GPs can be interpreted as distributions over the function space or, more precisely, over the set of evaluations of functions. Consider a function. We say thatfollows a GP defined by a mean functionand covariance (or kernel) function,i.e.,, if, for any finite set of input points, the set of function evaluationsfollows a multi-variate Gaussian distribution with meanand covariance matrix. That is,.\n\nSECTION: II-BDual formulation of Gaussian Processes in RKHS\n\nA Reproducing Kernel Hilbert Space (RKHS)is a Hilbert space of functions with the reproducing property, that is,such thatit verifies that, whereis the inner product on. By Moore\u2013Aronszajn theorem[2], ifis a positive definite kernel on, then, there exists a unique Hilbert space of functions onfor whichis a reproducing kernel. More precisely, letbe the linear span ofondefined as\n\nBy Moore\u2013Aronszajn theorem, the closure of, named asis a Hilbert space verifying the reproducing property with.\n\nAhas a dual representation as a Gaussian measure in a Banach space that contains the RKHS of its kernel function[20,8]. More precisely, consider a zero-mean GP prior withand the RKHS defined by the kernelassociated to the GP. For anyand a linear semi-definite positive\noperatorassociated to,i.e,, we can define a new GP with mean functionand kernel functiongiven by\n\nWe useas an abuse of notation to denote such Gaussian measure in the Banach space, withas the set of these measures:\n\nAs a result, there is a correspondence between GPsand Gaussian measuresin a Banach spacethat contains the samples of the GP and in whichis dense[20,9].\n\nThe zero-mean GP prioris obtained from the dual-formulation using. Furthermore, given a set\nof observationsfrom a regression task with Gaussian noise,\nthe GP posterior is obtained from the (posterior) Gaussian measurewhere:\n\nwithand.\n\nWith this construction, we aim to define a family of Gaussian measures in, whose corresponding GP verifies that. This means that the corresponding GP mean will match the output of the pre-trained neural network. Then, VI can be used to find an optimal Gaussian measure within such family.\n\nSECTION: II-CUniversal Kernels\n\nFollowing[37], we introduce the notion ofuniversal kernelsas kernel functions whose linear span can approximate any continuous function in a compact set. Given a kernel functionand its corresponding RKHS, assume that the kernel is continuous on. Letbe a fixed but arbitrary compact subset ofand, as usual, letdenote the space of all continuous real-valued functions fromtoequipped with infinity norm, which reduces to amaximum normin the compact set.\n\nThe space ofkernel sectionsis defined as, which consists of the set of all continuous functionswhich are limits of linear combinations ofunder the infinity norm.\n\nA kernel function is said to be universal if for any compact subsetof the input space, the kernel sectionis dense inwith the infinity norm. That is, for anyand any, there existssuch that.\n\nFrom the above definition, ifis universal,and, there exists a set ofscalar valuesand input space points, such that\n\nand hence,\n\nIntuitively, a universal kernel can approximate any continuous function in a compact set via linear combinations of kernel evaluations. As the approximation improves (i.e.decreases), the number of termsneeded in the linear combination increases.\n\nThe squared-exponential kernel with hyper-parameters, with, defined as\n\nis a universal kernel[37].\n\nSECTION: IIIFixed-Mean Gaussian Processes\n\nHere, we present a novel family of GPs,Fixed-Mean Gaussian Processes(FMGPs). This family of function-space distributions is defined using the dual formulation of sparse variational GPs,\nwhich are introduced next.\n\nSECTION: III-ASparse Variational Gaussian Processes\n\nSparse Variational GPs (SVGPs)[48]approximate the GP posterior using a GP parameterized byinducing points, with each, and associated process values. Specifically,\n\nwhere,andis fixed\nto the GP predictive distribution.\n\nFollowing[8], consider a restriction of the dual GP formulation introduced in SectionII-B,\nwhere the mean and covariance dual elements (and) must satisfy the linear structure:\n\nwhere,such thatand. This defines a family of Gaussian measuressuch that\n\nwhere we have omittedandfromnotation for simplicity as more sub-indexes will be used later.\n\nA SVGP withhas a dual representation inwhereand.\n\nConsequence of settingandin (10) and (11).\n\u220e\n\nBy definition, if, then, as in standard variational sparse GPs. However, in practice, and for scalability reasons,and. This leads to the issue of finding the measure inthat isclosestto. In this regard, variational inference (VI) can be used to minimize the KL divergence between Gaussian measures[8]and hence, to compute theoptimalvariational measure as:\n\nwherecan be computed in closed form\nwith,i.e., the GP prior. Namely,\n\nwith. After optimizing (13), one gets a Gaussian measurethat corresponds to the SVGP in[48]. See[8]for further details about this.\n\nSECTION: III-BDecoupled Basis\n\nIn[9], the authors propose to generalizeso thatandare defined using different sets of inducing points. Letandbe two sets of inducing points, for the mean and the variance respectively, of sizesand. The generalized dual representation is then defined as:\n\nThis decoupled parameterization is a clear generalization from standard SVGPs and cannot be obtained using the approach of[48]unless.\nThe decoupled space of Gaussian measures is now:\n\nwhere it is verified that. As shown in[8], VI can be\nused to find the optimalin this parametric family. That is,\n\nwhere the KL term, with, is:\n\nwithand.\n\nThe parameters for the mean of the variational distribution,i.e.,and, and the ones for the variance,i.e.,and, are separated in (18) and (19). Therefore, they can be independently optimized in practice.\n\nSECTION: III-CFixed-Mean Variational Family\n\nGiven a universal kernel, we now use the decoupled family of distributions to show that the mean function can befixedto any continuous function in compact subsets.\n\nLetbe any compact subset of the input space. If the kernel isuniversal, for any function, there exists, a set of inducing locations, and scalar valuessuch that\n\nverifies.\n\nDirect consequence of the definition of universal kernels applied to the dual formulation of SVGPs.\n\u220e\n\nAs a result, given an error rate, we can set the posterior mean of a decoupled GP toany continuous function in any compact set of the input space. More precisely, we can use the decoupled formulation of GPs tofixthe posterior mean to the outputof a given pre-trained DNN.\n\nFor any compact subset of the input space, continuous function, errorand universal kernel, the set of-mean Gaussian measuresis defined as\n\nwhereverifies.\n\nThus, for any, the corresponding GPverifies that\n\nWe will refer to this set of GPs as fixed-mean Gaussian processes (FMGPs).\n\nBy Proposition5, it is clear that for any, it is verified thatand its\ncorresponding set of FMGPs exists. Again, VI can be used to find the optimalfrom this parametric family.\nThat is,\n\nwhere the KL term is, setting:\n\nNote now, however, that onlyandneed to be optimized.\nWe refer to the method that solves (26) asfixed-mean Gaussian process(FMGP).\n\nParameterizingwithresults in\nan expression for the posterior covariances in (25) equivalent to the expression for the posterior\ncovariances in the SVGP[48]. Furthermore, (25) verifies that the posterior\ncovariances are less confident than the prior covariances[9].\n\nFig.2shows a set representation of the different families of Gaussian measures\nconsidered in this section. We observe that, in which there are different inducing\npoints for the mean and the covariances, is the largest family. This family includes\nboth, in which there is only a single set of inducing points for the mean and the covariances,\nand, in which the posterior mean is fixed to approximateinwith error at most. Note that there could potentially\nbe some overlap between the Gaussian measures inand in.\n\nSECTION: III-DApplication toPost-hocBayesian Deep Learning\n\nFMGP enables the conversion of DNN into approximate Bayesian models while\nmaintaining the DNN output as the predictive mean. The process is straightforward and can\nbe summarized as follows:\n\nGiven a pre-trained model, choose a parametric family of kernels that defines an RKHS and a family of Gaussian measures,e.g.squared exponential kernels.\n\nEnsure that there exists a compact setwhere inputs are expected. The parametric family of Gaussian measuresexists for any.\n\nInitialize a measure in this family,e.g.initializeusing K-Means[32]andas the identity matrix.\n\nPerform VI to optimize the variational measure (and), along with the kernel hyper-parametersand the noise variance, using (26). The predictive variance is computed as in (25), and, ifis large andsmall, the predictive meanapproximates the pre-trained model. Thus, in practice,can be replaced byin the computations.\n\nSECTION: III-ERegularization and Loss Function\n\nIn standard sparse GPs, tuning hyper-parameters involves balancing the fit of the mean\nto the training data versus reducing the model\u2019s predictive variance. However, FMGPs\nfix the predictive mean, which eliminates this trade-off. Thus, the kernel hyper-parametersonly adjust the predictive variance without affecting the mean. Consequently,\noptimizingby maximizing the VI ELBO in (26) can lead to undesirable\nsolutions where the predictive variance is set to zero. To address this, we introduce a regularization\ntechnique using an extra variational Gaussian measure. More precisely, we consider\nan extra auxiliary Gaussian measurethat shares\u2019s parameters\n(,and the kernel hyper-parameters)\nbut also incorporatesandas additional parameters for its predictive mean. This leads us to the loss function:\n\nwith.\nThis loss function implies that the predictive variances must account\nfor training data under two predictive means: the pre-trained one,, and the\none defined by. Additionally,cannot be adjusted solely\nto fit the variances, as it also affects\u2019s predictive mean.\n\nMoreover, the use of-divergences for approximate inference has been widely explored[19,6,50,44], with findings indicating that\nvalues ofenhance predictive mean estimation,\nwhileimprove predictive distributions, reflected in higher\ntest log-likelihood performance. Thus, instead of minimizing, our\nobjective is changed using a generalized view of VI[25]to minimize the-divergence betweenand,\nin an approximate way, for[29]. This can be\nachieved by changing the data-dependent term of the loss:\n\nwhere now the expectation is inside the logarithm function.\n\nMini-batch Optimization.The objective in (III-E) supports mini-batch optimization with a cost in:\n\nwhereis a mini-batch of points. The expectation can be computed in closed-form\nin regression. In classification, an approximation is available via the softmax method\nin[10].\n\nSECTION: III-FLimitations\n\nFMGP is limited by three factors:\n\nComputing the predictive distribution at each training iteration involves inverting,\nwith cubic cost in the number of inducing points. Therefore, FMGP cannot accommodate a very large number of inducing points.\nHowever, as shown in the experiments, this number can be set to a very low value, such as, even for classification tasks with a thousand classes.\n\nFMGP requires additional optimization steps compared to otherpost-hocapproximations,e.g.,[11]. However, these other methods often rely\non visitingevery training pointto compute specific updates. As a result, FMGP training can be faster in large datasets featuring millions of instances such as ImageNet.\n\nThe construction of FMGP requires choosing a (parametric) kernel. This is both an advantage,\nas it may better capture the underlying data patterns in the modelling process, and a disadvantage, as it may be difficult to efficiently use an effective kernel in some tasks, such as image classification.\n\nSECTION: IVRelated Work\n\nDue to itspost-hocnature, FMGP is highly related to the Linearized Laplace Approximation (LLA)\nfor DL. In[33], the Laplace Approximation (LA) was introduced by applying it to small DNNs. LA simply approximates the DNN posterior in parameter space with a Gaussian centered at its mode matching the posterior Hessian. LA can be made more scalable by considering a Generalized Gauss-Newton (GGN) approximation of the Hessian at the mode, which is equivalent to linearizing the DNN. When this linearization is used at prediction time, LA becomes apost-hocmethod known as LLA[43]. Moreover, LLA addresses the under-fitting issues associated with LA[27].\nDespite this, the GGN approximate Hessian of LLA is still intractable in real problems and has to be further approximated using,e.g., Kronecker factored (KFAC) approximations. Recently, many approaches have been developed to make LLA more scalable and accurate, trying to match LLA with the GGN approximate Hessian, including Nystr\u00f6n approximations[11], variational approaches[42,47]or sample-based approximations[1].\n\nAmong LLA methods, FMGP is most closely related to Variational LLA (VaLLA)[42]. VaLLA interprets the intractable LLA approximation as a GP[24,21], which is possible due to the DNN linearization. Then, it uses a VI sparse GP to approximate the posterior variances. If the Neural Tangent Kernel (NTK) is considered (which is simply given by the DNN Jacobian,i.e.), VaLLA can be recovered as a specific case of the FMGP formulation under certain hypotheses.\nThe key differences between FMGP and VaLLA are: (i) FMGP does not rely on the NTK, which allows it to make use of more suitable kernels for specific tasks. Furthermore, the NTK demands computing the Jacobian of the neural network at each iteration, which drastically increases prediction costs, making it impractical to apply VaLLA to large problems (e.g.ImageNet). Furthermore, as we will see in the experiments, the FMGP kernel flexibility allows both for enhanced predictive distributions as well as more efficiently computed predictions.\n(ii) FMGP does not rely on a LLA approximation. Therefore, LLA with the GGN Hessian approximation need not beoptimalunder this framework. (iii) The NTK kernel is not guaranteed to beuniversal, and VaLLA relies on the hypothesis that the DNN outputis in, which may not be the case. (iv) VaLLA does not consider a regularization technique to avoid over-fitting, requiring early-stopping and a validation set. In short, VaLLA is constructed by performing a variational approximation to the GP resulting from LLA. By contrast, FMGP uses VI to find the optimal measure within a set of Gaussian measures with a fixed mean.\n\nIn[11], the authors propose a Nystr\u00f6m approximation of the GGN Hessian approximation of LLA usingpoints chosen at random from the training set. The method, called ELLA, has cost. ELLA also requires computing the costly Jacobian vectors required in VaLLA, but does not need their gradients. Unlike VaLLA, the Nystr\u00f6m approximation needs to visit each instance in the training set. However, as stated in[11], ELLA suffers from over-fitting. Again, an early-stopping strategy using a validation set is proposed to alleviate it. In this case, ELLA only considers a subset of the training data. ELLA does not allow for hyper-parameter optimization, unlike VaLLA. The prior variancemust be tuned using grid search and a validation set, increasing the required training time significantly.\n\nSamples from LLA\u2019s corresponding GP posterior can be efficiently computed using stochastic optimization, without inverting the kernel matrix[30,1]. This approach avoids LLA\u2019scost. However, this method does not provide an estimate of the log-marginal likelihood for hyper-parameter optimization. Thus, in[1]it is proposed to use theEM-algorithmfor this task, where samples are generated (E-step) and hyper-parameters are optimized afterwards (M-step) iteratively. This significantly increases training cost, as generating a single sample is as expensive as fitting the original DNN on the full data. A limitation of this approach is that in[1]only classification problems are considered and there is empirical evidence showing that VaLLA is faster and gives better results.\n\nAnother GP-based approach for obtaining prediction uncertainty in the context of DNNs is the Spectral-normalized Neural Gaussian Process (SNGP)[31], which replaces the last layer of the DNN with a GP. SNGP allows to either (i) fine-tune a pre-trained DNN model, or (ii) train a full DNN model from scratch. We compare results with the former in our experiments. However, we have observed that replacing the last layer with a GP does not keep the predictive mean as the output of the pre-trained DNN and often results in a drop in prediction performance. This is also observed in[31].\n\nAnother simple option to transform a pre-trained DNN model to a Bayesian one is to consider a mean-field VI approximation of the DNN posterior where the means are initialized to the pre-trained optimal solution weights and kept fixed. This is known asmean-field VI fine-tuning[12]and, as demonstrated in our experiments, it can achieve good results in terms of both prediction performance and uncertainty estimation. However, this method demands full training of the variance of each weight, which can be very costly and may require several training epochs. Furthermore, this method provides no closed-form predictive distribution. It relies on generating Monte Carlo samples to make predictions. As a result, further approximations must be considered to reduce the training time, such asFlipout Trick[51]. Even though these techniques successfully reduce the training time, the required Monte Carlo samples significantly increase prediction time.\n\nSECTION: VExperiments\n\nWe compare our proposed method, FMGP, with other methods including: last-layer LLA with and without KFAC approximation, ELLA[11], VaLLA[42], a mean-field VI fine-tuning approach[12]and SNGP[31]. FMGP and VaLLA useinducing points, as in[42]. ELLA employsrandom points andrandom features as in[12]. All the timed experiments are executed on a Nvidia A100 graphic card. Finally, an implementation of FMGP is publicly available athttps://github.com/Ludvins/FixedMeanGaussianProcesses.\n\nSECTION: V-ASynthetic Experiment\n\nThe experiment in Fig.1illustrates the predictive distributions of commonly used Bayesian approaches on the synthetic 1-dimensional dataset from[22]. It compares the predictive distribution of FMGPs against other methods, including the pre-trained DNN with optimized Gaussian noise (MAP), the linearized Laplace approximation (LLA) with prior precision and Gaussian noise optimized to maximize the marginal likelihood estimate, mean-field VI (MFVI) fine-tuning of the pre-trained model with Gaussian noise optimized on the training data, a GP with a squared exponential kernel and hyper-parameters that maximize the marginal likelihood, and Hamiltonian Monte Carlo (HMC) using a uniform prior for the variance of both the Gaussian noise and the Gaussian prior over the DNN\u2019s weights.\n\nIn this simple problem, HMC\u2019s predictions serve as thegold standardfor assessing the predictive variances of other methods. Note, however, that HMC does not scale to large problems. Fig.1shows that MAP and MFVI tend to underestimate the predictive variance, while LLA tends to overestimate it by interpolating between data clusters. On the other hand, FMGP and the GP produce predictive variances comparable to those of HMC, with the GP yielding slightly larger variances.\n\nHowever, the GP\u2019s predictive mean does not align with the DNN output (given by the predictive mean of the MAP output) and suffers from a prior mean reversion problem, where the GP mean reverts to the prior mean between the second and third point clusters, which is expected to worsen the resulting predictive performance. Moreover, the GP does not scale well to large problems. By contrast, FMGP not only produces predictive variances similar to those of HMC but also retains the predictive mean equal to the DNN\u2019s output, which is expected to result in improved prediction accuracy.\n\nSECTION: V-BRegression Problems\n\nAs part of the experimental evaluation, we consider three different large regression datasets:\n\nTheYeardataset[3]withinstances andfeatures. The data is divided as: the firstinstances as train subset and the followingfor validation. The rest of instances are taken for the test set.\n\nTheUS flight delay (Airline)dataset[13]. Following[42], we use the firstinstances for training, the followinginstances for validation and the nextfor testing. Here,features are considered:month,day of the month,day of the week,plane age,air time,distance,arrival timeanddeparture time.\n\nTheTaxi dataset, with data recorded on January, 2023[41]. For this dataset,attributes are considered:time of day,day of week,day of month,month,PULocationID,DOLocationID,distanceandduration; while the predictive variable is theprice. Following[42], we filter trips shorter than 10 seconds and larger than 5 hours, resulting inmillion instances. The firstis used as train data, the nextas validation data, and the lastas testing data.\n\nIn all experiments, a pre-trained 3-layer DNN with 200 units withtanhactivations is employed, following[42].\nELLA is trained withoutearly-stoppingas over-fitting is not observed in these regression problems. Hyper-parameters are\nchosen using a grid search and the validation set. FMGP employs the squared-exponential kernel with hyper-parameters\ngiven by kernel amplitude and one length scale per input feature. MAP results are obtained by learning the optimal Gaussian\nnoise using a validation set. A last-layer Kronecker approximation is used for LLA.\n\nFig.3shows average results for each method over 5 different random seeds. We measure the quality of\nthe predictive distribution in terms of the negative log likelihood (NLL), the continuous\nranked probability score (CRPS)[15]and a centered quantile metric (CQM)[42].\nIntuitively, CRPS can be understood as a generalization of the mean absolute error to predictive distributions. CQM measures\nthedifferencebetween the models quantiles and the data quantiles under the same predictive mean, which is always the\ncase here for each method. CQM is like a generalization of expected calibration error for regression problems. It is defined as:\n\nwhere,andis the CDF of a Gaussian with meanand variance, specified by each model\u2019s predictive distribution.\n\nFig.3shows that FMGP performs best according to all three metrics (the lower the better), where the biggest\ndifference is obtained in terms of CQM. As a result, we can argue that FMGP provides better uncertainty estimates (in terms of NLL)\nand calibration (both in terms of CRPS and CQM) compared to state-of-the-art LLA variants in regression settings.\n\nSECTION: V-CCIFAR10 Dataset and ResNet Architectures\n\nWe perform experiments with various ResNets architectures[18]on the CIFAR10 dataset[26].\nTo facilitate reproducibility, the considered pre-trained models are publicly available and accessible athttps://github.com/chenyaofo/pytorch-cifar-models. The considered models are ResNet20 (parameters), ResNet32 (parameters), ResNet44 (parameters) and ResNet56 (parameters). Following[11]and[42], ELLA and VaLLA use as validation set a data-augmented subset oftraining points from the train set.\nThis validation set is obtained by performing random image crops of the training images of sizes in.\n\nIn multi-class classification problems, the kernel used in FMGP should model dependencies among the different DNN outputs, one per each class label. Therefore, we employ the following simple kernel in FMGP in that setting:\n\nwhich includes a p.s. matrixto model output dependencies,\na squared exponential kernel in the input space, and a linear kernel plus noise in the high-level features, that correspond to the output of the pre-trained model up to the second-to-last layer. The trainable hyper-parameters are the squared exponential amplitude and length scales of the RBF kernel (one per input feature), along with the matrix, parameterized by its Cholesky decomposition. This simple kernel gives good results in our experiments. More sophisticated kernels are possible, potentially leading to even better results. The inducing points are randomly assigned to a class label.\n\nFig.4shows the negative log-likelihood (NLL), expected calibration error (ECE), and Brier score of each method. Furthermore, we also report the out-of-distribution AUC of each method\nin a binary classification problem with the SVHN dataset as the out-of-distribution data[40]. In each method, we use predictive entropy as the threshold for classification between in and out-of-distribution. The training and evaluation times for each method are also reported. Recent work[38]shows how different uncertainty quantification metrics tend toclusterand the importance of measuring prediction uncertainty using as many as possible. Accuracy is not shown here as most methods barely change the pre-trained DNN accuracy. Notwithstanding, it is worth mentioning that SNGP tends to lower the accuracy of the model, as shown in[31], while MFVI tends to increase it slightly, as noticed in[11]and[42].\n\nFig.4shows that FMGP, MFVI, VaLLA and ELLA provide the highest performance in terms\nof NLL and Brier scores (the lower the better). However, in terms of ECE (also the lower the better),\nSNGP, VaLLA and FMGP provide better-calibrated uncertainties. As a result, FMGP and VaLLA seem to\nprovide better uncertainty quantification with better-calibrated predictive distributions.\nHowever, for out-of-distribution detection, the best AUC is obtained by MFVI, ELLA, VaLLA and SNGP.\nFig.5shows histograms of the entropy of the predictive distribution of each\nmethod for each type of test data (in and out-of distribution). We believe the poor results of FMGP\nin this task are due to the kernel choice. More sophisticated kernels may improve FMGP\u2019s results in this setting as well.\n\nRegarding training time, Fig.4shows that last-layer LLA approaches are the fastest to train, with VaLLA being the slowest method. At prediction time, SNGP, last-layer LLA and FMGP are quite similar to the pre-trained model. By contrast, VaLLA, ELLA and MFVI take larger prediction times. In VaLLA and ELLA this is due to the computation of the Jacobians, while in MFVI this due to Monte-Carlo sampling. Since FMGP is agnostic of the pre-trained model architecture, it only uses the DNN\u2019s predictions. Therefore, we also pre-computed all the model outputs and used them directly when training FMGP and making predictions using this model. As a result, a second bar is shown for FMGP indicating the training and evaluation time when pre-computing the outputs for both training and evaluation sets. In such a setting, the speed-up of FMGP is approximatelyfor training time andfor evaluation time.\n\nRegarding predictive robustness, in Fig.6we show the NLL and ECE of each method on rotated images of the CIFAR10 test set, as in[42]. These results indicate that FMGP is the most robust method in terms of NLL, while it lies around the middle ground in terms of ECE. ELLA and VaLLA achieve the best results in this regard.\n\nSECTION: V-DImageNet Dataset and Extra ResNet Architectures\n\nWe perform experiments with more ResNets architectures[18]on the ImageNet 1k dataset[46]. This dataset hasdifferent classes and over 1 million data instances. As pre-trained models, we considered those from TorchVision[36]available athttps://pytorch.org/vision/main/models/resnet.html. Specifically, the considered models are ResNet18 (parameters), ResNet34 (parameters), ResNet50 (parameters), ResNet101 (parameters) and ResNet152 (parameters). Importantly, due to the size of the DNNs and dataset, many methods became infeasible in these experiments. Specifically, LLA cannot be used even with last-layer approximations due to memory limitations. Furthermore, Monte Carlo sampling for MFVI testing takes longer than 1 day for models larger than ResNet18. For this reason, MFVI is only tested on the ResNet18 architecture. SNGP is not evaluated as it requires a training time of several days on the smallest architecture.\n\nTableIshows the results obtained for each method on each ResNet architecture.\nThe best method is highlighted in red and the second-to-best method is highlighted in green. We observe that, overall, FMGP obtains the best performance (NLL and ECE) while remaining the second-to-best in terms of computational time, only behind the MAP solution for the bigger models. As an additional detail, ELLA\u2019s validation set is computed using the same data-augmentation strategy proposed in[11].\n\nOur results for ELLA are slightly different from those reported in[11]since ELLA\u2019s performance highly depends on the particular data augmentation performed to create the validation set. Despite using the same hyper-parameters for this step, using the current PyTorch versions leads to different results.\n\nSECTION: V-EProtein Feature Prediction Dataset\n\nQM9 is a dataset which provides quantum chemical properties (at DFT level) for a\nrelevant, consistent, and comprehensive chemical space of aroundsmall organic\nmolecules[45]. In this experiment, we train a small convolutional neural network with message passing following the Torch-Geometric[14]tutorial available athttps://github.com/pyg-team/pytorch_geometric/blob/master/examples/qm9_nn_conv.py.\nThe model is trained to make predictions on thedipole momenttarget.\n\nIn this regression experiment, the input space consists of molecules\u2019 graphs and not the usual tabular data considered in supervised learning. Therefore, in each method evaluated, we considered the model up to the last two linear functions to be a feature embedding of the graphs and assumed the data live in such embedding space. We used the firstdata instances for testing, the followingdata instances for validation, and the restinstances for training. Here, ELLA is also trained withoutearly-stoppingand the hyper-parameters are chosen using a grid search on the validation set. FMGP employs the squared-exponential kernel with hyper-parameters including the amplitude parameter and one length scale per dimension. MAP results are obtained by estimating the Gaussian noise on the validation set.\n\nThe results obtained are displayed in TableIIfor MAP, last-layer Kronecker LLA, ELLA and FMGP in terms of the negative log-likelihood (NLL) and CRPS. We report average results acrossrepetitions of the experiments. The best result is again highlighted in red and the second-best result in green. We observe that FMGP provides the best performance (the smaller the better) in terms of both NLL and CRPS among the considered methods.\n\nSECTION: VIConclusions\n\nIn this work, we have introduced a method called fixed-mean GP (FMGP). FMGP leverages a family of variational distributions derived from the dual formulation of sparse GPs. This family corresponds to GPs where the predictive mean is fixed to any continuous function when using a universal kernel. Specifically, we set the continuous function to be the output of a pre-trained DNN. In this case, FMGP becomes apost-hocmethod that, given a pre-trained DNN, outputs error bars estimating the confidence of the DNN in its predictions. FMGP is both easy and efficient to train.\n\nAs demonstrated in our experiments, FMGP excels at computing error bars for pre-trained DNNs with a large number of parameters, across a wide variety of performance metrics, on extensive datasets, handling millions of training instances, parameters, and thousands of output dimensions. Furthermore, FMGP is applicable to a broad range of problems, including regression and classification tasks, where stochastic optimization enables sub-linear training costs with respect to the number of training instances.\n\nCompared to otherpost-hocstate-of-the-art methods for uncertainty estimation, FMGP provides robust predictive distributions with minimal evaluation time. This efficiency stems from FMGP relying solely on the outputs of the pre-trained DNN, without depending on its architecture or requiring the computation of DNN Jacobians, unlike related Linearized Laplace Approximation methods.\n\nSECTION: Acknowledgments\n\nThe authors acknowledge financial support from project PID2022-139856NB-I00 funded by MCIN/ AEI / 10.13039/501100011033 / FEDER, UE and from the Autonomous Community of Madrid (ELLIS Unit Madrid). They also acknowledge the use of the facilities of Centro de Computaci\u00f3n Cient\u00edfica, UAM.\n\nSECTION: References\n\nSECTION: VIIBiography Section", "text_file": "data\\paper_texts\\2412.04177v1_content.txt"}, {"title": "Methodology for Online Estimation of Rheological Parameters in Polymer\n  Melts Using Deep Learning and Microfluidics", "authors": ["Juan Sandubete-L\u00f3pez", "Jos\u00e9 L. Risco-Mart\u00edn", "Alexander H. McMillan", "Eva Besada-Portas"], "published_date": "2024-12-05T13:11:04Z", "summary": "Microfluidic devices are increasingly used in biological and chemical\nexperiments due to their cost-effectiveness for rheological estimation in\nfluids. However, these devices often face challenges in terms of accuracy,\nsize, and cost. This study presents a methodology, integrating deep learning,\nmodeling and simulation to enhance the design of microfluidic systems, used to\ndevelop an innovative approach for viscosity measurement of polymer melts. We\nuse synthetic data generated from the simulations to train a deep learning\nmodel, which then identifies rheological parameters of polymer melts from\npressure drop and flow rate measurements in a microfluidic circuit, enabling\nonline estimation of fluid properties. By improving the accuracy and\nflexibility of microfluidic rheological estimation, our methodology accelerates\nthe design and testing of microfluidic devices, reducing reliance on physical\nprototypes, and offering significant contributions to the field.", "arxiv_id": "2412.04142v1", "html_link": "https://arxiv.org/html/2412.04142v1", "search_term": "ti:\"deep learning\"", "html_content": "SECTION: Methodology for Online Estimation of Rheological Parameters in Polymer Melts Using Deep Learning and Microfluidics\n\nSECTION: ABSTRACT\n\nMicrofluidic devices are increasingly used in biological and chemical experiments due to their cost-effectiveness for rheological estimation in fluids. However, these devices often face challenges in terms of accuracy, size, and cost. This study presents a methodology, integrating deep learning, modeling and simulation to enhance the design of microfluidic systems, used to develop an innovative approach for viscosity measurement of polymer melts. We use synthetic data generated from the simulations to train a deep learning model, which then identifies rheological parameters of polymer melts from pressure drop and flow rate measurements in a microfluidic circuit, enabling online estimation of fluid properties. By improving the accuracy and flexibility of microfluidic rheological estimation, our methodology accelerates the design and testing of microfluidic devices, reducing reliance on physical prototypes, and offering significant contributions to the field.\n\nSECTION: 1Introduction and related work\n\nRheology is the science that studies how materials flow and deform. Among these rheological properties, viscosity is of special interest as it defines how much the fluid will flow when a force is applied. Viscosity estimation is crucial across various applications, from industrial processes like mold injection and 3D printing to medical diagnostics involving bodily fluids\\shortciteshaw:polyrheo,Meek2014,Zhao2023. The behavior of non-Newtonian fluids, such as polymer melts or gelled propellants, under different pressure gradient presents significant challenges due to the complex dynamics of their viscosities\\shortcitedlrgel2016.\n\nTraditional methods for measuring viscosity, such as rolling ball, rotational or capillary viscometers, while accurate, are often not suitable for inline measurement and real-time monitoring[Shaw (2011)]. These standardized methods such as those in[ASTM (2020)]typically require discrete liquid samples and are impractical for processes involving continuous or semi-continuous flow of changing fluid compositions. The objective of this study is to develop a methodology that integrates deep learning with microfluidic technology to enable online, real-time estimation of rheological parameters in polymer melts. This approach aims to overcome the limitations of traditional methods by providing a cost-effective, accurate, and flexible solution for continuous monitoring and adjustment of fluid properties.\n\nRecent developments in microfluidic devices are becoming a promising alternative. These devices require minimal fluid volumes and can be directly integrated into larger systems, facilitating real-time rheological estimation[Del\u00a0Giudice (2022)]. Various microfluidic designs have already been explored, ranging from devices measuring the deformation of flowing polymers\\shortciteLee_2005,DelGiudice2020 to systems using embedded sensors to measure pressure changes after a particular flow-path\\shortciteBie_2019.\n\nMoreover, in recent years deep learning has been successfully used to model many complex processes in fluid dynamics\\shortcitedlfluids2021 and in microfluidics\\shortciteMcIntyre_2022, as well as in the intersection of rheology and microfluidics to perform parameters identification of non-Newtonian fluids\\shortcitemlvisc2023,Jarujareet_2023. Nonetheless, estimating the viscosity\u2019s parameters from just pressure and flow signals could be convenient for their integration in microfluidic circuits, since these magnitudes are commonly measured.\\shortciteNleastsq2022 do so by combining least squares for the fittings. Their approach requires nevertheless of a particular stepped input signal, not being adequate for online estimation. The use of deep learning could help reducing the dependency of this restricted input sequences, allowing for estimating rheological properties, online, from pressure and flow rate signals.\n\nOne aspect that makes difficult the development of these systems is the fact that deep learning models requires of large training datasets. Simulating Newtonian and non-Newtonian fluids is often a computationally expensive and time-consuming task, which requires both expertise and resources because it is commonly done by means of Computational Fluid Dynamics (CFD,\\shortciteNPGalindoRosales2010). For microfluidics, the simulations might be performed applying a finite-volume method\\shortciteKeslerov_2010,ferziger2019cfd or the Lattice-Boltzmann method\\shortcitekruger2017lattice. Higher levels of abstraction are possible, using the geometry of the systems and their symmetries to approximate the microfluidic system as two-dimensional\\shortciteBoulais_2023 or one-dimensional\\shortciteelecanalog2012 problems. This allows to trade off the accuracy and complexity of the simulations. In this regard,\\shortciteNTakken_2024 have recently proposed a hybrid simulation framework in which high levels of abstraction are used to speed-up the simulations through analytic expressions (where the flow path is abstracted to 1D), using CFD only in cases where this expressions are not available (e.g. junctions of fluidic paths).\n\nOur methodology employs these principles of microfluidics to reduce the complexity of simulating generalized non-Newtonian fluids behaviors approximating the system as one-dimensional. By modeling the microfluidic system as a network of hydraulic resistances and capacitances, we can emulate the flow characteristics of various fluids under different conditions with reduced computational overhead. This model generates synthetic data used to train a deep neural network, an architecture containing Recurrent Neural Networks (RNN), which allows to discover time dependencies on the input data. This deep learning model is trained to predict the fluid\u2019s rheological parameters based on observed pressure changes and flow rates.\n\nThe main contributions of this work are: a methodology which can be applied to model, simulate and develop microfluidic systems flowing non-Newtonian fluids and integrated with deep learning, and an innovative approach to viscosity estimation for generalized Newtonian fluids, and more concretely, polymer melts, from commonly measured magnitudes such as pressure and flow signals. By providing a robust simulation framework, this work contributes to the broader field of fluid simulation and modeling, offering a practical tool for researchers and engineers working with complex fluid systems.\n\nExamples of potential applications might range from the development of deep neural networks for the estimation of viscosity\u2019s parameters (shown in this paper), to chemical process AI-based control in microfluidics, passing through cell-culture anomaly detection within complex fluids.\n\nThis paper is organized as follows. Section2describes the theoretical microfluidic background underpinning our methodology. Section3details our methodology, including the exposition of our microfluidic circuit design, of the physical model used for simulation, and of the architecture of the neural network model used to estimate parameters of the viscosity model. Section4presents the conducted experiments and the obtained results. Finally, Section5draws the conclusions and presents some future lines of research.\n\nSECTION: 2BACKGROUND\n\nThis section introduces the required background on two areas supporting our work: generalized Newtonian fluid models, and microfluidics systems modeling.\n\nThe viscosity of a fluid relates its rate of displacement or shear rate, usually denoted, for an imposed shear stress. This shear stress depends of how the material reacts to an applied force under some conditions. For Newtonian fluids, the ratio between these two magnitudes is independent of the applied shear stress, while in non-Newtonian fluids this does not always hold. Many different models have been proposed, although the most commonly used is the power-law fluid model\\shortciteshaw:polyrheo,\n\nwhere,stands for the zero shear rate viscosity,for the aforementioned shear rate, andfor its power index. For, shear rate and viscosity are perfectly independent, which is the behavior of an ideal Newtonian fluid. This model is used by\\shortciteNSrivastava2006 to semi-analytically approximate the solution to the flow of a non-Newtonian fluid through a hydraulic resistance of rectangular cross-section.\n\nAnalytic and semi-analytic expressions have been derived for the streamlined capillary geometries commonly used in microfluidics. These expressions are obtained by considering one-dimensional or two-dimensional approximations to the problem\\shortcitemortensen2004. They are used to speed up the simulations of these systems, accelerating the design process, for example, by simulating the system as analogous electrical problems, i.e., 1D approximation of the microfluidic circuits\\shortciteelecanalog2012.\n\nIn these analogous electrical problems, an equivalent Ohm\u2019s law\\shortcitetipler2004 is defined for Newtonian fluids under laminar flow. The equations of Hagen-Poiseuille[Anna (2013)]allows for solving microfluidic circuits in an analogous manner to electrical circuits. In particular, the flow rate,, through a hydraulic resistance can be estimated as\n\nwhereis thehydraulic resistancevalue, andis the pressure drop over it. Here, the subscriptofis used to denote that the flow is governed by viscous forces. If the circuit has only one branch with multiple hydraulic resistances interconnected in series, then the total resistance of the circuit can be estimated as. The total flow rate passing through the branch can be calculated using equation (2) over the total resistance.\n\nLastly, and continuing with the analogy, a hydrauliccircuit can be defined by adding a hydraulic capacitance,, to the circuit. Its transient response will be then governed by a time constant. The relation between volume,, and capacitanceis simply, wheredepends on the fluid, beingfor air.\n\nSECTION: 3METHODOLOGY\n\nFigure1shows the overall work-flow of the designed and implemented methodology, which is explained in this section. The aim of the methodology is to generate data for training a RNN model to estimate the rheological properties of a fluid.\n\nFirst, a set of values of the viscosity model\u2019s parameters is sampled from random uniform distributions within defined ranges. For this set, a random sequence of pressure signals is generated to be used as input to the microfluidic circuit simulator together with the sampled parameters, outputting signals of pressure drop along the resistances, as well as input and output flows to the capacitance. Simulations that do not contain significant information for the corresponding parameters are rejected, requiring the generation of a new random sequence of signals for the same set of parameters. This way, for each simulation, four output signals and one set of parameters of the viscosity model are collected.\n\nSecond, the generated dataset is used to perform a supervised training of the RNN model with the purpose of identifying the viscosity\u2019s parameters corresponding to each fluid from the signals produced by the simulator. This way, the model is trained for a number of epochs using mini-batches where each mini-batch is composed of sixteen simulations. Each simulation is made up of the four signals (two pressure drop signals, and two flow-rate signals) associated to the set of viscosity model\u2019s parameters that have been used to produce them. The RNN model receives these four signals, sample by sample, four values in parallel (one by input signal), outputting one prediction once the whole simulation has passed through.\n\nLast, the simulator is again used to verify the operation of the trained artificial neural network. The input signals from the test dataset are used to test the prediction quality of the RNN model. For each simulation, the RNN will predict the corresponding set of parameters. These estimated parameters are then simulated and the output four signals of pressure drop and flow rate are compared to the ones produced with the correct set of parameters of viscosity. The difference is evaluated to verify that, for each simulation, if the predicted parameters are different, the output of the simulation is different as well. This indicator is used as guide for the discovery of the right setup to estimate the parameters of the viscosity model.\n\nIn the following subsections, the proposed microfluidic circuit design is described along with the corresponding physical model. The simulation process is then introduced, followed by the description of the proposed artificial neural network architecture. Lastly, the used verification process is detailed.\n\nSECTION: 3.1Design and Modeling of the Microfluidic Circuit\n\nMicrofluidic systems development is pursued for many reasons (e.g. increased surface-volume ratio or low-waste applications,\\citeNPtabeling2005introduction), however, the most interesting feature of microfluidics related to this work is the reduction of complexity of the system under laminar flow. According to Hagen-Poiseuille\u2019s equations[Anna (2013)], the laminar flow of a Newtonian liquid passing through a capillary can also be analytically characterized for simple cross-sectional geometries. The expression depends of the geometry and dimensions of the capillary, the dynamic viscosity of the fluid,, and the applied pressure drop along the capillary[Tabeling (2005)]. In particular, for a microfluidic resistance of rectangular cross-section of width,, height,, and length,(when, and) becomes\n\nwherestands for the contribution of all the purely geometrical terms to thehydraulic resistance (i.e.). The subscripts,, in termsandhave been omitted for simplicity in equation (3).\n\nFigure2shows the equivalence between the designed microfluidic circuit and the corresponding ideal electrical circuit, which can be modeled by one resistance in series connected to another resistance with a capacitor in parallel, with. For Newtonian fluids, the rate at which the capacitance changes its internal pressure is given by a time constant,, so resolving by applying the Thevenin\u2019s theorem:. In this model, the hydraulic resistance corresponding to the tubing which connects the elements (i.e., from the reservoir to the, and fromto the waste) and the connectors (hydraulic interfaces between elements) have been neglected since the characteristic dimensions of the cross-section of the capillary are usually around two orders of magnitude above, and the hydraulic resistance decreases with the power of three over these dimension.\n\nIn this hydrauliccircuit in which a Newtonian fluid flows, if the values ofand, are known, the viscosity can straightaway calculated asafter measuring the circuit time constant. The introduced capacitance is thus important because it allows to extract information of the fluid from the transient response.\n\nAs it can be observed in Figure2, an extra resistance (calledhereafter) is included before the capacitance in the hydraulic circuit, or before thein the electrical analogy. This input resistance has the purpose of limiting the maximum flow rate to the capacitance. Following with the electrical analogy, it will short-circuit for fast changes in the input signal, leading to high flow-rates that could move the regime out of laminar flow. The value of this new resistance, although arbitrary, is set to be equal to the resistance of thepart (calledhereafter) for simplicity. Estimates of their values can be calculated from commercially available microfluidic resistance chips with,, and, which leads to.\n\nIn polymer melts and other polymeric dissolutions, equation (3) holds while the polymer chains recovery time is smaller than the characteristic time of the flow, because the elastic forces do not have a significant influence over the dynamics of the fluid. The dimensionless Deborah number can be used to estimate when these elastic forces must be taken into account[Anna (2013)]and are computed according to\n\nwhereis the relaxation time of the fluid, andthe shear rate. Being the rightmost part of the equation the corresponding solution at the wall of thecapillary of rectangular cross-section.\n\nFor, the chains are elongated with the flow and the elasticity of the fluid plays a significant role on the overall observed behavior which cannot be neglected. In this case, semi-analytic equation (5) obtained by\\shortciteNSrivastava2006 can be used to approximate the flow rate through the givenrectangular hydraulic resistance,\n\nwherestands for the zero-shear-rate viscosity (which corresponds toin Newtonian regime) andfor the power-law index. The subscriptinis used to denote the flow under influence of elastic forces.\n\nIn order to produce a set of equations that allow smooth transitions between the Newtonian and non-Newtonian behavior, the set of expressions is complemented with a generalized logistic function that receives as input the value ofand produces as output the value of, which is used to weight equation (3) and equation (5) during the transition according to:\n\nThe flow rate,, through each resistance,and, might be different during the transient response, while the capacitance,, charges or discharges fluid. Lastly, the only state variable considered in our microfluidic simulator is the charging of the capacitance,. Modeled as a capacitance of the compressed air, the evolution of the volume can be calculated as the balance of fluid flow experienced by the capacitor:\n\nwhereis the flow going into the capacitance,, andis the flow going out of it. Each of these flows will be calculated according to equation (6).\n\nThe rate of change of volume from equation (7), integrated, can be related to the pressure by means of Boyle-Mariotte formula[Tipler and Mosca (2004)]. For the first hydraulic resistance, the expression will become:\n\nwhereis the maximum volume of air stored in the capacitance,is its initial internal pressure, andis the volume of air stored inside, calculated by integrating, and,are the pressures at the input and the output of the first hydraulic resistance,. Equivalently, the pressure drop alongis calculated as.\n\nSECTION: 3.2Simulation\n\nTo generate the training and testing datasets for the RNN, simulations are run following the scheme presented in Figure3. The first block generates a set of random values for the parameters,and, from their corresponding uniform distributions within the ranges defined in Table1. These ranges has been selected according to the selected commercially-available hydraulic resistances to be able to observe Newtonian and non-Newtonian behaviors within experiments of 12.5 seconds. The timescale of the tool can be tuned by changing the time duration of the experiments, the values of the hydraulic resistances or the capacitance of the microfluidic circuit in order to study fluids of different degree of viscosity with this setup.\n\nAs Table1shows, shear-thinning (which happens when) and shear-thickening (when) are allowed behaviors for the fluid. Nevertheless the range ofremains close to 1.0, because the tubing and connections have been neglected in the current equivalent circuit. This is, high shear-thinning fluids, like xanthan solutions\\shortciteMrokowska_2019 or gelled propellants with\\shortciteCaldas_Pinto_2019, will be less accurately simulated under non-Newtonian regime the smaller their behavior exponent is, because higher viscosities will be observed at the tubing, increasing their resistance to flow. And, oppositely, shear-thickening fluids simulations will be more accurate the higheris.\n\nAfter the random values of the parameters are generated, a second block generates random sequences of sines, steps and ramps. The reason behind the randomness of the input signals is to increase the variance in the dataset to avoid having to impose patterns in the input signals for estimating the parameters, minimizing the interference with the user\u2019s work in a real setup.\n\nNext, in order to generate datasets with valid information, a type of rejection sampling scheme is applied. The process starts using the sampled values of the parameters and of the reference inputs to run the simulations, and continues checking if the simulations output signal fulfill the conditions required by theconditions checkblock.\n\nThe simulation is performed such that, at each instant,,is imposed from the input pressure signal, then equation (8) is used to calculate the pressure drop over. This is used in equation (3) to calculate the Newtonian flow-rate,, and in equation (5), to calculate the non-Newtonian flow rate,, through., is then used as input to calculate. From this number, the parameteris obtained and used to weightandfor the finalcalculation. The process is repeated for, obtaining the corresponding. Finally, the variation rate of the volume at the capacitance is calculated as the difference betweenand.\n\nThe condition block checks if the output signal contains information of Newtonian and non-Newtonian behavior of the fluid. In particular, ifis the sequence of values ofthat establishes the regime of the flow at equation (6) at different instant, the condition block checks if\n\nwheredenotes the size of a subset,andare thresholds to ensure predominant Newtonian and predominant non-Newtonian regimes for some corresponding percentages,and, of the total samples of the experiment,. The conditions are nevertheless checked for a maximum number of iterations to ensure that, if the conditions are too difficult to match for a given parameters set, the simulator will not remain blocked in an endless loop.\n\nThe input pressure signals are non-overlapping, randomly composed sequence of sine, ramp and step functions of different values generated according to a uniform distribution within the configured values. Each randomly generated pressure signal has a duration ofwith a resolution of, orpoints at 20Hz. This pressure signal, which corresponds within Figure2, remains constant while the dynamics of the rest of the system are calculated. Runge-Kutta\\shortciterk1986 is used to integrate the state variable equation (7). The used implementation assumes accuracy of order fourth to control the error, while steps are taken using the order five formulation. The generated dataset is composed of 5500 experiments of 12.5s (250 samples) each one. Figure5shows some of the main output signals from an example simulation.\n\nSECTION: 3.3Artificial Neural Network\n\nThe proposed architecture is composed of five layers, has four inputs (namely,,,) and three outputs (consisting of the estimated values of parameters,and).\n\nThe two first layers of the model constitute the recurrent part, where different implementations were tested. Namely unidirectional Long Short-Term Memory (LSTM,\\citeNPlstm1997), unidirectional Gated Recurrent Unit (GRU,\\shortciteNPgru2014) and bidirectional GRU (BGRU,\\shortciteNPbgru2017). In unidirectional RNN, information can only flow from past instants to the last instant, whereas in bidirectional RNN the signals are analyzed in both (time) directions\\shortcitebrnn1997. These BGRU layers were both implemented with twenty hidden units, Rectified Linear Units (RELU) as activation layers, leaky RELU for their recurrent activation, and a recurrent dropout of probability 10%.\n\nAfter the two RNN layers, two fully connected (i.e. dense) layers are sequentially connected, separated by a dropout layer. The first input layer is implemented with twenty hidden units, followed by a leaky RELU activation layer. The dropout layer is set with probability 10%. The output fully connected layer is composed by three units, one unit for each estimated parameter, with linear activation. The final model version has a total of 11.5k tunable parameters. Its architecture can be observed in Figure4.\n\nThe inputs to the system are two signals of pressure drop and two signal of flow-rate. Each signal is individually normalized in range, with a length of 250 samples per signal, 1000 samples in total. This configuration allows for its operation in real-time by implementing an input sliding window with the previous setup. The model produces only one estimate of each parameter, i.e.,, and, after inserting the 250 samples of each signal. The outputs are also in rangeand must be denormalized. The model was trained from zero. Adam optimizer is used with learning rate, no weight-decay, for 25 epochs, and Mean Squared Error (MSE) is used as loss function.\n\nSECTION: 3.4Verification\n\nIn order to verify the operation of the simulation, training, and estimation methodology, the following three steps procedure is performed: first, a K-fold cross validation method is applied to ensure the independence between the training and testing dataset. Thirty three percent of the generated dataset is used for the testing, and the remaining sixty seven percent is used for training. Thus, it is chosen, performing a complete rotation over the dataset after three iterations of the method. Since the generation of the parameters is uniformly performed within the defined range and the dataset is shuffled before the execution of the K-fold cross validation, the performance is expected to be similar for the three iterations. To evaluate it, an accuracy normal distribution curve is fitted of each model.\n\nAfterwards, for each model, simulations are performed using the estimated parameters for each sample of the dataset. The Mean Square Error (MSE) between the curves simulated with the reference parameters and the curves simulated with the estimated parameters is calculated.\n\nFinally, the Pearson correlation coefficients are calculated between the parameters estimation error and the simulated curves error.\nCorrelation coefficients close to the unit imply a high correlation between these two error evaluation. This potentially indicates that the mapping between the parameters and the simulated curves is injective, and thus, the experiments are properly set for the training of the RNN. Otherwise, a correlation coefficient for a particular estimated parameter closer to one half mean that the performed experiments might not be suitable for the RNN to discover the original values.\n\nSECTION: 4EXPERIMENTS AND RESULTS\n\nIn this section we explain how we have used the proposed methodology to explore the optimization of the simulation setup towards the inference of complex parameters, such as the rheological properties of a fluid, from pressure and flow-rate signals that do not have a straightforward linkage.\n\nIn order to define the ranges over which the current setup could be useful, the proposed framework can be used to perform a heuristic analysis. For the given resistances, capacitance, and maximum input pressure (arbitrarily set to 600mbar or 60e3 Pa), the objective is to find out what are the ranges for the parameters of the viscosity model for which both Newtonian and non-Newtonian regimes can be observed. The most influential parameter in this regard is the relaxation time,, because it regulates the threshold flow-rate at which the fluid will behave in one or the other way. A higher zero-shear viscosity,, will decrease the observed flow-rate,, for a given applied pressure,, making it harder to reach the non-Newtonian regime. Figure5shows two example simulations with same random input signal. Non-Newtonian regime is only achieved in one of them. The resulting parameters\u2019 ranges from this analysis are found in Table1.\n\nAs a first step, an exploration of different RNN-based model designs was performed aiming to reduce the possibilities of having a performance bottleneck at the model architecture. Three RNN implementations were tested: unidirectional LSTM, unidirectional GRU and BGRU. While the benchmark was equivalent for all of them, the unidirectional LSTM often turned unstable, showing more sensitivity to the training hyper-parameters. Unidirectional GRU and BGRU implementations were both stable, being BGRU chosen since it was observed to be faster converging towards the same results for both same and different hyper-parameters.\n\nAfter that, an exploration of the simulation setup is performed. Among the different parameters that can be configured are the properties of the randomized generation of pressure input signals: ranges for maximum and minimum values, ranges for the random frequency, the duration of each sequence inside the 10s experiment, etc. Figure6shows the fitted error distributions for the estimation of each parameter of the viscosity model during the exploration process.\nEach distribution set of three distributions corresponds to a different deep learning model. The error distribution is normalized to ease their comparison. Each of these models has been trained with a dataset generated with a different configuration. In the first dataset, corresponding to Figure6.a, the input pressure signal,, is a sequence made out of 8-10 sinusoidal signals. In the second set, Figure6.b,is a 8-10 segments sequence of steps, ramps, and sinusoidal signals. In the third one, Figure6.c, 3-4 segments of steps, ramps and signals are combined. The reduction of error can be observed, being the behavior exponent,, the easiest parameter for the models to learn to estimate.\n\nThe Pearson correlation coefficients are lastly calculated to be used as an indicator of sensitivity between the used input signals to the deep learning model and the parameters that it is asked to estimate. This is, if the chosen input signals to the model,, are good candidates to estimate the parameters,, simulating the inaccurate values of the parameters,, should produce wrong values of these input signals,. The error can be defined as. In Table2the correlation coefficients are showed for the model whose results are displayed at Figure6.c. The correlation shows that it is, indeed, the behavior index,, the parameter with a higher correlation over the simulated signals, potentially making it the easiest parameter of viscosity model to be estimated.\n\nSECTION: 5CONCLUSIONS AND FUTURE WORK\n\nIn this study, we developed a one-dimensional model for a hydrauliccircuit to simulate generalized Newtonian fluids. This model was crucial for generating synthetic data to train deep learning models. We used bidirectional GRUs in our methodology, which effectively predicted rheological parameters from the dynamic responses of microfluidic systems. Our findings show that the behavior indexof the fluid correlates strongly with the output signals, making it the most reliably estimated parameter.\n\nFor future work, we plan to improve the simulation setup by automating the exploration process to optimize input signal configurations and model parameters systematically. We also aim to build a physical prototype to validate our simulations and refine model assumptions. This step is essential for practical applications in industrial and medical diagnostics. Additionally, we will explore incorporating more complex fluid dynamic models and advanced neural network architectures to handle a wider range of fluids and flow conditions, expanding the applicability of our methodology to various industrial uses.\n\nSECTION: ACKNOWLEDGMENTS\n\nThis work has been supported by the Research Projects IA-GESBLOOM-CM (Y2020/TCS-6420) funded by the Synergic program of the Comunidad Aut\u00f3noma de Madrid (CAM), SMART-BLOOMS (TED2021-130123B-I00) by MCIN/AEI/10.13039/501100011033 and the European Union by NextGenerationEU/PRTR, and the MSCA-ITN-H2020 Project LasIonDef (GA n.956387).\n\nSECTION: References\n\nSECTION: AUTHOR BIOGRAPHIES\n\nJUAN SANDUBETE-L\u00d3PEZis an industrial PhD candidate by Universidad Complutense de Madrid, and at Microfluidic Innovation Center, Paris. He holds a masters degree in Systems and Control Engineering from the same university. His research interests include autonomous systems, instrumentation, deep learning, and simulation. His email address is .\n\nJOS\u00e9 L. RISCO-MART\u00edNreceived his Ph.D. from UCM, where he currently is Full Professor in the Department of Computer Architecture and Automation. His research interests include systems modeling, simulation, and optimization. His email address is .\n\nALEXANDER MCMILLANis an R&D project leader at the Microfluidics Innovation Center, where he oversees a team that develops new microfluidic tools through academia-industry collaborations. He earned his PhD from KU Leuven School of Bioscience Engineering, focusing on new polymers for microfluidic devices and their applications in flow chemistry and cell culture. His email address is .\n\nEVA BESADA-PORTASis an Associate Professor of Systems Engineering and Automation at UCM. She also holds a PhD in Computer Systems from UCM. Her research interests include uncertainty modeling and simulation, optimal control and planning of unmanned vehicles. Her email address is .", "text_file": "data\\paper_texts\\2412.04142v1_content.txt"}, {"title": "DeepFEA: Deep Learning for Prediction of Transient Finite Element\n  Analysis Solutions", "authors": ["Georgios Triantafyllou", "Panagiotis G. Kalozoumis", "George Dimas", "Dimitris K. Iakovidis"], "published_date": "2024-12-05T12:46:18Z", "summary": "Finite Element Analysis (FEA) is a powerful but computationally intensive\nmethod for simulating physical phenomena. Recent advancements in machine\nlearning have led to surrogate models capable of accelerating FEA. Yet there\nare still limitations in developing surrogates of transient FEA models that can\nsimultaneously predict the solutions for both nodes and elements with\napplicability on both the 2D and 3D domains. Motivated by this research gap,\nthis study proposes DeepFEA, a deep learning-based framework that leverages a\nmultilayer Convolutional Long Short-Term Memory (ConvLSTM) network branching\ninto two parallel convolutional neural networks to predict the solutions for\nboth nodes and elements of FEA models. The proposed network is optimized using\na novel adaptive learning algorithm, called Node-Element Loss Optimization\n(NELO). NELO minimizes the error occurring at both branches of the network\nenabling the prediction of solutions for transient FEA simulations. The\nexperimental evaluation of DeepFEA is performed on three datasets in the\ncontext of structural mechanics, generated to serve as publicly available\nreference datasets. The results show that DeepFEA can achieve less than 3%\nnormalized mean and root mean squared error for 2D and 3D simulation scenarios,\nand inference times that are two orders of magnitude faster than FEA. In\ncontrast, relevant state-of-the-art methods face challenges with\nmulti-dimensional output and dynamic input prediction. Furthermore, DeepFEA's\nrobustness was demonstrated in a real-life biomedical scenario, confirming its\nsuitability for accurate and efficient predictions of FEA simulations.", "arxiv_id": "2412.04121v1", "html_link": "https://arxiv.org/html/2412.04121v1", "search_term": "ti:\"deep learning\"", "html_error": "Error processing HTML: HTTP Error 404: Not Found"}, {"title": "PePR: Performance Per Resource Unit as a Metric to Promote Small-Scale\n  Deep Learning in Medical Image Analysis", "authors": ["Raghavendra Selvan", "Bob Pepin", "Christian Igel", "Gabrielle Samuel", "Erik B Dam"], "published_date": "2024-03-19T09:17:18Z", "summary": "The recent advances in deep learning (DL) have been accelerated by access to\nlarge-scale data and compute. These large-scale resources have been used to\ntrain progressively larger models which are resource intensive in terms of\ncompute, data, energy, and carbon emissions. These costs are becoming a new\ntype of entry barrier to researchers and practitioners with limited access to\nresources at such scale, particularly in the Global South. In this work, we\ntake a comprehensive look at the landscape of existing DL models for medical\nimage analysis tasks and demonstrate their usefulness in settings where\nresources are limited. To account for the resource consumption of DL models, we\nintroduce a novel measure to estimate the performance per resource unit, which\nwe call the PePR score. Using a diverse family of 131 unique DL architectures\n(spanning 1M to 130M trainable parameters) and three medical image datasets, we\ncapture trends about the performance-resource trade-offs. In applications like\nmedical image analysis, we argue that small-scale, specialized models are\nbetter than striving for large-scale models. Furthermore, we show that using\nexisting pretrained models that are fine-tuned on new data can significantly\nreduce the computational resources and data required compared to training\nmodels from scratch. We hope this work will encourage the community to focus on\nimproving AI equity by developing methods and models with smaller resource\nfootprints.", "arxiv_id": "2403.12562v2", "html_link": "https://arxiv.org/html/2403.12562v2", "search_term": "ti:\"deep learning\"", "html_content": "SECTION: PePR:PerformancePerResource Unit as a Metric to Promote Small-scale Deep Learning in Medical Image Analysis\n\nThe recent advances in deep learning (DL) have been accelerated by access to large-scale data and compute. These large-scale resources have been used to train progressively larger models which are resource intensive in terms of compute, data, energy, and carbon emissions. These costs are becoming a new type of entry barrier to researchers and practitioners with limited access to resources at such scale, particularly in theGlobal South. In this work, we take a comprehensive look at the landscape of existing DL models for medical image analysis tasks and demonstrate their usefulness in settings where resources are limited. To account for the resource consumption of DL models, we introduce a novel measure to estimate the performance per resource unit, which we call the PePR111Pronouncedpepper.score. Using a diverse family of 131 unique DL architectures (spanningtotrainable parameters) and three medical image datasets, we capture trends about the performance-resource trade-offs. In applications like medical image analysis, we argue that small-scale, specialized models are better than striving for large-scale models. Furthermore, we show that using existing pretrained models that are fine-tuned on new data can significantly reduce the computational resources and data required compared to training models from scratch.\nWe hope this work will encourage the community to focus on improving AI equity by developing methods and models with smaller resource footprints.222Source code:https://github.com/saintslab/PePR.\n\nSECTION: 1Introduction\n\nThe question of material costs of technology, even in light of their usefulness, should not be ignored[heikkurinen2021sustainability]. This is also true for technologies such as deep learning (DL) that is reliant on large-scale data and compute, resulting in increasing energy consumption and corresponding carbon emissions[strubell2019energy]. These growing resource costs can hamper their environmental and social sustainability.[kaack2022aligning,wright2023efficiency].\n\nConsiderations towards improving the environmental impact of DL are garnering attention across different application domains. This has resulted in calls for action broadly, and also within the medical image analysis community, to improve the resource efficiency of DL models[bartoldson2023compute,selvan2023operating]and to report the energy and carbon costs[selvan2022carbon]. Another important implication of the growing resource costs of DL is the risk of disenfranchising practitioners with limited access to resources. This is captured in Figure1which shows the number of publications (per capita) within DL across the world for 2013 and 2022333The data for the visualisation in Figure1was curated by querying for the number of research publications per country on the topic of \u201cArtificial Intelligence\u201d in OpenAlex.org. The population data per country was queried from data.WorldBank.org. Regional aggregation was performed using OECD standards and further refined into the ten regions. Curated data will be provided along with the source code.. Several regions in the world categorised asGlobal Southare continuing to lag behind in research in DL[oecd2024ai]. While there are also multiple other structural reasons for this trend, the increasing resource costs of performing research within DL can become a new form of entry barrier that can aggravate this disparity[ahmed2020democratization].\n\nIn light of these observations, this work argues for focusing on small-scale DL in the era of large-scale DL. We hypothesize that the current situation with the increasing resource consumption is due to the singular focus on task-specific performance metrics that are not grounded in material costs. We also argue that access is a prerequisite to improving equity in DL and in use of these methods in healthcare. These arguments are supported by a comprehensive analysis of performance and resource costs of DL-based computer vision models. We study the properties ofmodels ranging fromtotrainable parameters, on three medical image classification tasks to capture interesting trends. We provide qualitative evidence for the usefulness of using pretrained models in resource-constrained regimes. Finally, we present a novel composite measure of performance and resource consumption. We call this the performance per resource unit (PePR) score. Using the PePR-score we characterise the behaviour of small-scale and large-scale DL models. We demonstrate that in resource-constrained regimes, small-scale DL models yield a better trade-off between performance and resource consumption.\n\nSECTION: Related Work:\n\nPareto optimisation of performance and resource constraints has been primarily investigated within the context of neural architecture search (NAS)[elsken2019neural]. More recently, methods have been proposed to explore models using specific resource constraints such as energy consumption[evchenko2021frugal,bakhtiarifard:24]or carbon footprint[moro2023carburacy]. The work in[evchenko2021frugal]proposes a resource-aware performance metric similar to our contribution in this work which, however, is concerned with non DL models. Within application domains such as medical image analysis, there has been little emphasis on the joint optimisation of performance and energy consumption[selvan2021carbon]. The question of equitable AI within healthcare has been posed in works like[baumgartner2023fair]primarily from the context of fairness and not from resource/access perspectives.\n\nSECTION: 2PePR-score\n\nIn this work, we assume a DL model to be an entity that consumes resources such as data, energy, time, or CO2eq. budget as input and provides some measurable predictive performance on downstream tasks of interest. In contrast to conventional performance metrics that are not grounded in material costs, we envision a score that can take the resource costs into account. To this end, we introduce the notion of performance per resource unit (PePR), denoted as, which relates (normalised) performanceof a model with the resources consumed and defined as\n\nIn this definition,is the resource cost normalised to lie in, or explicitlyfor some absolute resource costand somefixed across models within an experiment.444Standard scaling might not always be appropriate. Outliers may have to be considered, and in other instancesmight depend on the experimental set-up.\n\nThe salient features of the PePR-score that make it useful as an alternative objective that takes resource costs into account are as follows:\n\nPerformance-dependent sensitivity:From the plot of the PePR isoclines (see Figure2-a)), it is clear that PePR is insensitive to resource consumption for models with low performance. For models with high performance, PePR attributes almost identical weight to performance and to resource consumption.\n\nPePR-score for a single model:PePR score is a relative measure of performance-resource consumption trade-off. In instances where a single model is considered, it is the same as performance. This is due to the fact thatand.\n\nComparing two models:Consider the case where only two models are compared with respective absolute resource consumptionsand test performances. If, then the normalized resource costs arebecause. Thus,and.\n\nPePR-score of random guessing:Consider a binary classification task with no class imbalance. In this setting, the performance of random guessing should be about. As thefor this \u201cmodel\u201d, the PePR-score is the same as performance.\n\n(a)\n\n(b)\n\n(c)\n\nDepending on what resource costs are used, different variations of the PePR-score can be derived. For instance, if energy consumption is used as the cost, thenresulting in the PePR-E score. Similarly, one can derive the PePR-C (CO2eq.), PePR-D (data), PePR-M (memory), or PePR-T (Time) scores. Idealised PePR-E scores are plotted in Figure2-a) which captures the trade-off between performance and resource consumption. Models with low resource consumption and high performance would gravitate towards the upper left corner where the PePR score approaches unity.\n\nWe also note that in cases where performance is deemed to be more important than resource consumption, PePR score can be adjusted to reflect this. For instance, one can employwith a scaling factor. Setting a largevalue, say, would prioritise performance and disregard the effect of the resource consumption. As an example, consider the PePR score for the most resource intensive model that also achieves the best performance (i.e.,). According to the definition in Eq.\u00a0(1), the PePR score is. Increasing the emphasis on performance usinggives, basically ignoring the resource costs, if the application warrants this. Adjustingoffers a spectrum of trade-offs between performance and resource costs. In this work, we are focussed on operating in resource constrained regimes, and are mainly interested in the setting.\n\nSECTION: Performance curve:\n\nFor a functionrepresenting a performance curve mapping resource costs to performance (e.g., if the resource is update steps or training data set size, it represents a rescaled learning curve), we define a PePR curve:\n\nwhere in cases of ties the smallest value is picked.\nFurthermore, in order to be able to compare models based on their performance curves, we define a scalar quantityby\n\nTo get some intuition on the PePR score, we can rewrite (2) as the integral of its derivative to obtain the integral representation\n\nHere,is the derivative ofwith respect to resource consumption, which can be interpreted as how much of a performance increase the model is able to get per resource consumed. First, note the presence of the weighting factorsand, which express that the score puts a higher weight on the performance of the model in low-resource regimes (small).\n\nSecond, we can see that the score emphasizes performance per resource consumed (first integral with) and de-emphasizes absolute performance (second integral with). Since all integrals are positive, the PePR score is always greater or equal to the performance of the model at zero resource consumption.\n\nSince, if we assumeto be increasing, we also have that PePR increases in intervals whereand decreases in intervals where.555Because of the bound for the second integrand in (2):This captures the idea that the maxima of the PePR curve lie at points of diminishing returns as captured by, which is also visualized in Figure2-b).\n\nSECTION: 3Data & Experiments\n\nTo demonstrate the usefulness of the PePR-score, we curated a large collection of diverse, neural network architectures and experiment on multiple datasets.\n\nSECTION: Space of models:\n\nThe model space used in this work consists ofneural network architectures specialised for image classification. The exact number ofarchitectures was obtained after seeking sufficiently diverse models which were also pretrained on the same benchmark dataset.\n\nWe used the Pytorch Image Models (timm) model zoo to access models pretrained on ImageNet-1k resulting inmodels spanningM toM trainable parameters. We randomly sub-sampled the available models inPytorch Image Modelslibrary[rw2019timm], which during our experiments had about 700 models. We chose as many unique architectures as possible that were all pre-trained on the same ImageNet dataset. This resulted in the 131 models used in our work, covering CNNs, vision transformers, hybrid models, and efficient architectures.\n\nWe categorise these models along two dimensions i)CNNorOtherdepending on if the architecture is a generic CNN primarily consisting of convolutional layers, residual connections, and other standard operators. This implies transformer-based models[vaswani2021scaling], for instance, are markedOtherii)EfficientorNot Efficientif the descriptions in the corresponding publications discuss any key contributions for improving some aspect of efficiency. Given these categorisations, we end up with a split of 80 and 51 forCNN, Other, respectively, and 31 and 100 forEfficient, Not Efficient, respectively. The median number of parameters is 24.6M. We further classify the models in the lower half to besmall-scaleand the upper half intolarge-scalefor simplicity. The model space is illustrated in Figure2-c)  and additional details are provided for each model in Table2.\n\nSECTION: Datasets:\n\nExperiments in this work are performed on three medical image classification datasets: Derma, LIDC, Pneumonia. Derma and Pneumonia datasets are derived from the MedMNIST+ benchmark[yang2023medmnist]and LIDC is derived from the LIDC-IDRI dataset[armato2004lung]. Images in all three datasets are ofpixel resolution with intensities rescaled to. All three datasets are split into train/valid/test splits: Derma (7,007/1,003/2,005), LIDC (9,057/3,019/3,020), and Pneumonia (4,708/524/624). Derma consists of seven target classes whereas the other two datasets contain binary labels.\n\nSECTION: Experimental design:\n\nAll models were implemented in Pytorch, trained or fine-tuned for 10 epochs with a learning rate ofusing a batch size ofon an Nvidia RTX3090 GPU workstation with 24 GB memory. Statistical significance is measured by-tests.\nWe considered training or fine-tuning of 10 epochs to reduce the compute resources used in this work. We expand on this choice in Sec.4. The training of models in this work was estimated to use 58.2 kWh of electricity contributing to 3.7 kg of CO2eq. This is equivalent to about 36 km travelled by car as measured by Carbontracker[anthony2020carbontracker].\n\nSECTION: Experiments and Results:\n\nWe performed three main experiments with our large collection of models: i) Study the influence of pretraining on test performance ii) Evaluate the role of number of training points iii) Compute PePR-E score and compare the trade-off between test performance and energy consumption as the cost. Results from all three experiments are summarized in Figure3.\n\nWe had access to pretrained weights for allmodels, which made it possible to investigate the influence of using pretraining when resources are constrained. We either fine-tune or train-from-scratch all models forepochs. In Figure3-a), across the board, we notice that using pretrained models are significantly better compared to training models from scratch for the same number of epochs ().\n\nAnother resource that can be lacking, on top of compute/energy, is the amount of training data. We study this by only usingof the training data, for each of the three datasets, and reporting the average test performance per model in Figure3-b). Even though there is a significant test performance difference () when only usingof the data compared to usingof the data, it could be still useful in making some preliminary choices.\n\nThe overall test performance averaged across the three datasets is plotted against the number of parameters, along with architecture classes, in Figure3-c). There was no significant group difference in test performance between small- and large-scale models. Similarly, there was no significant difference between models that areEfficientandNot Efficient, or betweenCNNandOther.\n\nFinally, in Figure3-d) we visualise the PePR-E score for all the models, which uses the energy consumption for fine-tuning for 10 epochs as the resource, which is then normalised within each experiment (dataset). The first striking observation is that the PePR-E scores for the larger models reduce, whereas for the smaller models there is no difference relative to other small-scale models. This is expected behaviour as PePR score is performance per resource unit, and models that consume more resources relative to other models will get a lower PePR score. We observed a significant difference in median PePR-E scores between small and large models for all three datasets, with the group of small models having a higher median PePR-E score (), shown in Figure5. We did not consistently observe any other significant difference across datasets in test performance or PePR-E score when stratifying by model type (CNNvs.Other) or betweenEfficientandNot Efficientmodels. Results for the top five models sorted based on their PePR-E score for each dataset along with their test performance, number of parameters, memory consumption, absolute energy consumption, training time for 10 epochs, are reported in Table1. We also report the best performing model when only test performance is used as the criterion for comparison.\n\nSECTION: 4Discussion & Conclusion\n\nOur experiments reported in Figure3and Table1reveal interesting trends about the interplay between test performance and resource consumption. We consider all models below the median number of parameters (24.6M) to be small-scale, and above as large-scale models, visualised demarcated using the gray-shaded regions in all relevant figures. We noticed no significant difference in performance between the small-scale and large-scale models in the regime where they were fine-tuned with pretrained weights for 10 epochs. This captures the problem with focusing only on test performance, as it could easily yield large-scale models even when small-scale models could be adequate. However, when using the PePR-E score, we see a significant performance difference with the small-scale models achieving a higher PePR-score (). This emphasises the usefulness of taking resource costs into account, which can be easily done using any variations of the PePR score.\n\nEnergy, or other resource, consumption awareness can also be incorporated using multi-objective optimisation[bakhtiarifard:24]. PePR score can be thought of as one way to access the solutions on the Pareto front with an emphasis on low-resource footprint. This is captured in Figure4which overlays the Pareto set (in orange) and all other models over the PePR scores. The knee point of this Pareto front is pointing towards maximising PePR-E score (brighter regions).\n\nPePR score is a composite metric that offers a trade-off between performance and resource consumption. It can be used instead of multi-objective optimisation of the two objectives separately. As shown in our experiments, PePR score can be used to compare models that use different extents of resources. Current reporting in deep learning for image analysis focus on performance metrics like accuracy while disregarding the resources expended[selvan2022carbon]. Furthermore, PePR can be used to choose the best model under a known resource constraint, such as maximum memory or energy consumption allowed.\n\nThe key experiments reported consider energy consumption as the main resource in the PePR-E score. Additional metrics (PePR-M for memory, PePR-C for carbon emissions, PePR-T for training time) reported in the Figure6show the versatility of the PePR score. We can envision a general PePR score which can consider all resources into account by weighting them differently. For example, usingwith, where the different weights can be adjusted depending on the application.\n\nSECTION: Limitations:\n\nWe used a training or fine-tuning budget of 10 epochs in this work to reduce the compute resources used. This can be a limitation, as different models learn at different rates. To show that our experimental results are not artifacts of this choice, we looked at the performance of models that have been trained to convergence on ImageNet (which formed the basis of pre-training) using the public dataset from[rw2019timm]. We performed a similar analysis of validation set performance of the converged models, The PePR-M scores are shown in Figure4-b), and they show similar trends as our experiments in Figures3and6.\n\nThe PePR score itself is agnostic to the downstream task. In this study, the experiments focussed on medical image classification, which may limit the generalisability of the results. While the findings were consistent across the considered data sets, expanding the study to other tasks (segmentation) and domains (non-image) in future work might provide further insights.\n\nSECTION: Conclusions:\n\nUsing a large collection of DL models we have shown that using pre-trained models yields significant gains in performance, and should always be considered. We have also shown that when resource consumption is taken into account, small-scale DL models offer a better trade-off than large-scale models. Specifically, the performance achieved per unit of resource consumption for small-scale models in low-resource regimes is higher. We proposed the PePR score that offers an inbuilt trade-off between resource consumption and performance. The score penalises models with diminishing returns for a given increase in resource consumption.\n\nQuestions around how best to improve equity in research and healthcare are neither easy nor straightforward, go far beyond the ways in which we use specific types of DL, and cannot be fixed through technological solutionism[morozov2013save]. Nevertheless, using small-scale DL can help mitigate certain types of inequities by reducing some of the barriers that are currently in place for researchers and practitioners with limited access to resources. Small-scale DL can be developed and run on end-point consumer hardware which is more pervasive than specialised datacenters with high performance computing in many parts of the world.\nWith this work, we sincerely hope that by focusing on reducing the resource costs of DL to improve access the larger question of equity in DL for healthcare will be grappled with by the community.\n\nAcknowledgments:\n\nRS, BP, CI, and ED acknowledge funding received under European Union\u2019s Horizon Europe Research and Innovation programme under grant agreements No. 101070284 and No. 101070408. CI acknowledges support by the Pioneer Centre for AI, DNRF grant number P1. GS would like to acknowledge Wellcome Foundation (grant number 222180/Z/20/Z).\n\nSECTION: Appendix AAdditional Results", "text_file": "data\\paper_texts\\2403.12562v2_content.txt"}, {"title": "Automated Medical Report Generation for ECG Data: Bridging Medical Text\n  and Signal Processing with Deep Learning", "authors": ["Amnon Bleich", "Antje Linnemann", "Bjoern H. Diem", "Tim OF Conrad"], "published_date": "2024-12-05T11:05:12Z", "summary": "Recent advances in deep learning and natural language generation have\nsignificantly improved image captioning, enabling automated, human-like\ndescriptions for visual content. In this work, we apply these captioning\ntechniques to generate clinician-like interpretations of ECG data. This study\nleverages existing ECG datasets accompanied by free-text reports authored by\nhealthcare professionals (HCPs) as training data. These reports, while often\ninconsistent, provide a valuable foundation for automated learning. We\nintroduce an encoder-decoder-based method that uses these reports to train\nmodels to generate detailed descriptions of ECG episodes. This represents a\nsignificant advancement in ECG analysis automation, with potential applications\nin zero-shot classification and automated clinical decision support.\n  The model is tested on various datasets, including both 1- and 12-lead ECGs.\nIt significantly outperforms the state-of-the-art reference model by Qiu et\nal., achieving a METEOR score of 55.53% compared to 24.51% achieved by the\nreference model. Furthermore, several key design choices are discussed,\nproviding a comprehensive overview of current challenges and innovations in\nthis domain.\n  The source codes for this research are publicly available in our Git\nrepository https://git.zib.de/ableich/ecg-comment-generation-public", "arxiv_id": "2412.04067v1", "html_link": "https://arxiv.org/html/2412.04067v1", "search_term": "ti:\"deep learning\"", "html_content": "SECTION: Automated Medical Report Generation for ECG Data: Bridging Medical Text and Signal Processing with Deep Learning\n\nRecent advances in deep learning and natural language generation have significantly improved image captioning, enabling automated, human-like descriptions for visual content. In this work, we apply these captioning techniques to generate clinician-like interpretations of ECG data. This study leverages existing ECG datasets accompanied by free-text reports authored by healthcare professionals (HCPs) as training data. These reports, while often inconsistent, provide a valuable foundation for automated learning. We introduce an encoder-decoder-based method that uses these reports to train models to generate detailed descriptions of ECG episodes. This represents a significant advancement in ECG analysis automation, with potential applications in zero-shot classification and automated clinical decision support.\n\nThe model is tested on various datasets, including both 1- and 12-lead ECGs. It significantly outperforms the state-of-the-art reference model by Qiu et al., achieving a METEOR score of 55.53% compared to 24.51% achieved by the reference model.\nFurthermore, several key design choices are discussed, providing a comprehensive overview of current challenges and innovations in this domain.\n\nThe source codes for this research are publicly available in our Git repository111https://git.zib.de/ableich/ecg-comment-generation-public.\n\nAttention mechanisms, Biomedical text generation, Clinical decision support, Deep learning, Electrocardiography (ECG), Encoder-decoder architectures, Image captioning adaptation, Long Short-Term Memory (LSTM), Medical datasets, Medical signal processing, Natural language processing (NLP), Neural networks, Transformers.\n\nSECTION: 1Introduction\n\nCardiovascular diseases remain a leading cause of morbidity and mortality worldwide, highlighting the importance of effective and timely diagnostic methods. Traditional ECG analysis, while indispensable, often requires manual interpretation by trained physicians, which can be time-consuming and subject to human error. At the same time, advances in AI-driven text generation algorithms present a novel opportunity to take advantage of existing ECG data sets for which physician-written free-text comments are available. These models have the potential to generate informative text about ECG episodes, which could offer decision support to clinicians during diagnostic tasks and thus reduce cognitive load on medical professionals and potentially improve the speed and accuracy of diagnosis.\n\nSignificant progress in encoder-decoder architectures, particularly in image captioning, has demonstrated their capacity for generating coherent descriptions[1,2]. While these methods have succeeded in domains such as image processing, their application to medical data, specifically electrocardiography (ECG), remains underexplored[3]. One major challenge is the scarcity of purpose-built, high-quality labeled datasets for ECG analysis, as generating such datasets requires the involvement of highly trained individuals, making the process resource-intensive[4]. However, a promising alternative lies in leveraging existing ECG records paired with free-text comments, which are often generated as a byproduct of routine clinical practices[5]. Although these free-text comments were not originally created for training machine learning models, they represent a rich source of data that can be adapted to train encoder-decoder architectures, capable of generating concise textual summaries of ECG episodes, as demonstrated in this work.\n\nThis study introduces an innovative approach to text generation designed for ECG analysis, validated on the publicly available PTB-XL dataset[6]. In addition to achieving state-of-the-art performance, a key contribution of this work is the establishment of a reproducible benchmark for future studies, as our work is, to the best of our knowledge, first of its kind to be tested on the official test subset of the PTB-XL dataset, ensuring both transparency and comparability.\nFurthermore, we present a case-study on a more challenging and larger dataset of single-lead, variate, and subcutaneous ECG (sECG) signals obtained from implantable cardiac monitors (ICMs)[4]. Unlike purpose-built datasets, this dataset comprises labels and free-text reports generated as a byproduct of routine clinical and administrative processes. These reports, often created to fulfill procedural requirements, have not been cross-checked for accuracy, introducing additional noise and complexity that make the dataset more difficult to leverage for automated learning. This case study highlights the robustness and adaptability of our method to diverse and less curated real-world data sources.\n\nThe method introduced in this study employs an encoder-decoder architecture, utilizing a ResNet-based encoder[7]paired with either an LSTM[8]or Transformer[9]decoder.\n\nIn the following sections, we provide an overview of related work to contextualize this study within the broader scientific landscape and facilitate comparison with existing methods. We then present a comprehensive description of our proposed model, detailing its architecture, tokenization techniques, data preprocessing procedures, and the hyperparameter optimization and training strategies employed. This is followed by a description of the datasets used, along with the specific preprocessing steps applied. Next, we outline the experiments conducted to evaluate the performance of our model and analyze the results obtained. Finally, we discuss the key findings and suggest directions for future research.\n\nSECTION: 2Related Work\n\nVarious methods have been proposed in recent years for the automatic diagnosis of ECG data (e.g.,[10,11,12]) and other feature-based heart data (e.g.,[13]). Stracina et al.[14]provide a comprehensive review of advancements and possibilities in ECG analysis since the first successful recording of the electrical activity of the human heart in 1887, including recent developments leveraging deep learning techniques.\n\nA key takeaway from these reviews is that the majority of existing ECG analysis methods focus primarily on the classification of ECG episodes, such as identifying atrial fibrillation (AF) or other abnormal heart rhythms. In contrast, the field of automated report generation for ECG data remains largely unexplored. Meanwhile, significant advancements in the task of generating textual descriptions from visual data (commonly referred to as image captioning) have been achieved through the development of sophisticated machine learning models and language models in particular. Seminal works in this domain include works by Vinyals et al.[1], and subsequently Xu et al.[2], which introduced an encoder-decoder architecture utilizing ResNet[7]as the encoder and a Long Short-Term Memory (LSTM) network[8]as the decoder to successfully generate descriptive text for images. This framework has since been extended to various domains, including medical signal processing. Pang et al.[3]conducted a survey of recent technologies in medical report generation, primarily addressing medical imaging, such as X-ray screenings, demonstrating the versatility and potential of such architectures. However this work offers limited insight into one-dimensional signals like ECG or EEG. This highlights a critical gap in the application of LLMs to medical data, emphasizing the need for further exploration in this area.\n\nGiven the substantial progress in automated data captioning and the relative scarcity of work in applying these techniques to ECG data, this area represents a significant research opportunity. Building on the foundational work of Vinyals et al. and Xu et al., this study explores the application of similar encoder-decoder architectures to the generation of free-text descriptions for ECG episodes, aiming to bridge this gap in the field.\n\nOne notable effort in this direction is the work by Qiu et al.[15], who investigated the potential of pre-trained large language models (LLMs) for ECG classification. They demonstrated that knowledge from natural language processing could be effectively transferred to ECG data, enabling the detection of cardiovascular diseases. Their approach involved fine-tuning pre-trained LLMs on the publicly available PTB-XL dataset[6], achieving highly encouraging performance. However, their method faced certain limitations, such as random data splitting, which can lead to overfitting, as evident by the minimal performance gains after incorporating the ECG data itself into the model (refer to Section6.4for further details). Since their work was not tested on the PTB-XL official splits and no other studies have reported results on publicly available datasets, we used their method as a reference model for performance comparison of our model (refer to Section5.2).\n\nBartels et al.[16]explored a method using a private dataset of ECG records with captions, partially machine encoded and corrected by healthcare professionals (HCPs), in addition to those generated by HCPs during routine medical practices. Their method used a pre-trained ECG classification ResNet (ECGNet[17]) with an additional classification layer to embed signals and feed the embedded signals into an LSTM or a Transformer. However, their approach did not include testing on publicly available datasets, which limits the generalizability of their model evaluations. Furthermore, they did not explore using non-pre-trained models or single-lead ECG signals, and their proposed model architecture lacked detailed explanation.\n\nIn conclusion, while caption generation for images has achieved satisfactory levels, the task remains complex for medical images and even more so for ECG data. Current advances in applying LLMs to ECG analysis are promising, but a proven approach to successfully evaluate public datasets with trustworthy evaluation metrics is still being investigated. Our work aims to address this gap by leveraging free-text data and exploring the potential of modern machine learning techniques in the challenging domain of ECG signal interpretation.\n\nSECTION: 3A New Method for ECG Caption Generation: Architecture and Training Optimization\n\nThis section introduces a novel approach for generating text descriptions of ECG data through encoder-decoder architectures, adapted from successful image captioning frameworks[1,2]. The method leverages a ResNet-based encoder for embedding ECG signals and incorporates Transformer or LSTM decoders to produce descriptive, clinically relevant reports.\n\nThe entire encoder-decoder architecture is visualized in Fig.2.\n\nSECTION: 3.1Encoder for ECG Embedding\n\nTo create an effective embedding for ECG signals, we employed a modified 34-layer ResNet architecture tailored for 1D inputs. The standard classification layer was removed and replaced with an average pooling layer, allowing for adjustable output sizes to preserve temporal information across 512 output channels. This adaptation enables the retention of temporal relationships, which together with attention mechanism in the decoder, can be utilized for ECG interpretation. The output embedding from this encoder forms the input to the decoding stage.\n\nSECTION: 3.2Decoder Architectures for Generating ECG Description\n\nThe decoder\u2019s task is to generate meaningful, contextual descriptions from the ECG embeddings. We tested two architectures for this purpose: a Transformer-based decoder and an LSTM-based decoder, described in detail below.\n\nThe Transformer decoder[9]receives a combined input of ECG embeddings and tokenized report sequences, leveraging self-attention to dynamically focus on different parts of the ECG signal during token prediction. The concatenation is formalized as follows:\n\nwheredenotes the embedding of the-th token in the report, andrepresents the features of the-th ECG segment provided by the encoder.is set by the user and determines the extent of downsampling of the ECG signal.\n\nUsing the combined input, the transformer calculates the context vectorat each time step:\n\nWhererepresents each element inwhich can be either an ECG-segment feature vectoror a token embedding. The attention weightsare calculated following the standard Transformer attention mechanism described in[9].\n\nThe LSTM decoder[8]generates one word at a time, conditioned on a context vector, the previous LSTM hidden state, and the last predicted token (or ground truth token during training). The attention mechanism dynamically weights different segments of the encoded ECG signal, creating a context vectorat each time step:\n\nwhereis the attention weight at time stepfor temporal segment, defined by:\n\nHere,,andare learnable weights within linear layers, andis the previous hidden state.\n\nUltimately, both the transformer and LSTM based models are trained using cross-entropy loss.\n\nAttention:In the realm of natural language processing (NLP), attention mechanisms have been shown to enhance the recognition of objects in images, as demonstrated by Ba et al.[18]. These mechanisms, which focus on relevant parts of the data, are hypothesized to enable the model to dynamically focus on relevant portions of the ECG signal and thus improve the quality of generated descriptions.\n\nWhile self-attention is natively integrated into the Transformer\u2019s structure and can be adjusted to attend to different segments of the ECG (refer to equation2), it is externally added in our LSTM-based model. In this case, in each prediction stepwe use anvector, sized to match the size of each channel output by the encoder (240 in our best-performing model, which represents temporal encoding of the ECG signal), and perform weighted aggregation of it. This results in a single weighted value for each encoder output channel, conditioned on the networks previous state. Refer to equation4for details about the calculation of. By applying weighted aggregation to the channels of the downsampled ECG signal using weights that account for both the ECG signal and the current network state, the model is able to attend to different parts of the ECG when predicting each word in the generated report. An example to this attention driven prediction process is visualized in Fig.1. Finally, as in[2], doubly stochastic attention regularization is applied to prevent overfitting. This encourages the model to distribute its attention more uniformly, avoiding a tendency to over-focus on specific time steps and ensuring diverse attention across the signal.\n\nSECTION: 3.3Training Optimization and Enhancement Techniques\n\nOur training pipeline incorporates several optimization strategies to improve model performance and ensure reliable generation of ECG descriptions. Key techniques include encoder pre-training and targeted hyperparameter tuning.\n\nEncoder Pre-Training:In one configuration, we pre-trained the encoder using rhythm class labels as a preliminary task to strengthen ECG embeddings before fine-tuning on free-text reports. This pre-training phase was intended to leverage rhythm labels for additional contextual learning; Moreover, as described in Section4.1the ICM dataset includes approximately 738K episodes with rhythm class labels but no accompanying reports, enabling us to tap into this large dataset, which would otherwise not be utilized. Pre-training resulted in an F-score of 0.69 on the PTB-XL dataset (6 classes) and 0.49 on the ICM dataset (11 classes).\n\nHyperparameters Tuning:Extensive hyperparameter tuning was conducted to optimize model performance and stability. Key parameters were adjusted as follows:\n\nEncoder architecture: We tested both ResNet-18 and ResNet-34, aiming to optimize both model performance and computational efficiency.\n\nDecoder architecture: We varied the layer depth for both LSTM and Transformer decoders, from a single layer up to 20 layers.\n\nLearning rates: Separate learning rates were optimized for the encoder and decoder.\n\nStopping Criteria: We explored various early stopping criteria such as cross-entropy loss (which was also used for back-propagation), BLEU-1, BLEU-4 and METEOR scores.\n\nAdditional Parameters:We also tuned various parameters, including embedding sizes, batch sizes, teacher-forcing probabilities[19], normalization techniques (at batch, dataset, and episode levels), and top-k sampling. These adjustments contributed to enhancing model robustness and performance consistency.\n\nThis comprehensive tuning process involved multiple repetitions of experiments to reduce the impact of random variations, resulting in a stable and reliable evaluation of model performance. The best-performing parameter configurations are as follows:\n\nEncoder:A learning rate ofwas optimal, with a stem kernel of 9 in the first convolutional layer and subsequent ResNet stage kernel sizes of 9, 7, 7, and 5, respectively. Each of the 512 output channels was set to an output size of 240.\n\nTransformer:The best configuration used 12 layers and 8 attention heads, a learning rate of, and an ECG input size of(as described in Section3.2, subsectionTransformer Decoder).\n\nLSTM:We used a single LSTM layer with a learning rate ofa teacher forcing probability of 1 during training (disabled during validation and testing), and an attention layer of size 512.\n\nFor both our LSTM and Transformer based models we used batch size of 32, no normalization, cross-entropy as the loss function and Adam optimizer. The token embedding size was set to 512, the dropout rate to 0.5 and the learning rate decay factor to 0.8 every 8 epochs without improvement, with the METEOR score (see section5.4under METEOR) as the target metric. Top-k sampling of 1 was used for deterministic token selection.\n\nSECTION: 3.4Complexity & Runtime\n\nThe model was trained on a single node equipped with 8 CPU cores, 62.5 GB of memory, and one NVIDIA A100 GPU (80 GB memory). The runtime was estimated based on training both the encoder and decoder from scratch (without pre-training) on the PTB-XL dataset, using the optimal hyperparameters described in section3.\n\nEncoder:The ResNet-34 architecture contains 13.76M parameters.\n\nDecoder:The Transformer decoder, with 38.36M parameters, has an estimated runtime of approximately 7 hours and 10 minutes on the specified hardware for training the entire model, including the encoder (without encoder pre-training). By comparison, the LSTM decoder\u2014with 5.52M parameters\u2014completes training in roughly 1 hour and 10 minutes (in both cases, runtime depends on the number of epochs before early stopping).\n\nWhile the best-performing Transformer model uses 12 layers, reducing this to 8 layers provides a runtime-optimized alternative with minimal impact on performance. This modification reduces the parameter count to 25.75M and shortens the runtime accordingly. Similarly, using ResNet-18 instead of ResNet-34 reduces the encoder\u2019s parameter count to 6.93M without substantially affecting accuracy.\n\nNote: Both the Transformer and the LSTM training are not optimized in terms of runtime and, presumably a relatively minor effort could decrease it significantly.\nFurthermore, using a pre-trained encoder reduces the runtime by about 10% (not including the time for pre-training, which, if included, results in a longer training time overall).\n\nSECTION: 4Data & Preprocessing\n\nIn this section, we introduce the datasets used for training and evaluating our ECG caption generation model. We outline the main characteristics of each dataset, followed by the preprocessing techniques employed to ensure compatibility with our model architecture. These steps are critical for enhancing model performance and ensuring robust, generalizable results.\n\nSECTION: 4.1Data\n\nThe success of our model relies on access to annotated ECG data that captures a wide range of cardiac conditions and data quality.\n\nTo achieve this, we selected two complementary datasets: the publicly available PTB-XL dataset, which provides clinical-grade, multi-lead ECG recordings, and the proprietary dataset provided by BIOTRONIK SE & Co KG, comprising single-lead sECG data from implantable cardiac monitor (ICM) devices. This combination allows us to evaluate the model\u2019s adaptability to varying ECG formats, sampling rates, and recording environments, thus ensuring its relevance for both clinical and personal health monitoring applications.\n\nThe PTB-XL dataset[6,20]comprises 21,801 10-second ECG episodes, with two optional sampling frequencies: 500 Hz (resulting in 5000 data points per ECG record) and 100 Hz (1000 data points per record). These episodes were collected from 18,869 unique patients, giving aunique patient proportionof 0.87 - indicating that 87% of the total episode count corresponds to unique individuals, while the remaining episodes are contributed by patients with multiple recordings. Each episode is annotated with a rhythm label and a corresponding report (for details see[6]).\n\nThe dataset includes 9839 unique reports, where uniqueness is determined by ignoring minor differences such as whitespace, case sensitivity, and punctuation. This results in a unique report proportion of 0.45, meaning that 45% of the total reports are distinct, with the remainder being repeated across multiple episodes. Using this dataset served multiple purposes in our research. Firstly, it enabled us to test our model on a publicly available dataset, ensuring the reproducibility of our results. Additionally, it allowed us to assess the model\u2019s performance on 12-lead, 500 or 100 Hz, 10-second ECG records and evaluate how it manages a relatively small sample size. Furthermore, we used this dataset for comparability with the reference model, which was also tested on PTB-XL[15](on 100Hz).\n\nThe ICM dataset contains records from Implantable Cardiac Monitors (ICMs) BIOMONITOR III and BIOMONITOR IIIm[21], which consists of 60-second sECG episodes recorded at a sampling frequency of 128 Hz, resulting in 7680 data points per sECG record. The report and rhythm class (a single label selected by HCPs out of given list) data were assembled as a byproduct of routine medical procedures conducted worldwide. The anonymized data underwent additional filtering to ensure that all personal information was removed from the manually entered reports. To avoid translation issues only reports in English were used. The final report dataset (Part 2below) comprised data from 1033 clinics. The dataset is divided into two parts:\n\nPart 1:737,999 episodes, each labeled with a rhythm class provided by an HCP. These labels are a byproduct of routine medical procedures and were not cross-checked for accuracy and are therefore potentially inaccurate.\n\nPart 2:206,768 episodes, where each episode includes both a rhythm class and a free-text report: a sentence written by an HCP as part of routine medical procedures, not necessarily intended as an ECG caption for learning or training purposes. These episodes come from 6687 implanted devices. Among these 200k episodes, there are 38,652 unique reports, (here too, ignoring minor differences like whitespace, case sensitivity, and punctuation), resulting in a unique report proportion of 0.19.\n\nSECTION: 4.2Data Pre-Processing\n\nConsistent data preprocessing ensures compatibility with our model architecture and optimizes performance by standardizing data across both datasets. Steps such as train-validation-test splits, abbreviation unification, and tokenization were carefully designed to maintain data integrity while enhancing comparability.\n\nIn the ICM dataset, we applied an episode deduplication process. An episode was considered duplicated if it originated from the same implanted device, had the same report, rhythm class, and recording date. During deduplication, one episode from each group of duplicates was randomly selected, and the others were removed from the dataset.\n\nTo prevent data leakage and enhance model generalizability, we applied device-level splits for training, validation, and testing. This approach ensures that each patient\u2019s data is confined to one split, reducing the risk of overfitting and supporting a more meaningful evaluation of the model\u2019s performance.\n\nPTB-XL:We tested two splitting approaches for this dataset. The first approach used the official splits provided by PhysioNet, which maintain patient exclusivity across splits and mitigate data leakage risks. These splits allocate 80% of data for training, 10% for validation, and 10% for testing, as further detailed in[6]. The second approach, used for comparability with the reference model[15], involved a random split into training, validation, and testing sets with proportions of 64%, 16%, and 20%, respectively. Results from both approaches are presented in Section6, Tables2and3.\n\nICM:For this dataset, splits were applied at the implanted device level to ensure no overlap between the training, validation, and test sets, preventing the model from leveraging morphological similarities across episodes from the same device. After deduplication, data was divided into training (80%), validation (10%), and test (10%) sets.\n\nDue to medical terminology variations in the datasets, an abbreviation unification process was applied to reduce prediction errors. Additionally, the OPUS-MT model[22,23]was used to translate German reports from the PTB-XL dataset into English, allowing for consistent application of abbreviation standardization and enhancing model compatibility. The abbreviations and their unified form are presented in Table1.\n\nSECTION: 4.3Report Tokenization\n\nReport tokenization was conducted by splitting comments based on non-letter characters (such as spaces, commas, slashes, etc.). These splitting characters were tokenized and retained within the text, with the exception of spaces. Spaces were not tokenized but assumed to occur between every two tokens to avoid skewing evaluation metrics (e.g., METEOR or BLEU-score) due to the abundance of spaces, which would otherwise inflate scores and reduce the relative significance of other tokens.\n\nThis process yielded an initial vocabulary size of 5,304 for the ICM dataset and 2,282 for the translated PTB-XL dataset, or 7,015 and 2,455, respectively, without applying the abbreviation script. As abbreviations only apply to English, they were not used on the non-translated PTB-XL reports. Filtering out words occurring fewer than twice further reduced the ICM and PTB-XL datasets vocabularies to 3,194 and 1,383 tokens, respectively. To ensure comparability across datasets, we truncated each vocabulary, retaining only the 1,024 most frequent tokens. Note that after vocabulary truncation, the least frequent word of the PTB-XL has a frequency of 3 and that of the ICM dataset a frequency of 14. Therefore, in the presented case filtering tokens based on their frequency deemed redundant.\n\nEach report was prefixed with a start token and suffixed with an end token, with padding tokens added to a maximum report length of 300 tokens (although in practice, no reports exceeded this length). A special token was assigned for unknown tokens: words appearing only once or outside the top 1,024 most common tokens.\n\nThe Byte-Pair-Encoding (BPE) method[24]was also tested, but it performed suboptimally compared to word tokenization, likely due to the specialized terminology in these datasets.\n\nIn summary, the PTB-XL and ICM datasets provide diverse ECG data well-suited for training and evaluation of our caption generation model. By carefully preprocessing the data, including patient/device-level splits, translation and abbreviation unification, we enhanced our model\u2019s ability to generate precise, contextually relevant ECG descriptions.\n\nSECTION: 5Experiments\n\nIn this section, we summarize extensive evaluations of our model across multiple design configurations (Section5.1) and its comparison with a state-of-the-art method (see Section5.2). Additionally, we present the results of our case study (Section6.3) and a sanity check conducted to confirm that the model\u2019s performance is attributable to its ability to learn ECG morphology (Section6.4).\n\nSECTION: 5.1Key Experiments\n\nThroughout our experimentation, we addressed several key research questions by running a series of targeted experiments. Table2, summarizes the performance of our model on the publicly available PTB-XL dataset across the following main variations:\n\nDecoder Architecture (LSTM vs. Transformer):As the choice of decoder architecture is fundamental to our model, we tested all configurations with both LSTM and Transformer decoders, allowing us to compare overall performance between the two.\n\nEncoder Depth (ResNet18 vs. ResNet34):To determine whether the depth of the ResNet encoder affects model performance, we experimented with both ResNet18 and ResNet34 encoders.\n\nEffect of Encoder Pre-Training:To quantitatively evaluate whether pre-training the encoder on rhythm class labels improves model performance, we compared results with and without encoder pre-training. In our case-study, as discussed in Section4.1, such pre-training also leverages an additional 738K episodes.\n\nTranslation to English:Since our study primarily uses English as the target language, we conducted all experiments with text translated to English. To maintain reproducibility, however, we also ran an experiment without translation or abbreviation unification, testing the model on raw text.\n\nAbbreviation Unification vs. Raw Text:To test the impact of the abbreviation unification process described in Section4.2, we compared results on such standardized text against results on the raw text. As the abbreviation unification applies exclusively to English, we included translation for this comparison as well.\n\nSECTION: 5.2Comparison with the Reference Model\n\nTo evaluate the effectiveness of our approach, we benchmarked its performance against the reference model proposed by Qiu et al.[15]. This model employs a ResNet-based encoder for feature extraction from ECG signals, which are then aligned with pretrained embeddings from large language models (LLMs) such as GPT-2 and BERT. Additionally, Qiu et al. introduced an Optimal Transport (OT)-based objective complementing the standard cross-entropy loss, to enhance alignment between ECG embeddings and language embeddings. Their method has achieved strong results in ECG disease classification and report generation tasks, particularly on the PTB-XL dataset, making it a relevant benchmark for comparison.\n\nHowever, Qiu et al.\u2019s use of random train/validation/test splits raises concerns regarding patient data overlap, potentially inflating results due to shared patient data across splits. Such overlap is problematic for generalizability, a critical factor for clinical applications. To address this, we adopt more rigorous splitting methods in our experiments, including the official PTB-XL splits that ensure patient exclusivity across sets, thereby enhancing reliability and comparability.\n\nA detailed comparison between the reference model and our model\u2019s performance is provided in Section6, particularly in Table3.\n\nSECTION: 5.3Experimental Setup\n\nOur primary experiments were conducted on the PTB-XL dataset, which serves as a comprehensive benchmark in this domain. To ensure reproducibility and generalizability, we used the train/validation/test splits recommended by the PTB-XL authors. These fixed splits enhance reliability as they are not random, and the validation and test datasets are considered gold standards, comprising reports manually reviewed by professionals[6].\n\nTo facilitate a direct comparison with the reference model, which is evaluated on random splits, we also tested our best-performing models on comparable random splits, applying translation but excluding abbreviation unification to align with the reference approach. Results from this experiment are presented in Table3.\n\nAdditionally, as a case-study, we evaluated our model on the ICM dataset. This dataset represents an increasingly relevant data source generated by personal ECG devices, and its automated analysis is of growing importance[4]. The results for this case-study can be found in Table4.\n\nFor detailed explanation of the different datasets used in the different setups please refer to section4.\n\nLastly, we performed asanity checkby substituting the ECG input with a uniform vector in which all entries are set to 1, effectively removing the ECG signal to assess whether the model was generating reports based on repetitive textual patterns rather than meaningful ECG data. The performance of our model in this test is discussed in Section6.4.\n\nSECTION: 5.4Evaluation\n\nEvaluating generated text is a challenging task, as even humans may struggle to assess the similarity in meaning between two sentences. We used three key metrics suited for such evaluation:\n\nBLEU[25]measures the precision of N-grams (1- to 4-grams) between generated and reference texts, applying a brevity penalty for shorter outputs to account for recall. This metric is widely used for its simplicity, but it emphasizes precision over recall.\n\nMETEOR[26]improves on BLEU by incorporating both precision and recall, offering a more balanced assessment. It also includes word-order sensitivity and explicit word-matching (exact, stemmed, and synonym-based), making it suitable for tasks where accurate phrasing is critical.\n\nROUGE[27], originally designed to emphasize recall (R), is particularly useful for assessing the completeness of information, as is often required in summarization tasks. However, beyond Recall, ROUGE also includes Precision (P) and F1-score (F), providing a more comprehensive evaluation of generated text.\n\nAnother metric worth noting isMRScore[28](preprint), which utilizes large language models (LLMs) such as GPT to evaluate generated reports based on human-like criteria. While it has not yet gained widespread adoption in clinical text generation, it represents a promising direction for incorporating LLM-based assessments in the future.\n\nMETEOR was chosen as our primary metric due to its ability to evaluate semantic similarity more effectively than BLEU or ROUGE. BLEU\u2019s reliance on exact matches makes it less suitable for tasks like clinical text generation, where paraphrasing or synonym usage is common. ROUGE, while including precision and F1-score, is primarily recall-focused, which can bias evaluations towards longer outputs. METEOR, on the other hand, balances precision and recall while incorporating linguistic features like stemming and synonyms, making it better suited for capturing subtle nuances in clinical reports.\n\nSECTION: 6Results\n\nSECTION: 6.1Key experiments\n\nThe results of our main experiments, as shown in Table2, indicate that our model outperforms the current state-of-the-art reference method across all metrics. This improvement is achieved even under a more rigorous setup, using patient-exclusive test sets, compared to the random splits used in the reference model.\n\nOur findings suggest that the optimal configuration employs a non-pretrained ResNet34 as the encoder and an LSTM as the decoder. However, performance across other configurations did not differ significantly. Notably, the performance gap between ResNet34 and the more compact ResNet18 encoder was minimal, suggesting that ResNet18 is a practical alternative when hardware or time constraints are considerations. This setup, combined with an LSTM decoder and bypassing time-intensive translation steps, still yielded comparable results.\n\nMoreover, pre-training the encoder on episode rhythm labels did not improve model performance; in fact, it appeared to slightly diminish predictive accuracy. Translating the reports to English, however, resulted in a noticeable METEOR score boost from 50.72% for the non-translated version to 55.01% for the translated version in the top-performing architecture. Abbreviation unification had a minor yet positive impact (about 0.5%) on the scores for translated reports.\n\nFig.3shows the progression of the METEOR score over training epochs for key experiments. The plot suggests the absence of overfitting and reveals that the LSTM-based model achieves accuracy comparable to the Transformer-based model in nearly half the epochs.\n\nSECTION: 6.2Experiment with the Reference Model Setup\n\nTo ensure a fair comparison, we conducted an additional experiment replicating the setup used by the reference model (detailed in Section5.3). The results, presented in Table3, demonstrate that both our LSTM-based and Transformer-based models outperform the reference model, with the LSTM-based model showing a slight advantage over the Transformer-based approach.\n\nSECTION: 6.3Case Study - Single lead, subcutaneous ECG\n\nIn our case-study, we evaluated performance on single-lead, subcutaneous ECG data with reports from ICM devices (See Section4.1). The model\u2019s performance on this dataset, though lower than on PTB-XL, still surpassed current state-of-the-art benchmarks, demonstrating the model\u2019s robustness in challenging data scenarios. Notably, the Transformer and LSTM-based models performed similarly, with the Transformer showing a slight edge, likely due to minor random variations.\n\nNotably, despite the 738K episodes containing only rhythm labels and thus available only for pre-training (alongside200K episodes with labels as well as reports which are used for both pre-training and training), pre-training the encoder did not enhance performance on this dataset.\n\nThe comparatively lower BLEU-4 score for the ICM dataset may be due to the brevity of the reports (median length of 4 tokens and an average of 5.67), reducing 4-gram match probability between generated and reference texts. This is supported by the relatively higher BLEU-1 and BLEU-2 scores, indicating more accurate matching in shorter text segments.\n\nLastly, abbreviation unification markedly improved performance on this dataset. While its effect was minimal on PTB-XL, it led to a pronounced increase (from 15.57% to 32.59%) on the ICM dataset. This is likely because a significant portion (31.2%) of the PTB-XL dataset is automatically generated, which may result in a higher degree of uniformity in terms and language. In contrast, the ICM dataset exhibits greater variability in terminology, an inconsistency mitigated effectively by abbreviation unification.\n\nSECTION: 6.4Sanity Check\n\nTo ensure our models were generating text based on ECG data rather than relying on common text patterns, we performed a sanity check. Here, we replaced the ECG input with a uniform vector in which all entries are set to, effectively removing ECG information. A successful sanity check would result in a substantial drop in performance, confirming that the model\u2019s output is informed by ECG morphology. As expected, this experiment led to prediction of uniform comments of optimal length comprising the most common words in the corpus.\n\nThe sanity check was conducted both on the PTB-XL and the ICM datasets, post-abbreviation unification.\nThe results on the ICM dataset demonstrate an 8.52% drop in METEOR score, from 32.59% in the regular experiment to 24.07% in the sanity check and a drop of 2.11% in BLEU4 score, from 10.61% in the regular experiment to 8.5% in the sanity check, reflecting 26% and 19% reduction in these scores respectively. While the relatively high performance in the sanity check suggests some dependency on recurring tokens, the observed drop highlights the model\u2019s ability to incorporate meaningful ECG data, even in the relatively challenging ICM dataset.\n\nAs expected, the sanity check performed on the official splits of the PTB-XL dataset resulted in a more pronounced performance drop across various metrics. For instance, BLEU4 decreased from 0.35 in the original experiment to 0.08 in the sanity check, reflecting a 77% reduction. METEOR score dropped from 0.56 to 0.31, a 45% reduction.\n\nThese results further validate the model\u2019s reliance on ECG morphology.\n\nSECTION: 7Discussion and Future Work\n\nThe successful application of free-text generation techniques to ECG episodes using an architecture traditionally employed in image captioning represents a notable advancement. It underscores the potential for adapting image-captioning methods to analyze and generate free-text descriptions for medical signals, thereby expanding the scope of cross-domain applications of such techniques.\n\nA central challenge identified in this study is the absence of a well-established, large-scale benchmark dataset specifically for ECG-based report generation. Although the PTB-XL dataset provides a foundation for evaluation, its relatively limited size poses constraints on training more sophisticated language models. An anticipated alternative is the MIMIC-IV-Notes dataset, which is expected to be linked with the MIMIC-IV-ECG dataset[29]. Future evaluation of our proposed model on this dataset could yield a more comprehensive performance assessment, and we anticipate that with larger datasets, the efficiency and accuracy benefits of Transformer-based models may become more pronounced. We also encourage other research groups to use this dataset upon its release to foster a standardized benchmark for the field.\n\nFurthermore, we would like to highlight the importance of our experiments that did not produce our best-performing results, as we believe these acknowledgments are valuable for future research, guiding it toward or away from potential approaches. Examples include our pre-training strategies, translation of reports versus non-translation, lack of abbreviation unification, and the exploration of different combinations thereof. Furthermore, we conducted numerous hyperparameter tests\u2014though not the entire field\u2014due to time constraints and limited computational resources. We encourage future researchers to examine these suboptimal experiments carefully and learn what might or might not be worth attempting in their own studies.\n\nFinally, building on the demonstrated adaptability of image captioning architectures to 1-dimensional medical data in this work, a particularly promising avenue for further exploration lies in applying these methods to other 1D datasets such as EEG (electroencephalogram), as well as data from other domains.\n\nSECTION: 8Conclusion\n\nThis work demonstrates the feasibility of generating clinically relevant free-text comments for ECG episodes by employing architectures commonly used in tasks like image captioning. Both LSTM-based and Transformer-based models achieved strong performance, with the LSTM-based model displaying overall advantage due to efficiency and training time.\n\nWhile these results are promising, they only represent an initial step toward fully automated ECG report generation. The current model, though effective in generating insightful text, is not yet sufficiently refined for standalone use in clinical settings. However, its performance suggests that even in its current state, the model could serve as a valuable assistive tool, providing diagnostic insights to physicians or enabling efficient reporting for large-scale ECG datasets.\n\nIn conclusion, this work introduces a state-of-the-art approach to generating ECG reports across a diverse range of episode characteristics, including variations in lead count, resolution, and report quality. The comprehensive pipeline and methodology developed here offer a robust foundation for further research and advancement, supporting future investigations into automated medical report generation and related topics.\n\nSECTION: Acknowledgment\n\nThis work was supported by the German Ministry for Education and Research (BMBF) within the Berlin Institute for the Foundations of Learning and Data\u2014BIFOLD (project grants 01IS18025A and 01IS18037I) and the Forschungscampus MODAL (project grant 3FO18501).\n\nSECTION: References\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.04067v1_content.txt"}]], ["ti:\"embeddings\"", [{"title": "Enhancing Cross-Language Code Translation via Task-Specific Embedding\n  Alignment in Retrieval-Augmented Generation", "authors": ["Manish Bhattarai", "Minh Vu", "Javier E. Santos", "Ismael Boureima", "Daniel O' Malley"], "published_date": "2024-12-06T16:22:32Z", "summary": "We introduce a novel method to enhance cross-language code translation from\nFortran to C++ by integrating task-specific embedding alignment into a\nRetrieval-Augmented Generation (RAG) framework. Unlike conventional retrieval\napproaches that utilize generic embeddings agnostic to the downstream task, our\nstrategy aligns the retrieval model directly with the objective of maximizing\ntranslation quality, as quantified by the CodeBLEU metric. This alignment\nensures that the embeddings are semantically and syntactically meaningful for\nthe specific code translation task. Our methodology involves constructing a\ndataset of 25,000 Fortran code snippets sourced from Stack-V2 dataset and\ngenerating their corresponding C++ translations using the LLaMA 3.1-8B language\nmodel. We compute pairwise CodeBLEU scores between the generated translations\nand ground truth examples to capture fine-grained similarities. These scores\nserve as supervision signals in a contrastive learning framework, where we\noptimize the embedding model to retrieve Fortran-C++ pairs that are most\nbeneficial for improving the language model's translation performance. By\nintegrating these CodeBLEU-optimized embeddings into the RAG framework, our\napproach significantly enhances both retrieval accuracy and code generation\nquality over methods employing generic embeddings. On the HPC Fortran2C++\ndataset, our method elevates the average CodeBLEU score from 0.64 to 0.73,\nachieving a 14% relative improvement. On the Numerical Recipes dataset, we\nobserve an increase from 0.52 to 0.60, marking a 15% relative improvement.\nImportantly, these gains are realized without any fine-tuning of the language\nmodel, underscoring the efficiency and practicality of our approach.", "arxiv_id": "2412.05159v1", "html_link": "https://arxiv.org/html/2412.05159v1", "search_term": "ti:\"embeddings\"", "html_content": "SECTION: Enhancing Cross-Language Code Translation via Task-Specific Embedding Alignment in Retrieval-Augmented Generation\n\nWe introduce a novel method to enhance cross-language code translation from Fortran to C++ by integrating task-specific embedding alignment into a Retrieval-Augmented Generation (RAG) framework. Unlike conventional retrieval approaches that utilize generic embeddings agnostic to the downstream task, our strategy aligns the retrieval model directly with the objective of maximizing translation quality, as quantified by the CodeBLEU metric. This alignment ensures that the embeddings are semantically and syntactically meaningful for the specific code translation task. Our methodology involves constructing a dataset of 25,000 Fortran code snippets sourced from Stack-V2 dataset and generating their corresponding C++ translations using the LLaMA 3.1-8B language model. We compute pairwise CodeBLEU scores between the generated translations and ground truth examples to capture fine-grained similarities. These scores serve as supervision signals in a contrastive learning framework, where we optimize the embedding model to retrieve Fortran-C++ pairs that are most beneficial for improving the language model\u2019s translation performance. By integrating these CodeBLEU-optimized embeddings into the RAG framework, our approach significantly enhances both retrieval accuracy and code generation quality over methods employing generic embeddings. On the HPC Fortran2C++ dataset, our method elevates the average CodeBLEU score from 0.64 to 0.73, achieving a 14% relative improvement. On the Numerical Recipes dataset, we observe an increase from 0.52 to 0.60, marking a 15% relative improvement. Importantly, these gains are realized without any fine-tuning of the language model, underscoring the efficiency and practicality of our approach.\n\nEnhancing Cross-Language Code Translation via Task-Specific Embedding Alignment in Retrieval-Augmented Generation\n\nManish Bhattarai1,\nMinh Vu1,\n Javier E. Santos2,Ismael Boureima1,Daniel O\u2019 Malley2,1Theoretical Division, Los Alamos National Laboratory, Los Alamos, NM 87544,2Earth & Environmental Science Division, Los Alamos National Laboratory, Los Alamos, NM 87544Correspondence:ceodspspectrum@lanl.gov\n\nSECTION: 1Introduction\n\nCross-language code translation is a critical task in modern software development, especially as legacy programming languages, such as Fortran, continue to be prevalent in scientific computing, while more contemporary languages like C++ are favored for their performance and versatility in production environments. The goal of automatic translation from Fortran to C++ is to preserve the functionality and structure of legacy code while benefiting from the optimizations and ecosystem of C++. However, achieving high-quality translations that adhere to the syntax and semantic norms of the target language remains a challenging problem, particularly when there is a lack of large, aligned datasets or evaluation metrics that cover both source and target languages effectively.\n\nTraditional approaches to cross-language translation, such as Retrieval-Augmented Generation (RAG)(Lewis et\u00a0al.,2020)typically involve two phases: first, retrieving relevant examples from a database, followed by a language model generating code conditioned on both the query and the retrieved examples. In prior efforts, the retrieval models in RAG systems have relied on general-purpose embedding models(Bhattarai et\u00a0al.,2024;Li et\u00a0al.,), which are not tailored to the specific nuances of code translation. These embeddings aim to retrieve relevant pairs from the source and target languages but do not directly optimize for the quality of the generated code. As a result, while the retrieved examples may be relevant in a broad sense, they often fail to guide the language model towards producing translations that maximize fidelity to the ground truth in the target language.\nThis gap is particularly problematic in scenarios where explicit metrics, such as CodeBLEURen et\u00a0al. (2020)-designed to assess both syntactic and semantic correctness of translated code\u2014are only available for the target language (e.g., C++ in this case). Without aligning the retrieval mechanism to such a task-specific metric, the system may retrieve suboptimal examples, leading to poor code generation performance. The inability to leverage task-relevant quality metrics during retrieval weakens the overall system, limiting its effectiveness in high-accuracy code translation tasks.\nTo address these limitations, we propose a novel contrastive learning framework that aligns the retrieval phase of the RAG system with the goal of maximizing the CodeBLEU(Feng et\u00a0al.,2020)score for the generated C++ code. We collect a dataset of 25,000 Fortran code examples from Stack V2(Lozhkov et\u00a0al.,2024)and use the LLaMA 3.1-8B(Touvron et\u00a0al.,2023)model to generate corresponding C++ translations. In the absence of ground truth C++ translations, we evaluate the quality of these translations using pairwise CodeBLEU similarity scores. This metric captures both syntactic correctness and semantic fidelity, providing a robust signal for aligning the retrieval model through contrastive learning.\n\nThe proposed approach aims to addresses the shortcomings of general-purpose embedding models by integrating task-specific metrics into the retrieval optimization process. By aligning the retrieval model with the downstream task of producing high-quality C++ code, our method ensures that the examples retrieved during inference are not just broadly similar but are semantically and syntactically aligned in a way that enhances the LLM\u2019s generative performance. The result is a significant improvement in translation quality, as measured by CodeBLEU, over previous methods that lack such alignment.\n\nOur contribution is twofold: first, we demonstrate the effectiveness of contrastive learning for fine-tuning retrieval models in the context of cross-language code translation, using a task-specific metric to guide alignment. Second, we show that optimizing retrieval for downstream generation tasks can lead to state-of-the-art results, particularly in cases where aligned datasets are not readily available for both source and target languages. This work not only advances the field of code translation but also opens up new possibilities for applying similar techniques to other language pairs and domains where task-specific evaluation metrics are available for only one side of the translation.\n\nSECTION: 2Related Work\n\nHistorically, code translation strategies before the advent of LLMs relied heavily on rule-based and statistical machine translation (SMT) systems(Koehn,2009). These systems used predefined rules or statistical mappings between the source and target programming languages, such as tree-based translation approaches that mapped syntax trees between languages. While these methods provided structured and interpretable outputs, they were limited in their ability to handle the semantic complexities of different programming languages and struggled with code diversity, edge cases, and idiomatic translations.\n\nWith the rise of deep learning and LLMs, fine-tuning models on large datasets became the go-to method for improving code translation. Models like CodeBERT(Feng et\u00a0al.,2020)and Codex(Chen et\u00a0al.,2021), when fine-tuned on specific language pairs, improved translation quality by leveraging vast amounts of parallel code data. However, the main limitation of LLM fine-tuning lies in the resource-intensive process. Fine-tuning requires substantial amounts of labeled data and computational resources, making it impractical for niche or legacy languages like Fortran, where parallel data may be scarce.\n\nAs a next step, task-specific alignment of LLMs emerged to improve translation by better guiding the model\u2019s output. While alignment techniques help improve output fidelity, they still necessitate fine-tuning or explicit modification of the LLM itself, which can be resource-intensive and may still fall short of generalization when translating between languages with significant structural differences(Mishra et\u00a0al.,2024).\n\nRAG introduced a more flexible approach by allowing LLMs to retrieve and condition their outputs on example pairs from a relevant dataset. While RAG improves translation by augmenting the model\u2019s input, the effectiveness of this strategy depends on the quality and relevance of the retrieved examples. In an example case(Bhattarai et\u00a0al.,2024), the retrieval step relies on general-purpose embeddings like Nomic-Embed or CodeBERT, which, although effective at retrieving semantically similar code, are not optimized for specific downstream metrics like CodeBLEU. As a result, the LLM might not always retrieve the examples that would best assist in producing translations aligned with target-specific quality metrics.\n\nThe approach we propose offers a significant advantage by focusing on semantic alignment of the retrieval mechanism without the need to fine-tune the LLM itself. Through contrastive learning, we optimize the embedding model to retrieve Fortran-C++ pairs that are more likely to maximize the downstream metric (e.g., CodeBLEU) when used by the LLM for generation. This strategy ensures that the most relevant examples are retrieved for each translation task, improving the generation quality without requiring computationally expensive fine-tuning of the LLM. This retrieval alignment makes RAG more efficient and better suited for translating between languages where high-quality paired datasets may not be available. By concentrating on improving the quality of retrieved examples, our method achieves high-quality translation with minimal additional model training, leveraging existing LLM capabilities more effectively.\n\nSECTION: 3Methods\n\nThis section provides the technical description of our proposed method.\n\nSECTION: 3.1Problem setting\n\nWe consider the standard code translation scenario leveraging a language model, in which a target translated codeof a query source codeis generated using:\n\nIn practice, conditioningonexample pairs of source and target code, can significantly enhance translation. This few-shot learning approach can be expressed as:\n\nIn a RAG framework, this process is further refined by integrating a retrieval mechanismthat identifies the most pertinentexample pairs from a large corpusbased on the query. By expressing this retrieval step as, we can describe the conventional translation scenario leveragingas\n\nIn practice, the input source code are embedded using a neural network, which are generally agnostic to the downstream task. We denoteas the embedding of the source codeunder the embedding. Hence, Eq.2can be expressed as\n\nunder the usage of the embedding model. Here, the notationrefers to the fact that the embedding is applied onto the corpus of.\n\nSome common embedding modules for code translation are Nomic-EmbedNussbaum et\u00a0al. (2024), StarEncoder(Li et\u00a0al.,2023), and CodeBERTFeng et\u00a0al. (2020). However, as the performance of the translation task heavily depends on the relevance and the alignment of the retrieved examples with respect to the query, as we will show in the following discussion, it is beneficial to optimizefor better code translation performance. In this manuscript, we use notationandto refer to aligned and unaligned embedding modules, respectively.\n\nSECTION: 3.2Task-Specific Embedding Alignment\n\nOur method involves aligning the Fortran embedding modelusing contrastive learning based on CodeBLEU similarity scores, followed by applying this aligned model within a RAG framework for improved cross-language code translation from Fortran to C++, as shown in Figure1I.\n\nEmbedding Similarity:Given a pre-trained embedding module, we directly leverage the CodeBLEU similarity computed from the language modelto train an aligned embedding modulefor the downstream code translation task. The following discusses how to extract the CodeBLEU similarity from.\n\nFrom a source dataset of Fortran code snippets, we generate the corresponding C++ translationsusingwithout RAG retrieval:\n\nThen, we compute the pairwise CodeBLEU similarity scores(Ren et\u00a0al.,2020)between all generated translation pairs:\n\nwhere the CodeBLEU score matrixis a weighted linear combination of four components: the n-gram match, the weighted n-gram match, the syntactic AST match, and the semantic data flow match. These components capture the syntactic and semantic similarities between the generated C++ translations:\n\nis the traditional BLEU score up to n-grams.\n\nassigns weights to n-grams based on their importance.\n\nmeasures the similarity between the abstract syntax trees (AST) of the code snippets.\n\nassesses the similarity in data flow between code snippets.\n\nIntuitively, a high value ofindicates that the source code snippetsand, when translated by, produce similar target code, suggesting thatandare semantically similar with respect to the translation task. Therefore, our approach aims to learn a fine-tuned embedding modulethat utilizesto enhance code embedding alignment. The approach is expected to guidein a way that enhances the code translation task leveraging.\n\nEmbedding Alignment:\n\nTo align the embedding space of code snippets with the semantic similarities measured by CodeBLEU, we propose the Soft Information Noise-Contrastive Estimation (S-InfoNCE) loss applied to the embeddings resulting from the trainable embedding module. On a high level, our proposed S-InfoNCE can be considered a soft version of the InfoNCE loss proposed for contrastive learning(van\u00a0den Oord et\u00a0al.,2018).\n\nGiven a batch ofcode snippets, we compute their embeddingsand then calculate the pairwise cosine similarities between those embeddings, scaled by a temperature parameter:\n\nOur proposed S-InfoNCE loss integrates these continuous similarity scores to weigh the contribution of each pair. Specifically, the loss component between codewith respect to codeis given as:\n\nand the S-InfoNCE loss is the sum over all code pairs:\n\nFinally, the embeddingis optimized by minimizingusing gradient descent.\n\nCompared to the conventional InfoNCE loss for contrastive learning(van\u00a0den Oord et\u00a0al.,2018), our proposed loss differs in its usage ofas a soft indicator for encoding a continuous similarity between the pair, rather than a binary indicator of class membership (same class or not). This gives rise to the termsoftInfoNCE, or S-InfoNCE. In the typical InfoNCE loss, the termis included only if the pairbelongs to the same class, assuming discrete classes are available. However, since such discrete class labels do not exist in the code translation task, we adoptas a soft version of this indicator function, allowing for a more nuanced representation of similarity between code pairs.\n\nThe stationary points of the S-InfoNCE loss (Equation8) satisfy:\n\nfor all.\n\nFurthermore, the optimal loss is the weighted sum of the entropy of the CodeBLEU similarity distribution for each input code:\n\nwhereis the entropy function andis a probability vector whose-th component is\n\nFor brevity, let us define:\n\n: the CodeBLEU similarity between the target code translationsand.\n\n, where: the normalized exponential of the cosine similarity between the embeddings of source code snippetsand.\n\nThe S-InfoNCE loss can be rewritten as:\n\nOur goal is to minimizewith respect to. This can be viewed as a constrained optimization problem over the variables, subject to the normalization constraints:\n\nWe formulate the Lagrangianas:", "text_file": "data\\paper_texts\\2412.05159v1_content.txt"}, {"title": "Multimodal Fact-Checking with Vision Language Models: A Probing\n  Classifier based Solution with Embedding Strategies", "authors": ["Recep Firat Cekinel", "Pinar Karagoz", "Cagri Coltekin"], "published_date": "2024-12-06T16:13:19Z", "summary": "This study evaluates the effectiveness of Vision Language Models (VLMs) in\nrepresenting and utilizing multimodal content for fact-checking. To be more\nspecific, we investigate whether incorporating multimodal content improves\nperformance compared to text-only models and how well VLMs utilize text and\nimage information to enhance misinformation detection. Furthermore we propose a\nprobing classifier based solution using VLMs. Our approach extracts embeddings\nfrom the last hidden layer of selected VLMs and inputs them into a neural\nprobing classifier for multi-class veracity classification. Through a series of\nexperiments on two fact-checking datasets, we demonstrate that while\nmultimodality can enhance performance, fusing separate embeddings from text and\nimage encoders yielded superior results compared to using VLM embeddings.\nFurthermore, the proposed neural classifier significantly outperformed KNN and\nSVM baselines in leveraging extracted embeddings, highlighting its\neffectiveness for multimodal fact-checking.", "arxiv_id": "2412.05155v1", "html_link": "https://arxiv.org/html/2412.05155v1", "search_term": "ti:\"embeddings\"", "html_content": "SECTION: Multimodal Fact-Checking with Vision Language Models: A\nProbing Classifier based Solution with Embedding Strategies\n\nThis study evaluates the effectiveness of Vision Language Models (VLMs) in representing and utilizing multimodal content for fact-checking. To be more specific, we investigate whether incorporating multimodal content improves performance compared to text-only models and how well VLMs utilize text and image information to enhance misinformation detection. Furthermore we propose a probing classifier based solution using VLMs. Our approach extracts embeddings from the last hidden layer of selected VLMs and inputs them into a neural probing classifier for multi-class veracity classification. Through a series of experiments on two fact-checking datasets, we demonstrate that while multimodality can enhance performance, fusing separate embeddings from text and image encoders yielded superior results compared to using VLM embeddings. Furthermore, the proposed neural classifier significantly outperformed KNN and SVM baselines in leveraging extracted embeddings, highlighting its effectiveness for multimodal fact-checking.\n\nMultimodal Fact-Checking with Vision Language Models: A\nProbing Classifier based Solution with Embedding Strategies\n\nRecep Firat Cekinel1,\nPinar Karagoz1,\n\u00c7a\u011fr\u0131 \u00c7\u00f6ltekin2,1Middle East Technical University, Turkiye2University of T\u00fcbingen, GermanyCorrespondence:rfcekinel@ceng.metu.edu.tr\n\nSECTION: 1Introduction\n\nSocial media platforms are increasingly becoming the primary source of news for many people. However, these platforms are susceptible to the rapid spread of fake stories, which can be used to manipulate public opinionAllcott and Gentzkow (2017). Fabricated posts may include false text, images, videos, or speech contentAlam et\u00a0al. (2022); Akhtar et\u00a0al. (2023); Comito et\u00a0al. (2023), designed to deceive social media users. Therefore, automated fact-checking systems should be able to consider information from different modalitiesAbdali et\u00a0al. (2024). For instance, on the Snopes website, a claim111https://www.snopes.com/fact-check/hitler-trump-image-fake/about an edited image was proven to be fake by providing the original image and explaining how it was fabricated to manipulate public opinion about public figures. To verify the truthfulness of such content, it is essential to process both text and image information (see Figure1).\n\nA vision language model (VLM) consists of an image encoder, a text encoder and a mechanism such as contrastive learningBordes et\u00a0al. (2024)and cross attentionChen et\u00a0al. (2022)to fuse text and image information. By this way, the model leverages the text and visual information while generating a response text. VLMs consist of billions of parameters and fine-tuning these models requires significant computational resources. Although parameter-efficient fine-tuning approachesHu et\u00a0al. (2022); Liu et\u00a0al. (2024c)have proven to be very effective for large language models, VLMs do not scale well horizontally. Consequently, such VLMs cannot be fine-tuned with moderate batch size and sequence length on a single GPU for problems like fact-checking that requires long text inputs.\n\nInstead of fine-tuning, probing classifiers are trained on the representations of a pre-trained modelKunz and Kuhlmann (2020)to predict linguistic features such as dependency parsingAdelmann et\u00a0al. (2021)and POS taggingKunz and Kuhlmann (2021). A key advantage of probing classifiers is their ability to assess how well the pre-trained model has captured linguistic properties. In this study, we aim to evaluate how VLMs leverage both text and images for the fact-checking task by training a probing classifier. The following research questions are addressed in the paper.\n\nRQ1: Validating the need for multimodality:Does incorporating multimodal data improve performance in the fact-checking task or are text-only models sufficient?\n\nRQ2: Leveraging multimodal content:How effectively do VLMs utilize both text and image information to enhance fact-checking performance?\n\nRQ3: Evaluating probing classifiers:How does a probing neural classifier compare to baseline models in the context of the fact-checking task?\n\nThis study proposes a probing classifier that involves extracting the last hidden layer\u2019s representation and using it as input for a neural network. By introducing this pipeline, we aim to elaborate on the utilization of multimodal information, text and image, compared to embeddings extracted from discrete text-only and image-only models for the fact-checking problem. The source code is available at the following anonymousGitHub repository222https://github.com/firatcekinel/Multimodal-Fact-Checking-with-Vision-Language-Models\n\nSECTION: 2Related Work\n\nShared tasks such as FEVERThorne et\u00a0al. (2018), CLEF2018Nakov et\u00a0al. (2018)and AVeriTeCSchlichtkrull et\u00a0al. (2023)evaluate fact-checking systems on textual claims. Although LLMs achieved high success rates on fact-checking with English data even in zero-shot settingsHoes et\u00a0al. (2023),Zhang et\u00a0al. (2024)emphasize the need for language models that are specifically pre-trained on the target language. SimilarlyCekinel et\u00a0al. (2024)investigate cross-lingual transfer learning using LLMs. Additionally,Cheung and Lam (2023)incorporate external evidence during instruction-tuning to enhance the knowledge of LLMs. Moreover,Yue et\u00a0al. (2023)focus on cross-domain knowledge transfer with in-context learning.Tang et\u00a0al. (2024)verify the factuality of synthetically generated claims against grounding documents. LLMs are also used for explanation generationBangerter et\u00a0al. (2024); Zeng and Gao (2024); Mediratta et\u00a0al. (2024)and neuro-symbolic program generationPan et\u00a0al. (2023)for fact-checking. While these works primarily focus on enhancing models\u2019 knowledge, we aim to explore how they can leverage different modalities.\n\nWhile SpotFake+Singhal et\u00a0al. (2020)concatenates extracted text and image features for further processing through feed-forward layers, CARMNSong et\u00a0al. (2021)fuses multimodal information using a cross-modal attention residual network. Pre-CoFactv2Du et\u00a0al. (2023)implements a multi-type fusion model that uses cross-modality and cross-type relations. COOLANTWang et\u00a0al. (2023)implemented a contrastive learning based fusion method for image-text alignment.Gao et\u00a0al. (2024)incorporates the information extracted from the tweet graph with text and image embeddings for improving fake news detection.Liu et\u00a0al. (2024b)examined the impact of audio in multimodal fact-checking by proposing a framework that fuses text, video and audio information with the cross-attention mechanism.Wang et\u00a0al. (2024a)align news text with images by cross-modal attention model.\n\nGeng et\u00a0al. (2024)propose an evaluation framework for VLMs that assesses the pre-trained knowledge of these models in fact-checking without evidence. RAGARKhaliq et\u00a0al. (2024)presents a RAG-based model that reframes the problem as question-answering for retrieved evidence pieces. MMIDRWang et\u00a0al. (2024b)trains a distilled model to generate explanations. SARD frameworkYan et\u00a0al. (2024)applies multimodal semantic alignment to integrate multimodal network features. LVLM4FVTahmasebi et\u00a0al. (2024)is an evidence-ranking approach and was evaluated on two benchmark datasets using LLMs and VLMs with zero-shot setting.\n\nAlthough recent studies have focused on developing multimodal models for fact-checking using various fusion approaches, we aim to explore how effectively VLMs utilize different modalities.Geng et\u00a0al. (2024)also evaluated the robustness of recent VLMs for this problem by comparing the pre-trained knowledge of selected models and their prediction accuracy and confidence rates in zero-shot and few-shot settings. In contrast, we aim to leverage VLM representations by proposing a pipeline that trains a classifier using these embeddings. Furthermore, our primary focus is on utilizing multimodal information. In the experiments, we evaluate the intrinsic fusion of multimodal information against the extrinsic fusion of separate text-only and image-only representations.\n\nSECTION: 3The Proposed Method\n\nSECTION: 3.1Feed-Forward Veracity Classifier\n\nWe introduce a probing classifier to examine the efficiency of multimodal embeddings compared to separate embeddings extracted from text-only and image-only models for veracity prediction. The VLM embeddings fuse text and image modalities intrinsically but distinct text and image encoder embeddings are fused extrinsically by the probing classifier as illustrated in Figure2.\n\nFirst, the last hidden layer representation is extracted from a VLM or a text/image encoder. The neural classifier either receives the VLM representation or embeddings from the corresponding text encoder and image encoder, then predicts veracity classes. If multiple input tensors are fed to the neural classifier, they are processed by a linear layer and after the first layer, all tensors are resized to a \"hidden_size\" \u2014 a hyper-parameter determined by validation experiments \u2014 and then concatenated. We concatenate after the first layer because the text and image embedding sizes vary significantly. To utilize both types of information equally, we resize these embeddings to the same dimension and concatenate them afterward. On the other hand, if only the VLM embedding is given to the network as input, two linear layers process the tensor sequentially without any concatenation.\n\nIn both of the probing classifier architectures, we implement a weighted cross-entropy loss, with weights determined by inverse class ratios to penalize the majority class more. Since PyTorch\u2019s cross-entropy loss implementation combines softmax with negative log-likelihood loss, the output tensor predicts class probabilities. Consequently, the classifier predicts the class with the highest probability for a given instance.\n\nSECTION: 3.2Models\n\nThe primary goal of this study is to examine whether merging image and text information provides gains for the fact-checking problem. To this end, we selected three multimodal models with different fusion mechanisms, as explained below.\n\nQwen-VLBai et\u00a0al. (2023b)is a multimodal model introduced by Alibaba Cloud. Qwen-VL is based on the Qwen-7BBai et\u00a0al. (2023a)language model and Openclip\u2019s ViT-bigGIlharco et\u00a0al. (2021)vision transformer. The model leverages both modalities through a cross-attention mechanism. Information from the vision encoder is fused into the language model using a single-layer cross-attention adapter with query embeddings optimized during the training phase. In this study, we employedQwen-VL-Chat-Int4checkpoint which was the 4-bit quantized version.\n\nIdefics2Lauren\u00e7on et\u00a0al. (2024)is a general-purpose multimodal VLM introduced by Huggingface. It is based on the Mistral-7BJiang et\u00a0al. (2023)language model and SigLIP\u2019s vision encoderZhai et\u00a0al. (2023)(SigLIP-So400m/14). The model employs a vision-language connector that takes the vision encoder\u2019s representation as input, using perceiver pooling and MLP modality projection. After these operations, the image information is concatenated with the encoded text representation and fed into the language model decoder.\n\nPaliGemmaBeyer et\u00a0al. (2024)is introduced by Google and is based on the Gemma-2BTeam et\u00a0al. (2024)language model and SigLIP\u2019s vision encoderZhai et\u00a0al. (2023)(SigLIP-So400m/14). Since Gemma-2B is a decoder-only language model, the vision encoder\u2019s representation is fed into a linear projection, concatenated with text inputs, and then fed into the Gemma-2B language model for text generation. In this study, we employedpaligemma-3b-mix-448checkpoint that was fine-tuned on a mixture of downstream tasks.\n\nSECTION: 3.3Datasets\n\nMochegYao et\u00a0al. (2023)consists of 15K fact-checked claims from Politifact333https://www.politifact.com/and Snopes.444https://www.snopes.com/These websites employ journalists to verify claims who collect evidence documents and write ruling comments. The Mocheg dataset includes both text and image evidence which were crawled from the reference articles linked on the fact-checked claims\u2019 webpages. In cases where multiple evidence images were available for a claim, some collected images were found to be irrelevant. Therefore, for the experiments, only the first image was used as the evidence image.\n\nFactify2Suryavardan et\u00a0al. (2023)is a challenge dataset containing 50K claims. The authors collected true claims from tweets by Indian and US news agencies and false claims from fact-checking websites. They scraped text and image evidence from external articles and also collected claim images from the headlines of the claims. The fact-verification task was reformulated as an entailment problem where claims were annotated to indicate whether the claim text and image were entailed by the evidence text and image.\n\nSECTION: 4Experiments\n\nWe conducted experiments on compute nodes with 4x40GB Nvidia A100 GPUs. While evaluating the models on the datasets, we ignore the instances that have missing text evidence or images. For the Mocheg dataset, we used the original train-dev-test splits. The dataset has three labels\"supported\",\"refuted\"and\"not enough info (NEI)\"and we used the labels as it is.\n\nRegarding the Factify2 dataset, since the labels in the test set were unavailable, the original validation data was kept for testing. Instead, we randomly selected 10% of the training set for validation but kept the same percentages of classes in each split. Similar toTahmasebi et\u00a0al. (2024), we reduced the original five labels to three classes:Support(Support_Multimodal & Support_Text),RefuteandNot enough info(Insufficient_Multimodal & Insufficient_Text) to evaluate the proposed approach.\n\nDuring the training of the probing classifier using the embeddings, validation experiments were conducted through grid search within the parameter space detailed below. Note that only the best parameter settings are presented in AppendixA. Last but not least, we reported F1-macro scores and F1 scores for each class in the following experiments.\n\nSECTION: 4.1Zero-Shot Inference\n\nIn this experiment, we evaluated the zero-shot inference performance of text-only language models and multimodal VLMs on selected datasets. The text-only models were the same language models used in the VLMs for text processing. The purpose of reporting the results on text-only models is to examine the necessity of image content for the fact-checking problem.\n\nFor the text-only models, the claim and evidence text were provided as a single prompt, as illustrated in Figure3. Similarly, for each claim statement, the evidence text and evidence image were fed to the VLMs using a similar prompt template. Note that we reported results only for instances where the models responded with \"supported,\" \"refuted,\" or \"not enough info.\" In other words, if the models did not provide a relevant justification, these cases were excluded from the reported results.\n\nAssess\u00a0the\u00a0factuality\u00a0of\u00a0the\u00a0following\u00a0claim\u00a0byconsidering\u00a0evidence.\u00a0Only\u00a0answer\u00a0\"supported\",\"refuted\"\u00a0or\u00a0\"not\u00a0enough\u00a0info\".Claim:\u00a0{claim}Evidence:\u00a0{evidence}\n\nWe also reported the performance of two baseline models, LVLM4VTahmasebi et\u00a0al. (2024)and MOCHEGYao et\u00a0al. (2023), for comparison. MOCHEG concatenates the claim, evidence and image to generate CLIPRadford et\u00a0al. (2021)representations, employing attention mechanisms to update the claim representation based on the evidence. LVLM4V uses two-level prompting, formulating the problem as two binary questions and utilizing the MistralJiang et\u00a0al. (2023)and LLaVaLiu et\u00a0al. (2024a)models.\n\nF1-macro scores along with F1 scores for each class are presented in Table1for both text-only and multimodal models. The results show that multimodality can enhance performance depending on the dataset and model configuration. For example, both Idefics-8b and LVLM4FV consistently outperformed their text-only counterparts, while Qwen-VL performed slightly better on the Factify2 dataset but worse on the Mocheg dataset. In contrast, PaliGemma consistently responded with, \"sorry, as a base VLM I am not trained to answer this question\" to test queries, suggesting that specific policies were implemented in the base VLM to prevent responses to ambiguous queries. As a result, PaliGemma\u2019s inference performance was significantly lower than that of its language model counterpart, Gemma-2b (see AppendixBfor response frequencies). The inference scores of Idefics2-8b suggest that images may provide additional information for fact-checking, likely due to its fine-tuning on a mixture of supervised and instruction datasets, which could explain its success on these datasets. Additionally, LVLM4V\u2019s prompting strategy appears more efficient, as it first checks whether the evidence is sufficient for verification before issuing a second prompt to verify or refute the claim.\n\nA qualitative analysis was conducted to explore the types of claims that were correctly predicted by multimodal models but incorrectly predicted by text-only models. In this analysis, the predictions from both the text-only (Mistral-7B) and multimodal (Idefics2-8b) models were employed on the Mocheg dataset. Although for the fact-checking problem, textual contents are the primary source, images are shown to be useful. After examining the instances that are correctly predicted by the VLM but misclassified by the LLM, we found that such instances required image information to accurately verify the claims, as illustrated in Figure4.\n\nFact-checking requires long evidence with supporting images, making it computationally challenging to fine-tune the VLMs with moderate batch sizes and sequence lengths on a single GPU. Therefore, we fine-tuned only thePaliGemma-3b-pt-224checkpoint using claim, evidence and claim image as input. The experimental details are given in AppendixC.\n\nEvidence in the Mocheg dataset was collected from reference web articles. In contrast, Factify2 used the justifications provided by fact-checkers as evidence. As a result, Factify2\u2019s evidence is more concise and self-explanatory. However, models should interpret the knowledge from Mocheg\u2019s evidence sources to make a final decision. Because of the GPU memory considerations, evidence texts were cropped if they exceeded 768 words.\n\nFine-tuning results, presented in Table2, show a significantly lower score of 0.366 on the Mocheg dataset compared to inference results, due to cropping of the evidence text. However, on the Factify2 dataset, the evidence texts were shorter and the model leveraged the key information for making a decision and achieved 0.835 F1-macro score. Note that, on the Factify2 challenge the best-performing model was LogicallyGao et\u00a0al. (2021)which was also fine-tuned on Factify2 dataset and it achieved 0.897 F1-macro score. Due to computational constraints, we were unable to utilize the long text evidence, particularly in the Mocheg dataset. As a result, we introduced a probing classifier instead of fine-tuning the selected VLMs.\n\nSECTION: 4.2Intrinsic Fusion of VLM Embeddings\n\nIn this experiment, we examined whether inherently multimodal models effectively utilize both text and image information. First, we extracted embeddings from selected VLMs and fed these vector representations into a feed-forward multi-class classifier. We extracted the last hidden states and applied mean pooling to each token\u2019s embedding. In other words, the extracted embedding size was(1, ntokens, ndim), wherentokensis the number of tokens andndimis the dimension of each token embedding. Mean pooling provided a single embedding for each instance.\n\nWe provided two sets of inputs for extracting embeddings:mm_claimandmm_evidence. Themm_claiminput consists of a claim and a corresponding image while themm_evidenceinput consists of text evidence and an evidence image. For the second setting, we fed two input vectors to the classifier network: themm_claimembedding and themm_evidenceembedding. This is becausemm_evidenceincludes only the evidence representation - evidence image and evidence text - so we provided the claim information by feeding a second input to the classifier.\n\nAccording to Table3, themm_evidenceinput setting improved F1-macro scores consistently for all models. This indicates that using both text and image evidence improved classification performance on both datasets. The results suggest that the selected VLMs effectively leverage information from evidence text and images on both the Mocheg and Factify2 datasets.\n\nSECTION: 4.3Extrinsic Fusion of Language Model and Vision Encoder Embeddings\n\nSeparate embeddings were extracted for text and image information from the vision encoders and language models, respectively. Afterward, we performed mean pooling to obtain one-dimensional vector representations for each instance. For this experiment, we had four input setups:\n\nInput1 (claim+image):The claim representation was taken from the language model and the corresponding image representation was taken from the vision transformer.\n\nInput2 (claim+claim_image+text+text_image):In addition to Input1, the evidence text representation was extracted from the language model and the evidence image representation was extracted from the vision transformer.\n\nInput3 (mm_claim+mm_image):The embeddings extracted when the claim text is given to the VLM and the embeddings extracted when only the claim image is given were used separately.\n\nInput4 (mm_text+mm_image):The embeddings extracted when all textual content is given to the VLM and the embeddings extracted when only the images are given were used separately.\n\nInputs, except Input2, had two separate text and image embeddings. Only the second setup had four embeddings: claim embedding, claim image embedding, text embedding, and text image embedding. After extracting the embeddings, we trained the proposed probing classifier as described in Section3.1for multi-class veracity prediction. We extracted the embeddings for Input1 and Input2 using the selected multimodels\u2019 text and vision encoders that were also mentioned in Section3.2.\n\nAccording to Table4, Idefics2 with the third input setup outperformed the other models on both datasets. Note that Idefics2 also performed better in zero-shot evaluations which could indicate that the model might have encountered similar data during pre-training. Therefore, it may leverage its pre-training knowledge while processing these claims.\n\nSECTION: 4.4Ablation Study\n\nOur feed-forward classifier, illustrated in Figure2, consists of two sequential linear layers. The first layer resizes each input tensor to a \"hidden size\" before concatenating the tensors. We chose this approach because there was a significant difference between the image and text embedding sizes. By reshaping each tensor to the same size before concatenation, we aimed to utilize both types of information more effectively.\n\nHowever, this approach has some limitations. If concatenation were performed before the first hidden layer, linear layers would be common for all models and input setups. In our approach, only the layers after concatenation are common so as the number of inputs increases, the number of learned parameters for the non-common layers also increases. Additionally, we did not validate the depth of the neural classifier and the network depth might be too shallow for the veracity detection task.\n\nTo assess whether the neural classifier effectively learns the intended task, we conducted an experiment using KNN and SVM classifiers with the same training embeddings as mentioned in Section4.2. We set the number of neighbors (k), to seven which was decided after exploring consecutive values. Similarly, we trained SVM classifier with a linear kernel. As shown in Table5, our approach outperformed the baselines on both datasets which implies that the proposed neural classifier leveraged the embeddings much better than the KNN and SVM classifiers on both datasets.\n\nSECTION: 5Discussion\n\nFirst, we addressed RQ1 by conducting a zero-shot experiment to verify that multimodality improves performance depending on the dataset and model configuration, with models like Idefics-8b and LVLM4FV outperforming their text-only counterparts. Idefics2-8b benefits from image information while LVLM4V\u2019s efficient prompting strategy further enhances verification accuracy.\n\nAdditionally, the proposed intrinsic fusion pipeline which utilizes VLM embeddings, outperformed the VLMs\u2019 base inference performance (see Table1and Table3). The only exception was the Idefics2 model on the Mocheg dataset, which had a 0.517 F1-macro inference score while the classifier achieved only a 0.501 F1-macro score. Since the probing classifier has only two layers, it might be too shallow for this dataset and model. Note that the primary goal of this study is not to achieve state-of-the-art scores for the selected datasets. Instead, we aim to evaluate whether recent VLMs improve performance on the fact-checking problem through multimodality or if fusing externally the information from distinct models achieves superior results.\n\nSecondly, we addressed RQ2 by assessing how VLMs leverage text and image information. According to the results, for Idefics2-8b and Qwen-VL, multimodal embeddings were outperformed by discrete models (see Table3and Table4). In other words, extracting separate embeddings resulted in higher F1-macro scores across all models. To be more specific, on the Mocheg dataset, the highest F1-macro scores for Qwen-VL and Idefics-8b were 0.514 and 0.528 respectively. Similarly, on the Factify2 dataset, the highest F1-macro scores were 0.629, 0.670 and 0.590 respectively. Although the best results were achieved with different input setups, for all of the best results, we extracted separate text and image embeddings. In contrast, when embeddings were extracted from inherently multimodal VLMs (as shown in Table3), the maximum F1-macro scores were lower except PaliGemma-3b on Mocheg dataset. This indicates that for the given evaluation framework, using discrete text and image embeddings yielded higher F1-macro scores.\n\nBesides, RQ3 was addressed by conducting an ablation study to examine how the proposed classifier leverages embeddings against KNN and SVM baselines. According to our evaluations, the proposed classifier utilized the extracted embeddings significantly better than the baseline approaches.\n\nFinally, on the Mocheg dataset, the selected models struggle more on \"not enough info\" cases, as their lowest success rates, even in the best settings, were consistently associated with this class. This may be due to class relabeling, where the authors of the Mocheg dataset reannotated the \"Mixture,\" \"Unproven,\" and \"Multiple\" cases as \"Not Enough Info\" which may lead to confusion for the models. In contrast, on the Factify2 dataset, the trained classifier was more successful in distinguishing fake claims compared to other classes. This could be linked to the difference of data domains, as the genuine news was sourced from news agencies while fake claims were crawled from fact-checking sites and satirical articles.\n\nSECTION: 6Conclusion\n\nIn this study, we utilize VLMs for multimodal fact-checking and propose a probing classifier-based approach. The proposed pipeline extracts embeddings from the last hidden layer of selected VLMs and fuses multimodal embeddings (extrinsic or intrinsic) into a simple feed-forward neural network for multi-class veracity classification. The experiments show that employing a probing classifier is more effective than the base VLM performance and extrinsic fusion usually outperforms the intrinsic fusion for the proposed approach. As future work, we plan to employ VLMs as assistants rather than as primary fact-checkers. To be more specific, the VLM can be used as an assistant that reviews the given text and image and returns a summary or justification to guide the text-only model for the fact-checking task. Since the LLMs are prone to hallucination and their accuracy depends on the quality of their training data which may be outdated or biased, incorporating knowledge grounding could be a more reliable strategy for real-world deployment.\n\nSECTION: 7Limitations\n\nWe tested a limited number of models which may not fully capture the variability across different models and configurations. Additionally, the evaluations were performed on English datasets, restricting the assessment of multilingual capabilities. Furthermore, there is a potential risk that some dataset instances may overlap with the training data of the VLMs which could bias the evaluation results.\n\nMoreover, while extracting embeddings from the selected VLMs and corresponding LLMs, we encountered some computational overhead. More specifically, for some claims, the evidence field exceeded the sequence length of the models or could not fit within our memory constraints. Therefore, we cropped the evidence fields for such instances. Furthermore, while LLMs and VLMs are prone to hallucination, we did not perform any analysis on this phenomenon within the scope of this study.\n\nSECTION: Acknowledgments\n\nThis research is supported by the Scientific and Technological Research Council of Turkey (TUBITAK, Prog: 2214-A) and the German Academic Exchange Service (DAAD, Prog: 57645447). We would like to thank the anonymous reviewers for their suggestions to improve the study. We also appreciate METU-ROMER and the University of T\u00fcbingen for providing the computational resources.\n\nThis project is partially supported by METU with grant no ADEP-312-2024-11484. Parts of this research received the support of the EXA4MIND project, funded\nby the European Union\u00b4s Horizon Europe Research and Innovation Programme,\nunder Grant Agreement N\u00b0 101092944. Views and opinions expressed are\nhowever those of the author(s) only and do not necessarily reflect those\nof the European Union or the European Commission. Neither the European\nUnion nor the granting authority can be held responsible for them.\n\nSECTION: References\n\nSECTION: Appendix AHyperparameter Values for the Best Models\n\nWe set the number of epochs to 20, enabling early stopping with the patience of 5 and monitoring the validation loss. We used the Adam optimizer in combination with a cosine scheduler, employing a warm-up ratio of 0.05. Moreover, we adjusted the cross-entropy loss weight of the neural network according to the inverse class ratios. In this way, the classifier was penalized more for the misclassifications of the minority classes.\n\nWe performed a grid search to explore the following parameter space for the results given in Table3and Table4:\n\nlearning rate: { 0.00001, 0.0001, 0.001, 0.01, 0.1},batch size: {32, 64, 128},hidden size(h in Figure2): {128, 256, 512 } anddropout: {0.05, 0.1, 0.2, 0.4}.\n\nThe parameter settings for the best results are detailed in Table6.\n\nSECTION: Appendix BZero-shot Model Response Frequencies\n\nWe used the prompt template shown in Figure3for all models in the zero-shot inference experiments. We expected the models\u2019 responses to contain either \"supported,\" \"refuted,\" or \"not enough info.\" If a model\u2019s response did not contain these labels, we ignored those instances. Additionally, we observed that PaliGemma consistently responded with \"sorry, as a base VLM I am not trained to answer this question,\" which could be due to injected policies. The frequencies of considered cases for each model (with percentages in parenthesis) are given in Table7.\n\nSECTION: Appendix CFine-tuning Parameter Settings\n\nWe employed QLoRADettmers et\u00a0al. (2024)adapter on top of attention weight matrices and fine-tuned only the LoRAHu et\u00a0al. (2022)adapters for 3 epochs. The batch size was set to 2 with an initial learning rate of 2e-5 using a cosine scheduler and the Adam optimizer. We used the checkpoint with the lowest validation loss. Additionally, we set warm up to 0.02, gradient accumulation to 4 and evaluated on validation set 10 times during fine-tuning. We set the rank of matrices for LoRA adapters to 16, the scaling factor (lora_alpha) to 16 and the dropout rate for the adapters to 0.05. Besides, 16-bit mixed precision, bfloat16, was employed for memory efficiency and faster fine-tuning.", "text_file": "data\\paper_texts\\2412.05155v1_content.txt"}, {"title": "StyleDiffusion: Prompt-Embedding Inversion for Text-Based Editing", "authors": ["Senmao Li", "Joost van de Weijer", "Taihang Hu", "Fahad Shahbaz Khan", "Qibin Hou", "Yaxing Wang", "Jian Yang", "Ming-Ming Cheng"], "published_date": "2023-03-28T00:16:45Z", "summary": "A significant research effort is focused on exploiting the amazing capacities\nof pretrained diffusion models for the editing of images.They either finetune\nthe model, or invert the image in the latent space of the pretrained model.\nHowever, they suffer from two problems: (1) Unsatisfying results for selected\nregions and unexpected changes in non-selected regions.(2) They require careful\ntext prompt editing where the prompt should include all visual objects in the\ninput image.To address this, we propose two improvements: (1) Only optimizing\nthe input of the value linear network in the cross-attention layers is\nsufficiently powerful to reconstruct a real image. (2) We propose attention\nregularization to preserve the object-like attention maps after reconstruction\nand editing, enabling us to obtain accurate style editing without invoking\nsignificant structural changes. We further improve the editing technique that\nis used for the unconditional branch of classifier-free guidance as used by\nP2P. Extensive experimental prompt-editing results on a variety of images\ndemonstrate qualitatively and quantitatively that our method has superior\nediting capabilities compared to existing and concurrent works. See our\naccompanying code in Stylediffusion:\n\\url{https://github.com/sen-mao/StyleDiffusion}.", "arxiv_id": "2303.15649v3", "html_link": "https://arxiv.org/html/2303.15649v3", "search_term": "ti:\"embeddings\"", "html_content": "SECTION: 1Introduction\n\nText-based deep generative models have achieved extensive adoption in the field of image synthesis. Notably, GANs[50,20,31,59], diffusion models[58,53,17,56], autoregressive models[74], and their hybrid counterparts have been prominently utilized in this domain.\n\nDiffusion models\nhave made remarkable progress due to their exceptional realism and diversity. It has seen rapid applications in other domains such as video generation[33,69,79], 3D generation[51,41,67]and speech synthesis[29,27,36]. In this work, we focus on Stable Diffusion (SD) models for real image editing.\nThis capability is important for many real-world applications, including object replacement in images for publicity purposes (as well as personalized publicity), dataset enrichment where rare objects are added to datasets (e.g. wheelchairs in autonomous driving datasets), furniture replacement for interior design, etc.\n\nResearchers basically perform real image editing with two steps:projectionandmanipulation. The former aims to either adapt the weights of the model[60,43,34,71]or project given\nimages into the latent code or embedding space of SD models[46,18,5,22].\nThe latter aims to edit the latent code or embedding to further manipulate real images[32,44,10,75,13]. In the firstprojectionstep, some works finetune the whole model[32,65,57,34,19,71]or partial weights of the pretrained model[37,72]. Yet, finetuning either the entire or part of the generative model with only a few real images suffers from both the cumbersome tuning of the model\u2019s weights and catastrophic forgetting[37,68,72].\nOther works onprojectionattempt to learn a new embedding vector which represents given real images (keeping the SD model frozen)[26,18,5,22,61,77,16]. They focus on optimizing conditional or unconditional inputs of the cross-attention layers of the classifier-free diffusion model[26].\nTextual Inversion[18]uses the denoising loss to optimize the textual embedding of the conditional branch given a few content-similar images.\nNull-text optimization[46]firstly inverts the real image into a series of timestep-related latent codes, then leverages a reconstruction loss to learn the null-text embedding of the unconditional branch (see Fig.1(middle)).\nHowever, these methods suffer from the following issues.Firstly, they lead to unsatisfactory results for selected regions, and unexpected changes in non-selected regions, both during reconstruction and editing (see the Null-text results in Fig.2; the structure-dist () for Null-text Editing and editing by our method are 0.046 and 0.012 in the first row, and 0.019 and 0.054 in the second row).Secondly, they require a user to provide an accurate text prompt that describes every visual object, and the relationships between them in the input image (see Fig.3).Finally, Pix2pix-zero[49]requires the textual embedding directions (e.g, catdog in Fig.9(up, the fourth column)) with thousands of sentences with GPT-3[9]before editing, which lacks scalability and flexibility.\n\nTo overcome the above-mentioned challenges, we analyze the role of the attention mechanism (and specifically the roles of keys, queries and values) in the diffusion process. This leads to the observation that the key dominates the output image structure (the \u201cwhere\u201d)[24], whereas the value determines the object style (the \u201cwhat\u201d).\nWe perform an effective experiment to demonstrate that the value determines the object style (the \u201cwhat\u201d). As shown in Fig.4(top), we generate two sets of images with prompt embeddingsand.\nWe use two different embeddings for the input of both key and value in the same attention layer. When swapping the input of the keys and fixing the input of the values, we observe that the content of generated images are similar, see Fig.4(middle). For example, The images of Fig.4(middle) are similar to the ones of Fig.4(top). When exchanging the input of the values and fixing the one of the keys, we find that the content swaps while preserving much of the structure, see Fig.4(bottom). For example, the images of the last row of Fig.4(bottom) have similar semantic information with the ones of the first row of Fig.4(top). It should be noted that their latent codes are all shared111The first row of top, middle, and bottom shares a common latent code, and the second row also shares a common latent code., so the structure of the results in Fig.4(middle) does not change significantly.\nThis experiment indicates that the value determines the object\u2019s style (the \u201cwhat\u201d).\n\nTherefore, to improve the projection of a real image, we introduceStylediffusionwhich maps a real image to the input embedding for the value computation (we refer to this embedding as theprompt-embedding), which enables us to obtain accurate style editing without invoking significant structural changes.\nWe propose to map the real image to the input of thevaluelinear layer in the cross-attention layers[8,66]providing freedom to edit effectively the real image in the manipulation step.\n\nWe take the given textual embedding as the input of thekeylinear layer, which is frozen (see Fig.1(right)). Using frozen embedding contributes to preserving the well-learned attention map from DDIM inversion, which guarantees the initial editability of the inverted image.\nWe observe that the system often outputs unsatisfactory reconstruction results (greatly adjusting the input image structure) (see Fig.2(first row, second column) and Fig.15(first row, second column)) due to locally less accurate attention maps (see Fig.15(first row, and third column)).\nHence, to further improve our method, we propose an attention regularization to obtain more precise reconstruction and editing capabilities.\n\nFor the second manipulation step, researchers propose a series of outstanding techniques[32,44,10,75,13,47,30,78,52,40]. Among them, P2P[24]is one of the most widely used image editing methods.\nHowever, P2P only operates on the conditional branch, and ignores the unconditional branch. This leads to less accurate editing capabilities for some cases, especially where the structural changes before and after editing are relatively large (e.g., \u201c\u2026tree\u2026\u201d\u201c\u2026house\u2026\u201d in Fig.5). To address this problem, we need to reduce the dependence of the structure on the source prompt and provide more freedom to generate the structure following the target prompt. Since the unconditional branch allows us to edit out concepts[4,64]. Thus, we propose to further perform the self-attention map exchange in the unconditional branch based on P2P (referred to asP2Plus), as well as in the conditional branch like P2P[24]. This technique enables us to obtain more accurate editing capabilities (see Fig.5(third column)). We build our method on SD models[56]and experiment on a variety of images and several ways of prompt editing.\n\nOur work thus makes the following contributions:\n\nState-of-the-art methods (e.g., Null-text inversion) struggle with unsatisfactory reconstruction and editing. To precisely project a real image, we introduceStyleDiffusion. We use a simple mapping network to map a real image to the input embedding forvaluecomputation.\n\nWe propose an attention regularization method to enhance the precision of attention maps, resulting in more accurate reconstructions and improved editing capabilities.\n\nWe propose the P2Plus technique, which enables us to obtain more powerful editing capabilities, especially when the source object is unrelated to the target object.\nThis approach addresses the limitations of P2P, which fails to function effectively in such cases.\n\nThrough extensive experiments, we demonstrate the effectiveness of our method for accurately reconstructing and editing images.\n\nSECTION: 2Related work\n\nSECTION: 2.1Transfer learning for diffusion models\n\nA series of recent works has investigated knowledge transfer on diffusion models[60,43,46,18,5,39,34,19,71,22,61,38,77,16]with one or a few images.\nRecent work[32,44,10,75,13,47,30,78,52,40,11]either finetune the pretrained model or invert the image in the latent space of the pretrained model.\nDreambooth[57]shows that training a diffusion model on a small data set (ofimages) largely benefits from a pre-trained diffusion model, preserving the textual editing capability. Similarly, Imagic[32]and UniTune[65]rely on the interpolation weights or the classifier-free guidance at the inference stage, except when finetuning the diffusion model during training. Kumari et al.[37]study only updating part of the parameters of the pre-trained model, namely thekeyandvaluemapping from text to latent features in the cross-attention layers.\nHowever, updating the diffusion model unavoidably loses the text editing capability of the pre-trained diffusion model. In this paper, we focus on real image editing with a frozen text-guilded diffusion model.\n\nSECTION: 2.2GAN inversion\n\nImage inversion aims to project a given real image into the latent space, allowing users to further manipulate the image. There exist several approaches[14,21,28,42,70,73]which focus on image manipulation based on pre-trained GANs, following literately optimization of the latent representation to restructure the target image. Given a target semantic attribute, they aim to manipulate the output image\nof a pretrained GAN. Several other methods[1,80]reverse a given image into the input latent space of a pretrained GAN (e.g., StyleGAN),\nand restructure the target image by optimization of the latent representation. They mainly consist of fixing the generator[1,2,54,62]or updating the generator[3,55].\n\nSECTION: 2.3Diffusion model inversion\n\nDiffusion-based inversion can be performed naively by optimizing the latent representation.[15]show that a given real image can be reconstructed by DDIM sampling[60]. DDIM provides a good starting point to synthesize a given real image. Several works[6,7,48]assume that the user provides a mask to restrict the region in which the changes are applied, achieving both meaningful editing and background preservation. P2P[24]proposes a mask-free editing method. However, it leads to unexpected results when editing the real image[46]. Recent work investigates the text embedding of the conditional input[18], or the null-text optimization of the unconditional input (i.e., Null-text inversion[46]). Although having the editing capability by combining the new prompts,\nthey suffer from the following challenges\n(i) they lead to unsatisfying results for the selected regions, and unexpected changes in non-selected regions, and (ii) they require careful text prompt editing where the prompt should include all visual objects in the input image.\n\nConcurrent work[49]proposes pix2pix-zero, also aiming to provide more accurate editing capabilities of the real image. However, it firstly needs to compute the textual embedding direction in advance using thousand sentences.\n\nSECTION: 3Method\n\nSECTION: 3.1Background\n\nDDIM inversion proposes an inversion scheme for unconditional diffusion models. However, this method fails when applied to text-guided diffusion models. This was observed by Mokady et al.[46], who propose Null-text inversion to address this problem. However, their methods has some drawbacks: (1) unsatisfying results for selected regions and unexpected changes in non-selected regions, and (2) they require careful text prompt editing where the prompt should include all visual objects in the input image.\n\nTherefore, our goal is to obtain a more accurate editing capability based on an accurate reconstruction of the real imageguided by the source prompt.\nOur method, called StyleDiffusion, is based on the observation that thekeysof the cross-attention layer dominate the output image structure (the \u201cwhere\u201d), whereas thevaluesdetermine the object style (the \u201cwhat\u201d). After faithfully projecting the real image, we propose P2Plus, which is an improved version of P2P[24].\n\nNext, we introduce SD models in Sec.3.2, followed by the proposed StyleDiffusion in Sec.3.3and P2Plus in Sec.3.4. A general overview is provided in Fig.6.\n\nSECTION: 3.2Preliminary: Diffusion Model\n\nGenerally, diffusion models optimize a UNet-based denoiser networkto predict Gaussian noise, following the objective:\n\nwhereis a noise sample according to timestep, andis the number of the timesteps. The encoded text embeddingis extracted by a Clip-text Encoderwith given prompt:.\nIn this paper, we build on SD models[56]. These first train both encoder and decoder. Then the diffusion process is performed in the latent space. Here the encoder maps the imageinto the latent representation, and the decoder aims to reverse the latent representationinto the image. The sampling process is given by:\n\nwhereis a scalar function.\n\nFor real-image editing with a pretrained diffusion model, a given real image is to be reconstructed by finding its initial noise.\nWe use the deterministic DDIM model to perform image inversion.\nThis process is given by:\n\nDDIM inversion synthesizes the latent noise that produces an approximation of the input image when fed to the diffusion process. While the reconstruction based on DDIM is not sufficiently accurate, it still provides a good starting point for training, enabling us to efficiently achieve high-fidelity inversion[24]. We use the intermediate results of DDIM inversion to train our model, similarly as[13,46].\n\nSD models achieve text-driven image generation by feeding a prompt into the cross-attention layer. Given both the text embeddingand the image feature representation,\nwe are able to produce the key matrix, the value matrixand the query matrix, via the linear networks:. The attention maps are then computed with:\n\nwhereis the projection dimension of the keys and queries. Finally, the cross-attention output is, which is then taken as input in the following convolution layers.\n\nIntuitively, P2P[24]performs prompt-to-prompt image editing with cross attention control. P2P is based on the idea that the attention maps largely control where the image is drawn, and the values decide what is drawn (mainly defining the style).\nImproving the accuracy of the attention maps leads to more powerful editing capabilities[46].\nWe experimentally observe that DDIM inversion generates satisfying attention maps (e.g., Fig.15(second row, first column)),\nand provides a good starting point for the optimization. Next, we investigate the attention maps to guide the image inversion.\n\nSECTION: 3.3StyleDiffusion\n\nAs shown in Fig.6(I), given a pair of a real imageand a corresponding prompt(e.g., \u201ddog\u201d), we perform DDIM inversion[15,60]to synthesize a series of latent noisesand attention maps, where, which is the extracted latent code of the input image222Note when generating the attention mapin the last timestep, we throw out the synthesized latent code..\nFig.6(II) shows that our method reconstructs the latent noisein the order of the diffusion process, where. Our framework consists of three networks: a frozen ClipImageEncoder, a learnable mapping networkand a denoiser network. For a specific timestep, the ClipImageEncodertakes the input imageas an input. The outputis fed into the mapping network, producing the prompt-embedding, which is fed into the value networkof the cross-attention layers. The input of the linear layeris the given textual embedding. We generate both the latent codeand the attention map.\nOur full algorithm is presented in algorithm1.\n\nThe full loss function consists of two losses:reconstruction lossandattention loss, which guarantee\nthat both the denoised latent codeand the corresponding attention mapat inference time are close to the ones:andfrom DDIM inversion, respectively.\n\nSince the noise representations () provide an initial trajectory which is close to the real image, we train the networkto generate the prompt embedding, which is the input of the value network. We optimize thein such a manner, that the output latent code () is close to the noise representations (). The objective is:\n\n,\n\nIt is known that a more accurate attention map is positively correlated to the editing capability[46].\nThe attention map, which is synthesized during the DDIM inversion, provides a good starting point. Thus, we introduce attention regularization\nwhen optimizing the mapping networkto further improve its quality. The objective is the following:\n\n,\n\nwhereandcan be obtained with Eq.4.\n\nThe full objective function of our model is:\n\nIn conclusion, in this section, we have proposed an alternative solution to the inversion of text-guided diffusion models which aims to improve upon existing solutions by providing more accurate editing capabilities, without requiring careful prompt engineering.\n\nSECTION: 3.4P2Plus\n\nHaving inverted the text-guided diffusion model, we can now perform prompt-based image editing (see Figs.2and9). We here outline our approach,\nwhich improves on the popular P2P\n\nP2P performs the replacement of both the cross-attention map and the self-attention map of the conditional branch, aiming to maintain the structure of the source prompt, see Fig.7(middle). However, it ignores the replacement in the unconditional branch. This leads to less accurate editing in some cases, especially when the structural changes before and after editing are relatively large (e.g., \u201c\u2026tent\u2026\u201d\u201c\u2026tiger\u2026\u201d ). To address this problem, we need to reduce the dependence of the structure on the source prompt and provide more freedom to generate the structure following the target prompt. Thus, as shown in Fig.7(bottom) we propose to further perform the self-attention map replacement in the unconditional branch based on P2P (an approach we callP2Plus), as well as in the conditional branch like P2P. This technique provides\nmore accurate editing capabilities. Like P2P, we introduce a timestep parameterthat determines until which step the injection is applied. Fig.8shows the results with differentvalues.\n\nSECTION: 4Experimental setup\n\nSECTION: 4.1Training details and datasets\n\nWe use the pretrained Stable Diffusion model. The configuration of the mapping networkis provided in Tab.1. We set.is a timestep parameter that determines which timestep is used by the output of the mapping network in the StyleDiffusion editing phase. Similarly, we can set the timestep(as in the conditional branch in P2P) to control the number of diffusion steps in which the injection of the unconditional branch is applied.\nWe use Adam[35]with a batch size of 1 and a learning rate of 0.0001. The exponential decay rates are. We randomly initialize the weights of the mapping network following a Gaussian distribution centered at 0 with 0.01 standard deviation. We use one Quadro RTX 3090 GPUs (24 GB VRAM) to conduct all our experiments. We randomly collect a real image dataset of 100 images (with resolution) and caption pairs from Unsplash(https://unsplash.com/) and COCO[12].\n\nSECTION: 4.2Evaluation metrics\n\nClipscore[25]is a metric that evaluates the quality of a pair of a prompt and an edited image. To evaluate the preservation of structure information after editing, we use Structure Dist[63]to compute the structural consistency of the edited image. Furthermore, we aim to modify the selected region, which corresponds to the target prompt, while preserve the non-selected region. Thus, we also need to evaluate change in the non-selected region after editing. To automatically determine the non-selected region of the edited image, we use a binary method to generate the raw mask from the attention map. Then we invert it to get the non-selected region mask. Using the non-selected region mask, we compute the non-selected region LPIPS[76]between the real and edited images, which we denoteNS-LPIPSfor non-selected LPIPS. A lower NS-LPIPS score means that the non-selected region is more similar to the input image. We also use both PSNR and SSIM to evaluate image reconstruction.\n\nSECTION: 4.3Baselines\n\nWe compare our method against the following baselines.Null-text[46]inverts real images with corresponding captions into the text embedding of the unconditional part of the classifier-free diffusion model.SDEdit[44]introduces a stochastic differential equation to generate realistic images through an iterative denoising process.Pix2pix-zero[49]edits the real image to find the potential direction from the source to the target words.DDIM with word swap[49]performs DDIM sampling with an edited prompt generated by swapping the source word with the target. We use the official published codes for the baselines in our comparison to StyleDiffusion.\nWe also ablate variants of StyleDiffusion.\n\nSECTION: 5Experiments\n\nSECTION: 5.1Qualitative and quantitative results\n\nFig.9presents a comparison between the baselines and our method.SDEit[44]fails to generate high-quality images, such as dog or cat faces (second column).Pix2pix-zero[49]synthesizes better results, but it also modifies the non-selected region, such as removing the plant when translating catdog. The official implementation ofpix2pix-zero[49]provides the editing directions (e.g, catdog), and we directly use them. Note thatpix2pix-zero[49]requires that the editing directions are calculated in advance, while our method does not require this. Fig.9(last three rows, fourth column) shows thatDDIM with word swaplargely modifies both the background and the structural information of the foreground. MasaCtrl[10]is designed for non-rigid editing and tries to maintain content consistency after editing. It often changes the shape of objects when translating from one to another (Fig.9(fifth column)). For example, when translating dog to cat, although MasaCtrl successfully performs the translation, it changes the shape, such as making the head of the cat larger than in the original image (Fig.9(second row, the fifth column)).\nOur method successfully edits the target-specific object, resulting in a high-quality image, indicating that our proposed method has more accurate editing capabilities.\n\nWe evaluate the performance of the proposed method on the collected dataset. Tab.2reports, in terms of both Structure distance and NS-LPIPS, that the proposed method achieves the best score, indicates that we have superior capabilities to preserve structural information. In terms of Clipscore, we get a better score than Null-text (i.e., 77.9vs 75.2), and a comparative result with SDEdit.DDIM with word swapachieves the best Clipscore. However,DDIM with word swapnot only changes the background, but also modifies the structure of the selected-regions (see Fig.9(last three rows, fourth column)). Note that we do not compare to pix2pix-zero[49]in Fig.9(last three rows), since it first needs to compute the textual embedding directions with thousands of sentences using GPT-3[9]. We also evaluate the reconstruction quality and the inference time for each timestep. As reported in Tab.3, we achieve the best PSNR/SSIM scores, with an acceptable time overhead.\n\nFurthermore, we conduct a user study, asking subjects to select the results that best match the following statement:which figure preserves the input image structure and matches the target prompt style(Fig.10). We apply quadruplet comparisons (forced choice) with 54 users (30 quadruplets/user).\nThe study participants were volunteers from our college. The questionnaire consisted of 30 questions, each presenting the original image, as well as the results of various baselines and our method. Users were tasked with selecting an image in which the target image is more accurately edited compared to the original image. Each question in the questionnaire presents five options, including baselines (DDIM with word swap, Nul-text, SDEdit, and MasaCtrl) and our method, from which users were instructed to choose one. A total of 54 users participated, resulting in a combined total of 1620 samples (30 questions1 option54 users) with 740 samples (45.68%) favoring our method. In the results of the user study, the values for DDIM with word swap, NullText, SDEdit, MasaCtrl, and Ours are 15.37%, 15.43%, 9.38%, 14.14%, and 45.68%, respectively.\n\nFig.11shows that we can manipulate the inverted image with attention injection (replacement), refinement (adding a new phrase) or re-weighting using P2Plus.\nFor example, we translateglassesintosunglasses(Fig.11(first row)). We addChinese style(new prompts) to the source prompt (Fig.11(third row)).\nWe scale the attention map of the \u201cflowers\u201d in prompt \u201ca tree with flowers in front of a house\u201d, resulting in a stronger effect (Fig.11(fourth row)).\nThese results indicate that our approach manages to invert real images with corresponding captions into the latent space, while maintaining powerful editing capabilities.\n\nWe observe that StyleDiffusion (Fig.12(last column)) allows for object structure modifications while preserving the identity within the range given by the input image cross-attention map, resembling the capabilities demonstrated by Imagic[32]and MasaCtrl[10](Fig.12(third and fourth columns)).\nFurthermore, StyleDiffusion can preserve more accurate content, such as the scarf around the dog\u2019s neck (Fig.12(fifth column)).\nIn contrast, Null-text[46]does not possess the capacity to accomplish such changes (Fig.12(second column)).\n\nStyleDiffusion can additionally be applied to many real-world applications, including object replacement in images for publicity purposes (as well as personalized advertising: see Fig.13(first row)), dataset enrichment by adding rare objects (e.g., wheelchairs in autonomous driving datasets: Fig.13(second row)), furniture replacement for interior design (Fig.13(third row)), etc.\n\nSECTION: 5.2Ablation study.\n\nHere, we evaluate the effect of each independent contribution to our method and their combinations.\n\nAlthough P2P obtains satisfactory editing results with attention injection in the conditional branch, it ignores attention injection in the unconditional branch (as proposed by our P2Plus in Sec.3.4).\nWe experimentally observe that the self-attention maps in the unconditional branch play an important role in obtaining more accurate editing capabilities, especially when the object structure changes before and after editing of the real image are relatively large, e.g., translatingbiketomotorcyclein Fig.14(left, third row). It also shows that the unconditional branch contains much useful texture and structure information, allowing us to reduce the influence of the unwanted structure of the input image.\n\nWe evaluate variants of our method, namely\n(i) learning the input prompt-embedding for thekeylinear layer and freezing the input of thevaluelinear layer with the one provided by the user, and (ii) learning the prompt-embedding for bothkeyandvaluelinear layers. As Fig.14(right) shows, the two variants fail to edit the image according to the target prompt. Our method successfully modifies the real image with the target prompt, and produces realistic results.\n\nWe perform an ablation study of attention regularization. Fig.15shows that the system fails to reconstruct partial object information (e.g., the nose in Fig.15(first row, second column)), and learns a less accurate attention map (e.g., the nose attention map in Fig.15(first row, third column). Our method not only synthesizes high-quality images, but also learns a better attention map even than the one generated by DDIM inversion (Fig.15(second row, first column)).\n\nFig.16illustrates the reconstruction and editing results of value-embedding optimization, that is, similar to our method extracting the prompt-embedding from the input image but directly optimizing the input textual embedding. Value-embedding optimization fails to reconstruct the input image. Null-text[46]draws a similar conclusion that optimizing both the input textual embedding for thevalueandkeylinear layers results in lower editing accuracy.\n\nAfter inverting a real image with StyleDiffusion, we leverage SDEdit to edit it. Only using SDEdit, the results suffer from unwanted changes, such as the orientation of the dog (Fig.18(first row, second column) and the texture detail of the leg of the dog (Fig.18(second row, second column)). While combining StyleDiffusion and SDEdit significantly enhances the fidelity to the input image (see Fig.18(third column)). This indicates our method exhibits robust performance when combining different editing techniques (e.g., SDEdit and P2Plus).\n\nRecently, some methods have been proposed[45,23]that do not use optimization.\nNegative-prompt inversion (NPI)[45]replaces the null-text embedding of the unconditional branch with the textural embedding in SD to implement reconstruction and editing.\nProximal Negative-Prompt Inversion (ProxNPI)[23]attempts to enhance NPI by introducing regularization terms using proximal function and reconstruction guidance based on the foundation of NPI.\nWhile these methods do not require optimizing parameters to achieve the inversion of real images, like to the method shown in Fig.1, they suffer from challenges when reconstructing and editing images containing intricate content and structure (see Fig.17(second and third columns, sixth and seventh columns)).\nDue to the absence of an optimization process in these methods, it is not possible to utilize attention loss to refine the attention maps like Null-text+(Fig.15), consequently limiting the potential for enhancing reconstruction and editing quality.\n\nAs a final illustration, we show that StyleDiffusion can be used to perform style transfer.\nFig.20(left) shows how, given a content image, we use DDIM inversion to generate a series of timestep-related latent codes. They are then progressively denoised using DDIM sampling. During this\nprocess, we extract the spatial features from the decoder layers. These spatial features are injected into the corresponding layers of StyleDiffusion model. Note that we first optimize StyleDiffusion to reconstruct the style image, then use both the well-learnedand the extracted content feature to perform the style transfer. Fig.20(right) shows that we can successfully combine both content and style images, and perform style transfer.\n\nSECTION: 6Conclusions and Limitations\n\nWe propose a new method for real image editing. We invert the real image into the input of thevaluelinear mapping network in the cross-attention layers, and freeze the input of thekeylinear layer with the textual embedding provided by the user. This allows us to learn initial attention maps, and an approximate trajectory to reconstruct the real image. We introduce a new attention regularization to preserve the attention maps after editing, enabling us to obtain more accurate editing capabilities. In addition, we propose attention injection in the unconditional branch of the classifier-free diffusion model (P2Plus), further improving the editing capabilities, especially when both source and target prompts have a large domain shift.\n\nWhile StyleDiffusion successfully modifies the real image, it still suffers from some limitations. Our method fails to generate satisfying images when the object in the real image has a rare pose (Fig.19(left)), or when both the source and the target prompts have a large semantic shift (Fig.19(right)).\n\nSECTION: Declaration of competing interest\n\nThe authors have no competing interests to declare relevant to the\ncontent in this study.\n\nSECTION: References", "text_file": "data\\paper_texts\\2303.15649v3_content.txt"}, {"title": "Mixed Blessing: Class-Wise Embedding guided Instance-Dependent Partial\n  Label Learning", "authors": ["Fuchao Yang", "Jianhong Cheng", "Hui Liu", "Yongqiang Dong", "Yuheng Jia", "Junhui Hou"], "published_date": "2024-12-06T13:25:39Z", "summary": "In partial label learning (PLL), every sample is associated with a candidate\nlabel set comprising the ground-truth label and several noisy labels. The\nconventional PLL assumes the noisy labels are randomly generated\n(instance-independent), while in practical scenarios, the noisy labels are\nalways instance-dependent and are highly related to the sample features,\nleading to the instance-dependent partial label learning (IDPLL) problem.\nInstance-dependent noisy label is a double-edged sword. On one side, it may\npromote model training as the noisy labels can depict the sample to some\nextent. On the other side, it brings high label ambiguity as the noisy labels\nare quite undistinguishable from the ground-truth label. To leverage the\nnuances of IDPLL effectively, for the first time we create class-wise\nembeddings for each sample, which allow us to explore the relationship of\ninstance-dependent noisy labels, i.e., the class-wise embeddings in the\ncandidate label set should have high similarity, while the class-wise\nembeddings between the candidate label set and the non-candidate label set\nshould have high dissimilarity. Moreover, to reduce the high label ambiguity,\nwe introduce the concept of class prototypes containing global feature\ninformation to disambiguate the candidate label set. Extensive experimental\ncomparisons with twelve methods on six benchmark data sets, including four\nfine-grained data sets, demonstrate the effectiveness of the proposed method.\nThe code implementation is publicly available at\nhttps://github.com/Yangfc-ML/CEL.", "arxiv_id": "2412.05029v1", "html_link": "https://arxiv.org/html/2412.05029v1", "search_term": "ti:\"embeddings\"", "html_content": "SECTION: Mixed Blessing: Class-Wise Embedding guided Instance-Dependent Partial Label Learning\n\nIn partial label learning (PLL), every sample is associated with a candidate label set comprising the ground-truth label and several noisy labels. The conventional PLL assumes the noisy labels are randomly generated (instance-independent), while in practical scenarios, the noisy labels are always instance-dependent and are highly related to the sample features, leading to the instance-dependent partial label learning (IDPLL) problem. Instance-dependent noisy label is a double-edged sword. On one side, it may promote model training as the noisy labels can depict the sample to some extent. On the other side, it brings high label ambiguity as the noisy labels are quite undistinguishable from the ground-truth label. To leverage the nuances of IDPLL effectively, for the first time we create class-wise embeddings for each sample, which allow us to explore the relationship of instance-dependent noisy labels, i.e., the class-wise embeddings in the candidate label set should have high similarity, while the class-wise embeddings between the candidate label set and the non-candidate label set should have high dissimilarity. Moreover, to reduce the high label ambiguity, we introduce the concept of class prototypes containing global feature information to disambiguate the candidate label set. Extensive experimental comparisons with twelve methods on six benchmark data sets, including four fine-grained data sets, demonstrate the effectiveness of the proposed method. The code implementation\nis publicly available athttps://github.com/Yangfc-ML/CEL.\n\nSECTION: 1.Introduction\n\nPartial label learning (PLL)(Tian et\u00a0al.,2023; Lv et\u00a0al.,2024; Jiang et\u00a0al.,2024; Jia et\u00a0al.,2024)has garnered significant attention over the past decade as a form of weakly supervised learning. In PLL, each sample is associated with a candidate label set, concealing the ground-truth label and several noisy labels within it. In the training phase, the ground-truth label is inaccessible. PLL does not rely on precise labeling, leading to substantial reductions in time and resource costs associated with sample annotation. Consequently, PLL finds extensive applications across various real-world domains, including automatic image annotation(Cour et\u00a0al.,2011), web mining(Luo and Orabona,2010), ecoinformatics(Liu and Dietterich,2012), and multimedia content analysis(Zeng et\u00a0al.,2013). The crux of addressing the PLL problem lies in label disambiguation, involving the identification of the ground-truth label while mitigating the impact of noisy labels within the candidate label set. Conventional label disambiguation techniques(Lv et\u00a0al.,2020; Wang et\u00a0al.,2022a; Wu et\u00a0al.,2022)have demonstrated strong efficacy in the typical PLL scenarios where noisy labels are randomly generated (instance-independent)(Wang et\u00a0al.,2022b; Jia et\u00a0al.,2023).\n\nRecently, a more realistic PLL framework called instance-dependent partial label learning (IDPLL)(Xu et\u00a0al.,2021; He et\u00a0al.,2023; Wu et\u00a0al.,2024)was proposed, where noisy labels are re-tailored to individual samples (instance-dependent). As shown in Fig.1, in the conventional PLL, the noisy labels in the candidate label set may have significant differences with the ground-truth label because the noisy labels are randomly generated. However, in IDPLL, the noisy labels are very similar to the ground-truth label. To be more specific, these birds all have yellow feathers, which is more easily to cause label ambiguity. Conventional PLL methods often struggle in IDPLL since they ignore the characteristics of IDPLL.\n\nIDPLL is a mixed blessing. On the positive side, as shown in Fig.2, the model achieves better classification performance in the early stage and converges faster in the IDPLL setting compared with the PLL setting. This is because in IDPLL the noisy labels in the candidate label set are related to the sample, which can describe the sample to some extent. Although the noisy labels in the candidate label set are incorrect, they can still act as supervision to promote model training. On the negative side, the model only achieves quite inferior classification performance in the later stage. The reason is that given the instance, the instance-dependent noisy labels in the candidate label set are highly similar to the ground-truth label, bringing more label ambiguity and making the candidate label disambiguation process more challenging. The current IDPLL methods have made different attempts to achieve better label disambiguation. For example, NEPLL(He et\u00a0al.,2023)proposed a well-disambiguated sample selection method based on normalized cross-entropy and trained the model progressively according to the selected samples. POP(Xu et\u00a0al.,2023)proposed to purify the candidate label set during the training phase to gradually reduce the difficulty of label disambiguation. DIRK(Wu et\u00a0al.,2024)proposed a label rectification strategy that ensured the model output on the candidate label set was always higher than that on the non-candidate label set. However, these methods did not comprehensively exploit both the positive side and negative side of IDPLL, limiting their performance.\n\nTo address this double-edged sword challenge in IDPLL, we propose a novel method calledCEL(Mixed Blessing:Class-WiseEmbedding guided Instance-Dependent PartialLabel Learning). Specifically, unlike previous PLL and IDPLL methods, where each sample corresponds to a single embedding, our method introduces the class-wise embedding for each sample, i.e., each sample has multiple embeddings corresponding to different classes, to comprehensively exploit the mixed blessing in IDPLL. First, we propose a class associative loss (CAL) to leverage the relationships among different classes of each sample to guide the learning of class-wise embeddings. In IDPLL, the class-wise embeddings within the candidate label set should exhibit high similarity to each other as the candidate labels can describe the instance to some extent. In contrast, the class-wise embeddings between the candidate label set and the non-candidate label set should display stark differences. In this way, we can obtain class-wise embeddings that are more suitable for IDPLL. Second, we propose a prototype discriminative loss (PDL) by constructing class prototype for each class which containts global feature information to guide the label disambiguation process. To be specific, we select the most high-confidence label for each sample based on the model output, and then we ensure the class-wise embedding of this particular high-confidence label is aligned with its corresponding class prototype while being distanced from other class prototypes, thereby enhancing the model\u2019s discriminative ability. By employing the above two losses, we can obtain embeddings that are tailor-made for IDPLL and enhance the model\u2019s label disambiguation performance simultaneously, thus addressing the mixed blessing issue in IDPLL. Extensive experiments on 6 benchmark\ndata sets demonstrate the effectiveness of\nthe proposed method when compared with 12 state-of-the-art methods.\n\nThe contributions of our work are summarized as follows:\n\nTo the best of our knowledge, we are the first to introduce class-wise embedding in IDPLL. Class-wise embedding enable the model to explore the nuances relationships of classes in each sample, thereby better addressing the mixed blessing problem inherent in IDPLL.\n\nWe comprehensively consider the positive and negative sides of IDPLL. To leverage the positive side, we utilize the class associative loss to exploit the relationships among the labels (including both candidate labels and non-candidate labels) of each sample. To address the negative side, we apply the prototype discriminative loss to utilize the relationships between high-confidence class and class prototypes.\n\nExtensive experiments on benchmark data sets demonstrate that our method achieves significantly superior performance when comparing with both PLL and IDPLL methods.\n\nSECTION: 2.RELATED WORK\n\nSECTION: 2.1.Partial Label Learning\n\nConventional PLL methods can be roughly divided into two categories based on the used information. The first category uses the information in the feature space to guide label disambiguation(H\u00fcllermeier and Beringer,2006; Zhang and Yu,2015; Zhang et\u00a0al.,2016; Feng and An,2018; Wang et\u00a0al.,2022b). The second category leverages the information of label space to achieve label disambiguation(Feng and An,2019; Li et\u00a0al.,2020; Jia et\u00a0al.,2023). These methods often rely on linear models and are difficult to apply to large-scale data sets. Deep PLL has received wide attention in recent years and it adopts deep neural networks to process large-scale data sets(Lv et\u00a0al.,2020; Wu et\u00a0al.,2022; Xia et\u00a0al.,2023). PRODEN(Lv et\u00a0al.,2020)proposed to progressively identify the ground-truth label during the self-training procedure. RC and CC(Feng et\u00a0al.,2020)proposed provably risk-consistent and classifier-consistent label disambiguation methods. LWS(Wen et\u00a0al.,2021)proposed a family of leveraged weighted loss functions. PICO(Wang et\u00a0al.,2022a)introduced the widely used contrastive loss(He et\u00a0al.,2020)to the PLL, which became the foundation for a large number of follow-up works. CR-DPLL(Wu et\u00a0al.,2022)employed consistency regularization to reduce the impact of noisy labels. PAPI(Xia et\u00a0al.,2023)constructed the similarity score between feature prototypes and sample embeddings, and improves the model performances by aligning the similarity score with the model output. CROSEL(Tian et\u00a0al.,2024)used two models to cross-select trustworthy samples from the data set for the training phase. The above methods have achieved superior results in the conventional PLL setting, where the noisy labels in the candidate label set are randomly generated. However, in practice, noisy labels are always instance-dependent(Xu et\u00a0al.,2021; Xia et\u00a0al.,2022; Qiao et\u00a0al.,2023). For example, in crowdsourced annotation tasks, the annotations from many annotators constitute the candidate label set of each sample, and the noisy labels within it are all instance-dependent. Consequently, the performances of conventional PLL methods are often limited due to the lack of consideration of the characteristics of IDPLL.\n\nSECTION: 2.2.Instance-Dependent Partial Label Learning\n\nIDPLL is a PLL framework that is closer to real-world scenarios. VALEN(Xu et\u00a0al.,2021)was the first work to introduce the concept of IDPLL, with a two-stage disambiguation process. The first stage involves recovering samples\u2019 latent label distribution, and then training the model using the recovered label distribution in the second stage. ABLE(Xia et\u00a0al.,2022)proposed an ambiguity-induced positive selection contrastive learning framework for IDPLL to achieve label disambiguation. NEPLL(He et\u00a0al.,2023)introduced a sample selection method based on normalized entropy, intending to separate well-disambiguated samples and under-disambiguated samples. It also established a dynamic candidate-aware thresholding for the further refinement of sample selection. POP(Xu et\u00a0al.,2023)presented a method that progressively purified the candidate label set and optimized the classifier. IDGP(Qiao et\u00a0al.,2023)modeled the candidate label generation process for IDPLL, simulating the ground-truth label and noisy labels generation processes with Categorical distribution and Bernoulli distribution respectively. DIRK(Wu et\u00a0al.,2024)proposed a self-distillation-based label disambiguation method, i.e., the training of the student model is directed by the output of the teacher model. It also developed a label confidence rectification method that meets the prior knowledge of IDPLL. However, the aforementioned IDPLL methods did not consider that the IDPLL setting is a mixed blessing, i.e., on the positive side, noisy labels can describe the sample to some extent and help the model training, while on the negative side, identifying the ground-truth label within the candidate label set becomes more challenging. They only focused on developing better disambiguation methods while ignoring the noisy label information, thereby limiting their performance.\n\nSECTION: 3.Proposed Method\n\nSECTION: 3.1.Class-Wise Embedding\n\nDenote bythe-dimensional feature space andthe corresponding label space withclasses. Letdenote a partial label data set comprisingsamples, whereis the candidate label set of sampleand the ground-truth label of sample is concealed in. The objective of IDPLL is to induce a multi-class classifier that maps elements fromto.\nPrevious models in the realm of PLL and IDPLL typically consist of two modules.\nThe first module is the backbone responsible for deriving the feature mapfor each sample, where,andare the height, width, and channel respectively. Afterward,is processed through average global pooling to obtain the low dimensional embedding. The second module encompasses a linear layer that translates this low-dimensional embedding into the final prediction.\n\nIn this paper, we use a different paradigm that consists of three modules. As shown in Fig.3, the first module is as same as other models, i.e., a backbone, which extracts the feature mapof each samplethrough the deep neural network. The second module is aclass-wise embeddingencoder(Lanchantin et\u00a0al.,2021; Liu et\u00a0al.,2021; Ridnik et\u00a0al.,2023)which produces class-wise embeddings, whereis the length of each class-wise embedding. Note, theindicates the representation of the-th class of sample. The third module is a group of linear layersthat output the predicted probabilities.\n\nIn conventional PLL and IDPLL representation methods, the features of each sample are extracted as a single embedding, so they can only consider relationships at the sample level. However, in IDPLL, the relationships between each sample\u2019s candidate labels and non-candidate labels are valuable and worth leveraging. By employing class-wise encoder, we can represent each sample\u2019s embeddings on different labels. This allows us to explore the internal relationships among different classes and fully utilize the prior knowledge of IDPLL, making it a more suitable representation method for IDPLL.\n\nSECTION: 3.2.Label Disambiguation Loss\n\nAs aforementioned, the pivotal process in addressing PLL is label disambiguation, which mitigates the impact of noisy labels within the candidate label set. Here, we adopt a widely used deep PLL disambiguation strategy(Lv et\u00a0al.,2020), which constructs sample label confidences based on model outputs during training. Initially, the label confidence vectorof sampleis initialized as, ifand, otherwise, wherereturns the number of candidate labels of sample. Throughout the training, we update the label confidence according to the model output:\n\nwhererepresents the model prediction on-th class of sample. The goal of Eq. (1) is to eliminate the influence of non-candidate labels, so that the model can focus on the candidate labels. Based on the disambiguated confidence, we can obtain the following classification loss to guide the model training:\n\nwhereindicates the cross-entropy loss. Eq. (2) is usually viewed as a self-learning process and has demonstrated efficacy across various PLL scenarios.\n\nSECTION: 3.3.Class Associative Loss based on Class-Wise Embedding\n\nAs previous analyzed, IDPLL is a double-edged sword. On the positive side, the noisy labels in the candidate label set are very similar to the ground-truth label because they often share common features and they can depict the sample to some extent, which is also the fundamental reason leading to label ambiguity.\nTherefore, an important characteristic of IDPLL is that the labels within the candidate label set should be very similar, while the labels in the candidate label set should exhibit significant differences from the labels in the non-candidate label set. To fully leverage this prior knowledge, we can measure the relationships among classes of each sample through class-wise embeddings. To be specific, the class-wise embeddings corresponding to each label in the candidate label set should be similar to each other, while the class-wise embeddings between the candidate label set and the non-candidate label set should display stark differences. Therefore, we can construct the following class-wise relationships:\n\nwhereandrepresent the-th class-wise embedding and the candidate label set of samplerespectively.returns the cosine similarity of two vectors. Note that the class-wise embeddings have been normalized before calculating the cosine similarity.in Eq. (3) denotes the average similarity of classes in the candidate label set, whilein Eq. (4) indicates the average similarity of classes between the candidate label set and the non-candidate label set.\n\nBy considering the similarities of Eqs. (3) and (4) simultaneously, we can obtain the following class associative loss (CAL) function:\n\nwhereis a trade-off parameter that balances two different terms.is the absolute value operator given. The first term in Eq. (5) means that for each sample, the class-wise embeddings in its candidate label set should be pulled as close as possible. In the meanwhile, the second term implies that the class-wise embeddings between the candidate label set and the non-candidate label set should be pushed as far away as possible, and in the ideal situation, their class-wise embeddings should be orthogonal, i.e.. By minimizing the loss function, we can obtain a model that is more suitable for the IDPLL setting.\n\nSECTION: 3.4.Prototype Discriminative Loss based on Class-Wise Embedding\n\nIn IDPLL, we can distinguish the labels between the candidate label set and the non-candidate label set easily, however, it becomes more difficult to identify which label in the candidate label set is the ground-truth label, as the candidate labels are similar to each other, bringing more label ambiguity. Therefore, to tackle this negative side of IDPLL and identify the ground-truth label, we use the global information of samples to guide the model\u2019s disambiguation. Therefore, we first construct prototypes for each class,\n\nwhereis the-th class prototypes anddenotes thenormalization operator. To ensure the quality of the class prototypes, we only select the class with the highest model output probability in the candidate label set as the high-confidence label of each sample and add the corresponding class-wise embedding into the class prototype.\n\nSimilar to Eq. (3) and Eq. (4), we construct the similarity relationships between class-wise embeddings and class prototypes as follows:\n\nwherein Eq. (7) represents the similarity between the class-wise embedding of the class with the highest model prediction in the candidate label set and the corresponding class prototype. Meanwhile,in Eq. (8) denotes the average similarity between this selected class-wise embedding and all other class prototypes. By combing Eq. (7) and Eq. (8), we have the following prototype discriminative loss (PDL) function:\n\nwhereis the trade-off parameter that balances the two different prototype level terms. By minimizing Eq. (9), we can guide the model training through class prototypes, i.e., the most reliable class predicted by the model should be as close as possible to the corresponding class prototype, while this reliable class should be far away from other class prototypes, and in the most ideal case, their similarity relationship should be 0. By utilizing the global information of class prototypes, we can effectively improve the discriminative performance of the model and select the ground-truth label from the candidate label set.\n\n75.51\u00b10.28%\n\n75.77\u00b10.09%\n\n78.36\u00b10.19%\n\n78.18\u00b10.12%\n\n86.22\u00b10.08%\n\n68.60\u00b10.10%\n\nSECTION: 3.5.Overall Objective\n\nConsidering the low quality of class prototypes obtained in the early stages of model training, it is difficult to ensure the effectiveness of label disambiguation. Therefore, we divide the model training into two stages. In the first stage, our training uses the classification loss and class associative loss which aims to learn a model more suitable for IDPLL. The training objective of the first stage can be written as follows:\n\nAfter training forepochs, we add the prototype discriminative loss into the training objective to further improve the model\u2019s disambiguation performance. The objective function of the second stage can be summarized as follows:\n\nwhereandare two trade-off parameters. The overall pseudo-code of our method is summarized inAlgorithm 1.\n\nSECTION: 4.EXPERIMENTS\n\nSECTION: 4.1.Experimental Setting\n\nTo demonstrate the effectiveness of our method, we conducted comparisons on several challenging data sets with at least 100 classes. To be specific, we conducted experiments on two common image data sets including CIFAR-100(Krizhevsky,2009), CIFAR-100H(Krizhevsky,2009)and four fine-grained(Wei et\u00a0al.,2022; Zhao et\u00a0al.,2017)image data sets including CUB-200-2011(Wah et\u00a0al.,2011), Stanford Cars(Krause et\u00a0al.,2013), FGVC Aircraft(Maji et\u00a0al.,2013), Stanford Dogs(Khosla et\u00a0al.,2011)which are more easily to cause label ambiguity. Table1records the detailed information of all the data sets, where Avg. CLs represent the average number of candidate labels per sample. For data sets CIFAR-100 and CIFAR-100H, the image size was set to 3232, while for fine-grained data sets, we resized the images to 224224. We employed the IDPLL noisy label generation method proposed by VALEN(Xu et\u00a0al.,2021)to generate instance-dependent noisy labels. Note that for CIFAR-100H, noisy labels only appear in other subclasses that belong to the same superclass as the ground-truth label.\n\nTotal\n\nTo demonstrate the effectiveness of the proposed method, we compared our method with 12 methods including 6 IDPLL methods and 6 PLL methods.IDPLL methods: (i) DIRK(Wu et\u00a0al.,2024), a self-distillation based label disambiguation method. (ii) NEPLL(He et\u00a0al.,2023), a normalized entropy guided sample selection method. (iii) POP(Xu et\u00a0al.,2023), a method that progressive purifies candidate label set and refines classifier. (iv) IDGP(Qiao et\u00a0al.,2023), a generation method that models the candidate label generation process. (v) ABLE(Xia et\u00a0al.,2022), a contrastive learning-based framework that uses ambiguity-induced positives selection method. (vi) VALEN(Xu et\u00a0al.,2021), a label enhancement guided latent label distribution recovery method.PLL methods: (i) PICO(Wang et\u00a0al.,2022a), a method that combines PLL and contrastive learning for the first time. (ii) CR-DPLL(Wu et\u00a0al.,2022), a consistency regularization label disambiguation method. (iii) LWS(Wen et\u00a0al.,2021), a method that uses leveraged weighted loss to balance candidate label set and non-candidate label set. (iv) PRODEN(Lv et\u00a0al.,2020), a method that progressively identifies the\nground-truth label during the self-training procedure. (v) RC(Feng et\u00a0al.,2020), a risk-consistent weighting method. (vi) CC(Feng et\u00a0al.,2020), a classifier-consistent that uses a transition matrix. The parameters of all methods were set according to their original papers.\n\nFor fair comparisons, we employed ResNet-18(He et\u00a0al.,2016)as the backboneon data sets CIFAR-100 and CIFAR-100H, while using a ResNet-34(He et\u00a0al.,2016)pre-trained on ImageNet(Deng et\u00a0al.,2009)as the backboneon fine-grained data sets for all methods. We used ML-Decoder(Ridnik et\u00a0al.,2023)as our class-wise encoder. Stochastic gradient descent (SGD) was used as the optimizer with a momentum of 0.9 and all methods were trained for 500 epochs. We selected learning rate in {0.5, 0.1, 0.05, 0.01, 0.005, 0.001} and weight decay in {0.001, 0.0005, 0.0001} respectively. Additionally, we applied the cosine annealing learning rate schedule for all methods. The batch sizes of all data sets were set to 128. We repeated the experiments three times under the same random seeds and recorded the mean accuracy and standard deviation. As for our method, parametersandwere selected from, whileandwere selected from. Thewas set to 250, which is half of the total training epochs. The length of each class-wise embeddingwas set to 512 on all data sets. Following DIRK(Wu et\u00a0al.,2024), we used the same weak augmentation and strong augmentation for our method.\n\nSECTION: 4.2.Experimental Results\n\nTable2reports the classification accuracies of all methods on two common data sets and four fine-grained data sets. According to Table2, our method ranks first in all benchmark data sets when compared with 6 PLL methods and 6 IDPLL methods. It is worth noting that there is only minor difference in classification accuracies between the previous PLL and IDPLL methods on common data sets like CIFAR-100 and CIFAR-100H. While our method improves classification accuracy from 74.40\u00b10.18% to 75.51\u00b10.28% on the CIFAR-100 data set compared to the best previous method. On fine-grained data sets, the performance gaps between PLL and IDPLL methods are significant. PLL methods generally fail to achieve satisfactory accuracy because all labels in the fine-grained data sets belong to the same superclass, which is more challenging. Consequently, IDPLL methods tailored to this setting often outperform the conventional PLL methods. Our method CEL constantly excels on these fine-grained data sets, improving classification accuracy from 66.60\u00b11.07% to 68.60\u00b10.10% on the data set CUB200 and from 75.97\u00b10.29% to 78.18\u00b10.12% on the data set DOGS120, compared to the best previous method.\n\nTable3reports win/tie/loss counts of our method CEL against each comparing method based on the pairwise t-test at 0.05 significance level. As shown in Table3, on common data sets, our method CEL significantly outperforms the comparing methods in 83.3% (20/24). While on fine-grained data sets, our method significantly outperforms the comparing methods in 95.8% (46/48). Furthermore, our method achieves superior performance than all comparing methods except DIRK on fine-grained data sets. Considering all benchmark data sets, our method significantly outperformed the comparing methods in 91.7% (66/72), demonstrating the effectiveness of our method.\n\nFig.4shows the classification accuracy curves of all methods on data set CUB200. As shown in Fig.4, compared to other methods, our method CEL maintains a relatively fast learning speed in the early stages, only slightly slower than POP, demonstrating that our class associative loss can enhance the model\u2019s learning speed. Moreover, CEL achieves the best classification accuracy in the later stages of training, proving that our prototype discriminative loss further improves the model\u2019s disambiguation performance.\n\nAVERAGE\n\n75.51\u00b10.28%\n\n75.77\u00b10.09%\n\n78.36\u00b10.19%\n\n78.18\u00b10.12%\n\n86.22\u00b10.08%\n\n68.60\u00b10.10%\n\n77.11%\n\nSECTION: 4.3.Further Analysis\n\nIn Table4, we conduct ablation studies on all benchmark data sets to demonstrate the necessity of each component in our method. The first row represents our method with only a classification loss, the second row represents our method with the classification loss and our proposed class associative loss, i.e.,, and the third row represents our method with the classification loss, class associative loss, and our prototype discriminative loss, i.e.,. According to the results in Table4, both the class associative lossand prototype discriminative losscan improve the model\u2019s classification performance. To be specific, the class associative loss improves the classification accuracy by an average of 1.22% on six data sets, and the prototype discriminative lossloss also promotes the classification accuracy by an average of 0.71%. Therefore, incorporating both terms into the model is the optimal choice.\n\nFig.5shows the classification accuracy of our method CEL on benchmark data sets CIFAR-100 and CUB200 under different parameter settings. Fig.5(a), (b), (c) and (d) correspond to the parameters,,, and, respectively. To be specific,andcontrol the weights of the class associative loss and the prototype discriminative loss, whileandcontrol the relative importance of the pull close and push away components within each loss. As illustrated in Fig.5, whenis set to 0.5,to 1, andto 1, the model achieves the best classification performance. Specifically, whenis set to 1 and 2, our method achieves the highest classification accuracy on data sets CUB200 and CIFAR-100, respectively. Therefore, setting,, andto 0.5, 1, and 1, and choosingfrom {1, 2} are the suggested parameters for our method.\n\nWe conduct experiments on the data sets CIFAR-100 and CUB200 to verify the impact of different lengths of class-wise embeddingon model classification performance. To be specific, we selectin. As shown in Fig.6, for data set CIFAR-100, which has smaller image sizes (), the classification accuracy of the model is higher when the length of the class-wise embedding is less than or equal to 512. This is because excessively large embeddings can dilute important features in data sets with smaller feature dimensions, which reduces classification performance. Conversely, for the data set CUB200, which has larger image sizes (), the classification accuracy is higher when the length of the class-wise embedding is greater than or equal to 512, this is because for data sets with larger feature dimensions, excessively small embeddings can compress important feature information, leading to a decline in performance. Therefore, considering both cases, setting the class-wise embedding length to 512 is a good choice.\n\nSECTION: 5.CONCLUSION\n\nIn this paper, we have presented a novel method named CEL to address the IDPLL problem. For the first time, we realize that IDPLL is a mixed blessing with both positive side and negative side. We, therefore, propose to construct the class-wise embeddings to explore the relationships among the candidate labels and the non-candidate labels. To leverage the positive side of IDPLL, we introduced the class associative loss to learn representations that are more suitable for IDPLL. This is achieved by leveraging the similarity among labels within the candidate label set and the differences between the candidate label set and the non-candidate label set through class-wise embeddings. To mitigate the negative side of IDPLL, i.e., identifying the ground-truth label in the candidate set becomes more challenging, we constructed prototype discriminative loss to guide the model\u2019s disambiguation process using class prototypes which include global sample information. Extensive experiments on both common and fine-grained data sets demonstrate that our method significantly outperforms twelve state-of-the-art PLL and IDPLL methods. Moreover, compared with previous methods, our method converges faster in the early stages of model training, while produces the highest classification accuracy in the later training stages.\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.05029v1_content.txt"}, {"title": "Unifying Dual-Space Embedding for Entity Alignment via Contrastive\n  Learning", "authors": ["Cunda Wang", "Weihua Wang", "Qiuyu Liang", "Feilong Bao", "Guanglai Gao"], "published_date": "2024-12-06T13:25:09Z", "summary": "Entity alignment aims to match identical entities across different knowledge\ngraphs (KGs). Graph neural network-based entity alignment methods have achieved\npromising results in Euclidean space. However, KGs often contain complex\nstructures, including both local and hierarchical ones, which make it\nchallenging to efficiently represent them within a single space. In this paper,\nwe proposed a novel method UniEA, which unifies dual-space embedding to\npreserve the intrinsic structure of KGs. Specifically, we learn graph structure\nembedding in both Euclidean and hyperbolic spaces simultaneously to maximize\nthe consistency between the embedding in both spaces. Moreover, we employ\ncontrastive learning to mitigate the misalignment issues caused by similar\nentities, where embedding of similar neighboring entities within the KG become\ntoo close in distance. Extensive experiments on benchmark datasets demonstrate\nthat our method achieves state-of-the-art performance in structure-based EA.\nOur code is available at https://github.com/wonderCS1213/UniEA.", "arxiv_id": "2412.05028v1", "html_link": "https://arxiv.org/html/2412.05028v1", "search_term": "ti:\"embeddings\"", "html_content": "SECTION: Unifying Dual-Space Embedding for Entity Alignment via Contrastive Learning\n\nEntity alignment aims to match identical entities across different knowledge graphs (KGs).\nGraph neural network-based entity alignment methods have achieved promising results in Euclidean space.\nHowever, KGs often contain complex structures, including both local and hierarchical ones, which make it challenging to efficiently represent them within a single space.\nIn this paper, we proposed a novel method UniEA, which unifies dual-space embedding to preserve the intrinsic structure of KGs.\nSpecifically, we learn graph structure embedding in both Euclidean and hyperbolic spaces simultaneously to maximize the consistency between the embedding in both spaces.\nMoreover, we employ contrastive learning to mitigate the misalignment issues caused by similar entities, where embedding of similar neighboring entities within the KG become too close in distance.\nExtensive experiments on benchmark datasets demonstrate that our method achieves state-of-the-art performance in structure-based EA. Our code is available athttps://github.com/wonderCS1213/UniEA.\n\nUnifying Dual-Space Embedding for Entity Alignment via Contrastive Learning\n\nCunda Wang1,\nWeihua Wang1,2,3,\u2020\u2020thanks:Corresponding Author. Email:wangwh@imu.edu.cn.,\nQiuyu Liang1,\nFeilong Bao1,2,3,\nGuanglai Gao1,2,31College of Computer Science, Inner Mongolia University, Hohhot, China2National and Local Joint Engineering Research Center of Intelligent\nInformationProcessing Technology for Mongolian, Hohhot, China3Inner Mongolia Key Laboratory of Multilingual Artificial Intelligence Technology, Hohhot, China\n\nSECTION: 1Introduction\n\nKnowledge graphs (KGs) represent real-world knowledge in the form of graphs.\nThey typically store data in the form of triples, whererepresents the head entity,the relation, andthe tail entity.\nThe completeness of KGs affects tasks such as knowledge-driven question answering(Sun et\u00a0al.,2024)and recommendation(Cai et\u00a0al.,2023; Liang et\u00a0al.,2025).\nHence, it is essential to integrate multiple source KGs to build a comprehensive KG.\nEntity alignment (EA) serves as an important step in this process.\nIt aims to identify the same real-world entities referenced across different KGs.\n\nRecently, due to the strong neighborhood learning capabilities of graph neural networks (GNNs), GNN-based EA have achieved significant progress(Xie et\u00a0al.,2023; Wang et\u00a0al.,2024a; Sun et\u00a0al.,2020b).\nHowever, GNNs face two issues in Euclidean space embedding: 1) limited performance when handling complex hierarchical structures, and 2) the embeddings of neighboring entities are overly similar.\n\nAs shown in Figure1, this is a common type of hierarchical structure found in KGs.\nTraditional GNN-based EA methods often embed entities like \u201cIron Man\u201d and \u201cAmerica\u201d directly according to their Euclidean distance. Nevertheless, this does not reflect the true distance between these two entities, leading to distortion in the graph structure embeddings.\nThe hyperbolic space can capture the hierarchical structure of graphs(Wang et\u00a0al.,2024b; Liang et\u00a0al.,2024b). The hyperbolic distance better represents the true distance between the entities \u201cIron Man\u201d and \u201cAmerica\u201d.\nMoreover, these methods(Wang et\u00a0al.,2018; Yu et\u00a0al.,2021)cause similar entities within the same KG to have embeddings that are too close in distance.\nFor example, entities like \u201cRobert Downey Jr.\u201d and \u201cChris Evans\u201d share multiple neighboring entities, such as \u201cThe Avengers\u201c and \u201cAmerica\u201c. These shared neighbors often lead to homogenization, resulting in incorrect entity alignment.\nCurrent methods have proposed various solutions to these two challenges(Sun et\u00a0al.,2020a; Guo et\u00a0al.,2021; Xie et\u00a0al.,2023; Wang et\u00a0al.,2024a). For instance,Sun et\u00a0al. (2020a)andGuo et\u00a0al. (2021)explore EA task in hyperbolic space embedding, demonstrating that hyperbolic space is more effective for learning the hierarchical structure of graphs, which aids in entity alignment.Xie et\u00a0al. (2023)alleviates over-smoothing through graph augmentation techniques.\nHowever, the augmentation strategies, which randomly perturb the graph topology, may degrade the quality of the graph embeddings(Shen et\u00a0al.,2023).\nOur motivation is to consider hyperbolic space embedding as an augmentation of graph embedding.\nThis approach not only avoids the drawbacks of traditional graph augmentation techniques but also leverages the hierarchical structure information provided by hyperbolic embedding.\n\nTo address the aforementioned issues, we propose a novel method named UniEA, whichUnifies the Euclidean and hyperbolic spaces embedding forEA.\nOur method is not limited to embedding in a single space.\nSpecifically, we introduce graph attention networks (GATs)(Velickovic et\u00a0al.,2018)to aggregate neighboring entities in Euclidean space and employ hyperbolic graph convolutional networks (HGCNs)(Chami et\u00a0al.,2019)to learn the hierarchical structural information of the graph in hyperbolic space.\nWe maximize the consistency between the embedding in Euclidean space and hyperbolic space throughcontrastive learning, which leads to more accurate entity embeddings.\nMoreover, the close distances of similar neighboring embedding severely affect the final alignment of entities. We employcontrastive learning once againto address the issue.\nThe contributions of this work can be summarized as follows:\n\nWe propose a novel EA method called UniEA. To the best of our knowledge, this is the first method for EA that leverages contrastive learning to unify Euclidean and hyperbolic space embeddings.\n\nWe also employ contrastive learning to mitigate misalignment issues caused by overly close distances between similar entity embeddings.\n\nThe extensive experiments on four public datasets demonstrate that UniEA consistently outperforms the state-of-the-art methods for structure-based EA.\n\nSECTION: 2Related work\n\nIn line with our work, we review related work in three areas: EA in Euclidean space, representation learning in hyperbolic space and improving EA with graph augmentation.\n\nSECTION: 2.1EA in Euclidean Space\n\nCurrent embedding-based EA methods can be broadly categorized into three types: TransE-based EA, GNN-based EA and other methods. All of these primarily aim to learn embeddings for entities and relations from relational triples.\n\nDue to the strong performance of TransE(Bordes et\u00a0al.,2013)in capturing local semantic information of entities, several methods have proposed variants of TransE for application in EA.\nFor instance,Chen et\u00a0al. (2017)addresses the inconsistency in cross-lingual embedding spaces.Zhu et\u00a0al. (2017)emphasizes path information.Sun et\u00a0al. (2018)treats EA as a classification task.Pei et\u00a0al. (2019)enhances knowledge graph embedding by leveraging nodes with varying degrees.\n\nTransE-based EA methods lack the ability to effectively model global structural information. As a result, recent research increasingly favors GNN-based approaches for EA.\nStacking multiple layers of GNNs enables the capture of information from more distant neighbors, which facilitates learning of global structural information.\nFor example,Wang et\u00a0al. (2018)directly stacks multiple layers of vanilla GCN(Kipf and Welling,2017)to obtain entity embeddings.\nDue to the heterogeneity of KGs, the alignment performance is limited.Sun et\u00a0al. (2020b)employs a gating mechanism to attempt capturing effective information from distant neighbors. MRAEA(Mao et\u00a0al.,2020), RAEA(Zhu et\u00a0al.,2021), KE-GCN(Yu et\u00a0al.,2021), RSN4EA(Guo et\u00a0al.,2019), GAEA(Xie et\u00a0al.,2023), RHGN(Liu et\u00a0al.,2023), and GSEA(Wang et\u00a0al.,2024a)utilize rich relational information to obtain entity embeddings.Xin et\u00a0al. (2022)encoded neighbor nodes, triples, and relation paths together with transformers.\nUnfortunately, the ability to handle complex topological structures in graphs is limited in Euclidean space.\n\nAdditionally, some methods integrate the rich information within KGs to enhance the performance of EA tasks. This includes leveraging attributes(Liu et\u00a0al.,2020), entity names(Tang et\u00a0al.,2020)and more(Chen et\u00a0al.,2023).Jiang et\u00a0al. (2024)explores the potential of large language models for EA task.\nSince our method focuses on structural information, we do not compare it with the above methods to ensure experimental fairness.\n\nSECTION: 2.2Representation learning in hyperbolic space\n\nHyperbolic space has recently garnered considerable attention due to its strong potential for learning hierarchical structures and scale-free characteristics.\nFor example,Chami et\u00a0al. (2019)first introduced the use of graph convolutional networks (GCNs) and hyperbolic geometry through an inductive hyperbolic GCN.\n\nHyperbolic space representation learning has been applied to various downstream tasks, achieving excellent performance in areas such as node classification(Liang et\u00a0al.,2024b)and completion(Liang et\u00a0al.,2024a,c). Notably, existing work has successfully completed EA using hyperbolic space embedding.Sun et\u00a0al. (2020a)extends translational and GNN-based techniques to hyperbolic space, and captures associations by a hyperbolic transformation.Guo et\u00a0al. (2021)integrates multi-modal information in the hyperbolic space and predict the alignment results based on the hyperbolic distance.\nAlthough these methods demonstrate the advantages of hyperbolic embedding, they are limited to embedding solely in hyperbolic space.\n\nSECTION: 2.3Improving EA with graph augmentation\n\nGraph augmentation techniques primarily generate augmented graphs by perturbing the original graph through node dropout or edge disturbance, effectively enhancing the model\u2019s robustness to graph data.\n\nGraph augmentation techniques have been proven effective in entity alignment tasks. GAEA(Xie et\u00a0al.,2023)opts to generate augmented graphs by removing edges rather than adding new ones, as introducing additional edges can lead to extra noise. GSEA(Wang et\u00a0al.,2024a)employs singular value decomposition to generate augmented graphs, capturing the global structural information of the graph. It leverages contrastive loss to learn the mutual information between the global and local structures of entities. However, these methods fall short in effectively learning the hierarchical structure of graphs.\n\nSECTION: 3Preliminaries\n\nIn this section, we define the EA task and explain the fundamental principles of hyperbolic space. This foundation is essential for comprehending our approach.\n\nSECTION: 3.1Entity alignment\n\nFormally, we repesent a KG as, wheredenotes entities,denotes relations,repesents triples.\nGiven two KGs,repesent source KG,repesent target KG.\nEA aims to discern each entity pair,,whereandcorrespond to an identical real-world entity.\nTypically, we use pre-aligned seed entitiesto unify the embedding spaces of two KGs in order to predict the unaligned entities.\n\nSECTION: 3.2Hyperbolic space\n\nHyperbolic geometry is a non-Euclidean geometry with a constant negative curvature, where curvature measures how a geometric object deviates from a flat plane(Chami et\u00a0al.,2020).\nHere, we use the-dimensional Poincar\u00e9 ball model with negative curvature.\nFor each point, the tangent space (a sub-space of the Euclidean space)is a-dimensional vector space at point, which contains all possible directions of path inleaving from. Then, we introduce two basic operations that exponential and logarithmic maps in the hyperbolic space.\n\nLetbe the feature vector in the tangent space;is a point in the hyperbolic space, which is also as a reference point. Letbe the origin,, the tangent spacecan be mapped tovia the exponential map:\n\nConversely, the logarithmic map which mapstois defined as:\n\nHere,is hyperbolic space embedding.\n\nSECTION: 4Method\n\nIn this section, we elaborate on our approach in four parts.\nAs shown in Figure2, our method includes: 1) Euclidean space embedding, 2) hyperbolic space embedding, 3) relation encoding and fusion, and 4) the loss function.\n\nWe randomly initialize the entity and relation embedding of, represented asand, respectively. Similarly, the entity and relation embedding ofare represented asand.\nHere,denotes Euclidean space embedding;andstand for the dimensionality of entity and relation, respectively.\n\nSECTION: 4.1Euclidean space embedding\n\nThe ability of GAT to aggregate neighbor information in heterogeneous graphs has been well demonstrated(Chen et\u00a0al.,2023; Wang et\u00a0al.,2024a). We stack multiple layers of GAT to obtain Euclidean space embedding:\n\nwhereMdenotes the graph adjacency matrix,is a diagonal weight matrix for linear transformation.\n\nDue to the varying importance of the neighborhoods aggregated by different layers of GAT.\nFor example, in Figure1, aggregating the first-order neighbors of \u201cChris Evans\u201d is most beneficial. While aggregating higher-order neighbors can capture some implicit relationships of the entity, it often introduces noise.\nTherefore,Xie et\u00a0al. (2023)introduce an attention mechanism(Vaswani et\u00a0al.,2017)to assign different weights to the embeddings obtained from different layers:\n\nwhereis the scaling factor,andare the learnable paramenter matrices. Finally, the Euclidean space embedding.\n\nSECTION: 4.2Hyperbolic Space embedding\n\nOur method equips HGCN(Chami et\u00a0al.,2019)to learn the hierarchical structure of graphs in hyperbolic space.\n\nSpecifically, we project Euclidean space embeddingsto hyperbolic space using exponential map (Equation1):\n\nwhere, in other words, we obtain the first layer of embeddingin hyperbolic space.\n\nFor the hyperbolic space embedding of the-th layer, by hyperbolic feature aggregation, we can get the hyperbolic embedding of the next layer.\nThe hyperbolic aggregation process is as follows:\n\nArepresents the symmetric normalized adjacency matrix,isandis a trainable weight matrix.\n\nFor example, for the inputin-th layer, we can getusing Equation6.\n\nFinally, we can obtain the final outputin Hyperbolic Space.\nThe \u2018\u2019 is a hyper-parameter denoting the number of layers of the HGCN.\n\nSECTION: 4.3Relation encoding and fusion\n\nThe same entities often share similar relations, and relational semantic information is also highly beneficial for EA.Mao et\u00a0al. (2020)reveals that relying solely on the inflow direction to accumulate neighboring information through directed edges is insufficient. Accumulating information from the outflow direction as well would be highly beneficial.\nThis idea facilitates the bridging and propagation of more information in such a sparse graph.\nHence, following this work, we use both in-degree and out-degree relation encoders to learn the representation of relations:", "text_file": "data\\paper_texts\\2412.05028v1_content.txt"}, {"title": "A coisotropic embedding theorem for pre-multisymplectic manifolds", "authors": ["Luca Schiavone"], "published_date": "2024-12-06T10:58:17Z", "summary": "We prove a coisotropice embedding theorem \\`a l\\`a Gotay for\npre-multisymplectic manifolds", "arxiv_id": "2412.04941v1", "html_link": "https://arxiv.org/html/2412.04941v1", "search_term": "ti:\"embeddings\"", "html_content": "SECTION: A coisotropic embedding theorem for pre-multisymplectic manifolds\n\nSECTION: Acknowledgements\n\nI am indebted to prof. A. Ibort for making me aware of the existence of the coisotropic embedding theorem and its potential applications as well as for the huge amount of inspiring discussions of the last years.\nI am also indebted to prof. G. Marmo for encouraging me to think of alternative ways to depict the geometry behind Gotay\u2019s coisotropic embedding theorem.\nI also want to thank prof. A. Zampini, F.M. Ciaglia and F. Di Cosmo for many useful discussions.\nI acknowledge financial support from Next Generation EU through the project 2022XZSAFN \u2013 PRIN2022 CUP: E53D23005970006.\nI am a member of the GNSGA (Indam).\n\nSECTION: 1Introduction\n\nMultisymplectic geometry has emerged as a fundamental framework for the geometric description of Field Theories, extending the role that symplectic geometry plays in Classical Mechanics, as testified by the huge, even if not exhaustive, list of contributions[GPR69,GPR71,Kij73,GS73,KS76,TK79,AA80,Got91a,Got91b,BSF88,GMS97,CCI91,FF03,EEMLRR96,EEMLRR00,RR09,Kru15,CDI+20b,CDI+20a,CDI+24,CDI+22b,CDI+22c](see also references therein).\nIn this setting, the tangent and cotangent bundles of a configuration space are replaced by the first-order jet bundle of a fibration and its dual, providing a natural arena for analyzing the equations of motion and symmetries of Field Theories.\nThese jet bundles are examples of pre-multisymplectic manifolds, which generalize the notion of pre-symplectic geometry to higher-degree forms.\nIn this framework, to any Lagrangian (or Hamiltonian), a pre-multisymplectic structure is associated, that allows for an intrinsic formulation of a variational principle providing the equations of motion of the theory.\n\nIn previous contributions[CDI+20b,CDI+20a,CDI+24,CDI+22b,CDI+22c], we showed how, at least locally close to some Cauchy hypersurface, such equations of motion can be formulated in terms of (infinite-dimensional) pre-symplectic Hamiltonian systems.\nIn particular, we established that the space of solutions of the equations of motion inherits a pre-symplectic structure.\nThe kernel of this structure descends from the kernel of the pre-multisymplectic form mentioned above and encodes the gauge symmetries of the theory.\n\nTo define a Poisson structure on the space of solutions, we employed a regularization technique based onM. Gotay\u2019scoisotropic embedding theorem[Got82](see also[GS90]for an equivariant version of the theorem and[OP05]for a more modern approach).\nGotay\u2019s theorem provides a powerful tool in symplectic geometry, ensuring that any pre-symplectic manifold can be embedded as a coisotropic submanifold into a symplectic manifold, referred to as itssymplectic thickening.\nIn[CDI+24,CDI+22b,CDI+22c], we used Gotay\u2019s theorem to coisotropically embed the space of solutions of the equations of motion of Classical Field Theories into a symplectic manifold where a Poisson structure is canonically defined.\nWe then studied whether and how this Poisson structure projects back to a Poisson structure on the space of solutions.\n\nHaving in mind the previous statement that Field Theories can be locally formulated in terms of (infinite-dimensional) pre-symplectic Hamiltonian systems, the coisotropic embedding theorem has been used by the author also to provide a one-to-one correspondence between symmetries and constants of the motion[CDI+22a]and to provide a solution for the inverse problem of the calculus of variations for a class of implicit differential equations[Sch24b,Sch24a].\n\nWhile effective in all the cases mentioned above, this regularization approach requires handling infinite-dimensional spaces, as the space of solutions is typically infinite-dimensional.\nTo circumvent the technical challenges associated with infinite-dimensional spaces, it may be useful to develop a multisymplectic analogue of Gotay\u2019s theorem, allowing coisotropic regularization to be carried out directly at the finite-dimensional level of the underlying pre-multisymplectic manifold.\n\nThe aim of this manuscript is to prove a coisotropic embedding theorem for pre-multisymplectic manifolds.\nSpecifically, we will show that any pre-multisymplectic manifold can be embedded as a coisotropic submanifold into a larger multisymplectic manifold.\nThis generalization provides a finite-dimensional framework for coisotropic regularization and opens new avenues for the study of multisymplectic geometry and its applications to Field Theories.\n\nIt is worth pointing out that recent work has focused on the interplay between coisotropic submanifolds and multisymplectic geometry, particularly in the opposite direction taken in this manuscript, namely that of multisymplectic reduction.\nIn particular, in[de 24]the authors analyze coisotropic submanifolds of pre-multisymplectic manifolds and study conditions under which they inherit multisymplectic structures.\n\nThe structure of this paper is as follows.\nInSection2, we recall preliminary notions about pre-multisymplectic manifolds that will be used throughout the manuscript.Section3is devoted to the statement and proof of the main theorem.\n\nSECTION: 2Preliminaries\n\nA-multisymplectic manifoldis a smooth differential manifoldequipped with a closed and non-degenerate differential-form.\n\nA-pre-multisymplectic manifoldis a smooth differential manifoldequipped with a closed differential-form.\n\nGiven a multisymplectic manifoldand a submanifold, the-multisymplectic orthogonalofinatis\n\nEvidently\n\nand\n\nA submanifoldof a multisymplectic manifoldis-coisotropic if.\n\nGiven a smooth differential manifold, we will denote bythe bundle of differential-forms on, namely the vector bundle overwhose typical fibre atis.\nWe will denote bythe canonical projection.\n\nGiven a smooth-dimensional differential manifold, we will usually denote by\n\na system of local coordinates on it and by\n\nan adapted system of local coordinates on.\n\nConsider a-pre-multisymplectic manifold.\nThe kernel ofis, at each, a subspace of, denoted by.\nWe will always assumeto have constant rank, so thatandare isomorphic for any pair.\nMoreover, we will also always assume thatprovides a completely integrable distribution on111Note that the complete integrability of the distribution generated byis ensured by the closure ofonly whenbyFrobenius theorem[AMR88].so that there exists a foliation ofsuch that the tangent space to each leaf at each pointcoincides withand there exists a unique leaf passing through any point of.\nThe space of leaves of the foliation is the space of equivalence classes of points onbelonging to the same leaf and will be denoted by\n\nSometimes, we will consider ona system of local coordinates adapted to such a foliation\n\nwhereis the rank ofandis the dimension of its kernel.\nThe coordinatesare a system of coordinates on the space of leaves of the foliation, namely, locally each leaf is a level set of the type\n\nwhereas the coordinates(for fixed values of the\u2019s) individuates a point on each leaf.\nIn this system of coordinates,reads\n\nA complement toat eachis not canonically defined.\nIndeed, each choice of a complementsuch that\n\namounts to the choice of aconnectionon.\n\nAconnectiononis an idempotent smooth-tensor field onwhose image, at each, is.\n\nLocally a connection can be written as\n\nwhere the functionsare the so-calledconnection coefficients(see, for instance[GMS10, App.]or[GMS09, Sec.]).\nBy means of, the tangent space toat each point splits into the direct sum of tangent vectors in the image ofand tangent vectors in the kernel of.\nThe image ofreads\n\nand is usually referred to as the space ofvertical vectors, whereas the kernel reads\n\nand is usually referred to as the space ofhorizontal vectors.\nIt is easy to show that under a change of local chart, the point-wise splitting into vertical and horizontal tangent vectors is preserved and, thus, the tangent bundle tosplit as the direct sum\n\nwhere(resp.) is the vector subbundle ofwhose sections are, at each point, the vertical (resp. horizontal) vectors defined by.\n\nGiven any connection, there exists a related connectionwhose horizontal space coincides with the vertical space ofand vice-versa.\nWe will denote by\n\nthe splitting ofinduced by the connection.\n\nThe splitting generated byon vector fields induces a splitting for differential-forms on.\nIndeed, given a-format some point of, it can always be written as\n\nwhere\n\nand\n\nWe will refer toandas theparallelandtransversalcomponents ofassociated with the connection.\n\nIn the system of local coordinates chosen, if one writes the-form in terms of the basis of differential-forms\n\ndual to the basis of vertical and horizontal vectors associated to, one has\n\nwhere\n\nIn this system of local coordinates, one gets\n\nwhereis a multi-indexwhere the-th component isand the sum is taken over all the multi-indices of this type.\nWe will denote bythe set of these multi-indices.\nAs for vertical and horizontal tangent vectors, it is easy to show that this splitting of-forms is stable under a change of local chart, and, thus, it defines two subbundles of.\nWe will denote by\n\nthe splitting of the fibre bundleinto the direct sum of the fibre subbundlesandwhose sections are parallel (resp. transversal) differential-forms with respect to.\nWe will denote by\n\na system of local coordinates onand by\n\na system of local coordinates on.\n\nAnalogously, by means of the connection, one can construct the splitting\n\nwhere\n\nand\n\nIn the system of local coordinates chosen\n\nwhereis a multi-indexwhere the-th component isand the sum is taken over all the multi-indices of this type.\nWe will denote bythe set of these multi-indices.\n\nWe will denote by\n\nthe splitting of the fibre bundleinto the direct sum of the fibre subbundlesandwhose sections are parallel (resp. transversal) differential-forms with respect to.\nClearly, if,is empty, and.\n\nWe will denote by\n\na system of local coordinates onand by\n\na system of local coordinates on.\n\nSECTION: 3The theorem\n\nHere, we provide the statement and the proof of the multisymplectic coisotropic embedding theorem.\n\nSECTION: 3.1Statement\n\nLetbe a-pre-multisymplectic manifold.\nThen, there exists a multisymplectic manifold, referred to as themultisymplectic thickeningof, and an embedding, such that:\n\nis equipped with a-multisymplectic form.\n\nbecomes a-coisotropic submanifold of.\nIn particular.\n\nSECTION: 3.2Proof\n\nConsider a-dimensional-pre-multisymplectic manifold.\nIn the system of coordinates adapted to the foliation induced by the kernel ofdescribed inSection2,reads:\n\nConsider the bundledescribed inSection2, with the system of local coordinates\n\nConsider the-form ondefined by\n\nwherehas to be understood as a point inon the left hand side and as a (transversal)-form onon the right hand side,is the projection fromto, and.\nIn the system of local coordinates chosen\n\nConsider onthe following-form\n\nIn the system of local coordinates chosen\n\nThe-formis closed by construction.\n\nTo prove whether it is also non-degenerate, we will consider its contraction along the following basis of vector fields on:\n\nVia a direct computation one shows\n\nwhereand.\nSince none of the right-hand sides of the equations above vanish, one concludes thatis non-degenerate.\n\nNote that the caseis a \"degenerate\" situation, where the second line of (41) does not appear and, thus,does not vanish unless eitheror(namely, if we are in a tubular neighborhood of the zero section of).\nThis is, indeed, the classical coisotropic embedding theorem for pre-symplectic manifolds.\n\nNote that the departing-pre-multisymplectic manifoldcan be embedded intoas the zero-sectionof the projection.\nA straightforward computation also shows that\n\nTo show thatis coisotropic inside, let us consider a vector field tangent to\n\nand a vector field tangent to\n\nand let us compute\n\nA direct computation shows that\n\nThe term involving\n\nis independent on the others.\nThus, imposingto vanish for allforces all theto be zero.\nImposing this condition into the other terms leads to the fact that also all thevanish.\nConsequently,is empty, and, thus,is trivially-coisotropic inside.\n\nThe arguments above can be iterated up to the orderto show thatis-coisotropic inside.\n\nSECTION: Conclusions and Future Plans\n\nThis paper has established a coisotropic embedding theorem for pre-multisymplectic manifolds.\nWe constructed a suitable multisymplectic thickening of any given pre-multisymplectic manifold such that the original manifold can be embedded as a coisotropic submanifold.\nThis embedding facilitates a more accessible finite-dimensional approach to regularization techniques, which traditionally require handling infinite-dimensional spaces.\n\nBuilding on these results, several directions for future research emerge.\n\nWe will apply the coisotropic embedding theorem proved to Field Theories, particularly in constructing a Poisson bracket on the space of solutions, as we already did in[CDI+24,CDI+22b,CDI+22c].\nWe will study the relation between the two approaches and the potential advantages of the one based on the multisymplectic coisotropic embedding theorem proved in this manuscript.\n\nImplicit partial differential equations can often be formulated using pre-multisymplectic structures.\nThe regularization method introduced in this paper allows for the definition of a multisymplectic structure on an extended manifold, enabling (under some conditions that have to be studied) the formulation of an explicit PDE.\nFuture work will investigate:\n\nThe correspondence between solutions of the original implicit PDE and the explicit PDE on the extended manifold.\n\nThe conditions under which the integrability of the explicit PDE ensures the integrability of the original system.\n\nThe development of tools to apply this framework to specific classes of implicit PDEs, thereby improving the understanding of their geometric and analytic properties.\n\nSeveral results proven by the author, including those in[CDI+22a,Sch24b,Sch24a], rely on a local formulation of Field Theories as pre-symplectic Hamiltonian systems.\nUsing the coisotropic embedding theorem, we aim to extend these results to global contexts by avoiding passing through the above-mentioned pre-symplectic Hamiltonian system, which is only locally defined close to some Cauchy hypersurface.\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.04941v1_content.txt"}, {"title": "Acquired TASTE: Multimodal Stance Detection with Textual and Structural\n  Embeddings", "authors": ["Guy Barel", "Oren Tsur", "Dan Vilenchik"], "published_date": "2024-12-04T19:23:37Z", "summary": "Stance detection plays a pivotal role in enabling an extensive range of\ndownstream applications, from discourse parsing to tracing the spread of fake\nnews and the denial of scientific facts. While most stance classification\nmodels rely on textual representation of the utterance in question, prior work\nhas demonstrated the importance of the conversational context in stance\ndetection. In this work we introduce TASTE -- a multimodal architecture for\nstance detection that harmoniously fuses Transformer-based content embedding\nwith unsupervised structural embedding. Through the fine-tuning of a pretrained\ntransformer and the amalgamation with social embedding via a Gated Residual\nNetwork (GRN) layer, our model adeptly captures the complex interplay between\ncontent and conversational structure in determining stance. TASTE achieves\nstate-of-the-art results on common benchmarks, significantly outperforming an\narray of strong baselines. Comparative evaluations underscore the benefits of\nsocial grounding -- emphasizing the criticality of concurrently harnessing both\ncontent and structure for enhanced stance detection.", "arxiv_id": "2412.03681v2", "html_link": "https://arxiv.org/html/2412.03681v2", "search_term": "ti:\"embeddings\"", "html_content": "SECTION: AcquiredTASTE:Multimodal Stance Detection with Textual and Structural Embeddings\n\nStance detection plays a pivotal role in enabling an extensive range of downstream applications, from discourse parsing to tracing the spread of fake news and the denial of scientific facts. While most stance classification models rely on textual representation of the utterance in question, prior work has demonstrated the importance of the conversational context in stance detection.\nIn this work we introduce TASTE \u2013 a multimodal architecture for stance detection that harmoniously fuses Transformer-based content embedding with unsupervised structural embedding. Through the fine-tuning of a pretrained transformer and the amalgamation with social embedding via a Gated Residual Network (GRN) layer, our model adeptly captures the complex interplay between content and conversational structure in determining stance. TASTE achieves state-of-the-art results on common benchmarks, significantly outperforming an array of strong baselines.\nComparative evaluations underscore the benefits of social grounding \u2013 emphasizing the criticality of concurrently harnessing both content and structure for enhanced stance detection.\n\nAcquiredTASTE:Multimodal Stance Detection with Textual and Structural Embeddings\n\nGuy BarelBen Gurion Universityguybare@post.bgu.ac.ilOren TsurBen Gurion Universityorentsur@bgu.ac.ilDan VilenchikBen Gurion Universityvilenchi@bgu.ac.il\n\nSECTION: 1Introduction\n\nStance detection is the task in which the attitude of a speaker towards a target (individual, idea, proposition) is identified. A typical example provided bySomasundaran and Wiebe (2010): Given an utterance U (\u201cGovernment is a disease pretending to be its own cure\u201d) and a target T (universal health care), we wish to determine the speaker\u2019s stance, as reflected in U toward the target T. Notice that the target is not explicitly mentioned by the speaker. A shared task with a similar setting was introduced in SemEval byMohammad et\u00a0al. (2016).\n\nHowever, opinions are not expressed in a vacuum but rather in a conversational context. The speaker\u2019s stance may emerge as the conversation evolves. This idea echoes Goffman\u2019s classic assertion in Response CriesGoffman (1978): \u201cUtterances are not housed in paragraphs but in turns at talk, occasions implying a temporary taking of the floor as well as alternation of takers\u201d.\n\nIndeed, some works look beyond the utterance level and address its context \u2013 whether the speaker level or the conversational levelWalker et\u00a0al. (2012a); Sridhar et\u00a0al. (2015); Johnson and Goldwasser (2016); Joseph et\u00a0al. (2017); Li et\u00a0al. (2018); Pujari and Goldwasser (2021), among others. Stance detection based on the conversation structure alone was recently demonstrated byPick et\u00a0al. (2022).\n\nFocusing on conversational stance, we are interested in detecting stance in a multi-participant conversation, both on the utterance and speaker level. To this end, we propose TASTE \u2013 a multimodal architecture combining Textual And STructural Embeddings. Multi-participant conversations unfold in tree structures that can be converted to speaker graphs (see details in Section3.2). TASTE uses LLMs to represent utterances; The max-cut graph optimization problem is used to derive contextual node (speaker) embeddings from the conversation graph. The embeddings of the two modalities are fused through a GRN layer. We considered several other text and node representations as well as different ways to combine them in a principled way.\n\nOur architecture has two key benefits:\nFirst, it consistently outperforms an array of strong baselines, including the state-of-the-art, across topics in commonly used datasets.\nSecond, through ablation and careful analysis, we get a glimpse of the interdependence of text and conversation structure.\n\nWe find that the heavy lifting is achieved by the conversation structure. Teasing text and structure modalities apart, we find that, in most cases, using the structure alone outperforms text-based models. Combining the textual modality with the structural further adds, on average, 12% to the accuracy.\nThis result should not be read as \u201cstructure is more important than text\u201d. Rather, we maintain that the stance signal is encoded in a structure with a higher signal-to-noise ratio compared to text. This makes sense as texts are produced in a conversational context by subjective individuals. We further discuss this idea in Section7as it goes back to the sociolinguistics concepts of faceGoffman (1955), conversational normsGrice (1975), and communication groundingClark and Brennan (1991).\n\nSECTION: 2Related Work\n\nEarly work tackled stance detection by identifying specific lexical forms.Biber and Finegan (1988)consider six adverbial categories that mark the speaker\u2019s stance. Lexicons of word categories reflecting psychological states such as LIWCTausczik and Pennebaker (2010)and MPQAWilson et\u00a0al. (2005)were used effectively bySomasundaran and Wiebe (2010); Murakami and Raymond (2010); Yin et\u00a0al. (2012); Walker et\u00a0al. (2012a); Wang and Cardie (2014); Elfardy and Diab (2016)among others.\n\nRecent works fine-tune LLMsSamih and Darwish (2021); Kawintiranon and Singh (2021), or employ zero-shotAllaway and McKeown (2020); Liang et\u00a0al. (2022); Allaway and McKeown (2023); Weinzierl and Harabagiu (2024)and few-shot frameworksLiu et\u00a0al. (2021,2022); Wen and Hauptmann (2023); Khiabani and Zubiaga (2024).\nThese works use the textual content of an utterance as the sole or primary signal.\n\nSpeaker (user) and social attributes are extracted and used to enrich textual attributes in training regression and SVM models to predict utterance-level stanceAldayel and Magdy (2019); Lynn et\u00a0al. (2019), whileMurakami and Raymond (2010); Walker et\u00a0al. (2012b); Yin et\u00a0al. (2012)use the conversation structure with textual markers. Branch-BertLi et\u00a0al. (2023)uses part of the conversation structure, i.e. a branch in the conversation tree in order to provide their stance classifier with some contextual information. While it is shown that the naive turn-taking structure that is captured in a branch does not fair well with graph-based embeddingsPick et\u00a0al. (2022), a turn-taking modeling combined with a LLM achieved improved results comparing to text-only modelsWen et\u00a0al. (2024).\n\nThe relationship between the post and the user level is addressed bySridhar et\u00a0al. (2015); Benton and Dredze (2018); Li et\u00a0al. (2018); Conforti et\u00a0al. (2020).\nA hierarchical model combining text and user representation is proposed byPorco and Goldwasser (2020).Pick et\u00a0al. (2022)andZhang et\u00a0al. (2023)demonstrate that structure alone can achieve competitive results on both the utterance and the speaker levels.\n\nThese works, using or combining the different modalities, serve as our baseline models (see Section5) and as the reference point for our analysis and discussion (Sections6&7).\n\nModalities indicate the differentmediumsthat information is conveyed through text (in natural language), code (formal language), audio, visual, etc. Using more than a single modality achieves an improvement in the performance of many ML systems on multiple tasks. In multimodal learning, the representations of the different modalities are used explicitly in the training phaseRamachandram and Taylor (2017); Xu et\u00a0al. (2023), rather than just defining a feature vector, e.g.,Aldayel and Magdy (2019); Lynn et\u00a0al. (2019).\n\nNaturally, the common modalities that are used are those with mature uni-modal frameworks such as Transformers and CNNs for text and image processing and generationMao et\u00a0al. (2016); Yu et\u00a0al. (2017); Sharma et\u00a0al. (2018); Ramesh et\u00a0al. (2021); Ding et\u00a0al. (2022); Rombach et\u00a0al. (2022);Ramesh et\u00a0al.; Kwak et\u00a0al. (2023); modalities that are closely related such as conversational audio with its transcribed text, e.g.,Lai et\u00a0al. (2019); Yao et\u00a0al. (2020); Yao and Mihalcea (2022), or code segments coupled with documentation, e.g.,Kwak et\u00a0al. (2023)and commercial products like Github\u2019s Copilot, OpenAI\u2019s Codex, and Google\u2019s Gemini.\n\nTwo of the works mentioned above explicitly address multi-modality in the context of stance detection. However, we note that the second modality used byWeinzierl and Harabagiu (2024)is the visual (image) modality; Multimodality inKhiabani and Zubiaga (2024)refers to the use the different modalities independently to train a number of independent classifiers.\n\nIn this paper, we apply the multimodal framework over two modalities: text and social context.\n\nSECTION: 3Methodology\n\nSECTION: 3.1Task Definition\n\nGiven a set of authorsparticipating in a discussion, the setdenotes the utterances produced bythrough the discussion, where each utteranceand userhave a label(pro,con).\n\nIn this work, we address two distinct stance prediction tasks: utterance-level and author-level. At the utterance level, the classification task is straightforward \u2013 learning a classifierthat minimizes some loss function. The dot inrepresents additional input that the classifier may take besides the utterance. In our case, this extra input is the speakers\u2019 network graph (see Figure1). In other cases that can be meta-data like the speaker\u2019s age, gender, etc.\n\nAt the author/speaker level, we assume stance labels are assigned to speakers. This assumption is valid if a speaker holds a pre-formed and stable stance throughout the discussion. This is inherently the case in debate-like discussions like those in our datasets.\nThe task, in this case, is to train a classifier, in which all the utterances of a user are fed to the classifier, plus additional data (the graph in our case).\n\nSECTION: 3.2Model Architecture and Components\n\nThe model depicted in Figure2is the utterance-level classifier. It receives as input two modalities: textual and structural. The textual modality is the embedding of the utterance, and the structural is an embedding of the speaker (details below). The two embeddings are then fused using a GRN unit and fed into an MLP classifier. The MLP outputs a vector that quantifies the likelihood of each tagin the tagset(in our case), and the binary prediction is the tag with maximum likelihood.\n\nThe author\u2019s stance is computed by feeding all her utterances into the model (Figure2), getting the prediction for each utterance separately, and taking the majority vote.\n\nIn the remainder of this section, we describe the model components in more detail.\n\nThe representation of an utteranceis derived directly from the pretrained model. Specifically,, the content embedding for, is obtained as the [CLS] token vector, which is extracted by feedingto pretrained Sentence-BERTReimers and Gurevych (2019).\n\nWe followPick et\u00a0al. (2022)in the structure representation and user embedding. The discussion tree is first converted to a conversation graph (see Figure1); each node represents a speaker, and an edgebetween nodesandindicates a direct interaction between them.indicates the weight of, determined by the number and type of interactions. The edge weights in the interaction graph are calculated as:\n\nwhereandindicate the number of direct replies and quotes, between usersand. The hyper-parametersandare used to reflect the significance of the type of interaction, often based on the conversational norms expected in the specific platform. Through empirical testing, we determined the optimal values for these weights. For the 4Forums dataset, where interactions typically consist of direct replies to the original post (OP) and selective quoting of pertinent content, the optimal values were found to beand. Conversely, for CreateDebate,was set at 1.0 andat 0.0, reflecting the infrequent use of quotes in this platform.\n\nThe intuition underpinning the structural embedding is that stronger disagreements lead to more intense interactions in the discussion tree, reflected as heavier-weight edges in the network. Thus, the user embeddings are trained to maximize the distance between users that are connected by heavy edges (max-cut). Mathematically, the divergence in views betweenandcan be encoded in a distance metricbetween two vectors. The maximal value ofis 1 (for antipodal vectors/stances), and the minimum is 0 (when, same stance). This all leads to the following optimization problem over all pairs of interacting authors:\n\nwhereandare the unit vector embeddings for usersandandis the-dimensional unit sphere (is the number of users).\n\nThe optimization program in Eq.(2) is called a Semi-Definite Program (SDP) and can be efficiently solved using algorithms like the Ellipsoid method. We use the SDP proposed byGoemans and Williamson (1995)as a relaxation for the max-cut problem. Figure1illustrates the relation between the max-cut problem and the SDP solution.\n\nGated Residual Networks (GRNs), as proposed byLim et\u00a0al. (2021), present an innovative approach to integrating a primary input vector with multiple context vectors of which relevance may vary. Notably, GRNs have proven to be particularly effective in handling datasets that are both relatively small and characterized by noise.\nIn its basic form, the GRN unit processes a primary input vector, denoted, along with a context vector:\n\nWe take the user content embedding as the primary vector and the user embedding as the context, fusing them through the GRN (taking the structural embedding as the primary vector and the text embeddings as the context yielded suboptimal results).\n\nSECTION: 4Data\n\nOur analysis is conducted on two distinct datasets: 4Forums, introduced byWalker et\u00a0al. (2012a), and CreateDebate, presented byHasan and Ng (2014). These datasets have been widely used in prior work on stance detection, e.g.,Walker et\u00a0al. (2012a); Sridhar et\u00a0al. (2015); Abbott et\u00a0al. (2016); Li et\u00a0al. (2018); Pick et\u00a0al. (2022). For the reader\u2019s convenience, we provide a brief overview of the datasets, emphasizing their unique characteristics.\nDescriptive statistics, comparing the two datasets are presented in Table1.\n\n4Forums (no longer online) was a platform for political debates. Introduced byWalker et\u00a0al. (2012a), the dataset includes annotations for agree/disagree stances in 202 debates on four major topics: abortion, evolution, gay marriage, and gun control. Annotations are provided at the user level. Gold labels of utterances are derived by broadcasting the author\u2019s label to his posts.\n\nCreateDebate, is an online platform developed as \u201ca social tool that democratizes the decision-making process through online debate\u201d.\nA user (the \u2018OP\u2019) starts a debate by posing a question (e.g., \u201cShould abortion be illegal: Yes or No?\u201d). Other users can respond to the OP or to other users by adding a support, dispute, or clarification message. A benchmark containing 200 debates over four topics (abortion, gay rights, the legalization of marijuana, and Obama) was introduced byHasan and Ng (2014). Self-annotation by debate participants provides gold labels at the utterance level. The most frequent self-assigned tag serves as the gold label of user.111Indeed, over 95% of the users self-annotated all of their utterances with the same label.\n\nSECTION: 5Experimental Settings\n\nSECTION: 5.1Baselines\n\nWe compare our architecture to four other models based on text, structure or both:\n\nThe STEM algorithmPick et\u00a0al. (2022)is a stance classification algorithm that uses only the conversation structure. At its core, it is based on the SDP spelled in Eq.\u00a0(2) and has shown superior results in author-level stance detection across datasets. It proceeds in three steps, the first is to compute the 2-core of the conversation graph; next compute the SDP embedding on the 2-core and derive author classifications from the vectors; finally, propagate the labels to the non-core part of the graph in a greedy manner. The derivation of author labels from SDP vector embedding is done via hyper-plane roundingGoemans and Williamson (1995). The geometric positioning of the user vectors on the-dimensional sphere is illustrated in Figure1.\n\nA simplified version of STEM, which we call for simplicity SDP, skips the 2-core computation and applies SDP to the entire graph. The labels are derived using the hyperplane rounding technique.\n\nThe Probabilistic Soft Logic (PSL)Sridhar et\u00a0al. (2015)approach combines the expressiveness of first-order logic with the probabilistic modeling capabilities of graphical models. It allows for the flexible specification of complex, relational structures and dependencies in the data, making it well-suited for tasks such as stance detection. PSL formulates the problem as a joint probabilistic inference task, where the goal is to infer the most likely values of the unobserved variables (such as stance labels) given the observed data (such as textual features).\n\nSentence-BERT is a modification of the BERT model specifically designed for sentence embeddings. Developed byReimers and Gurevych (2019), S-BERT fine-tunes BERT by training it on a siamese and triplet network architecture, which allows it to learn better sentence embeddings. These embeddings are capable of capturing semantic similarity between sentences, making them useful for various natural language processing tasks like sentence classification, semantic search, and clustering. S-BERT has been shown to outperform traditional BERT embeddings in tasks that require an understanding of sentence-level semantics.\n\nThe Global Embedding model, as introduced byLi et\u00a0al. (2018), leverages both text and structural information within online debates to create a unified global embedding. Unlike traditional methods that might treat text and structure separately, their method captures the nuanced interplay between an author\u2019s contributions and the broader conversational context. While their approach closely aligns with our methodology in considering both structural and textual information, their model integrates these two dimensions into a single global embedding, contrasting with our technique of generating distinct embeddings for text and structure separately.\n\nSECTION: 6Results and Analysis\n\nTable2(a\u2013d) compares TASTE and the baseline models for utterance and user level over the two benchmarks and across topics. The table also provides results of three variations of the TASTE architecture: (1)uses GRN to fuse the SDP and the S-BERT embedding; (2)skips the GRN, concatenates the two embeddings and pushes them into the MLP layer; (3)uses a node2vecGrover and Leskovec (2016)instead of SDP.\n\nKeeping in line with previous works, we use accuracy as the base metric to evaluate the models. Reported values are average accuracy of a 5-fold cross-validation setting. All results achieved statistical significance with-values less thanin paired-tests comparing themodel against the baseline models.\n\nachieves best performance in 3 out of 4 tasks (2a-c), across all topics.was ranked second, trailing STEM, in the fourth task at Table2d. However, replicating the work ofPick et\u00a0al. (2022)we observed that they excluded users lacking strong inter-annotator agreement; these are probably the more challenging cases. In contrast, our application of TASTE and the SDP considers all users, hence the observed differences in performance in some cases.\nThe other two versions of TASTE are often competitive but are outperformed by some of the baselines. This trend emphasizes the advantage provided by the learning SDP embeddings, compared to node2vec, and of the use of the GRN unit for vectors fusion, compared to concatenation.\nFinally, we observe that SDP alone achieves excellent results \u2013 being the strongest of the baselines in tasks a\u2013c.\n\nUsing uni-modal approach, we can tease the content and the structure apart and gain some interesting insights about interplay between them. intense (dense) though short (in utterance length) interactions, such as those in the Abortion, Evolution, and Gay Marriage topics, are better captured by structural models such as the SDP, while less intense interactions that exhibit longer utterances, such as in Gun Control, are better captured by text based embeddings such as S-BERT. The uni-modal results along with the number of interactions and the number of tokens in each topic are provided in Table3in AppendixA.\n\nThis pattern can be explained in a number of ways: (i) in highly interactive but less verbose environments, the structure of interactions becomes more important in indicating stance. The structure of the discourse, encompassing aspects such as the frequency and network of replies, becomes a potent indicator of the users\u2019 stances, and (ii) long argumentation is correlated with sparser networks since participation is more demanding. On the other hand \u2013 the longer texts provide stronger signal to be exploited by text-based models.\n\nThese results suggest that the effectiveness of structure vs. content-focused models also depends on the conversational dynamics that constitute the (local) social context.\n\nIntegrating content and structural information in a multimodal manner proves robust across datasets, topics, and conversational dynamics. The multimodal approach grounds the potentially rich textual content in the relevant social context. An additional analysis of a concrete example is provided in AppendixB, underscoring the benefits of our multimodal approach.\n\nUsing SDP to obtain node embeddings proved superior to the more traditional node2vec approach, with performance gains between 0.09 and 0.17. We attribute the difference in performance to two factors: (i) node2vec performs best on networks much larger than the conversational networks in our datasets, and (ii) The node2vec approache aims at minimizing the distances between neighbouring nodes, while in our case we actually try to separate them, given the intuition that they reflect opposite stances. The SDP, on the other hand is designed to achieve exactly that via the max-cut.\n\nWe considered a naive yet commonly used fusion strategy, namely concatenation of the representations (see results in Table4in AppendixC). Using a GRN layer significantly outperforms concatenation across all topics. This notable difference in the results can be attributed to the GRN layer\u2019s ability to scrupulously combine the distinct properties of both content and structure embeddings. Unlike simple concatenation, which merely amalgamates or adds up the information, the GRN layer applies a more nuanced approach. It effectively \u2018gates\u2019 the information from each embedding, allowing for a selective integration process. This gating mechanism is particularly advantageous in handling smaller and noisier datasets where discerning relevant information is critical. The GRN layer\u2019s capacity to attend to the the most pertinent features from both content and structural data results in a more accurate and robust stance detection model.\n\nExamining the error rate of TASTE at the author level with regards to the number of utterances per user, we observed that the error rate over users with only one or two utterances isthe average error rate, and an order of magnitude higher than the error rate over users with twenty utterances or more (which drops to only 0.05). Nevertheless, TASTE outperforms other models even over the users with a low number of utterances. These results again confirm our intuition: Sharp stance differences are reflected by the conversation dynamics. The more intense the conversation \u2013 the more utterances a user makes (high engagement), providing a stronger signal in both modalities. However, the structural embeddings enhance the signal even in cases of a curtailed contribution of one or two utterances.\n\nThe performance gap between TASTE and other models is notably smaller at the author level than at the post level. This can be explained by the inference stage methodology, which determines a user\u2019s stance through majority voting across all their posts. Consequently, if a model accurately predicts the majority stance of a user\u2019s posts, it achieves perfect accuracy for that user, which is a harder task than correctly classifying each individual post by that same user. Therefore, the latter task is more effective at highlighting performance differences between models.\n\nSECTION: 7Discussion\n\nIn linguistics, cognitive science, and communication studies the concept ofgroundingrefers to a communication phase in which the interlocutors assume or establish the common ground required for mutual understandingHarnad (1990); Clark and Brennan (1991); Lewis (2008). In the field of AI (NLP, Autonomous Agents, etc.), the term \u2018grounding\u2019 usually means that models are trained with respect to other modalities that reflect the \u201cenvironment\u201d. Typical examples include visual and audio modalities, e.g.,Ngiam et\u00a0al. (2011); Mao et\u00a0al. (2016); Yao and Mihalcea (2022). This latter practical approach to grounding could be viewed as a limited implementation of the broader concept of communication groundingChandu et\u00a0al. (2021). Our results empirically confirm fundamental frameworks such as face workGoffman (1955)and the cooperative principleGrice (1975)\u2013 in which texts are interpreted upon grounding in the (limited) social context \u2013 the conversation graph.\n\nSome recent work (see Section2) use other, more recent, datasets. The focus of this work is the interplay between content and speaker dynamics, hence whese datasets cannot be used for proper comparison of the utterance level, nor for speaker-level stance.\n\nThe work ofLi et\u00a0al. (2018), explicitly modeling text and structure, is the most similar to our work. However, while their strategy for integrating text, speaker, and structure can be viewed as early fusion222We note that Li et al. do not refer to their model as multimodal and do not explicitly refer to the fusion strategy., resulting in global embeddings in one shared space, our methodology combines embeddings of the different modalities using joint (hierarchical) fusion strategyHuang et\u00a0al. (2020); Xu et\u00a0al. (2023). We believe that the joint fusion of separate embeddings provides flexibility that results in higher efficiency. Specifically, it allows the use of the SDP for speaker representation. These non-orthodox embeddings are learned efficiently at the conversation level in a completely unsupervised manner.\n\nSECTION: 8Conclusion\n\nWe introduced TASTE, a multimodal model fusing Text and STurcture Embedding model, designed for stance detection. The model effectively leverages the intricate interplay of conversation content and structure to compute comprehensive user embeddings. We have shown that the application of a GRN layer, initially utilized for multi-horizon time-series forecasting as described byLim et\u00a0al. (2021), is particularly advantageous for the task we are addressing, especially considering the constraints of our relatively small dataset. Our evaluation of the 4Forum and CreateDebate datasets, alongside comparisons with state-of-the-art models, highlights the distinct advantages of TASTE.\n\nFurthermore, our analysis revealed a notable correlation between stance classification accuracy and the balance of textual depth and interaction frequency. Specifically, in scenarios where\nparticipant interactions are frequent yet text contributions are succinct, structural-based models gain an upper hand. Conversely, in conversations characterized by lengthier texts but fewer interactions, content-based models excel.\n\nWhile uni-modal approaches have their respective advantages in certain scenarios, it is the integration of both these elements that consistently leads to the most solid and reliable outcomes.\nThis combined approach not only delves into the detailed and subtle aspects of the textual content but also leverages the patterns and complexities in human interactions, in line with theoretical frameworks established byGoffman (1955,1978); Grice (1975)and others.\n\nSECTION: 9Limitations\n\nOur approach suffers from a number of limitations that would be addressed in future work.\n\nFirst, although the 4Forums and CreateDebate datasets are commonly used and allow us to compare our work to relevant previous works, further evaluation on more diverse and contemporary datasets, such as social media platforms like X (Twitter) or Reddit, would improve the generalizability of our findings.\n\nA second, though related, limitation stems from the focus on English datasets. The performance of the model highly depends on conversational norms and dynamics, which may vary across languages and platforms.\n\nFinally, the model, at least when applied on the user level, is based on the implicit assumption that a speaker holds a pre-formed and stable stance throughout the discussion. While this assumption holds in the datasets we use, it may not hold in other datasets, e.g., Reddit\u2019s Change My View in which users are encouraged to persuade each other: \u201cA place to post an opinion you accept may be flawed, in an effort to understand other perspectives on the issue. Enter with a mindset for conversation, not debate.\u201d Future work should address the (optimistic) case in which speakers are less dogmatic and open to change of hearts. We plan to address this by combining our architecture that depends on the global conversation structure with the local turn taking recently explored byWen et\u00a0al. (2024).\n\nSECTION: References\n\nSECTION: Appendix AUni-modal Results\n\nTable3presents uni-modal results, along with the number of interactions and the number of tokens in each topic. The table promotes an interesting observation regarding the interplay between content and structure: intense (dense) though short (in utterance length) interactions, such as those in the Abortion, Evolution, and Gay Marriage topics, are better captured by structural models such as the SDP, while less intense interactions that exhibit longer utterances, such as in Gun Control, are better captured by text based embeddings such as S-BERT.\n\nSECTION: Appendix BAnalysis of an Illustrative Example\n\nThe exchange presented in Figure3provides an example for the value of the multimodal framework and the contribution of the SDP to understanding the global structure of a discussion. In this discussion on gun control, participants engage in a more complex interaction pattern than simple turn-taking, where each comment directly opposes the previous one. Instead, participants often respond supportively to others\u2019 comments, creating a nuanced dynamic which locally violates the max-cut assumption, a violation that may also affect the tone of the comment. The first comment of C, which locally violates the max-cut hypothesis, is such an example of supportive utterance which reads quite ambiguous when taken out of context. However, by adding to the model the global overview of the SDP, our model can accurately capture these interactions and correctly classify the participants\u2019 stances, circumventing the local max-cut violation and the diluted tone of the utterance. Diving into specific results, relying solely on SBERT led to a 71% accuracy for the entire conversation from which this excerpt is taken, while our best TASTE model achieved a 92% accuracy. This significant improvement underscores the effectiveness of combining textual and structural embeddings for nuanced stance detection.\n\nSECTION: Appendix CGRN and Naive Fusion\n\nTable4provides the GRN and the the concatenations results for the two datasets.\n\nSECTION: Appendix DTechnical Specifications\n\nWe trained TASTE with a maximum of 10 epochs, employing the AdamW optimizerLoshchilov and Hutter (2019)with a batch size of 16. A learning rate decay strategy was utilized, starting the learning rate within the range of. This rate was halved each time the validation loss showed no improvement every three epochs. The training was terminated when either the learning rate was reduced to the minimum threshold of, or when the maximum epoch limit of 10 was reached.\nTo avoid data leakage, we ensured that posts from the same author were not included in both training and test sets simultaneously.\nIn both training and testing we used Google\u2019s co-lab environment with T4 GPU. Training each TASTE version, and also running each experiment ran for on average for no more than two hours.", "text_file": "data\\paper_texts\\2412.03681v2_content.txt"}, {"title": "HEAL: Hierarchical Embedding Alignment Loss for Improved Retrieval and\n  Representation Learning", "authors": ["Manish Bhattarai", "Ryan Barron", "Maksim Eren", "Minh Vu", "Vesselin Grantcharov", "Ismael Boureima", "Valentin Stanev", "Cynthia Matuszek", "Vladimir Valtchinov", "Kim Rasmussen", "Boian Alexandrov"], "published_date": "2024-12-05T23:10:56Z", "summary": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by\nintegrating external document retrieval to provide domain-specific or\nup-to-date knowledge. The effectiveness of RAG depends on the relevance of\nretrieved documents, which is influenced by the semantic alignment of\nembeddings with the domain's specialized content. Although full fine-tuning can\nalign language models to specific domains, it is computationally intensive and\ndemands substantial data. This paper introduces Hierarchical Embedding\nAlignment Loss (HEAL), a novel method that leverages hierarchical fuzzy\nclustering with matrix factorization within contrastive learning to efficiently\nalign LLM embeddings with domain-specific content. HEAL computes\nlevel/depth-wise contrastive losses and incorporates hierarchical penalties to\nalign embeddings with the underlying relationships in label hierarchies. This\napproach enhances retrieval relevance and document classification, effectively\nreducing hallucinations in LLM outputs. In our experiments, we benchmark and\nevaluate HEAL across diverse domains, including Healthcare, Material Science,\nCyber-security, and Applied Maths.", "arxiv_id": "2412.04661v1", "html_link": "https://arxiv.org/html/2412.04661v1", "search_term": "ti:\"embeddings\"", "html_content": "SECTION: HEAL: Hierarchical Embedding Alignment Loss for Improved Retrieval and Representation Learning\n\nRetrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by integrating external document retrieval to provide domain-specific or up-to-date knowledge. The effectiveness of RAG depends on the relevance of retrieved documents, which is influenced by the semantic alignment of embeddings with the domain\u2019s specialized content. Although full fine-tuning can align language models to specific domains, it is computationally intensive and demands substantial data. This paper introducesHierarchicalEmbeddingAlignmentLoss (HEAL), a novel method that leverages hierarchical fuzzy clustering with matrix factorization within contrastive learning to efficiently align LLM embeddings with domain-specific content. HEAL computes level/depth-wise contrastive losses and incorporates hierarchical penalties to align embeddings with the underlying relationships in label hierarchies. This approach enhances retrieval relevance and document classification, effectively reducing hallucinations in LLM outputs. In our experiments, we benchmark and evaluate HEAL across diverse domains, including Healthcare, Material Science, Cyber-security, and Applied Maths.\n\nSECTION: 1Introduction\n\nLarge Language Models (LLMs), such as GPT-4(OpenAI,2023), have demonstrated exceptional capabilities in natural language understanding and generation. However, LLMs are prone tohallucinations, generating plausible but incorrect or nonsensical content(Ji et\u00a0al.,2023). Retrieval-Augmented Generation (RAG) frameworks(Lewis et\u00a0al.,2020)mitigate this issue by integrating external knowledge through document retrieval, enhancing the factual accuracy of LLM outputs. A critical component of RAG systems is the embedding model used for document retrieval. Standard embedding models, however, often fail to capture the hierarchical and semantic relationships within domain-specific corpora, leading to suboptimal retrieval and, consequently, increased hallucinations. This issue is particularly pronounced in domains with increased specificity such as Healthcare, Legal sytem, and Scientific research.\n\nCorpus of documents for a specialized domain inherently exhibit a high degree of semantic coherence, presenting an opportunity to align embedding models for retrieving the most contextually relevant information. Hierarchical Non-negative Matrix Factorization (HNMF)(Eren et\u00a0al.,2023)is a powerful technique for semantically categorizing documents into clusters that exhibit thematic coherence. By grouping documents into hierarchical clusters of supertopics and subtopics, HNMF provides a rich semantic categorization of the corpus, enabling a deeper understanding of document relationships. Leveraging this semantic knowledge in the form of hierarchical cluster labels, we can align embedding models to preserve hierarchical information within the embedding space. This alignment enhances the embeddings to capture both coarse-grained and fine-grained document similarities, improving contextual relevance in retrieval tasks and enabling better downstream capabilities.\n\nTo tackle the challenges of hallucination and suboptimal retrieval in RAG systems, we introduce theHierarchical Embedding Alignment Loss (HEAL), a refined extension of the Hierarchical Multi-label Contrastive Loss(Zhang et\u00a0al.,2022). HEAL leverages an improved hierarchical weighting scheme to align embeddings more effectively with the underlying hierarchical structure. By incorporating hierarchical label structures, HEAL fine-tunes embedding models to align with document clusters derived from HNMF. The method computes contrastive losses at each hierarchical level, combining them with depth-specific penalties to emphasize distinctions at higher levels of the hierarchy.\n\nOur contributions are summarized as follows:\n\nIntroduce a refined contrastive learning framework, named HEAL, that incorporates hierarchical label structures to align embeddings with hierarchical document relationships.\n\nIntegrate HEAL into RAG systems, fine-tuning embedding models to improve retrieval accuracy and reduce hallucinations in LLM outputs.\n\nValidate and benchmark HEAL through extensive experiments on domain-specific datasets from specialized scientific sub-domains of Healthcare, Material Science, Tensor Decomposition, and Cyber-security.\n\nShowcase significant improvements in retrieval relevance and downstream tasks compared to baseline method.\n\nSECTION: 2Related Work\n\nContrastive learning has become a cornerstone of representation learning, particularly in computer vision and natural language processing. Methods like SimCLR(Chen et\u00a0al.,2020)and MoCo(He et\u00a0al.,2020)have achieved state-of-the-art performance in unsupervised settings by learning representations that are invariant to data augmentations. In supervised contrastive learning,Khosla et\u00a0al. (2020)extended the contrastive loss to utilize label information, improving performance on classification tasks. Similarly, the SciNCL framework employs neighborhood contrastive learning to capture continuous similarity among scientific documents, leveraging citation graph embeddings to sample both positive and negative examplesOstendorff et\u00a0al. (2022). However, these methods generally assume flat label structures and do not exploit hierarchical relationships.\n\nHierarchical classification has been studied extensively, with approaches such as hierarchical softmax(Goodman,2001)and hierarchical cross-entropy loss(Deng et\u00a0al.,2014). These methods aim to leverage hierarchical label structures to improve classification efficiency and accuracy. In the context of representation learning,Deng et\u00a0al. (2011)introduced hierarchical semantic embedding, aligning image embeddings with WordNet hierarchies. More recent works, such asBertinetto et\u00a0al. (2020), have explored hierarchical prototypes to capture hierarchical relationships.Zhang et\u00a0al. (2022)propose a hierarchical multi-label contrastive learning framework that preserves hierarchical label relationships through hierarchy-preserving losses. Their method excels in scenarios with hierarchical multi-label annotations, such as biological or product classifications. In contrast, our approach focuses on enhancing information retrieval to mitigate hallucinations.\n\nRAG frameworks combine retrieval models with generative models to enhance the factual accuracy of language generation(Lewis et\u00a0al.,2020). These systems rely heavily on the quality of the embeddings used for retrieval. Prior work has focused on improving retrieval through better indexing and retrieval algorithms(Karpukhin et\u00a0al.,2020), but less attention has been given to aligning embeddings with hierarchical document structures.\n\nSECTION: 3Method\n\nIn this section, we propose an embedding alignment framework comprising hierarchical label extraction with HNMF, embedding alignment using HEAL, and retrieval with aligned embeddings as outlined in Figure1.\n\nSECTION: 3.1Hierarchical Document Clustering with HNMFk.\n\nHierarchical Non-negative Matrix Factorization with automatic latent feature estimation (HNMFk)Eren et\u00a0al. (2023)is an advanced technique for uncovering hierarchical patterns within document collections. It builds on traditional Non-negative Matrix Factorization (NMF)Vangara et\u00a0al. (2021)by\ndynamically and automatically determining the optimal number of latent features at each level.\nEffective contrastive learning relies on well-separated document cluster labels to align embeddings effectively. HNMFk\u2019s ability to automatically balance stability and accuracy using a bootstrap approach enhances the quality of clustering results. In this work, we utilize the publicly available HNMFk implementation from the TELF library111TELF is available athttps://github.com/lanl/T-ELF.\n\nGiven a Term Frequency-Inverse Document Frequency (TF-IDF) matrix, whererepresents the vocabulary size anddenotes the number of documents, HNMFk performs a sequence of matrix factorizations across hierarchical levels to capture the nested structure of topics. At each level, the factorization is expressed as, whereis the basis matrix representing latent topics, andis the coefficient matrix quantifying the contribution of each topic to the composition of documents. Here,is the number of topics at level, which is determined automatically through stability analysisVangara et\u00a0al. (2021). This analysis involves bootstrapping the data to create resampled versions of the TF-IDF matrix, applying NMF across a range ofvalues, and evaluating the stability of clusters across the resampled datasets. The optimalis selected as the value that produces the most consistent clustering results, indicating a robust underlying structure in the data.\n\nTo construct hierarchical labels for each document, the coefficient matrixis used to determine topic assignments. For each level, the topic for documentis identified by selecting the index of the maximum value in the corresponding column of, expressed as. The hierarchical label for documentis then formed by aggregating the topic assignments across all levels, resulting in. Here,is the total number of hierarchical levels, or hierarchical depth that is the number of NMFk operations from the first one to the leaf.is the label of sampleat level, withcorresponding to theshallowest(most general or root node) level andto thedeepest(most fine-grained, or leaf node) level.\n\nSECTION: 3.2Hierarchical Multilevel Contrastive Loss (HEAL)\n\nUpon the unsupervised data decomposition with HNMFk, the datasets have clusters with hierarchical structures. To incorporate such structures, we propose the HEAL, which extends supervised contrastive loss(Khosla et\u00a0al.,2020)by introducing level-wise contrastive losses and aggregating them with level-specific penalties.\n\nFor a batch ofsamples, whereis the input andis the hierarchical cluster label, we obtain normalized embeddingsusing an encoder network:\n\nFor a given level, the set of positive samples for sampleis:\n\nThe contrastive loss at levelfor sampleis:\n\nIfis empty (i.e., no positive samples at levelfor),is excluded from the total loss.\n\nTo prioritize discrepancies at shallower levels, we assign penaltiesto each level, where shallower levels have higher penalties. The penalties are defined as:\n\nThe penaltiessatisfy:\n\nfor, i.e., penalties decrease for deeper levels.\n\n, i.e., the penalties are normalized.\n\nThe total HEAL loss is then:\n\nAlgorithm1outlines the computation offor a mini-batch.\n\nSECTION: 3.3Fine-tuning Embedding Models with HEAL for RAG\n\nTo enhance retrieval performance in RAG systems, we fine-tune the embedding model to align with the hierarchical structure of the document corpus. Given a specialized document corpus, we first apply HNMFk (as described in Section3.1) to the corresponding TF-IDF matrixproducing hierarchical cluster labelsfor each document. Next, we generate embeddings from each documentusing a pretrained embedding model. The embedding model is initialized with pre-trained weights and produces normalized embeddingsfor document.\nTo align embeddings with the hierarchical structure, we optimize the HEAL presented in3.3.\n\nThe embedding model is trained by minimizingusing gradient-based optimization:\n\nwhereare the parameters of the embedding model.\n\nAfter fine-tuning, the updated embeddingsare used to replace the initial embeddings in the vector store. During inference, a queryis embedded usingas, and\nretrieves topdocuments based on cosine similarity:\n\nTo maximize retrieval performance in RAG systems, it is essential to align the query embeddings with the hierarchically aligned document embeddings. Since queries are typically shorter and may not capture the full semantic richness of the documents, we need to semantically align queries and documents in the embedding space. To achieve this, we generate question-answer (Q&A) pairs using a language model (e.g., LLaMA-3.1 70B) for each document and leverage HEAL to jointly align both query and document embeddings during training.\nFor each document, we generate a set of queries, whereis the number of queries generated for document. Each queryis associated with the same hierarchical labelsas its source document, since it is derived from the content of.We extend the HEAL framework to include both documents and queries by defining a unified set of samples:\n\nEach samplehas an associated hierarchical label, where:\n\nBased on this dataset, the HEAL is leveraged to finetune the embedding model\u00a0.\n\nSECTION: 4Experiments\n\nSECTION: 4.1Datasets\n\nWe evaluate our method on datasets specifically constructed from scientific publications in the domains of Material Science, Medicine, Tensor Decomposition, and Cybersecurity. To construct our datasets, we leveraged the Bibliographic Utility Network Information Expansion (BUNIE) method, a machine learning-based approach that integrates subject-matter expertise in a human-in-the-loop frameworkSolovyev et\u00a0al. (2023).\nFor completeness, we briefly summarize the BUNIE approach in this paper. BUNIE begins with a small core corpus of documents selected by subject-matter experts (SMEs). From this starting point, it constructs a citation network to identify additional relevant documents, leveraging BERT based text embeddings to assess semantic similarity. Through iterative cycles of dataset expansion and pruning\u2014guided by embedding visualization, topic modeling, and expert feedback\u2014the method ensures the corpus is both comprehensive and domain-specific. We apply this procedure to each scientific domain with guidance from SMEs, who provide target keywords/phrases and/or a core set of papers relevant to the sub-topic of interest within the domain. Using this knowledge base, we employ BUNIE to expand the dataset from the initial core papers to a larger collection of domain-specific documents.\n\nMaterial Science: A collection of 46,862 scientific articles, which explore 73 Transition Metal Dichalcogenides (TMD) compounds, combining transition-metal and chalcogen atoms (S, Se, or Te). With a layered structure similar to graphite, TMDs excel as solid lubricants and exhibit unique quantum phases like superconductivity and charge density waves. Their atomically thin layers offer tunable properties, with applications in spintronics, optoelectronics, energy harvesting, batteries, and flexible electronics.\n\nHealthcare: A collection of 9,639 scientific articles, which examine Pulmonary Hypertension (PH) disease - a rare condition causing elevated pulmonary arterial pressure, right heart strain, and reduced oxygen delivery. The WHO classifies PH into five groups based on causes, including pulmonary arterial hypertension (PAH), which has a prevalence of 15-25 cases per million in the U.S. Treatments such as endothelin receptor antagonists and prostacyclin analogs aim to improve symptoms, but prognosis varies, with untreated PAH having a median survival of less than three years.\n\nApplied Mathematics:A collection of 4,624 scientific articles, which explore tensor network techniques, such as Tensor-Train (TT) decomposition, which recently emerged as a powerful mathematical tool for solving large-scale Partial Differential Equations (PDEs). Tensor network PDE solvers efficiently manage high-dimensional data by mitigating the curse of dimensionality, drastically reducing computational costs and memory usage while maintaining high solution accuracy. These advancements hold significant promise for breakthroughs in scientific computing, including material science, climate modeling, and engineering design optimization.\n\nCyber-security: We created a dataset of 8,790 scientific publications focusing on the application of tensor decomposition methods in cybersecurity and ML techniques for malware analysis. This dataset serves as a knowledge base covering topics for cyber-security such as ML-based anomaly detection, malware classification, novel malware detection, uncertainty quantification, real-world malware analysis challenges, tensor-based anomaly detection, malware characterization, and user behavior analysis.\n\nSECTION: 4.2Experimental Setup\n\nFor training, we used the Adam optimizer with a learning rate of, a batch size of, and early stopping based on validation performance with a patience ofepochs. The experiments were conducted on a high-performance computing cluster, with each node equipped withNVIDIA GH200 GPUs. Document metadata, comprising the title and abstract combined, were used as input. Hierarchical labels were generated using HNMF with dataset-specific factorization depths: Material Science (depth 3), Healthcare (depth 4), Applied Mathematics (depth 3), and Cybersecurity (depth 3). HEAL loss was applied with a temperature parameter of. The embedding base model, SciNCLOstendorff et\u00a0al. (2022), was chosen for its robust contrastive pretraining on scientific documents, serving as a strong baseline for fine-tuning.\nThe data was split intotraining,validation, andtest sets, with early stopping monitored on the validation set. Evaluation metrics were reported on the test set, while Q&A retrieval analysis used the entire dataset (train + validation + test) for constructing the vector store.\n\nThe efficacy of the RAG system was evaluated at two levels.First, we characterized the embeddings on document-level tasks, including hierarchical classification, retrieval, and hallucination measurement. For hierarchical classification, we used a hierarchical classifier applying random forests to each node(Miranda et\u00a0al.,2023). The classifier is trained on embeddings corresponding to train dataset and evaluated against the test set. We perform this for embeddings derived from aligned and unaligned embedding model. Retrieval performance was assessed by measuring whether retrieved documents belonged to the same hierarchical class as the query document. Hallucination likelihood was evaluated based on the retrieval of incorrect documents for a given query.Second, we evaluated the performance of the embedding model within a RAG framework. To support retrieval and hallucination analysis, we used the LLaMA-3.1 70B model to generateQ&A pairs per document using abstracts as input, providing a robust test for embedding alignment and retrieval capabilities. Next, we leveraged the questions as queries to the embedding model to retrieve the best metadata and assessed whether the model retrieved the exact document that generated the query during Q&A analysis, as well as the rank of the returned document within the top 10 results. Furthermore, the retrieved documents were augmented with LLaMA-3.1 70B LLM to generate responses, with hallucinations evaluated based on response accuracy and relevance.\n\nGiven the specialized nature of our dataset and the requirement for hierarchical labels, fine-tuning is essential. Comparing our method to approaches that do not leverage hierarchical labels is inequitable, as they are inherently less effective for this task. Our approach simplifies training by eliminating HEAL loss hyperparameter tuning, unlike HiMulConZhang et\u00a0al. (2022), which requires extensive tuning of penalty parameters for optimal results. While HiMulCon focuses on root-level classification in vision datasets, our method aligns embeddings across all hierarchical depths. We optimize hierarchical metrics such as classification, retrieval, and hallucination indirectly through the HEAL loss, ensuring a robust alignment with the hierarchical structure.\n\nFor these reasons, we evaluate the performance of HEAL using the baseline model SciNCL, both without and with hierarchical alignment on our diverse specialized datasets.\nWe evaluate performance using hierarchical metrics to capture nuances of hierarchical label structures in retrieval, classification, and hallucination assessments as presented in Table1.\n\nSECTION: 4.3Results\n\nTable2summarizes the performance metrics for three datasets (Healthcare, Materials, Applied Mathematics, and Cybersecurity) across three tasks: classification, retrieval, and hallucination evaluation. The aligned model corresponds to the embedding model trained using the HEAL loss, whereas the non-aligned model corresponds to the original embedding model without HEAL-based training. The metrics are reported for both non-aligned and aligned SciNCL embeddings, demonstrating the significant impact of HEAL on improving performance.\nFigure2illustrates hierarchical embedding alignment achieved through HEAL training, resulting in well-separated super and sub-clusters for the Materials and Healthcare datasets which enhances the performance of downstream tasks.\n\nFirst, we evaluate the performance on document-level tasks using hierarchical labels. Specifically, we assess the ability of the hierarchical classifier to predict hierarchical labels in the classification task. Additionally, we quantify the retrieval of documents from the same hierarchical category based on a query document to characterize retrieval accuracy and evaluate hallucinations.\nThe results presented in table2demonstrate that HEAL significantly improves hierarchical classification metrics across all datasets.\nFor the Healthcare dataset, the Hierarchical F1 Score improves from 0.5164 to 0.6588, reflecting a more accurate representation of hierarchical labels. Similarly, the Materials dataset achieves near perfect classification metrics (F1 Score, Precision, Recall = 0.99) with aligned embeddings, while the most challenging Healthcare dataset (4 depth cluster label) sees improvements in F1 Score from 0.5164 to 0.6588. In retrieval tasks, HEAL aligned embeddings consistently outperform non-aligned embeddings across all metrics. For the Healthcare dataset, Hierarchical MRR improves from 1.6259 to 2.2525, and nDCG@k increases from 0.3752 to 0.5908 where, indicating better ranking and retrieval relevance. The Materials dataset achieves a dramatic increase in retrieval precision, with Precision@k rising from 0.4787 to 0.9707, while nDCG@k reaches 0.99, showcasing near-perfect retrieval performance. For the Cyber dataset, aligned embeddings yield an MRR improvement from 2.7538 to 3.1482 and a corresponding nDCG@k increase from 0.6781 to 0.7908. Hallucination metrics further underscore the superiority of HEAL. Aligned embeddings reduce hallucination rates significantly across all datasets. For the Healthcare dataset, FPR@k drops from 0.9386 to 0.8771, and severity decreases from 0.7306 to 0.5533, indicating fewer irrelevant or misleading retrievals. The Materials dataset shows the most striking improvement, with FPR@k reduced from 0.8534 to 0.0878 and severity declining from 0.6041 to 0.0644, nearly eliminating hallucination tendencies. For the Cyber dataset, aligned embeddings lower FPR@k from 0.7968 to 0.6236 and severity from 0.4402 to 0.3654.\n\nNext, we evaluate the performance of aligned RAG in retrieving the correct documents for generated queries to augment the LLM and minimize hallucinations. From each test dataset, we randomly sampled 100 documents and generated 10 Q&A pairs per document using the LLAMA-3.1 70B model, resulting in a total of 1,000 Q&A pairs for each dataset. Each Q&A pair was tagged with the corresponding document from which it was generated.\nThe prompt used for Q&A generation was as follows:\u201cFirst, provide a concise summary of the following abstract that emphasizes its key concepts and hierarchical relationships. Then, based on this summary, generate 10 unique, nuanced Q&A pairs. Focus on creating questions that delve into specialized details of the hierarchical concepts discussed.\u201dThe generated queries were used to fetch documents via both aligned and unaligned models. We assessed the ability of each model to correctly retrieve the original document and evaluated the rank/order of retrieval. On average, the unaligned model achieved an MRR of 0.273 and a Recall@10 of 0.415. These metrics represent regular retrieval scores, not hierarchical scores. In contrast, the aligned model significantly improved performance, achieving an MRR of 0.514 and a Recall@10 of 0.731, demonstrating its superior ability to retrieve the correct set of documents.\nFurthermore, when integrating RAG with LLAMA-3.1 70B for generating answers from the queries and retrieved documents, the unaligned model produced a ROUGE score of 0.42, while the aligned model achieved a ROUGE score of 0.68. This highlights the impact of alignment on improving the quality and relevance of generated responses.\n\nSECTION: 5Conclusion\n\nIn this work, we introduced HEAL, a novel framework for aligning embeddings in RAG systems through hierarchical fuzzy clustering and matrix factorization, integrated within a contrastive learning paradigm. HEAL effectively computes level-specific contrastive losses and applies hierarchical penalties to align embeddings with domain-specific structures, enhancing both retrieval relevance and classification performance. Experimental results across diverse domains \u2014 Healthcare, Materials Science, Cybersecurity, and Applied Mathematics \u2014 demonstrate HEAL\u2019s capability to significantly improve retrieval accuracy and mitigate hallucinations in LLM-based systems. By bridging hierarchical semantics with contrastive alignment, HEAL establishes itself as a versatile and robust tool for advancing RAG methodologies, enabling more precise, reliable, and domain-adaptive applications of large language models.\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.04661v1_content.txt"}, {"title": "Chain-linked multiple matrix integration via embedding alignment", "authors": ["Runbing Zheng", "Minh Tang"], "published_date": "2024-12-03T19:43:06Z", "summary": "Motivated by the increasing demand for multi-source data integration in\nvarious scientific fields, in this paper we study matrix completion in\nscenarios where the data exhibits certain block-wise missing structures --\nspecifically, where only a few noisy submatrices representing (overlapping)\nparts of the full matrix are available. We propose the Chain-linked Multiple\nMatrix Integration (CMMI) procedure to efficiently combine the information that\ncan be extracted from these individual noisy submatrices. CMMI begins by\nderiving entity embeddings for each observed submatrix, then aligns these\nembeddings using overlapping entities between pairs of submatrices, and finally\naggregates them to reconstruct the entire matrix of interest. We establish,\nunder mild regularity conditions, entrywise error bounds and normal\napproximations for the CMMI estimates. Simulation studies and real data\napplications show that CMMI is computationally efficient and effective in\nrecovering the full matrix, even when overlaps between the observed submatrices\nare minimal.", "arxiv_id": "2412.02791v2", "html_link": "https://arxiv.org/html/2412.02791v2", "search_term": "ti:\"embeddings\"", "html_content": "SECTION: Chain-linked Multiple Matrix Integration via Embedding Alignment\n\nMotivated by the increasing demand for multi-source data integration in various scientific fields, in this paper we study matrix completion in scenarios where the data exhibits certain block-wise missing structures \u2013 specifically, where only a few noisy submatrices representing (overlapping) parts of the full matrix are available.\nWe propose the Chain-linked Multiple Matrix Integration (CMMI)\nprocedure to efficiently combine the information that can be extracted from these individual noisy submatrices.\nCMMI begins by deriving entity embeddings for each observed submatrix, then aligns these embeddings using\noverlapping entities between pairs of submatrices, and finally aggregates them to reconstruct the entire matrix of interest.\nWe establish, under mild regularity conditions, entrywise error bounds and normal approximations for the CMMI estimates.\nSimulation studies and real data applications show that CMMI is computationally efficient and effective\nin recovering the full matrix, even when overlaps between the observed submatrices are minimal.\n\nKeywords:norm, normal approximations, matrix completion, data integration\n\nSECTION: 1Introduction\n\nThe development of large-scale data collection and sharing has sparked considerable research interests in integrating data from diverse sources to efficiently uncover underlying signals. This problem is especially pertinent in fields such as healthcare research(Hong et\u00a0al.,2021; Zhou et\u00a0al.,2023),\ngenomic data integration(Maneck et\u00a0al.,2011; Tseng et\u00a0al.,2015; Cai et\u00a0al.,2016),\nsingle-cell data integration(Stuart et\u00a0al.,2019; Ma et\u00a0al.,2024),\nand chemometrics(Mishra et\u00a0al.,2021).\nIn this paper we consider a formulation of the problem where each sourcecorresponds to a partially observed submatrixof some matrix, and the goal is to integrate theseto recoveras accurately as possible.\n\nAs a first motivating example, consider pointwise mutual information (PMI) constructed from different electronic healthcare records (EHR) datasets.\nPMI quantities the association between a pair of clinical concepts, and matrices representing these associations can be derived from co-occurrence summaries of various EHR datasets(Ahuja et\u00a0al.,2020; Zhou et\u00a0al.,2022).\nHowever, due to the lack of interoperability across healthcare systems(Rajkomar et\u00a0al.,2018), different EHR data often involve non-identical concepts with limited overlap, resulting in substantial differences among their PMI matrices.\nThe analysis of PMI matrices from different EHR datasets can thus be viewed as a\nmulti-source matrix integration problem. Specifically, letrepresent some concept set and\nsuppose there is a symmetric PMI matrixassociated with, where. For theth EHR, we denote its clinical concept byand let.\nThe PMI matrix derived from theth EHR,, then corresponds to the\nprincipal submatrix ofassociated with. As it is often the case that the union of all the entries inconstitutes only a strict subset of those in, our aim is to integrate theseto recover the unobserved entries in.\n\nFor another example, consider single-cell matrix data where rows represent genomic features, columns represent cells, and each entry records some specific information about a feature in the corresponding cell.\nA key challenge in the joint analysis for this type of data is to devise efficient computational strategies to integrate different data modalities(Ma et\u00a0al.,2020; L\u00e4hnemann et\u00a0al.,2020), as the experimental design\nmay lead to a collection of single-cell data matrices for different, but potentially overlapping, sets of cells and features. More specifically,\nletbe the population matrix for all involved features and cells where(withanddenoting the sets of genomic features and cells, respectively). Each single-cell data matrixis then a submatrix ofcorresponding to someand; here we denoteand. Our aim is once again to integrate the collection ofto reconstruct the original.\n\nThe above examples involving EHR and single-cell data are special cases of the matrix completion with noise and block-wise missing structures.\nHowever, the existing literature on matrix completion mainly focuses on recovering a possibly low-rank matrix based on uniformly sampled observed entries\nor independently sampled observed entries which may be contaminated by noise; see, e.g.,Cand\u00e8s and Tao (2010); Candes and Recht (2012); Cai et\u00a0al. (2010); Candes and Plan (2011); Koltchinskii et\u00a0al. (2011); Tanner and Wei (2013); Chen et\u00a0al. (2019); Fornasier et\u00a0al. (2011); Mohan and Fazel (2012); Lee and Bresler (2010); Vandereycken (2013); Hu et\u00a0al. (2012); Sun and Luo (2016); Cho et\u00a0al. (2017); Chen et\u00a0al. (2020); Yan et\u00a0al. (2024); Srebro and Salakhutdinov (2010); Foygel et\u00a0al. (2011); Cai and Zhou (2016); Keshavan et\u00a0al. (2010)for an incomplete list of references.\n\nThese assumptions of uniform or independent sampling in standard matrix completion models are generally violated in applications of matrix integration, thus necessitating the development of efficient methods for tackling the block-wise missing structures.\nSome examples of this development include the generalized integrative principal component analysis (GIPCA) ofZhu et\u00a0al. (2020), structured matrix completion (SMC) ofCai et\u00a0al. (2016), block-wise overlapping noisy matrix integration (BONMI) ofZhou et\u00a0al. (2023), and symmetric positive semidefinite matrix completion (SPSMC) ofBishop and Yu (2014).\nThe GIPCA procedure operates under the setting where each data matrix have some common samples and completely different variables, and furthermore assumes that each entry in these matrices are from some exponential family of distribution, with entries in the same matrix having the same distributional form.\nSMC is a spectral procedure for recovering the missing block of an approximately low-rank matrix when a subset of the rows and columns are observed; thus, SMC is designed to impute only a single missing block at a time.\nBONMI is also a spectral procedure for recovering a missing block (or submatrix) in an approximately low-rank matrix but, in contrast to SMC,\nassumes that this missing block is associated with a givenpairof observed submatrices that share some (limited) overlap.\nSPSMC has a similar spectral procedure with BONMI to recover a low-rank symmetric positive semidefinite matrix using some observed principal submatrices. While BONMI combines submatrices pair by pair, SPSMC sequentially integrates each new submatrix with the combined structure formed by all previously integrated submatrices.\nThe key idea behind BONMI and SPSMC is to align (via an orthogonal transformation) the spectral embeddings given by the leading (scaled) eigenvectors of the two overlapping submatrices and then impute the missing block by taking the outer product of these aligned embeddings.\nThe use of embedding alignments also appeared in other applications including bilingual dictionary induction(Kementchedjhieva et\u00a0al.,2018), knowledge graphs integration(Lin et\u00a0al.,2019; Fanourakis et\u00a0al.,2023), and vertex nominations(Zheng et\u00a0al.,2022).\n\nIn this paper, we extend the BONMI procedure, which handles only two overlapping submatrices, tosubmatrices and propose the Chain-linked Multiple Matrix Integration (CMMI) for more efficient and flexible matrix completion. As a motivating example, suppose we have two overlapping pairs of submatricesand. Using the overlapping entries betweenand(resp.and) we can find an orthogonal transformation(resp.) to align the embeddingsand(resp.and). Then by combiningand, we can also aligntoand recover the missing block associated withandeven when these submatrices arenon-overlapping. Generalizing this observation we can show that as long asareconnectedthen we can integrate them simultaneously to recover all the missing entries; here two submatricesandare said to be connected if there exists a sequencewith,such thatandare overlapping for all.\nThe use of CMMI thus enables the recovery of many missing blocks that are unrecoverable by BONMI and furthermore allows for significantly smaller overlap between the observed submatrices. CMMI considers all possible overlapping pairs without relying on the integration order of submatrices, unlike SPSMC, enabling a more optimal recovery result.\n\nThe structure of our paper is as follows. In Section2we introduce the model for multiple observed principal submatrices of a whole symmetric positive semi-definite matrix, and propose CMMI to integrate a chain of connected overlapping submatrices.\nTheoretical results for our CMMI procedures are presented in Section3.\nIn particular we derive error bounds in two-to-infinity norm for the spectral embedding of the submatrices and entrywise error bound for the recovered entries. Using these error bounds we show that our recovered entries are approximately normally distributed around their true values and that our algorithm yields consistent estimate even when there are only minimal overlap between the submatrices. We emphasize that the results in Section3also hold for BONMI (which is a special case of our results for) and SPSMC, thereby providing significant refinements over those inZhou et\u00a0al. (2023)andBishop and Yu (2014), which mainly focus on bounding the spectral or Frobenius norm errors of the missing block and embeddings.\nAnd our analysis handles both noisy and missing entries in the observed submatrices whileZhou et\u00a0al. (2023)andBishop and Yu (2014)only consider the case of noisy entries.\nNumerical simulations and experiments on real data are presented in Sections4and5. In Section6, we extend our embedding alignment approach to the cases of symmetric indefinite matrices and asymmetric or rectangular matrices.\nDetailed proofs of stated results are provided in the supplementary material.\nSectionAin the supplementary material explores more complex matrix integration challenges, such as scenarios where the connected submatrices do not form a single chain but rather multiple chains with possibly quite different lengths and thus we need to select a suitably optimal chain among these candidates. The theoretical results in Section3allows us to develop several effective strategies for addressing these issues.\n\nSECTION: 1.1Notations\n\nWe summarize some notations used in this paper.\nFor any positive integer, we denote bythe set.\nFor two non-negative sequencesand, we write(resp.) if there exists some constantsuch that(resp.) for all, and we writeifand.\nThe notation(resp.) means that there exists some sufficiently small (resp. large) constantsuch that(resp.).\nIfstays bounded away from, we writeand, and we use the notationto indicate thatand.\nIf, we writeand.\nWe say a sequence of eventsholds with high probability if for anythere exists a finite constantdepending only onsuch thatfor all.\nWe write(resp.) to denote that(resp.) holds with high probability.\nWe denote bythe set oforthogonal\nmatrices.\nFor any matrixand index sets,, we denote bythe submatrix offormed from rowsand columns, and we denote bythe submatrix ofconsisting of the rows indexed by. The Hadamard or entrywise product between two conformal matricesandis denoted by.\nGiven a matrix, we denote\nits spectral, Frobenius, and infinity norms by,, and, respectively.\nWe also denote the maximum entry (in modulus) ofbyand thenorm ofby\n\nwheredenotes theth row of, i.e.,is the maximum of thenorms of the rows of. We note that thenorm isnotsub-multiplicative. However, for any matricesandof conformal dimensions, we have\n\nsee Proposition\u00a06.5 inCape et\u00a0al. (2019). Perturbation bounds using thenorm for the eigenvectors and/or\nsingular vectors of a noisily observed matrix had recently\nattracted interests from the statistics community, see e.g.,Chen et\u00a0al. (2021); Cape et\u00a0al. (2019); Fan et\u00a0al. (2018); Abbe et\u00a0al. (2020)and the references therein.\n\nSECTION: 2Methodology\n\nWe are interested in an unobserved population matrix associated withentities denoted by. We assumeis positive semi-definite with rank; extensions to the case of symmetric but indefiniteas well as asymmetric or rectangularare discussed in Section6. Denote the eigen-decomposition ofas,\nwhereis a diagonal matrix whose diagonal entries are the non-zero eigenvalues ofin descending order, and the orthonormal columns ofconstitute the corresponding eigenvectors.\nThe latent positions associated to the entities are given byand any entry incan be written as the inner product of these latent positions, i.e.,so thatfor any, whereanddenote theth andth row of, respectively.\n\nWe assume that the entries ofare only partially observed, and furthermore, that the observed entries can be grouped into blocks. More specifically, suppose that we havesources and\nfor anywe denote the index set of the entities contained in theth source by.\nFor ease of exposition we also requirefor allas otherwise there exists somesuch that it is impossible to integrate observations fromwith those from. We denoteand the population matrix for theth source by. We then have\n\nwhereis the submatrix offormed from rows and columns in,contains the rows ofin, andcontains the latent positions of.\n\nWe also allow for missing and corrupted observations in each source, i.e., for theth source we only get to observefor allHereindicates the indices of the observed entries andrepresent the random noise.\nIn particularandare both symmetric, and we assume the upper triangular entries ofare i.i.d. Bernoulli random variables with success probabilitywhile\nthe upper triangular entries ofare independent, mean-zero sub-Gaussian random variables\nwith Orlicz-2 norm bounded by.\nFor this model, the matrix\n\nis an unbiased estimate of, and thus a natural idea is to use the scaled leading eigenvectorsas an estimate for, whereandcontain the leading eigenvalues and the leading eigenvectors of, respectively.\nWe now propose an algorithm to integrate and alignfor recovery of the unobserved entries in.\n\nSECTION: 2.1Motivation of the algorithm\n\nWe first summarize the BONMI algorithm ofZhou et\u00a0al. (2023). We start with thenoiselesscase for overlapping submatricesandto illustrate the main ideas. Our goal is to recover the unobserved entries in the white block of Figure1; this is part of.\n\nBased onandwe can obtain latent position estimates for entities inand, which we denote asand.\nNext note that\n\nand hence there existssuch that\n\nEq.\u00a0(2.2) then implies\n\nwhere, and thus we only needto recover.\n\nNote that for entities in, we have two equivalent representations of their latent positions. More specifically, letandbe the rows ofandcorresponding to entities in. Then by\nEq.\u00a0(2.2) we haveand thuscan be obtained by aligningand.\nThe resultingis unique\nwhenever.\n\nThe same approach also extends to the case where theandare partially and noisily observed. More specifically, suppose\nwe observeandas defined in Eq.\u00a0(2.1). We then obtain estimated latent positionsforandforfromand, respectively. To alignand, we solve the orthogonal Procrustes problem\n\nand then estimate the unobserved block as part of\n\nSECTION: 2.2Chain-linked Multiple Matrix Integration (CMMI)\n\nWe now extend the ideas in Section2.1to a chain of overlapping submatrices.\nSuppose our goal is to recover the entries in the yellow block of Figure2, given a collectionsuch thatfor all.\nThen for each pair, we align the estimated latent position matricesandby solving the orthogonal Procrustes problem\n\nNote that the solution of the orthogonal Procrustes problem between matricesandis given bywhereandcontain the left and right singular vectors of, respecitvely; seeSch\u00f6nenmann (1966).andcan be aligned by combining thesewhich then yields\n\nas an estimate for. See Algorithm1for more details.\n\nFor, obtain estimated latent position matrix for, denoted by, whereand the diagonal matrixcontain theleading eigenvectors and eigenvalues of, respectively.\n\nFor, obtainby solving the orthogonal Procrustes problem\n\nCompute.\n\nCompared to BONMI inZhou et\u00a0al. (2023), which handles only two submatrices, our proposed CMMI can combine allconnectedsubmatrices, where two submatricesandare said to be connected if there exists a path of overlapping submatrices between them. Indeed, for the example in Figure2, BONMI can only recover the entries associated with pairs of overlapping submatrices, namely, while CMMI can recover the whole matrix. In general, BONMI only recoversfraction of the entries recoverable by CMMI.\nMoreover, our theoretical results indicate that increasinghas a minimal effect on the estimation error of CMMI (see Theorem2), and\nsimulations and real data experiments in Sections4and5show that accurate recovery is possible even when.\nOur theoretical results also show that CMMI requires only minimal overlap betweenand, e.g.,can be as small as, the embedding dimension of; see Remark6and Section4.3for further discussion and experimental results.\n\nFor more general cases encountered in practice, there\nmay exist multiple chains to recover a given unobserved entry.\nWe will present in SectionAof the supplementary material several strategies to select and/or combine these chains.\nAlthough in certain cases, such as when considering a simple chain, CMMI is identical to the sequential integration approach SPSMC inBishop and Yu (2014), CMMI offers a more optimal strategy in more complex scenarios by considering all overlapping pairs. In contrast, the restriction to sequential integration imposes limitations on SPSMC, and how to determine a valid and effective sequential integration order is an unresolved issue inBishop and Yu (2014). Furthermore, our theoretical results are significantly stronger than those inBishop and Yu (2014)andZhou et\u00a0al. (2023); see Section3.1for further discussion.\n\nSECTION: 3Theoretical Results\n\nWe now present theoretical guarantees for the estimateobtained by Algorithm1.\nWe shall make the following assumptions on the underlying population matricesfor the observed blocks.\nWe emphasize that, because our results address either large-sample approximations or limiting distributions, these assumptions should be interpreted in the regime whereis arbitrarily large and/or.\n\nFor each, the following conditions hold for sufficiently large.\n\nWe have. Letanddenote the largest and smallest non-zero eigenvalues of, and letcontain the eigenvectors corresponding to all non-zero eigenvalues.\nWe then assume\n\nfor some finite constant.\n\nwhereis a symmetric matrix whose (upper triangular) entries are independent mean-zero sub-Gaussian random variables with Orlicz-2 norm bounded byandis a symmetric binary matrix whose (upper triangular) entries are i.i.d. Bernoulli random variables with success probability.\n\nDenote\n\nWe supposeand\n\nFor the entire population matrix, we have. Letanddenote the largest and smallest non-zero eigenvalues of, and letcontain the eigenvectors corresponding to non-zero eigenvalues. Suppose (1)has bounded condition number, i.e.,for some constant, andhas bounded coherence, i.e.,; (2) for each,are drawn uniformly at random from. Then\n\nwith high probability (LemmaD.11) and Eq.\u00a0(3.1) holds.\n\nWe first consider the case where we only have two overlapping submatricesand.\nTheorem1presents an expansion for.\n\nLetandbe overlapping submatrices satisfying Assumption1. For their overlap, suppose, and define\n\nLetfor any. We then have\n\nwhereandare random matrices satisfying\n\nwith high probability. Furthermore suppose\n\nThenis the dominant term and\n\nwith high probability.\n\nThe expansion in Eq.\u00a0(3.5) consists of four terms, with the first two terms being linear transformations of the additive noise matricesand. The third termcorresponds to second-order estimation errors forand, and hence Eq.\u00a0(3.6) only depends\non quantities associated withand. Finallycorresponds to the error when aligning the overlapsandand hence Eq.\u00a0(3.7) depends on.\n\nZhou et\u00a0al. (2023)requiresso that the number of overlapping entities must grow\nwith the sizes of the submatrices. However, our derivations of\nTheorem1show that the overlap can be bounded or even\nbe as small as(the rank of), which is the minimum value\nrequired to align the embeddings in; see Remark6and\nSection4.3for more detailed discussion and\nsimulations verifying this result.\n\nNext we consider a chain of overlapping submatrices as described in Algorithm1. Theorem2presents the expansion of the\nestimation error forand\nTheorem3leverages this expansion to derive an\nentrywise normal approximation for. While the statements of Theorem2and Theorem3appear somewhat complicated at first glance, this is intentional as they make the results more general and thus applicable to a wider range of settings.\nIndeed, we allow forto have different magnitudes as well as the overlaps\nto be of very different sizes. For example we can havebutwhilebut.\nIfthen these results can be simplified considerably; see Remark6.\n\nConsider a chain of overlapping submatricessatisfying Assumption1.\nFor all overlaps, suppose,\nand defineas in Eq.\u00a0(3.4).\nLetfor all. We then have\n\nwhereandare random matrices satisfying\n\nwith high probability. Furthermore suppose\n\nThenis the dominant term and\n\nwith high probability.\n\nWe note that the only difference between Theorem1and Theorem2is in the upper bound forcompared to that for. Indeed,andin Theorem2only depend onand, but not on the chain linking them, and thus their upper bounds are the same as that in Theorem1forand. In contrast, from our discussion in Remark2, the termcorresponds to the alignment error betweenand. Asandneed not share any overlap, this alignment is obtained via a sequence of orthogonal Procrustes transformations betweenandfor. The accumulated error for these transformations is reflected in the termand depends on the whole chain.\nWhen the length of the chainis not too large relative to the overlaps, the error inis negligible compared to that in the dominant term, and consequently, our entrywise error rate depends only onand, rather than on the chain linking them.\n\nConsider the setting of Theorem2. For, letbe amatrix whose entries are of the form\n\nDefineas\n\nFor any fixed, defineas\n\nFurthermore, denote\n\nwhereanddenote the-th row and-th row ofandrespectively.\nNote that.\nSuppose the following conditions\n\nare satisfied, whereandare upper bounds forandgiven in Theorem2.\nWe then have\n\nas.\n\nIfthen all terms depending onare dropped from these conditions. Specifically,in Assumption1simplifies to, and\nthe condition in Eq.\u00a0(3.10) simplifies tofor.\nIfthen all terms depending onare also dropped. Specifically, the condition in Eq.\u00a0(3.10) simplifies to.\n\nUsing Theorem3we can also construct aconfidence interval foraswheredenotes the upperquantile of the standard normal distribution andis a consistent estimate ofbased on\n\nwithand;\nwe leave the details to the interested readers.\n\nWe now provide an example to illustrate the above results.\nWe first assumeare of the same order, i.e., there exists anwithfor all. We also assume,for all.\nWe further supposehasentries that are lower bounded by some constantnot depending on. Then, asis also low-rank with bounded condition number, we have, and thus.\nBy Eq.\u00a0(3.3) we have,\nsofor all.\nFor the overlaps we assumeand.\nUnder this setting, the condition in Eq\u00a0(3.2) simplifies to\n\nthe error bounds in Eq.\u00a0(3.8) of Theorem2simplify to\n\nwith high probability. Furthermore we also have\n\nwith high probability, provided that\n\nAll conditions are then trivially satisfied and the estimate error converges towhenever,, and.\nNote that the overlap sizecan be as small as the minimal.\n\nFor the normal approximation in Theorem3, the condition in Eq.\u00a0(3.10) is trivial, and the condition in Eq.\u00a0(3.11) simplifies to\n\nNote thatis bounded whenfor some constant(this condition is dropped when). Then Eq.\u00a0(3.13) is satisfied whenand, i.e. the overlap size is slightly larger than the minimal overlap.\n\nSECTION: 3.1Comparison with related work\n\nAs mentioned in Section2, our theoretical results are comparable toZhou et\u00a0al. (2023)for two observed submatrices and toBishop and Yu (2014)for a simple chain of observed submatrices.\nIn particular, while the error bounds inZhou et\u00a0al. (2023)are in the spectral norm and those inBishop and Yu (2014)are in the Frobenius norm, our error bounds are in the maximum entrywise norm and allow for heterogeneity among blocks.\nIn additional, the error bound in Theorem\u00a04 ofBishop and Yu (2014)grows exponentially with the length of the chain (due to its dependency onwhereis the chain length), whereas our error bound forincludes only a non-dominant term that grows linearly with the chain length; see Eq.\u00a0(3.8) or Eq.\u00a0(3.12).\nAnd the bound in Theorem\u00a04 ofBishop and Yu (2014)is only applicable for smallwhere, whereas our noise model allowsto be of orderwithof order, rendering their result ineffective.\n\nWe emphasize that the block-wise observation models in this paper, BONMI(Zhou et\u00a0al.,2023)and SPSMC(Bishop and Yu,2014)differ significantly from those in the standard matrix completion literature, which typically focuses on a single large matrix and assumes uniformly or independently sampled observed entries.\nNevertheless, the authors of BONMI have compared their results with other results in the standard matrix completion literature. For example, Remark\u00a010 inZhou et\u00a0al. (2023)shows that the upper bound for the spectral norm error of\nBONMI matches the minimax rate for the missing at random setting.\nAs CMMI is an extension of BONMI to more thanmatrices, the above comparison is still valid.\nFurthermore, our results for CMMI are in terms of the maximum entrywise norm and normal approximations, which are significant refinements of the spectral norm error in BONMI, and are thus also comparable with the best available results for standard matrix completion such as those inAbbe et\u00a0al. (2020)andChen et\u00a0al. (2021). More specifically, consider the case ofwith. Also supposeand. Then CMMI has the maximum entrywise error bound of, which matches the rate in Theorem 3.4 ofAbbe et\u00a0al. (2020)and Theorem\u00a04.5 ofChen et\u00a0al. (2021)up to a factor of, as the number\nof observed entries in our model is onlytimes that for the standard matrix completion models.\nFinally the normal approximation result in Theorem3is analogous to Theorem\u00a04.12 inChen et\u00a0al. (2021), with the main difference being the expression for the normalizing variance as our model considers individual noise matricesandwhereasChen et\u00a0al. (2021)consider a global noise matrix(which includesandas submatrices).\n\nAnother related work isChang et\u00a0al. (2022)which considers matrix completion for sample covariance matrices with a spiked covariance structure. Sample covariance matrices differ somewhat from the data matrices considered in our paper as, while both our population data matrixin Section2and their population covariance matrixare positive semidefinite, the entries of the sample covariance matrixaredependent. Consequently, the settings in the two papers are related but not directly comparable.\nNevertheless, if we were to compare our results against Theorem\u00a0C.1 inChang et\u00a0al. (2022)(where we setin our model, asChang et\u00a0al. (2022)assume that the sample covariance submatrices are observed completely) then (1) we allow block sizes(theirsare our) to differ significantly in magnitude; (2) more importantly, our error bounds depend at most linearly on the chain length, whereas their bounds grow exponentially with the chain length due to the dependency on, (theiris our). As(see Proposition\u00a0C.2 inChang et\u00a0al. (2022)), this results in a factor ofthat is highly undesirable asincreases.\n\nSECTION: 4Simulation Experiments\n\nWe now present simulation experiments to complement our theoretical results and compare the performance of CMMI against\nexisting state-of-the-art matrix completion algorithms.\n\nSECTION: 4.1Estimation error of CMMI\n\nWe simulate a chain ofoverlapping observed submatricesfor the underlying population matrixas described in Figure3, and then predict the yellow unknown block by Algorithm1.\nEachhas the same dimension, i.e.for all, and the overlap betweenandare set tofor all.\nWe generateby samplinguniformly at random from the set ofmatrices with orthonormal columns and set, so thatin this setting.\nWe then generate symmetric noise matriceswithfor alland allwith. Finally we setwhereis a symmetricmatrix whose upper triangular entries are i.i.d. Bernoulli random variables with success probability.\n\nRecall that, by Theorem2, the estimation error forcan be decomposed into the first order approximationand the remainder term. Furthermore we also have\n\nwith high probability.\nWe compare the error rates foragainstandby varying the value of one parameter among,,,,andwhile fixing the values of the remaining parameters.\nEmpirical results for these error rates, averaged overMonte Carlo replicates, are summarized in Figure4. We note that the error rates in Figure4are consistent with the bounds in Eq.\u00a0(4.1) obtained by Theorem2.\n\nWe next compare the entrywise behavior ofagainst the limiting distributions in Theorem3. In particular, we plot in Figure5histograms (based onindependent Monte Carlo replicates) of theth entries where, and it is clear that the empirical distributions in Figure5are well approximated by the normal distributions with parameters given in Theorem3.\n\nSECTION: 4.2Comparison with other matrix completion algorithms\n\nWe use the same setting as in Section4.1, but with, so that the observed submatrices fully span the diagonal of the matrix.\n\nWe varyand compare the performance of Algorithm1(CMMI) with some existing state-of-art low-rank matrix completion algorithms, including generalized spectral regularization (GSR)(Mazumder et\u00a0al.,2010), fast alternating least squares (FALS)(Hastie et\u00a0al.,2015), singular value thresholding (SVT)(Cai et\u00a0al.,2010), universal singular value thresholding (USVT)(Chatterjee,2015), iterative regression against right singular vectors (IRRSV)(Troyanskaya et\u00a0al.,2001). Note that increasingleads to more\nobserved submatrices but, as each submatrix is of smaller dimensions, the total number of observed entries decreases withat rate of.\nOur performance metric for recovering the yellow unknown block is in terms of the relative Frobenius norm error.\nPlots of the error rates (averaged overindependent Monte Carlo replicates) for different algorithms and their running times are presented in the left and right panels of Figure6, respectively.\nFigure6shows that CMMI outperforms all competing methods in terms of both recovery accuracy and computational efficiency.\n\nSECTION: 4.3Performance of CMMI with minimal overlap\n\nWe now examine the performance of CMMI when the overlap between the submatrices are very small. More specifically, we use the setting from Section4.2withand; as, this is the smallest overlap for which the latent positions for thecan still be aligned.\nWe fixand compute the estimation errorfor several values of. The results are summarized in Figure7. Note that the slope of the line in the left panel of Figure7is approximately the same as the theoretical error rate ofin Remark6. In summary, CMMI can integrate arbitrarily large submatrices even with very limited overlap.\n\nSECTION: 5Real Data Experiments\n\nWe compare the performance of CMMI against other matrix completion algorithms on the MNIST database of grayscale images and MEDLINE database of co-occurrences citations.\n\nSECTION: 5.1MNIST\n\nThe MNIST database consists ofgrayscale images of handwritten digits for the numbersthrough. Each image is of sizepixels and can be viewed as a vector in. Letdenote thematrix whose rows represent these images, where each row is normalized to be of unit norm.\nWe consider a chain ofoverlapping blocks, each block corresponding to a partially observed (cosine) kernel matrix for some subset ofimages. More specifically,\n\nfor eachwe generate amatrixwhose rows are sampled independently anduniformlyfrom rows ofcorresponding to one of the digits, with the lastrows ofand the firstrows ofhaving the same labels;\n\nwe setfor all;\n\nfinally,whereis asymmetric matrix whose upper triangular entries are i.i.d. Bernoulli random variables with success probability.\n\nGiven above collection of, we compare the accuracy for jointly clustering the images in the first and last blocks. In particular, for CMMI we first construct an embeddingusing theleading scaled eigenvectors offor each, and aligntoviaand. We then concatenate the rows ofandinto amatrixand cluster the rows ofintoclusters using-means. Finally we compare the accuracy of the cluster assignment against the true labelsusing Adjusted Rand Index (ARI). Note that\nARI values range fromto, with higher values indicating closer alignment between two sets of labels. For other low-rank matrix completion algorithms, we first reconstructfrom. Lettingdenote the resulting estimate, we findsuch thatis the best rank-approximation toin Frobenius norm (among all positive semidefinite matrices). We then subsetto keep onlyrows corresponding to images inand, and finally cluster theserows intoclusters using-means and compute the ARI of the resulting cluster assignments.\n\nComparisons between the ARIs of CMMI and other matrix completion algorithms, for different numbers of submatrices, are summarized in Figure8. Note that the black dotted line in the left panel of Figure8are ARIs when applying-means directly onand, and thus represent the best possible clustering accuracy. We observe that CMMI outperforms all competing methods on this dataset, and its ARIs remain close to optimal even asincreases. Finally, the right panel of Figure8shows that the running time for CMMI is orders of magnitude smaller than that of other algorithms.\n\nSECTION: 5.2MEDLINE co-occurrences\n\nThe MEDLINE co-occurrences database(National Library of Medicine,2023)summarizes the MeSH Descriptors that occur together in MEDLINE citations from the MEDLINE/PubMed Baseline over a duration of several years. A standard approach for handling this type of data is to first transform the (normalized) co-occurrence counts into pointwise mutual information (PMI), an association measure widely used in natural language processing(Church and Hanks,1990; Lu et\u00a0al.,2023). More specifically, the PMI between two conceptsandis\ndefined aswhereandare the (marginal) occurrence probability ofand, andis the (joint) co-occurrence probability ofand.\n\nFor our analysis of the MEDLINE data, we first selectclinical concepts which were most frequently cited during the twelve years period fromto, and construct the total PMI matrixbetween these concepts. Next we split the dates intotime intervals of equal length, and for each time intervalwe construct the individual PMI matrix. We randomly sample, for each interval, a subsetofconcepts from thosecited concepts such thatfor all. Finally we setas the principal submatrix ofinduced by. The collectionforms a chain of perturbed overlapping submatrices of.\n\nGiven, we apply CMMI and other low-rank matrix completion algorithms to constructfor the PMIs between clinical concepts inand those inin the total PMI matrix. Note that we specifyfor both CMMI and FALS, where this choice\nis based on applying the dimensionality selection procedure ofZhu and Ghodsi (2006)to. In contrast we setfor GSR as its running time increase substantially for larger values of. The values offor SVT and USVT are not specified, as both algorithms automatically determineusing their respective eigenvalue thresholding procedures.\nWe then measure the similarities between the estimated PMIs inand the true total PMIs inin terms of the Spearman\u2019s rank correlation(note that, for ease of exposition, we only compare PMIs for pairs of concepts with positive co-occurrence). The Spearman\u2019sbetween two set of vectors takes value inwith(resp.) denoting perfect monotone increasing (resp. decreasing) relationship andsuggesting no relationship.\nThe results, averaged overindependent Monte Carlo replicates, are summarized in Figure9. CMMI outperforms competing algorithms in terms of both accuracy and computational efficiency.\n\nSECTION: 6Extensions to Indefinite or Asymmetric Matrices\n\nWe now describe how the methodologies presented in this paper can be extended to block-wise data integration of symmetric\nindefinite matrices and asymmetric/rectangular matrices.\n\nSECTION: 6.1CMMI for symmetric indefinite matrices\n\nSupposeis a symmetric\nindefinite low-rank matrix.\nLetandbe the number of positive and negative eigenvalues ofand set.\nWe denote the non-zero eigenvalues ofbyLet,, and the orthonormal columns ofandconstitute the corresponding eigenvectors.\nThen the eigen-decomposition ofis, whereand.\n\nThencan be written aswith, and thus the rows ofrepresent the latent positions for the entities. For any, letdenote the set of entities contained in theth source, and letbe the corresponding population matrix. We then haveFor each observed submatrixon, we compute the\nestimated latent position matrix. Hereand,contain thelargest positive andlargest (in-magnitude) negative eigenvalues of, respectively.contains the corresponding eigenvectors.\n\nWe start with thenoiselesscase to illustrate the main idea. Consideroverlapping block-wise submatricesandas shown in Figure1. Now\n\nand hence there exist matricesandsuch that\n\nHereis the indefinite orthogonal group.\nEq.\u00a0(6.1) implies\n\nwhere we defineand we can recoverby aligning the latent positions for overlapping entities,\n\nIfthenis theuniqueminimizer of Eq.\u00a0(6.2).\nHeredenotes the Moore-Penrose pseudoinverse.\n\nNow supposeandare noisy observations ofand. Letandbe estimates ofandas described above.\nThen to alignand, we can consider solving the indefinite orthogonal Procrustes problem\n\nHowever, in contrast to the noiseless case, there is no longer an\nanalytical solution to Eq.\u00a0(6.3). We thus replace Eq.\u00a0(6.3) with the unconstrained least squares problem\n\nwhose solution is once again.\nGiven, we estimatebyExtending the above idea to a chain of overlapping submatrices is also straightforward; see SectionB.1in the supplementary material for the detailed algorithm and simulation results.\n\nThe following result extends Theorem2to the indefinite setting. We note that the main difference in this extension is in the upper bound forand this is due to the fact that the least square transformationshave spectral norms that can be smaller or larger than,\nand the accumulated error induced by these transformations need not grow linearly with. Finally ifthen the bounds in Theorem4are almost identical to those in\nTheorem1, but with a slightly different definition for.\n\nConsider a chain of overlapping submatriceswhere, for each,haspositive eigenvalues andnegative eigenvalues, satisfying Assumption1.\nSet. Here we define,for any, and supposefor.\nFor all overlaps, suppose, and define\n\nSupposefor all.\nWe then have\n\nwhereandare random matrices satisfying\n\nwith high probability. Hereis a quantity defined recursively byand\n\nfor.\n\nSECTION: 6.2CMMI for asymmetric matrices\n\nData integration for asymmetric matrices has many applications including genomic data integration(Maneck et\u00a0al.,2011; Cai et\u00a0al.,2016),\nsingle-cell data integration(Stuart et\u00a0al.,2019; Ma et\u00a0al.,2024).\nSupposeis a low-rank matrix.\nLetbe the rank of, and write the singular decomposition ofaswhereis a diagonal matrix whose diagonal entries are composed of the singular values ofin a descending order, and orthonormal columns ofandconstitute the corresponding left and right singular vectors, respectively.\nThe left and right latent position matrices associated to the entities can be represented byand, respectively. For theth source we denote the index set of the entities for rows and columns byand, and letFor each noisily observed realizationof, we obtain the estimated left latent positionsfor entities inand right latent positionsfor entities in.\n\nLetandbe two overlapping submatrices shown in Figure10without noise or missing entries. Suppose.\nNow\n\nThen there existmatricesandsuch that\n\nSuppose we want to recover the unobserved yellow submatrix in Figure10as part ofwhere, and thus our problem reduces to that of recovering. By straightforward algebra, we have\n\nandcan be obtained by aligning the latent positions for the\noverlapping entities, i.e.,\n\nNow supposeandare noisy observations ofandwith possible missing entries. Letandbe the estimated latent positions matrices obtained fromand.\nWe can align these estimates by solving the least squares problems\n\nand setting.\nWe then estimatebyNote that the unobserved white submatrix in Figure10is part ofand can\nbe recovered using the same procedure described above.\n\nFinally we emphasize that to integrate any two submatricesandof an asymmetric matrix, it is not necessary for them\nto have any overlapping entries, i.e., it is not necessary that bothand. Indeed, the above\nanalysis shows that if,or(inclusive or)then we can recover. Consider, for example, the situation in Figure11and\nsupposeis of rank. We can\nthen set\n\nExtending these ideas to a chain of overlapping submatrices is\nstraightforward; see SectionB.2in the supplementary\nmaterial for the detailed algorithm and simulation results.\n\nFinally,\nwe note that extending Theorem4to the asymmetric setting is also straightforward if we assume the entries ofare independent and thatfor all. Indeed, we can simply apply\nTheorem4to the Hermitean dilations\nof. However, the asymmetric case also allows for\nricher noise models such as the rows ofbeing independent\nbut the entries in each row are dependent, or imbalanced dimensions\nwhereor vice versa for some indices. We leave theoretical results for these more general settings to future work.\n\nSECTION: References\n\nSupplementary Material for \u201cChain-lined Multiple Matrix Integration via Embedding Alignment\u201d\n\nSECTION: Appendix ADiscussion for Multiple Matrix Data Integration\n\nIf we are given a chain of overlapping submatrices of some larger matrix, then Algorithm1provides a simple and computationally efficient procedure for recovering the unobserved regions of.\nIn practice, before applying Algorithm1we need to resolve two other crucial issues: 1) determining if a chain exists for a particular unobserved entry and 2) if there exists more than one feasible chain, how to select one chain or combine multiple chains.\nWe discuss the recoverability of entries in SectionA.1.\nIn SectionA.2and SectionA.3we discuss the issue of chain selection for two cases: 1) we want to recover a specific unobserved entry; 2) integrating several block-wise overlapping noisy matrices. Finally in SectionA.4we discuss the use of a preprocessing step proposed inZhou et\u00a0al. (2023)to aggregate multiple observed values by introducing weights to each source.\nThe algorithms are illustrated for positive semidefinite matrices, and it can be easily extended to the cases of symmetric indefinite matrices and asymmetric or rectangular matrices.\n\nSECTION: A.1Determine the recoverability of entries\n\nFor a matrix ofentities, suppose that we haveobserved submatrices associated with entities setsfrom different sources.\nWe then construct an undirected graphto determine which unobserved entries can be recovered. More specifically,hasverticessuch thatandare adjacent if and only ifandare overlapping, i.e.,.\nFor any unobserved entryin, we setand. As long as there existandsuch thatandare connected in, we can estimateby at least one chain of overlapping submatrices. Furthermore, the connected components ofalso provide information on all recoverable entries. More specifically, supposehasconnected components, which can be found by depth-first search (DFS) efficiently.\nFor each component, let, and construct the entity setto contain all entities involved in this component.\nThen wheneverandare both elements offor some,\nthe entryis recoverable using only the observed. See Algorithm2for details.\n\nSECTION: A.2Choosing among multiple chains\n\nFor any arbitrary unobserved entry, we can use the graphdescribed in SectionA.1to determine all feasible chains of overlapping submatrices for recovering; indeed, each path incorresponds to one such chain.\nWe now describe how our theoretical analysis in Section3can provide guidance for choosing a \u201cgood\u201d chain.\nFor ease of exposition, we will henceforth assume thatis connected and thus all entries that appeare in our matrix is recoverable. Ifhas more than one connected component then we can consider each connected component separately.\n\nFirst recall thatis the dominant error term in\nTheorem2. And if the noise levels ofandare homogenous then\n\nwith high probability.\nNote that the bounds on the right sides of Eq.\u00a0(A.1) and Eq.\u00a0(A.2) depend only on the first and last submatrices of the chain, respectively, and they can be estimated from the observed data. Indeed,andare given, whilecan be approximated by, with a similar approximation for.\nWe thus only need to specify the start and end points of a chain. Let\n\nfor any. Then for any arbitrary unobserved entry, set\n\nAs the influence of the middle nodes of the chain on the estimation error is negligible (see Theorem2), for simplicity we set these nodes to form the shortest path betweenandin, which can be obtained by breadth-first search (BFS). See Algorithm3for details.\n\nSECTION: A.3Algorithm for holistic recovery\n\nSuppose we have observed submatricesforand want to integrate them.\nGiven Algorithm3in SectionA.2, a straightforward idea is to recover all unobserved entries one by one, but this involves a significant amount of redundant and repetitive calculations.\nWe now describe an approach for simplifying these calculations.\n\nSuppose we have a graphas visualized in panel (a) of Figure12. Recall that each vertexinis associated with an observed submatrixwith estimated latent position matrix. Furthermore, ifandare adjacent inthen the corresponding submatricesandare overlapping and we can find an orthogonal matrixto alignand. Note thatin panel (a) of Figure12has cycles, and thus there exists at least one pair of verticesandwith multiple paths between them. We now want to find a unique path from every vertex to every other vertex, so that all the latent position matricescan be aligned together and all the unobserved entries can be recovered simultaneously.\nThis problem can be addressed using a spanning tree of; see panel (b) of Figure12. Recall thatdefined in Eq.\u00a0(A.3) reflects the magnitude of the error foras an estimate of, and thus we want our tree to have paths passing through vertices with small estimation errors. This can be achieved by setting the weight of any edgeintoand then finding the minimum spanning tree (MST) of; see panel (c) of Figure12.\n\nGiven a minimum spanning tree, we choose aat random and align the remainingtousing the unique path.\nSee Algorithm4for details.\n\nhavevertices, andare adjacent if and only if.\n\nFor each, compute, and\nobtain estimated latent positions for, denoted by.\n\nSet the weight of each edgeas.\n\nFind the minimum spanning tree ofby Prim\u2019s algorithm or Kruskal\u2019s algorithm, and denote its edge set by.\n\nFor each edge, obtainvia the orthogonal Procrustes problem\n\nChoose one of the vertex denoted by(for example,), and let.\n\nFor each, apply BFS to find a path fromto, denoted by, and let.\n\nSortin descending order and record their indices as.\n\nInitializeas anmatrix, and\n\nfortodo\n\n;\n\nend for\n\nSECTION: A.4Aggregating multiple observed values for the same entry\n\nFinally we note that as the same entry can appear in more than one observed submatrix, we can also apply a\npreprocessing step proposed inZhou et\u00a0al. (2023)to aggregate these multiple observed values. This preprocessing is especially useful when the observed submatrices have many overlapping regions as it can reduce the noise and proportion of missing observations for each submatrix.\nLetbe arbitrary, and denote bythe set of matrices in which theentry appears.Zhou et\u00a0al. (2023)show that one can weight each sourceby.\nAsis generally unknown, it can be estimated by. Hereis the observed matrix,\nandis the rank-eigendecomposition of.\nGiven, the aggregated observed value foris then set to\n\nHereis the value for theentry inandare the normalized weights.\n\nSee Algorithm5for details.\n\nFor each, estimateby\n\nwhereis the rank-eigendecomposition ofwith.\n\nInitialize.\n\nFor each, set.\n\nifthen\n\nCompute.\n\nSetfor each.\n\nSetand.\n\nend if\n\nFor each, setand.\n\nEstimatebyand set.\n\nSECTION: Appendix BAlgorithms and Simulation Results for Section6\n\nSECTION: B.1Symmetric indefinite matrices integration\n\nFor each, obtain the estimated latent positions.\n\nFor each, obtainby solving the least square optimization problem\n\nCompute.\n\nWe compare the performance of Algorithm6with other matrix completion methods. Consider the setting of Section4.2, but\nwith, and thus.\nFigure13shows the relative-norm estimation error results for CMMI against other matrix completion\nalgorithms.\n\nSECTION: B.2Asymmetric matrices integration\n\nFor each, obtain estimated left latent positions forasand right latent positions foras.\n\nFor each, obtain:\n\nifandthen\n\nComputewhere\n\nelse ifthen\n\nCompute\n\nelse\n\nComputeby\n\nend if\n\nCompute.\n\nWe compare the performance of Algorithm7with other matrix completion methods. We simulate a chain ofoverlapping observed submatricesfor the underlying population matrixas described in Figure14, and then predict the yellow unknown block by Algorithm7.\nWe let all observed submatrices have the same dimension, and let all overlapping parts have the same dimension.\nFor the observed submatrices, we generate the noise matricesbyfor alland all, and we let all observed submatrices have the same non-missing probability.\nFor the low-rank underlying population matrix, we randomly generateandfromand, respectively. We fix the rank as, and set.\nWe fix the dimensions of the entire matrix atand, and we vary, the length of the chain, while ensuring that the observed submatrices fully span the diagonal of the matrix Recall that asincreases, we have more observed submarices but each observed submatrix is of smaller dimensions, which then increases the difficulty of recovering the original matrix.\nFigure15shows the relative-norm estimation error results of recovering the yellow region.\n\nSECTION: Appendix CProof of Main Results\n\nSECTION: C.1Proof of Theorem1\n\nWe first state two important technical lemmas, one for the error ofas an estimate of the true latent position matrixfor each, and another for the difference betweenand. The proofs of these lemmas are presented in SectionD.2and SectionD.5.\n\nFix anand consideras defined in Eq.\u00a0(2.1).\nWrite the eigen-decompositions ofandas\n\nLet.\nNext defineas a minimizer ofover allorthogonal matrices.\nSuppose that\n\nis amatrix with bounded coherence, i.e.,\n\nhas bounded condition number, i.e.,\n\nfor some finite constant; hereanddenote the largest and smallest non-zero eigenvalues of, respectively.\n\nThe following conditions are satisfied.\n\nWe then have\n\nwhere the remainder termsatisfies\n\nwith high probability.\nIf we further assume\n\nthen we also have\n\nwith high probability.\n\nAs, we generally have, e.g.,hasentries that are lower bounded by some constantnot depending onor. Thus, asis low-rank with bounded condition number, we also haveand the second condition in Eq.\u00a0(C.1) simplifies to\n\nSimilarly, the condition in Eq.\u00a0(C.3) simplifies to\n\nBoth conditions are then trivially satisfied wheneverand.\nFinally when, the bounds in LemmaC.1simplify to\n\nwith high probability.\n\nConsider the setting of Theorem1. We then have\n\nwith high probability.\n\nWe now proceed with the proof of Theorem1. Recall Eq.\u00a0(C.2) and\nletfor any. Also denote.\nWe then have\n\nwhere we set\n\nWe now boundand. Note that,\nfor matricesandof conformal dimensions, we have\n\nWe therefore have\n\nNext, by LemmaC.1, for anywe have\n\nwith high probability. Finally, by LemmaC.2we have\n\nwith high probability. Substituting the above bounds in Eq.\u00a0(C.6) and Eq.\u00a0(C.7) into Eq.\u00a0(C.5), we obtain the desired bounds ofand.\n\nSECTION: C.2Proof of Theorem2\n\nTheorem2follows the same argument as that for Theorem1, with the only change being the use of\nLemmaC.3below (see SectionD.7for a proof) to bound the difference betweenand.\n\nConsider the setting of Theorem2and\nlet. We then have\n\nwith high probability.\n\nSECTION: C.3Proof of Theorem3\n\nBy Theorem2, for any fixed, we have\n\nAs,\nwe have for anythat\n\nSimilarly, we also have that for any,.\nNote thatare independent.\nWe thus have\n\nLet\n\nand note thatare mutually independent zero-mean random variables.\nLet\n\nWe now analyzefor any fixed; the same analysis also applies tofor any fixed.\nRewriteas\n\nThen by Eq.\u00a0(C.10) we have\n\nThe condition in Eq.\u00a0(3.9)implies\n\nwhereanddenote theth row andth row ofand, respectively.\nNext we have\n\nFor any fixed but arbitrary, we have\n\nwhere the last inequality follows from the fact thatwhenever.\n\nNow ifthen by Eq.\u00a0(C.11) and Eq.\u00a0(C.12) we have\n\nSimilarly, ifwe have\n\nEq.\u00a0(C.14) and Eq.\u00a0(C.15) together imply\n\nasymptotically almost surely, provided by the condition in Eq.3.10.\nReturning to Eq.\u00a0(C.13), note thatis a deterministic function ofand hence\n\nWe now bound the terms appearing in the above display.\nFirst, by Eq.\u00a0(C.14), we have\n\nNext, asis sub-Gaussian with, we have by a similar analysis to Eq.\u00a0(C.14) thatis also sub-Gaussian with\n\nThere thus exists a constantsuch that\n\nsee Eq.\u00a0(2.14) inVershynin (2018)for more details on tail bounds for sub-Gaussian random variables.\nWe therefore have\n\nCombining Eq.\u00a0(C.13) and Eq.\u00a0(C.16) through Eq.\u00a0(C.19), we have\n\nas, under the assumption thatprovided by Eq.3.10.\nUsing the same argument we also have\n\nBy Eq.\u00a0(C.20), Eq.\u00a0(C.21), and applying the Lindeberg-Feller central limit theorem (see e.g.,\nProposition\u00a02.27 inVan\u00a0der Vaart (2000)) we have\n\nas. Finally, invoking Theorem2and the assumption in Eq.\u00a0(3.11), we havein probability. Then applying Slutsky\u2019s theorem we obtainas claimed.\n\nSECTION: Appendix DTechnical Lemmas\n\nSECTION: D.1Technical lemmas for LemmaC.1\n\nConsider the setting of LemmaC.1.\nThen for any, forwe have\n\nwith high probability.\n\nFirst writeas the sum of two matrices, namely\n\nIfthen,\nfollowing the same arguments as that for Lemma\u00a013 inAbbe et\u00a0al. (2020)we obtain\n\nwith high probability. Next for any arbitrarywith, let\n\nwheredenotes theth row offor any.\nThen; note thatare independent, random, self-adjoint matrices ofdimension withand\n\nNow for any square matrix, we have, wheredenotes the Loewner ordering for positive semidefinite matrices. Therefore for anywe have\n\nFurthermore, asfor all,\nwe have\n\nTherefore according to Theorem\u00a01.4 inTropp (2012), for all, we have\n\nand hence\n\nwith high probability.\n\nNext note that, whereis theth row of.\nThus, for any fixed,; note thatare independent, random matrices of dimensionwithand\n\nLet.\nWe then have\n\nTherefore, by Theorem\u00a01.6 inTropp (2012), for anywe have\n\nand hence\n\nwith high probability, where the final inequality follows from the assumption. Taking a union over allwe obtain\n\nwith high probability.\n\nFor, its upper triangular entries are independent random variables. Because for anyis a sub-gaussian random variable with, we haveand it follows that\n\nFor sub-gaussian random variables, we also havewith high probability; hereis some finite constant not depending onor.\nThen with high probability\n\nThen by combining Corollary 3.12 and Remark\u00a03.13\ninBandeira and Van\u00a0Handel (2016)with Proposition\u00a0A.7 inHopkins et\u00a0al. (2016),\nthere exists some constantsuch that for any\n\nLetfor some, thenfromwe have\n\nwith high probability.\n\nForand, with the similar analysis withandwe have\n\nwith high probability\nand\n\nwith high probability.\n\nFinally we combine Eq.\u00a0(D.2) and Eq.\u00a0(D.5), Eq.\u00a0(D.3) and Eq.\u00a0(D.6), Eq.\u00a0(D.4) and Eq.\u00a0(D.7) and obtain the desired results for,, and.\n\u220e\n\nConsider the setting of LemmaC.1.\nThen for anywe have\n\nwith high probability, and forandwe have\n\nwith high probability.\n\nBy perturbation theorem for singular values (see Problem\u00a0III.6.13 inHorn and Johnson (2012)) and LemmaD.1, for anywe have\n\nwith high probability. Then the condition in Eq.\u00a0(C.1) yields the stated claim for.\nNext, by Wedin\u2019sTheorem (see e.g., Theorem\u00a04.4 in Chapter\u00a04 ofStewart and Sun (1990)) and LemmaD.1, we have\n\nwith high probability, and hence\n\nwith high probability.\n\u220e\n\nConsider the setting of LemmaC.1.\nThen for each, we have\n\nwith high probability.\n\nFor ease of exposition we will fix a value ofand thereby drop the index from our matrices.\n\nFor, because\n\nby LemmaD.1and LemmaD.2we have\n\nwith high probability.\n\nFor, we notice\n\nFor the right hand side of the above display, we have bounded the second term in Eq.\u00a0(D.8).\nFor the first term and the third term, by LemmaD.2we have\n\nwith high probability.\nCombining Eq.\u00a0(D.8) and Eq.\u00a0(D.9), we have\n\nwith high probability.\n\nFor, for anytheentry can be written as\n\nWe defineas amatrix whose entries are. Eq.\u00a0(D.11) means\n\nwheredenotes the Hadamard matrix product, and by Eq.\u00a0(D.10) and LemmaD.2it follows that\n\nwith high probability.\n\nFor, notice\n\nwhereis a matrix-valued function, and for anyinvertible matrix,. Then if, according to Theorem\u00a01 inLi (1995)we have\n\nWe now bound.\nBy LemmaD.2and Eq.\u00a0(D.12) we have\n\nwith high probability. Now by Eq.\u00a0(C.1)\nwe havewith high probability.\nIn summary we have\n\nwith high probability.\n\u220e\n\nSECTION: D.2Proof of LemmaC.1\n\nWe first state a lemma for bounding the\nerror ofas an estimate of; see SectionD.3for a proof.\n\nConsider the setting of LemmaC.1.\nDefineas a minimizer ofover allorthogonal matrices.\nWe then have\n\nwhereandis arandom matrix satisfying\n\nwith high probability. Furthermore, suppose\n\nWe then have\n\nwith high probability.\n\nNow recall that\n\nAs, there exists an orthogonalsuch that.\nDefine\n\nand recall\n\nNote that.\nNext, by Eq.\u00a0(D.13) we have\n\nwhere the remainder termis\n\nBy LemmaD.4, LemmaD.2and LemmaD.3we obtain\n\nwith high probability and\n\nwith high probability.\n\nSECTION: D.3Proof of LemmaD.4\n\nFor ease of exposition we fix a value ofand drop this index from our matrices.\nFirst note that\n\nHence for anyorthogonal matrix, we have\n\nLetbe the minimizer ofover allorthogonal matrices. We now bound the spectral norms ofwhen.\n\nFor, by LemmaD.2we have\n\nwith high probability.\n\nFor, by LemmaD.2and LemmaD.3we have\n\nwith high probability.\n\nFor, by LemmaD.1, LemmaD.2and LemmaD.3we have\n\nwith high probability.\n\nFor, by LemmaD.1, LemmaD.2and LemmaD.9, we have\n\nwith high probability.\nCombining the above bounds we obtain\n\nwith high probability\nasas implied by Eq.\u00a0(C.3).\n\nSECTION: D.4Technical lemmas forin LemmaD.4\n\nOur bound forin the above proof of LemmaD.4is based on a series of technical lemmas which culminate in a high-probability bound for. These lemmas are derived using an adaptation of the leave-one-out analysis presented in Theorem\u00a03.2 ofXie (2024); the noise model forin the current paper is, however, somewhat different from that ofXie (2024)and thus we chose to provide self-contained proofs of these lemmas here.\nOnce again, for ease of exposition we will fix a value ofand thereby drop this index from our matrices in this section.\n\nWe introduce some notations.\nForwhose entries are independent Bernoulli random variables sampled according to,\nwe define the following collection of auxiliary matricesgenerated from.\nFor each row index, the matrixis obtained by replacing the entries in theth row ofwith their expected values, i.e.,\n\nDenote the singular decompositions ofandas\n\nConsider the setting in LemmaC.1, we then have\n\nwith high probability. Furthermore, letbe the solution of orthogonal Procrustes problem betweenand. We then have\n\nwith high probability.\n\nThe proof is based on verifying the conditions in Theorem\u00a02.1 ofAbbe et\u00a0al. (2020), and this can be done by following the exact same derivations as that in Lemma\u00a012 ofAbbe et\u00a0al. (2020).\nMore specifically,inAbbe et\u00a0al. (2020)corresponds toin our paper whileinAbbe et\u00a0al. (2020)corresponds toin our setting, whereappears in the assumptions of LemmaC.1and is bounded,\nandinAbbe et\u00a0al. (2020)can be set to befor some sufficiently large constant, based on the bound offrom LemmaD.1. Then all desired results of LemmaD.5can by obtainedunder the conditions thatand condition in Eq.\u00a0(C.3).\u220e\n\nConsider the setting in LemmaC.1.\nRecall in LemmaD.1, we decomposeas.\nLetdenote theth row of.\nThen for any deterministicmatrix, we have\n\nwith high probability.\n\nFor any, letanddenote theth row ofand. Following the same arguments as that forandin the proof of LemmaD.1, we have\n\nWe therefore have\n\nwith high probability.\nCombining the above bounds yields the desired claim.\n\u220e\n\nConsider the setting in LemmaD.4, we then have\n\nwith high probability.\n\nBy the construction ofand LemmaD.1we have\n\nwith high probability, and hence\n\nwith high probability.\nThen by Weyl\u2019s inequality we have\n\nwith high probability. The condition in Eq.\u00a0(C.3) impliesand hence.\nFurthermore, by LemmaD.2we havewith high probability. Applying\nWedin\u2019sTheorem (see e.g., Theorem\u00a04.4 ofStewart and Sun (1990)) we have\n\nwith high probability.\nWe now bound. Note that\n\nwhereis theth row of.\nFor the first term on the right side of Eq.\u00a0(D.18), by Cauchy-Schwarz inequality, LemmaD.5and LemmaD.1we have\n\nwith high probability.\nFor the second term on the right side of Eq.\u00a0(D.18),\nasandare independent, we have by LemmaD.6, LemmaD.5, and the assumptionthat\n\nwith high probability. Combining Eq.\u00a0(D.18), Eq.\u00a0(D.19) and Eq.\u00a0(D.20), we have\n\nwith high probability. Substituting Eq.\u00a0(D.21) into Eq.\u00a0(D.17)\nyields the desired claim.\n\u220e\n\nConsider the setting in LemmaD.4, we then have\n\nwith high probability.\n\nEq.\u00a0(D.15) and Eq.\u00a0(D.16) impliesandwith high probability.\nThen by Wedin\u2019sTheorem (see e.g., Theorem\u00a04.4 inStewart and Sun (1990))\nwe have\n\nwith high probability.\nLetbe the orthogonal Procrustes alignment betweenand. Then\n\nwith high probability.\n\nNow let. By Eq.\u00a0(D.22) and LemmaD.5we obtain\n\nwith high probability, where the final inequality follows from the fact that, under the conditions in Eq.\u00a0(C.3) we have.\n\nFinally, asandare independent, by LemmaD.6and the assumptionwe have\n\nwith high probability. \u220e\n\nConsider the setting in LemmaD.4, we then have\n\nwith high probability.\n\nFor each, letdenote theth row of. Notice\n\nWe now bound all terms on the right hand side of Eq.\u00a0(D.23).\nFor, by LemmaD.2we have\n\nwith high probabilityasas implied by Eq.\u00a0(C.3).\n\nFor, by LemmaD.1and LemmaD.2we have\n\nwith high probability.\n\nFor, by LemmaD.1and LemmaD.7we have\n\nwith high probability.\n\nFor, by LemmaD.8we have\n\nwith high probability.\n\nCombining Eq.\u00a0(D.23), Eq.\u00a0(D.24), \u2026, Eq.\u00a0(D.27) we finally obtain\n\nwith high probabilityasas implied by Eq.\u00a0(C.3).\n\u220e\n\nSECTION: D.5Proof of LemmaC.2\n\nRecall that\n\nDenote\n\nWe therefore have, by perturbation bounds for polar decompositions, that\n\nIndeed, we supposeis invertible in Theorem1.\nNow suppose. Thenis also invertible and hence, by Theorem\u00a01 inLi (1995)we have\n\nOtherwise ifthen, as, Eq.\u00a0(D.28) holds trivially.\n\nWe now bound. First note that\n\nNext, by LemmaC.1, we have\n\nwhereandcontain the rows inandcorresponding to entities, respectively. A similar expansion holds for.\nWe therefore have\n\nFor, by LemmaC.1we have\n\nwith high probability. For, by LemmaD.10we have\n\nwith high probability. The same argument also yields\n\nwith high probability. For, once again by LemmaD.10we have\n\nwith high probability. The same argument also yields\n\nwith high probability.\nCombining the above bounds forand simplifying, we obtain\n\nwith high probability. Substituting the above bound forinto Eq.\u00a0(D.28) yields the stated claim.\n\nSECTION: D.6Technical lemmas for LemmaC.2\n\nConsider the setting of Theorem1. We then have\n\nwith high probability.\n\nFrom LemmaC.1we have\n\nwith high probability. Furthermore, following the same derivations as that forin the proof of LemmaD.1, we have\n\nwith high probability, provided that.\n\u220e\n\nSuppose the entities inare selected uniformly at random from allentities.\nWrite the eigen-decomposition ofaswhereare the eigenvectors corresponding to the non-zero eigenvalues. Letanddenote the largest and smallest non-zero eigenvalue of, respectively. Then forwe have\n\nwith high probability.\n\nIf the entities inare chosen uniformly at random from allentities then, by Proposition\u00a0S.3. inZhou et\u00a0al. (2023)together with the assumption, we have\n\nwith high probability. As, given the above bound we have\n\nand hence\n\nFinally, from\n\nwe obtain\n\nas claimed.\n\u220e\n\nSECTION: D.7Proof of LemmaC.3\n\nWe proceed by induction on. The casefollows from LemmaC.2.\nNow for any, define\n\nand suppose the stated bound holds for, i.e.\n\nwith high probability. Next note that\n\nand hence, by combining Eq.\u00a0(D.31) and LemmaC.2(for),\nwe obtain\n\nwith high probability.\n\nSECTION: D.8Extension to symmetric indefinite matrices\n\nWe first state an analogue of LemmaC.1for symmetric but possibly indefinite matrices.\n\nFix anand consideras defined in Eq.\u00a0(2.1).\nWrite the eigen-decompositions ofandas\n\nLetanddenote the number of positive and negative eigenvalues of, respectively, and denote.\nLetand.\nSuppose that\n\nis amatrix with bounded coherence, i.e.,\n\nhas bounded condition number, i.e.,\n\nfor some finite constant; hereanddenote the largest and smallest non-zero singular values of, respectively.\n\nThe following conditions are satisfied.\n\nThen there exists an matrixsuch that\n\nwhere the remainder termsatisfies\n\nwith high probability. Recall thatanddenote the set oforthogonal andindefiniteorthogonal matrices, respectively.\nIf we further assume\n\nthen we also have\n\nwith high probability.\n\nThe proof of LemmaD.12follows the same argument as that for LemmaC.1and is thus omitted. The main difference between the statements of\nthese two results is that LemmaC.1boundswhile\nLemmaD.12bounds. Ifis positive semidefinite thenfor some orthogonal matrixand thus we can combine bothandinto a single orthogonal transformation; see the argument in SectionD.2. In contrast, ifis indefinite thenfor some indefinite orthogonal. As indefinite\northogonal matrices behave somewhat differently from orthogonal matrices, it is simpler to considerandseparately.\n\nConsider the setting of Theorem4for just two overlapping submatricesand.\nLetbe the least square alignment betweenand.\nHeredenotes the Moore-Penrose pseudoinverse of a matrix.\nAlso noticeis the corresponding alignment betweenand. We then have\n\nwith high probability.\n\nFor ease of notation, we will letand, and defineand. We then have\n\nwhere the first equality follows from the fact that ifis amatrix andis aorthogonal matrix then.\n\nNow under the assumption, we havewith high probability, and hence bothandare invertible with\nhigh probability. We thus haveand\n\nFurthermore, asandshare the same column space, we haveso that\n\nCombining Eq.\u00a0(D.33) and Eq.\u00a0(D.34) we obtain\n\nand hence\n\nWe now bound the terms on the right side of Eq.\u00a0(D.35).\nBy LemmaD.12we have\n\nwith high probability.\nRecall that\n\nThusBecausewith high probability, we also have\n\nwith high probability.\nThen by Theorem\u00a04.1 inWedin (1973)we have\n\nwith high probability.\nNext, by Eq.\u00a0(D.32), we have\n\nBy similar results to LemmaD.10we have\n\nwith high probability. Therefore we have\n\nwith high probability.\nCombining Eq.\u00a0(D.35), Eq.\u00a0(D.36), Eq.\u00a0(D.37), and Eq.\u00a0(D.38) we have the desired error rate of.\n\u220e\n\nWe now prove Theorem4.\nIn the case of a chainwe have\n\nwhereis matrix product and. Furthermore, for anywe also have\n\nTherefore, for,\n\nDefinefor. We then havewith high probability by LemmaD.13, and have\n\nforwith high probability by Eq.\u00a0(D.39), where\n\nFinally, we have\n\nwhere the last equality follows from the facts thatand.\nLetfor. Following the same derivations as that for Eq.\u00a0(C.4), with LemmaD.12replacing LemmaC.1,\nwe obtain\n\nwhere we set\n\nSubstituting the bounds forandin LemmaD.12, we obtain\n\nwith high probability under the assumptionfor. Finally, asfor, with, we have after some straightforward algebra that\n\nas desired.", "text_file": "data\\paper_texts\\2412.02791v2_content.txt"}, {"title": "Mixed Delay/Nondelay Embeddings Based Neuromorphic Computing with\n  Patterned Nanomagnet Arrays", "authors": ["Changpeng Ti", "Usman Hassan", "Sairam Sri Vatsavai", "Margaret McCarter", "Aastha Vasdev", "Jincheng An", "Barat Achinuq", "Ulrich Welp", "Sen-Ching Cheung", "Ishan G Thakkar", "J. Todd Hastings"], "published_date": "2024-12-05T21:28:04Z", "summary": "Patterned nanomagnet arrays (PNAs) have been shown to exhibit a strong\ngeometrically frustrated dipole interaction. Some PNAs have also shown emergent\ndomain wall dynamics. Previous works have demonstrated methods to physically\nprobe these magnetization dynamics of PNAs to realize neuromorphic reservoir\nsystems that exhibit chaotic dynamical behavior and high-dimensional\nnonlinearity. These PNA reservoir systems from prior works leverage echo state\nproperties and linear/nonlinear short-term memory of component reservoir nodes\nto map and preserve the dynamical information of the input time-series data\ninto nondelay spatial embeddings. Such mappings enable these PNA reservoir\nsystems to imitate and predict/forecast the input time series data. However,\nthese prior PNA reservoir systems are based solely on the nondelay spatial\nembeddings obtained at component reservoir nodes. As a result, they require a\nmassive number of component reservoir nodes, or a very large spatial embedding\n(i.e., high-dimensional spatial embedding) per reservoir node, or both, to\nachieve acceptable imitation and prediction accuracy. These requirements reduce\nthe practical feasibility of such PNA reservoir systems. To address this\nshortcoming, we present a mixed delay/nondelay embeddings-based PNA reservoir\nsystem. Our system uses a single PNA reservoir node with the ability to obtain\na mixture of delay/nondelay embeddings of the dynamical information of the\ntime-series data applied at the input of a single PNA reservoir node. Our\nanalysis shows that when these mixed delay/nondelay embeddings are used to\ntrain a perceptron at the output layer, our reservoir system outperforms\nexisting PNA-based reservoir systems for the imitation of NARMA 2, NARMA 5,\nNARMA 7, and NARMA 10 time series data, and for the short-term and long-term\nprediction of the Mackey Glass time series data.", "arxiv_id": "2412.04622v1", "html_link": "https://arxiv.org/html/2412.04622v1", "search_term": "ti:\"embeddings\"", "html_content": "SECTION: Mixed Delay/Nondelay Embeddings Based Neuromorphic Computing withPatterned Nanomagnet Arrays\n\nPatterned nanomagnet arrays (PNAs) have been shown to exhibit a strong geometrically frustrated dipole interaction. Some PNAs have also shown emergent domain wall dynamics. Previous works have demonstrated methods to physically probe these magnetization dynamics of PNAs to realize neuromorphic reservoir systems that exhibit chaotic dynamical behavior and high-dimensional nonlinearity. These PNA reservoir systems from prior works leverage echo state properties and linear/nonlinear short-term memory of component reservoir nodes to map and preserve the dynamical information of the input time-series data into nondelay spatial embeddings. Such mappings enable these PNA reservoir systems to imitate and predict/forecast the input time series data. However, these prior PNA reservoir systems are based solely on the nondelay spatial embeddings obtained at component reservoir nodes. As a result, they require a massive number of component reservoir nodes, or a very large spatial embedding (i.e., high-dimensional spatial embedding) per reservoir node, or both, to achieve acceptable imitation and prediction accuracy. These requirements reduce the practical feasibility of such PNA reservoir systems. To address this shortcoming, we present a mixed delay/nondelay embeddings-based PNA reservoir system. Our system uses a single PNA reservoir node with the ability to obtain a mixture of delay/nondelay embeddings of the dynamical information of the time-series data applied at the input of a single PNA reservoir node. Our analysis shows that when these mixed delay/nondelay embeddings are used to train a perceptron at the output layer, our reservoir system outperforms existing PNA-based reservoir systems for the imitation of NARMA 2, NARMA 5, NARMA 7, and NARMA 10 time series data, and for the short-term and long-term prediction of the Mackey Glass time series data.\n\nSECTION: IIntroduction\n\nPatterned nanomagnet arrays (PNAs) are typically fabricated by etching nanomagnets in ferromagnetic thin films (e.g., permalloy films) in the desired geometric arrangement using lithography[1]. Each nanomagnet in a PNA behaves as a microscopic Ising spin whose dipole interaction with the neighboring spins is geometrically frustrated[2]. Such frustrated interactions often lead to numerous nearly degenerated energy states in the PNA because of which not all spins in the PNA attain the ground-energy state when the PNA is subjected to an external magnetic field perturbation[3]. The lingering of nanomagnet spins in the degenerated energy states gives rise to richly nonlinear magnetization dynamics in PNAs[3].\n\nThe nonlinear magnetization dynamics give rise to the echo-state property and nonlinear/linear short-term memory in PNAs. These properties enable PNAs to memorize and adaptively learn spatio-temporal features and hidden patterns in complex time series data when the time series data are encoded and applied to PNAs as a sequence of perturbing external magnetic fields[3,4]. These capabilities of PNAs make them very attractive for realizing neuromorphic reservoir systems that can be used to imitate and predict time series data[5,6,7,8,9,10,2,11,12,13,14].\n\nDesigning a quality reservoir system using PNAs requires probing the magnetization dynamics and the nearly degenerated spin energy states of PNAs so that the spatial embeddings of the dynamical information of the input time series data, preserved in the magnetization states of PNAs, can be extracted for postprocessing[2,15]. Such spatial embeddings are referred to as nondelay spatial embeddings as they provide the preserved dynamical information at a particular time step of the input time-series data[16,17]. To extract nondelay spatial embeddings of PNA-based reservoir systems, the orientations of nanomagnet spins in PNAs can be probed by direct magnetic imaging of PNAs[3]. In addition to magnetic imaging, other common tools for probing PNAs and extracting nondelay spatial embeddings include ferromagnetic resonance[4,13,14], resonant soft X-ray scattering, X-ray photon correlation spectroscopy, and magnetoresistance transport measurement (using giant magnetoresistance (GMR) or anisotropic magnetoresistance (AMR) or tunneling magnetoresistance (TMR))[2]. Several prior works have used these tools to demonstrate PNA-based reservoir computing capabilities[5,6,7,8,9,10,2,11,12,13,14].\n\nUnfortunately, however, these tools face several shortcomings that diminish their use in practical reservoir computing systems and applications. For instance, the ferromagnetic resonance and X-ray based tools are powerful for (i) distinguishing the nearly degenerated spin energy states, (ii) understanding the physics of PNAs, and (iii) extracting the nondelay spatial embeddings of reservoir states that are rich in the spatially separable dynamical information of the input data, but they are not practical for co-packaged hardware-based implementations of reservoir computing applications. On the other hand, the TMR/GMR/AMR-based transport measurements are amenable to hardware-based implementations of practical reservoir computing applications, but they provide nondelay spatial embeddings of reservoir states that are scarce in the spatially separable dynamical information of input data. This scarcity of spatially separable dynamical information results in poor accuracy for imitation and prediction tasks.\n\nTo address these shortcomings, we propose a mixed delay/nondelay spatial embeddings-based PNA reservoir system. Our system extracts the nondelay spatial embeddings of PNA reservoir states using TMR-based probing of PNA spins and augments the extracted nondelay spatial embeddings with delayed spatial embeddings from a total of past h=50 time steps. The combination of delay and nondelay-based spatial embeddings dramatically increases the richness of the spatially separable dynamical information readily available to our reservoir system, increasing the imitation and prediction accuracy of our system compared to the TMR/GMR/AMR-based PNA reservoir systems from prior works. In addition, the TMR-based probing increases the practicality of our reservoir system compared to the ferromagnetic resonance and X-ray probing-based PNA reservoir systems from prior works.\n\nThis paper makes the following key contributions.\n\nWe propose a TMR probing-based PNA reservoir system that combines delay and nondelay spatial embeddings to improve the practicality and accuracy of reservoir computing;\n\nWe evaluate our proposed PNA reservoir system for imitation of the NARMA 2, NARMA 5, NARMA 7, and NARMA 10 time series data;\n\nWe evaluate our proposed PNA reservoir system for short-term and long-term prediction of the Mackey-Glass time series data;\n\nWe explore the accuracy impacts of using a single-layer versus multilayer perceptron at the output layer of our PNA reservoir system;\n\nWe compare the imitation and precision accuracy results for our system with the results for five different reservoir systems from prior works.\n\nSECTION: IIPreliminaries\n\nSECTION: II-ANeuromorphic Reservoir Computing: Overview\n\nThe core idea of the neuromorphic reservoir computing is to use a neuromorphic dynamical system as a physical reservoir to adaptively learn spatio-temporal features and hidden patterns in complex time series[15,18](see Fig.1). A perspective on emerging challenges and opportunities for reservoir computing is available in[15]. As described in[15], the basic process of reservoir computing involves preprocessing the external input signal, mapping the input signal onto interacting physical nodes within the reservoir to transform it into complex spatiotemporal patterns in a high-dimensional space, probing the physical reservoir state, and finally postprocessing the reservoir state to generate output data. Various physical devices and systems with disparate physical phenomena can be used to realize a reservoir[18,19,20,21,15](see Fig.1).\n\nThe computational flow of reservoir computing comprises three layers, namely the input, reservoir, and output layers (see Fig.1). At the input layer, the raw inputs are encoded into temporal sequences of physical perturbations that can directly excite the dynamics of the nonlinear physical nodes of the reservoir. The complex interaction dynamics of the physical nodes of the reservoir, governed by predefined physical rules, determine the reservoir state. This reservoir state is then transformed into the reservoir output using a perceptron typically a single-layer linear perceptron. In a standard setup, the input encodings and the physical rules defining the dynamical behavior of the reservoir are fixed, and only the perceptron at the output layer is trained.\n\nTo achieve high accuracy from a reservoir system, it is generally desirable to have a very high- dimensional reservoir state. The dimensionality of the reservoir state referes to the number of embeddings of spatio-temporally dynamical information in the reservoir state vector. Typically, there is a one-to-one correspondence between the number of embeddings in the reservoir state vector and the number of physical nodes in the reservoir system; therefore, increasing the dimensionality of the reservoir state often results in a large number of physical nodes in the reservoir, making it less feasible to implement. To make reservoir systems more practical and encourage their wider adoption, it is essential to increase the dimensionality of the reservoir state without proportionally increasing the number of physical nodes. Fortunately, previous studies[16,17,22,23,24]have demonstrated that this can be achieved by introducing time-delayed embeddings in addition to the general nondelay embeddings per physical reservoir node.In this paper, we investigate the use of time-delayed embeddings in PNA-based reservoir systems for the first time.\n\nSECTION: II-BComputing with Patterned Nanomagnet Arrays (PNAs)\n\nPNAs can be manufactured using standard lithography techniques in various geometric topologies and structures (e.g., square[25], pinwheel[26], kagome[27]topologies and various other structures[28,29,30,31,32,33,1]). Realizing any arbitrary more exotic PNA topologies is also possible, e.g., artificial quasicrystals[34,35], distorted topologies[36,37], and connected nanomagnet networks where domain walls can travel through the network[8]. Individual nanomagnets of a PNA system can be realized in various shapes, e.g., elongated[26,27]and circular[11].\n\nIn a PNA, the nanomagnets are typically arranged in a way that not all dipole interactions among the nanomagnets can be satisfied simultaneously. This gives rise to collective dynamical magnetization behavior, which can be driven by an external magnetic field. In Fig.2, we demonstrate an experimentally probed trigger of such dynamical magnetization behavior in an electron-beam lithography-fabricated sample of a square PNA.\n\nThe collective dynamical behavior of the nanomagnets gives rise to a variety of emergent phenomena including magnetic monopoles, vertex-based frustration, phase transition, and chiral dynamics[38,39,40,41,42,43,44]. This collective dynamical behavior of PNAs has been shown to give rise to the echo state property and nonlinear/linear memory capacity in the PNAs. Due to these properties, PNAs have been used as high-quality neuromorphic reservoirs in several prior works[5,6,7,8,9,10,2,11,12,13].\n\nSECTION: II-CInput Encoding and Readout for PNA Reservoirs\n\nSpin-wave spectral fingerprinting has been a common method to investigate the physics and computing capabilities of PNAs[45,9,13]. This is typically enabled by flip-chip integration of the PNA chip with an RF coplanar waveguide[45]. The coplanar waveguide can be connected to a microwave generator to enable the coupling of an external RF magnetic field with the PNA sample, constituting a method for applying the input signal to the PNA-based reservoir[13]. The coplanar waveguide can also be used to probe the ferromagnetic resonance spectra of the PNA sample, constituting a method for reservoir state readout. Apart from applying an external magnetic field via the coplanar waveguide, the input signal can also be applied through spin-orbit torque (SOT) or magnetic tunnel junction (MTJ)-based switching of the actuating nanomagnets in the PNA[11,6]. Similarly, alternative methods also exist for the state readout of PNA-based reservoirs. These include magnetization probing using TMR[11]or GMR/AMR[2]. An analysis of the energy consumption of the SOT-based input application and the MTJ-based readout enabled by TMR has been carried out in[11], and evidently, the TMR-based readout and SOT-based input application can be substantially more energy efficient than the coplanar waveguide-based input and readout methods.\n\nSECTION: IIIProposed Delay/Nondelay Embeddings Based PNA Reservoir System\n\nSECTION: III-ADesign\n\nOur proposed PNA reservoir system, illustrated in Fig.3, takes as input the time-series data(a sequence of sampled signal amplitudes). At each time step,is encoded intocycles of global external magnetic field amplitude pairs,. The magnetic field amplitudes of the field cycles corresponding toare linearly proportional to the amplitude of. This method of encodingas cycles of magnetic field amplitudes is inspired from[13,12]. We set N=5 for our PNA reservoir system, though other values may also be valid. Determining the optimal value of N is beyond the scope of this work.\n\nOur system consists of a single PNA reservoir node with configurable parameters such as geometric topology, dimensions, and magnetic properties. We have used square and pinwheel topologies[25]in this work, though other geometric topologies could also be explored (e.g., kagome[46,27]). When the input-encoded magnetic field cycles are applied sequentially to the PNA reservoir node, they induce hysteretic and dynamical magnetization interactions among the constituent nanomagnets of the PNA. After the magnetization interactions in the PNA node stabilize, the magnetization status at each nanomagnet is probed. The physical probing of the magnetization of each nanomagnet can be achieved by using the Tunneling Magnetoresistance (TMR) effect[11]or the Giant/Anisotropic Magnetoresistance (GMR/AMR) effects[2].\nThe magnetization values of all the nanomagnets in the PNA node are then combined into the reservoir node\u2019s state vector, denoted as the nondelay embedding. This nondelay embedding represents the response of the reservoir system at the current time step. This embedding preserves the dynamical information of multiple past time steps of sequence.\nThis preservation of dynamical information is due to the echo state property and linear/nonlinear short-term memory capacity of the PNA reservoir node, as expected.\n\nIn addition to this nondelay embedding, our system incorporates a delay embedding to explicitly capture historical information on the input signal dynamics for imitation and prediction. This delay embedding at timeis formed by concatenating the reservoir\u2019s nondelay embeddings from pasttime steps, denoted aswhere.\nThe final reservoir state at timeis the combination of delay and nondelay embeddings, represented by the concatenated state vector.\nThis concatenated state vector spatially encapsulates the dynamical behavior of the system up to the current time step, and is used for subsequent tasks such as signal imitation and prediction.\n\nFor the training process, a perceptron (either single-layer or multilayer) is used to transform the augmented state vectorinto a target signal (sequence of sampled signal amplitudes). In the case of the imitation task, the output is trained to reproduce the target signal. For prediction tasks, the output forecasts a time-advanced input signal, whereis the prediction horizon. Designating the target output signal (sequence) asand the output signal (sequence) generated by the PNA reservoir system as, we train the perceptron employing a linear combination of mean square error (MSE) and inverted Pearson correlation coefficient (CC) as the loss function:\n\nHere, we take= 0.15. Our approach allows the system to effectively capture temporal dependencies in the input signal, thereby enhancing its ability to imitate and predict time-series data with high accuracy.\n\nSECTION: III-BSimulation and Implementation\n\nFor the implementation and evaluation of our PNA reservoir system, we model a PNA with sizeand square topology (containing a total ofnanomagnets) using flatspin[25].\nThe size of the PNA represents a hyperparameter for controlling the reservoir quality. It directly influences the expressivity and computational complexity of the reservoir[47]. Increasing the PNA size is theoretically favorable for higher expressivity; however, in practice, realizing high-quality reservoirs (i.e., with high echo state property, memory capacity, and stability) with large PNAs requires exploration and co-optimization of several physical properties of PNAs, including the\nshape, size,\nand interspacing of the individual nanomagnets. We leave such exploration and co-optimization for future work and select the PNA size offor this work because this size has been accepted as one of the standard PNA sizes in prior work[25].\nWe simulate the magnetization dynamics of this PNA from our flatspin-based model[25]to extract the delay and nondelay spatial embeddings. We incorporate this flatspin-based PNA modeling environment with a PyTorch-based framework for input data pre-processing, perceptron training, testing, data analysis, and accuracy evaluation.\nFlatspin[25]does not\nimpose\na hard limit on the PNA size; however, its main drawback is the\nrapidly increasing simulation time as the array size grows.\nIn the future, we plan to explore more efficient simulation platforms such as HotSpice[48,49]in studying the effect of scaling the PNA size to thousands of nanomagnets.\n\nSECTION: IVTime Series Imitation\n\nSECTION: IV-AMethod\n\nIn the Non-linear Autoregressive Moving Average (NARMA) imitation task, we aim to replicate the behavior of NARMAwhere, where higher values of N represent systems with increased temporal complexity and nonlinear dependencies. The sequencesare generated using the nonlinear difference equation from[50]with the same parameters:and. The input signalis randomly generated by uniformly sampling between 0 and 0.5. Sequences generated from NARMA systems exhibit a noisy, cyclical behavior where increasing the order leads to more complex dependencies on past values. We set the history length to, meaning that the mixed delay/nondelay embeddings include the current reservoir state as well as the reservoir states from the previous 50 time steps. The perceptron used for training consists of a single hidden layer with a SeLU activation function.\n\nThe goal is to estimate the target signalwhereis generated according to the NARMA equation, using the mixed delay/nondelay embeddingderived from the PNA reservoir system. The training is performed by minimizing a loss function combining the mean squared error (MSE) and the inverted Pearson correlation coefficient, as described in Eq.1. We use data sequences of length, withof the input-output pairs randomly selected for testing and the remaining for training.\n\nSECTION: IV-BResults and Discussion\n\nTableIshows the MSE, normalized mean square error (NRMSE), and Pearson Correlation Coefficient (CC) betweenandfor the square ICE and Pinwheel ICE configurations. The correlation decreases for both the training and testing data as the NARMA order increases for both topologies, with the square configuration yielding marginally better results. On the testing data, MSE and NRMSE increase as the order rises, reflecting the increasing complexity in the relationship between the target and input signals. However, both metrics show a slight decrease when moving from NARMA7 to NARMA10. This behavior can be attributed to the skewed nature of the loss function, where 85% of the weight is assigned to the inverted Pearson correlation coefficient, and only 15% to the MSE. As a result, the best loss value does not necessarily align with the lowest MSE, which explains the non-intuitive decrease in MSE for higher NARMA orders during training. Overall, the imitation performance remains strong as we progress from NARMA2 to NARMA10, with the CC consistently above 99%.\n\nFigure4shows a visual comparison betweenand, demonstrating that our method successfully reproduces the NARMA signals across all four orders. By explicitly incorporating past reservoir states, the delay embeddings enhance the reservoir node\u2019s memory capacity, allowing for better extraction of dynamical information from the input. This makes it possible to achieve high performance using relatively simple learning architectures such as a multilayer perceptron (MLP).\n\nTableIIpresents a comparison of the NRMSE results from the proposed method with two other reservoir computing systems[51,18]on NARMA prediction tasks. For both the NARMA5 and NARMA10 series, our proposed mixed embeddings method, which uses 51 embeddings in the reservoir state vector, outperforms both the unconnected and connected versions of[51], which use 100-dimensional state vectors. This advantage is because[51]relies on hierarchical echo state network (ESN) reservoirs, which depend solely on internal recurrence for temporal memory, whereas our method explicitly captures both past and current states using mixed delay/nondelay embeddings.\n\nHowever,[18]slightly outperforms our method on NARMA10. This is due to its use of feedback mechanisms introduced by silicon microrings, which enhance the system\u2019s ability to handle long-term memory tasks. On the other hand, their approach involves both input signal masking and encoding, adding complexity to the pre-processing stage.", "text_file": "data\\paper_texts\\2412.04622v1_content.txt"}, {"title": "The Evolution of Binaries Embedded Within Common Envelopes", "authors": ["Alejandra Rosselli-Calderon", "Ricardo Yarza", "Ariadna Murguia-Berthier", "Valeriia Rohoza", "Rosa Wallace Everson", "Andrea Antoni", "Morgan MacLeod", "Enrico Ramirez-Ruiz"], "published_date": "2024-04-11T18:00:01Z", "summary": "Triple stellar systems allow us to study stellar processes that cannot be\nattained in binary stars. The evolutionary phases in which the stellar members\nundergo mass exchanges can alter the hierarchical layout of these systems. Yet,\nthe lack of a self-consistent treatment of common-envelope (CE) in triple\nstar-systems hinders the comprehensive understanding of their long-term fate.\nThis letter examines the conditions predicted around binaries embedded within\nCEs using local 3D hydrodynamical simulations. We explore varying the initial\nbinary separation, the flow Mach number, and the background stellar density\ngradients as informed by a wide array of CE conditions, including those invoked\nto explain the formation of the triple system hosting PSR J0337+1715. We find\nthat the stellar density gradient governs the gaseous drag force, which\ndetermines the final configuration of the embedded binary. We observe a\ncomparable net drag force on the center of mass but an overall reduction in the\naccretion rate of the binary compared to the single object case. We find that\nfor most CE conditions, and in contrast to the uniform background density case,\nthe binary orbital separation increases with time, softening the binary and\npreventing it from subsequently merging. We conclude that binaries spiraling\nwithin CEs become more vulnerable to be disrupted by tidal interactions. This\ncan have profound implications on the final outcomes of triple star-systems.", "arxiv_id": "2404.08037v2", "html_link": "https://arxiv.org/html/2404.08037v2", "search_term": "ti:\"embeddings\"", "html_error": "Error processing HTML: HTTP Error 404: Not Found"}, {"title": "Multi-Scale Node Embeddings for Graph Modeling and Generation", "authors": ["Riccardo Milocco", "Fabian Jansen", "Diego Garlaschelli"], "published_date": "2024-12-05T17:12:45Z", "summary": "Lying at the interface between Network Science and Machine Learning, node\nembedding algorithms take a graph as input and encode its structure onto output\nvectors that represent nodes in an abstract geometric space, enabling various\nvector-based downstream tasks such as network modelling, data compression, link\nprediction, and community detection. Two apparently unrelated limitations\naffect these algorithms. On one hand, it is not clear what the basic operation\ndefining vector spaces, i.e. the vector sum, corresponds to in terms of the\noriginal nodes in the network. On the other hand, while the same input network\ncan be represented at multiple levels of resolution by coarse-graining the\nconstituent nodes into arbitrary block-nodes, the relationship between node\nembeddings obtained at different hierarchical levels is not understood. Here,\nbuilding on recent results in network renormalization theory, we address these\ntwo limitations at once and define a multiscale node embedding method that,\nupon arbitrary coarse-grainings, ensures statistical consistency of the\nembedding vector of a block-node with the sum of the embedding vectors of its\nconstituent nodes. We illustrate the power of this approach on two economic\nnetworks that can be naturally represented at multiple resolution levels:\nnamely, the international trade between (sets of) countries and the\ninput-output flows among (sets of) industries in the Netherlands. We confirm\nthe statistical consistency between networks retrieved from coarse-grained node\nvectors and networks retrieved from sums of fine-grained node vectors, a result\nthat cannot be achieved by alternative methods. Several key network properties,\nincluding a large number of triangles, are successfully replicated already from\nembeddings of very low dimensionality, allowing for the generation of faithful\nreplicas of the original networks at arbitrary resolution levels.", "arxiv_id": "2412.04354v1", "html_link": "https://arxiv.org/html/2412.04354v1", "search_term": "ti:\"embeddings\"", "html_content": "SECTION: Multi-Scale Node Embeddings for Graph Modeling and Generation\n\nLying at the interface between Network Science and Machine Learning, node embedding algorithms take a graph as input and encode its structure onto output vectors that represent nodes in an abstract geometric space,\nenabling various vector-based downstream tasks such as network modelling, visualization, data compression, node classification, link prediction, and community detection.\nTwo apparently unrelated limitations affect these algorithms. On one hand, it is not clear what the basic operation defining vector spaces, i.e. the vector sum, corresponds to in terms of the original nodes in the network. On the other hand, while the same input network can be represented at multiple levels of resolution by coarse-graining the constituent nodes into arbitrary block-nodes, the relationship between node embeddings obtained at different hierarchical levels is not understood. Here, building on recent results in network renormalization theory, we address these two limitations at once and define a multiscale node embedding method that, upon arbitrary coarse-grainings, ensures statistical consistency of the embedding vector of a block-node with the sum of the embedding vectors of its constituent nodes. We illustrate the power of this approach on two economic networks that can be naturally represented at multiple resolution levels: namely, the network of international trade between (sets of) countries and the network of input-output flows among (sets of) industries in the Netherlands. We confirm the statistical consistency between networks retrieved from coarse-grained node vectors and networks retrieved from sums of fine-grained node vectors, a result that cannot be achieved by alternative methods.\nSeveral key network properties, including a large number of triangles, are successfully replicated already from embeddings of very low dimensionality, allowing for the generation of faithful replicas of the original networks at arbitrary resolution levels.\n\nSECTION: IIntroduction\n\nComplex networks capture a variety of socially relevant processes, from economic activities to interactions among brain regions[1,2,3].\nIndeed, every dyadic interaction can be described by properly defining which are the actors (nodes) and the type of connections among them (edges). By defining the nodes as the sectors and the edges as the transactions among them, the Input-Output network (ION)[4]is recovered. Again, by defining the nodes as the states and the edges as the trade among them, the World Trade Web (WTW)[5]is obtained. On the other hand, if nodes were seen as the brain regions connected by electrical stimuli, the brain network is represented. In particular, this flexibility allows for arbitrary definitions of the nodes even when looking at the samephenomenonthat is generating the data (generative process). For example, one could have access to the network involving the most detailed sector classification (National Industry), whereas another one only at the community level (Industry) - see the upper part ofFigure\u00a01. The same reasoning applies naturally in other contexts (neuroscience, social sciences,), as community structures help in simplifying the heterogeneity of the graph[6,7]. Therefore, seen at a coarser resolution, the graph represents the interactions among block-nodes, and it would beuniquelyrecovered after the specification of the partitions. This scheme could be iterated at wish to produce amulti-scaleunfolding of the original graph withnestedpartitions: pictorially, the base of a pyramid is the observed network, whereas the coarser levels are the cross-sections of the pyramid. Lastly, it is worth noticing that the properties of each lower-resolution graph change with levels. For instance, the firm graph is less dense than the sector one since there will be fewer nodes to redistribute links to.\n\nHere, assuming to know the nodes, we aim at modeling their interactions by assigning a probability for every pair of nodes (or edge or link)[8]: the higher the probability, the more likely is that edge to exist in a sampled graph. Ultimately, the measurements over the observed graph should coincide with the average over the sampled networks. This exercise is called, in general,network modelingbut alsonetwork reconstruction[9]or (binary)edge classification[10]in the machine-learning literature.\n\nTo tackle this problem, many machine-learning models usenode embeddings[11,12,2]. As highlighted by the arrows inFigure\u00a01, one mayassigna set of coordinates for every node and, then, extract the probability for the network\u2019s edges. The optimalnode embeddingsare the vector of parameters that optimize a functional involving the observed graph and the model, for e.g. the likelihood. Therefore, these vectors are interpreted as informative features about the nodes that can be deployed to different tasks, such as community detection or node classification[13].\n\nTwo hallmarks of real-world graphs are 1)low density(sparsity) and 2)high triangle density, where there are many triangles incident to low-degree vertices[14]. It was recently proven thatlinearnode embedding methods, such as node2vec[11], are not capable of reproducing the triangle density[14]. To overcome this drawback, LogisticPCA[15]optimized the vectors for anon-linearlogistic function. Furthermore, Chanpuriya et colleagues proposed the \u201csymmetric\u201d LogisticPCA (LPCA)[16]to deal with undirected networks (seesubsection\u00a0III.1).\n\nAs mentioned, a phenomenon can be studied at different scales. By combining nodes into communities we go from a microscopic to a coarse grained scale. Nevertheless, most of the models, e.g. LPCA, regard a network only at one scale, providing the optimalembeddingsfor that level. If nodes were merged into communities, the block-vectorshave to be recomputed. In other words, the two sets of vectors are completely unrelated, as if the models see the two networks as realizations of two distinctprocesses- even though it is not the case. there is no prescription to use the node embeddings from a lower level to create an embedding for a community. For this reason, we would refer to this class of models as \u201csingle-scale\u201d models (SSM).\n\nTo tackle the renormalization problem on networks, several methods have been proposed in the literature, but they all rely on strong assumptions that limit their use cases[17]. More concretely, the most promising one[3]assumes that the nodes are embedded in a hyperbolic plane, all the coarser networks arescale-freeand the block-nodes containmicro-nodes. The latter restriction doesn\u2019t allow to aggregate the nodes with the \u201cmost natural\u201d way induced by the studied phenomenon, e.g. geographical distances for the WTW or the sector (industry) for the ION.\nTo overcome this limitation, themulti-scalemodel[17]was proposed, which isgeneralizableat higher levels and allows for any arbitrary partition of the microscopic nodes. Indeed, the block parameters areuniquelyobtained by summing thevectorsof the nodes belonging to a community (renormalization rule) - this mimics theuniqueidentification of the coarser network starting from the microscopic one.\n\nIn this work, we enhance the scalarmulti-scalemodel withnode embeddings(MSM). Therenormalization rulefor vectors is shown inFigure\u00a01: each communityembeddingis the summation of its lower-level ones. Therefore, the MSM provides an interpretation of thesumof node embeddings, which is rarely addressed in the literature111The successes of Natural Language Processing are also due to the effective representation of a phrase obtained by summing the embedding for each word of it[18]. This is hardly replicated in a graph setting as it is intrinsically more complex than a language.[19]. Due to therenormalization rule, the MSM has the additional benefit of having to be fitted only at the ground level, further implying a lower computational complexity with respect to a single-scale model to be tailored at every level.\n\nThe rest of the paper is organized as follows.\nInsection\u00a0III, we introduce the LogisticPCA and the Multi-Scale Model (MSM) alongside its renormalization rule.\nInsubsection\u00a0IV.1, we describe the ING Input-Output Network and the World Trade Web datasets. Furthermore, we present thecoarse-grainingprocedure to obtain thehigher-scalenetwork.\nInsection\u00a0VI, we show the multi-scale results of the two models and discuss the implications of the theoretical results over either network- and machine-learning scores.\nFinally, in the Appendices, we store all the technical details supporting the results in the main text.\n\nSECTION: IIGraph Renormalization\n\nIn this section, we will introduce the mathematical description of a graph and its coarse-graining procedure. We use the subscriptto identify the base graph whereasfor its aggregated versions. In addition, we refer to a generic quantity at levelas-quantity, e.g. the2-vectorsand2-nodesare, respectively, the node embeddings and block-nodes at level.\n\nConsider a binary undirected graph with\u201cmicroscopic\u201d nodes(labelled as)\nand their connections (called edges or links), i.e.\n\nwhereif there was an edge amongandotherwise.\nThis system could be represented by anadjacency matrixwhich has to be symmetric, i.e., since the graph isundirected. We don\u2019t account for multiple edges, whereas we do for self-loops in the diagonal of, i.e.. Unless explicitely specified, we would use the notationwithas a chosen quantity when the results are valid for any level, e.g. if.\n\nSECTION: II.1Coarse-Graining\n\nIn order find the coarse-grained version of the graph, we definenon-overlapping arbitrarypartitionsof the microscopic nodes into block-nodes\n\nSecondly, we assume that two communities are connected if there was at least one edge between their internal nodes, i.e.\n\nwhere\nthe operation used is thelogical ORover the edges connecting two communities. That is, only the zeros are preserved whereas the possible multiple-edges among the lumped nodes are projected to one. Moreover, note thatis onlysurjectivebut notinjective, since multiple nodesare merged into, and that we allow for self-loops as we didn\u2019t require. At a higher level, the self-loop proxies there isat least one connectioninside the community either because links among nodes or self-loops edges. In this way, we built theadjacency matrixwhich is binary and symmetric as. To ease the recalling, we will refer toas0-graph,as1-graph, themicroscopic nodes asand the block-nodes as.\n\nThe lumping procedure could be iteratedtimes by specifying the partitionofintoblock-nodes\n\nHowever, since the partitionsare not overlapping, one can define their compositionas\n\nThis provides adirectmapping ofinto block-nodes.\nTherefore, theis recovered both applying iteratively theEquation\u00a01ordirectlyviaEquation\u00a02\n\nwhereis the inverse ofEquation\u00a02.\n\nThenestedcan be uniquely parametrized in terms of a dendrogram as shown in[17]. By following an \u201chorizontal\u201d cut of the dendrogram, one obtains the community partitions at the same scale, e.g. for the WTW by merging states nearer than a fixed geographical distance. In contrast, by cutting the dendrogram at different heights, one recovers the \u201cmultiscale\u201d clusters, e.g. a state interacting with a continent.\n\nIn many cases, the problem setting suggests a partitioning of the nodes into communities without an explicit distance matrix, e.g. the classification of the Industries based on the NAICS-codes digits. They characterize each Industry by a number atdigits, e.g. Full-Service Restaurants (722511), but also their clusters by progressively removing the last number up todigits, e.g. Accommodation and Food Services (72). More concretely, the industries sharing the firstdigits are merged into a block-node at thelevel (cfr. Hamming Distance). The dendrogram can be built by setting the height where the blocks split into sub-cluster as.\n\nLastly, if the problem doesn\u2019t come with any \u201cnatural\u201d partition, it would be possible to merge the0-nodesrandomly by creating a dendrogram from a random distance matrix.\n\nSECTION: IIIModels\n\nIn the previous sections, we highlighted that onephenomenoncould be studied at different resolutions.\nIn particular, since every adjacency matrixis binary and undirected, every edgecould be seen as a Bernoulli random variable. Since the theory is valid for every scale, we will remove the dependence on the levelon each quantity, e.g.(seesection\u00a0II). Hence, for every resolution the adjacency entries are\n\nTo model this hierarchical structure, we will use the LogisticPCA[16]and the new MSM. We selected LPCA as the representative for the SSM whereas, at the best of our knowledge, only the MSM accounts for arenormalization ruleover arbitrary partitions of the nodes.\n\nSECTION: III.1Logistic PCA\n\nLPCA222The authors have historically called LPCA thedirectedLPCA, but there was no straightforward acronym for itsundirectedcounterpart[16]. Since in this work, we will use only the symmetric one, we will call it LPCA for easiness.[16]is a probabilistic model which aims to classify each edge as existing (class) or non-existing (class). This is a common \u201cspam / non-spam\u201d problem[13], but applied to objects (the links) that are not carrying explicit features - that is why node embeddings come into play.\n\nThe connection probability reads\n\nwhere the node embeddings\n\nwere introduced by the Chanpuriya et colleagues to grasp the \u201chomophily\u201d and \u201cheterophily\u201d natures of a node respectively (see Supp.Mat.I).\nBy defining the non-negative matrices asand similarly for, the scalar product among them becomesimplying.\nTherefore, the node embeddings are obtained by maximizing the likelihood[20]\n\nsubject to.\n\nSECTION: III.2Multi-Scale Model\n\nThe natural way of modellingis deployingrenormalizationmodels that have to beself-consistentacross scales and allowing for arbitrary partitions. The multi-scale model[17]is the only one that takes into account all these properties (cfr.[3]for homogeneous partitions). In addition, by equipping the (global) multi-scale model with node embedding, the connection probability for the new multi-scale model (MSM) reads\n\nsubject tosince the probability has to be bounded (). In principle, one should impose, but this is equivalent to restricting every component to be non-negative, i.e.(see Supp.Mat.II.6). The normis the one induced by the scalar product. In general,could be interpret as the propensity ofto create a link with another node. Theoretically, this role in encoded by its modulus (significance) and its relative angle (similarity) with the other embeddings. On the other hand,rules the self-loop of node. A more detailed interpretantion of the node embeddings is lacking in the literature, but it is out of the scope for this work.\n\nAs for LPCA,are obtained by maximizing the likelihood\n\nwhereis the vertical stacking of the embedding vectors, i.e.. We optimized only the vectors of thestructural inequivalentnodes (see Supp.Mat.II.4).\nTheloopparametersare fixed to\n\nafter the training of the node embeddings.\nIndeed, for each node, the likelihood of the self-loopsdisplays only one term related to the presence () or absence of the self-loop (). Therefore, by maximizing the likelihood the reached values for the loop parameters will be exactly the ones set above.\n\nThe MSM enforces the equality among therenormalizedand thecoarse-grainedprobabilities333The notation for the different probabilities uses the(hat) to refer to thePwith fitted parameters. On the contrary, the(check) reuses the 0-level parameters (the opposite of), and the(tilde) resembles the \u201cS\u201d of \u201csummed\u201d for the renormalized probabilities.with the same functional form ([17], Supp.Mat.II), namely\n\nThese two conditions are calledself-consistencyandscale-invariance.\nHere, we will use again the subscriptto highlight thatare thefittedvectors at finest level, whereasare the parameters at the levelwhere we set the block-nodes as. Therefore, by assuming that the block-node parameters are thesumof the lower level ones, i.e.\n\nEquation\u00a014are satisfied by means ofEquation\u00a010(seesection\u00a0II).\nIn this way, the MSM at levelis recovered by inserting them in theEquation\u00a010, namely\n\nThanks to thesummation rule, the MSM has a lower computational complexity with respect to the single-scale models (seesubsection\u00a0II.7) as they need to be refitted at every scale.\n\nIn this essay, we took advantage of this property by fitting theembeddingsvector from the observed network to recover all the coarserfor everyby means ofEquation\u00a015. Finally, the probabilityamongwas obtained by inserting theinEquation\u00a010.\n\nSECTION: IVApplications\n\nSECTION: IV.1ING Input-Output Network\n\nING Bank N.V. regularly reports the economic transactions of all ING clients for different years.\nWe focused on the payments for the year 2022 between ING firms by removing theindividualor non-Dutch clients, the flows of money sent/received by a non-ING account and the payments circulating inside a firm, i.e. self-payments. Since ING is the biggest bank in The Netherlands[21], this gave us the possibility of analyzing a major portion of the market.\n\nMore precisely, we chose the year 2022 both to ease the numerical calculations and to avoid skewed distributions by the aftermath of the COVID-19 pandemic. However, the procedure may be easily replicated for other time intervals, e.g. 3 years span, quarterly,\n\nAt the firm-to-firm (f2f) resolution, the dataset is composed bywhich imply a density. Therefore, the network isbig and sparse. From this network, we aggregated the firms by NAICS (North American Industry Classification System) codes and set an edge among two sectors if there wasat least onelink among the firms of each community (seeEquation\u00a03). Then, we filtered out the \u201cPublic Administration\u201d (92), \u201cFinance and Insurance\u201d (52), \u201cManagement of Companies and Enterprises\u201d/\u201cHoldings\u201d (55) sectors to retain aproductionION. Roughly, the reasons underneath the payments from/to the sectors52-55-92are not directly connected with a product/service. In particular, the \u201cPublic Administration\u201d fluxes includes taxes and fees; the \u201cFinance and Insurance\u201d, money management, e.g. loans, that are not part of the production chain of any good; and the \u201cManagement of Companies and Enterprises\u201d/\u201cHoldings\u201d are a collection of business entities, controlling stocks in other companies. Lastly, we mapped, for simplicity, the 6-digits NAICS codes to integers, i.e..\n\nThis is a first application of the MSM on themultiscale structurebuilt from the ION. Therefore, we studied only theeconomic relationshipsamong the sectors discarding the directionality and the amount of money of the link. In other words, each edge isreciprocated and binary. For example444To be precise, the notation for the nodesrefers to the observed microscopic nodes, namely, ifwas the total amount of money sent fromat level, wereciprocatedthe weight by setting[5]andifand imposedto create abinaryedge. In this way, a bidirectional link is created, i.e.every time there wasat leastone directed flow among two sectors.\nThe empirical ION is composed bysectors andlinks, which imply a densityby 4 order of magnitudes.\n\nIn the previous section, we obtained thebinary undirectedadjacency matrixrepresenting the interactions among the 6-digits sectors (0-nodes). Here, we describe the coarse-graining procedure producing themulti-scaleunfolding of.\n\nAt first, the0-nodeswith the firstdigits, with, are lumped together in the-nodes, e.g. the sectorswould be merged in the samecommunity starting from555In general, the model accepts every othernon-overlappingpartition of the 0-nodes[17], e.g. Louvain[7,22]..\n\nSecondly, we set an edge among twoif there wasat least onelink among their higher resolution members. More formally, thecoarse-grainedis calculated by applying theEquation\u00a04. We chose thisone-stepprocedure to easily generate every level without passing into the intermediate scales. However, the MSM accept everyarbitrary-stepsscheme, e.g. ifthenis also possible. Note that the coarse-grained graphs becomefully-connectedfrom. Therefore, thestatisticalmodelling would be possible up to.\n\nSECTION: IV.2World Trade Web\n\nAs a second application, we considered the World Trade Web (WTW) from the Gleditsch dataset[23], which reports the international trade flows (imports and exports) among all the world countries. We selected the year 2000 (the most recent one) and removed the states that were not reported in the BACI-CEPII GeoDist[24]as we would use the geographical distances to coarse-grain the WTW.\nThis results in0-nodes. Although we analyzed the year 2000 the methodology can be applied also to other years.\n\nThe dataset provides two columns which display, for every pair of nodes: theexportandimportamongin USA dollars. However, by flipping, theexportandimportdon\u2019t coincide with the previous value. Therefore, we used only the redefined[25]as the amount of trade from.\n\nAs done for the ION, wesymmetrizedthe connections by mapping, i.e. the average flow between the two directions. By renamingas, it follows that the weights are symmetric, i.e.. Note that the WTW has a highreciprocityof links, i.e. the connections in the WTW are almost always corresponded[26]. Therefore, its undirected approximation is a legitimate starting point to study the system.\nMoreover, webinarizedthe import-export matrix to get an adjacency matrix among the states. Formally, we projected all the positive values of the weighted matrixto one, i.e.where theis the Heaviside function.\n\nTo coarse-grain the empirical WTW, assumed at level, we used the geographical distances[24]to iteratively merge \u201cclose\u201d nodes into block-nodes as in[17]. Technically, this is done by means of asingle-linkage agglomerative clusteringmodel which returns a dendrogram where theleavesare the 0-nodes, thebranching pointsare the block-countries, and theheightof each branching point represents the distance, obtained via the single-linkage, between two leaves following the corresponding branches. Therefore, to calculate the partitionswe cut the dendrogram athierarchical heights, such that the number of block-countries, namely, are. Note that the coarse-grained graphs become fully-connected from. Therefore, thestatisticalmodelling would be possible up to.\n\nSECTION: VMethodology\n\nIn this section, we will define the scores used to evaluate the models over the ION and the WTW.\n\nSECTION: V.1Embedding Dimension\n\nThe choice of thebestembedding dimension is still an active Research topic[27,28,29]. Here, we relied on the \u201cMinimum Description Length\u201d principle approximated by the Bayesian Information Criteria (BIC)[28]. In the Supp.Mat.Table\u00a01, we reported the BIC scores over the dimensionsand also, for completeness, the AIC ones[27]. Thebestdimension, according to BIC, depends on the resolution levels: for the ION,whereas. Since the WTW is a less complex network, the BIC selects everytime the lowest dimension, i.e.(see Supp. Mat.). This result could be expected since having more nodes, as for the lower levels, it implies more heterogeneity and, therefore, a bigger embedding dimension to grasp the ION. However, since this is the first time the MSM has been proposed, we will show the scores for all thedimensions.\n\nSECTION: V.2Renormalization of LPCA\n\nHaving fitted the(at level), in order to model, one may either proceed with theEquation\u00a08or by refitting LPCA at that level. However, both solutions have similar drawbacks as there is no renormalization rule enforcing thescale-invariance(cfr.Equation\u00a015). In the following, we will analyze them in detail.\n\nFirstly, one may calculatefrom the RHS of theEquation\u00a013. Nevertheless, as shown in Supp.Mat.I.2, it wouldn\u2019t be possible to rearrange theto recover alogisticfunction with renormalized parameters. Hence, the resulting method would no longer belong to the logistic parametric family. In other words, LPCA is notself-consistentunder coarse-graining.\n\nIn addition, note that the calculation ofrequires a greater complexity than the MSM Supp.Mat.subsection\u00a0II.7.\n\nSecondly, by refitting LPCA at level, one would find another set of vectors that, in general, are unrelated with the(-1)-vectors. To the point of view of LPCA, the different levelsare realizations of differentgenerative process.\n\nTo enforce relatedness of thevectors, we forced the lower-resolution parametersto be a function of the higher-resolution ones. Since there is no natural way of combining them, we used theEquation\u00a015, namely\n\nwhereare the block-nodes at level.\nThen, to imposed self-consistency, the parameters are inserted in LPCA activation function, i.e.\n\nAt this point, the machinery providesself-consistency and relatednessof LPCA across scales as the MSM does naturally. Hence, we can fairly compare their expected values with respect to.\n\nSECTION: V.3Comparison Among Probabilities\n\nFor every level, both LPCA and MSM give rise to 3 probability functions666For the MSM, the symbols on top ofrefers to different values of theinnerparameters since its functional form does not vary by construction.: thefitted,summedand thecoarse-grained. Hence, we produced a cross comparison among them to understand their hallmarks.\n\nFirstly, we will compare for each model how therelates with, i.e.Equation\u00a018againstEquation\u00a08andEquation\u00a016againstEquation\u00a014.\n\nSecondly,against, namelyEquation\u00a018againstandEquation\u00a016against.\n\nThe insets, displayed in some figure, are reporting thehistogram of the density of points inside each bin777We set the number of bins equal toboth along the x- and y- axis., i.e.\n\nwhere(\u201cxy\u201d refers to its center of mass) is the total number of points andthe number of pairs.\nFinally, we colored the bins according to(creating a heatmap) - the bigger the value, the lighter the color.\n\nThe missing evaluation ofagainstis due to the fact thecoarse-grainedprobability spoils the LPCA functional form (seesubsection\u00a0V.2) whereasfor the MSM. Therefore, we used it only to check numerically theEquation\u00a013in the first comparison.\n\nSECTION: V.4Scores\n\nSECTION: V.5Network Measurements\n\nThe fundamental topological properties of a network are thedegree, theaverage nearest neighbor degree(ANND) and thebinary clustering coefficient(CC)[8].\nFormally, each of such measurements is a functionof anadjacency matrix representing a graph.\n\nHere, we compute them both in the observed networkand as expected by the model. More precisely, thedegreecounts the number of edges that are incident to a node, i.e.\n\nand its expected value is given by\n\nwheredenotes the expected value over the ensemble of graphs sampled from.\nMovingtwo-hopsaway from, theANNDreports the average degree of the neighbors of the node, i.e.\n\nwhereas its expected value reads\n\nwhere in the second passage we took advantage on the first order approximation(delta approximation)[30].\nLastly, theCCis defined as the ratio among the number of triangles of nodeand its number of wedges, namely\n\nwhereas the expected one is\n\nFor more complicated measurements, e.g. the variance of theANND, thedelta approximationwon\u2019t be valid and one has to estimate them as the average over asufficiently largeensemblewhereis the number of graphs.\nIn particular, having optimized the parameters of the model, we can generate unbiased realizationsby sampling eachindependently with probability[30,17].\n\nIn the limit of, thesampledaverage of any measuremeets itsanalyticalestimations[17], i.e.\n\nwhereis a matrix drawn from the set of the undirected binary graphsofnodes.\n\nLastly, to estimate the uncertainty of the model over the sampled realizations, we calculated the-th (-th) percentile ofcalculated withlinear approximation(see[31]). These values are seen as upper and lower bounds of thedispersion intervals[26]which containsof the measurementsover the sampled graphs. Note that in the whole procedure we arbitrarily fix the percentage of \u201cdispersion\u201d to[26], but other values are also allowed.\n\nSECTION: V.6Reconstruction Accuracy\n\nIn order to have a cross-comparison among all the levels and models, we exploited thereconstruction accuracy[26]. This measure is defined as the fraction of times an observed statisticsfalls within thedispersion interval(seesubsection\u00a0V.5). More formally, the reconstruction accuracy at levelfor the statisticsis defined as\n\nwhereis the indicator function888Further refinements are possible, but we stick with this definition for the sake of simplicity..\nRoughly, it counts the frequency at which the sampled ensemble includes the observed statistics. If all the observed statistics, e.g. degrees, were included in the interval, the accuracy would be, whereas the accuracy would beif none of them were included.\n\nSECTION: V.7Rescaled ROC and PR Curves\n\nThe LPCA and MSM could be seen asbinary classifiersthat predict the presence of a link between two nodes. For this reason, we evaluated them also for the common metrics used in theMachine Learningfield: theexpectedconfusion matrix, the Receiver Operating Characteristic (ROC) and the Precision-Recall (PR) curves[32,33].\nFirstly, theexpectedconfusion matrix is amatrix that reports the expected value of True Positives (TP), i.e., False Positives (FP), i.e., True Negatives (TN), i.e., and False Negatives (FN), i.e.[34]. By combining these scores, one recovers the True Positive Rate (TPR), the False Positive Rate (FPR) and the Positive Predictive Value (PPV)[33], namely\n\nwhere,.\n It\u2019s possible to \u201cactivate\u201d these scores by mapping the entriestoand otherwise to. For convenience, we will call them.\nThe ROC and PR curves are obtained by spanning[32]and plotting, respectively, the TPR against the FPR, and the TPR against the PPV (seeFigure\u00a04). Interestingly, if, then, whereas if,.\n\nAs a reference model, it is commonly employed a random classifier predicting the majority class, i.e.. In the ROC plane, this naive model spans the identity line, and an \u201cL\u201d shape in the PR plane which depends on the link density999As the density increases the corner will be right-shifted and vertical line bent forming a \u201c\u201d shape. Nonetheless, since we are interested in ranking thesummedmodels, we rescaled the Area Under the Curve (AUC) to highlight the advantage with respect to thenaiveclassifier. Specifically, we defined\n\nTherefore, the perfect classifier would still havebut the random one. ThenewAUCs can be negative, as a signal of a worse performance than the random classifier.\n\nSECTION: V.8Triangle Density\n\nInspired by[15], we computed theexpectednumber of triangles for every model at disposal101010In this essay, our objective was to modelprobabilisticallythe observed network rather than describing itexactly, namely the limit where..\nSpecifically, the expected density of triangles at a certain levelis defined as\n\nwhereis the number of observed triangles (seeEquation\u00a026) calculated on the subgraphcomposed by the nodeswith degree lower (or equal) than a threshold[14,15].\nIts expected value reads\n\nwhereand the probabilitiesrefers to the summed model.\n\nSECTION: VIResults and Discussions\n\nHere, we present the results of the LPCA and MSM models applied to the ION and WTW datasets.\n\nSECTION: VI.1Scale-Invariance Evidence and Multi-Scale Clustering Coefficient\n\nInFigure\u00a02a, it is represented the behavior of thesummedprobability against thecoarse-grained(seeEquation\u00a013) as described insubsection\u00a0V.3.\n\nWe chose the lowest embedding dimensionsandto highlight the differences among the models even in the simplest case. Specifically, the identity line depicts the scale-invariant nature (Equation\u00a013) which is met only by MSM. On the other hand, LPCA systematically underestimates the coarse-grained version, since the0-vectors, maximizing the likelihood at, get lower values than the ones needed to enforce scale-invariance.\n\nBy taking asufficiently largeembedding dimensionsand, we reported inFigure\u00a02ba cross-comparison between the LPCA-(8,8) and MSM-16 focusing on the multi-scale CC. At a plain eye inspection, LPCA-(8,8) is outperforming MSM-16 at level 0 (where we fitted the models); whereas it is the other way around at level. More quantitatively at level, thewhile. Additionally, we exploited the relative Froebenius error among thefittedprobabilityand the adjacency matrix defined as[15]\n\nto obtain that. The comparison is further enriched by the insets displaying the summedagainst the fittedateither for LPCA and MSM - the identity line represents the perfect match. Technically, we created ahistogram as described insubsection\u00a0V.3.\n\nHere, we verifynumericallythat LPCA is not scale-invariant and the agreement of the expected CCs with respect to the observed ones. Comparable results were also obtained for the other levels and measures, namely the DEG and ANND - for ION at(seeFigure\u00a03), but refer to the Supplementary Material for the WTW and the other levels. Hence, as expected from the theory, the MSM provides the best modelling of the multi-scale ION.\n\nSECTION: VI.2Expected Values of the Network Measurements\n\nInFigure\u00a03, we display the key network properties at level(seesubsection\u00a0V.5) calculated for the empirical network, as expected by the summed modeland by the refitted one(seesection\u00a0IIIandV.2). The dimensionwe used arefor the LPCA(LPCA-(8,8))andfor the MSM(MSM-16).\n\nWe computed the observed properties as insubsection\u00a0V.5. Then, focusing on the fitted model, we sampledrealizations in order tocalculateas reported inEquation\u00a031. In addition, we obtained the dispersion interval for each measurement with- the bars attached to every sampled average in the plot. Lastly, we re-applied the same procedure to the summed model.\n\nIn the upper panel, we report the observed measurements (x-axis) and the expected ones (y-axis) both for the summed model (orange points) and the fitted one (blue points). Needless to say, the identity line represents the perfect match of the predicted quantities with the observed properties. The single inset depicts the scattered plots ofagainstas described insubsection\u00a0VI.1.\n\nAs expected fromFigure\u00a02a, LPCA does not recover the measurements on average whereas the MSM can approximate them including most of the observed points in the dispersion intervals. In addition, by looking at the inset in the upper-left plot,approximatesonly for the MSM. This result, differently fromEquation\u00a013, was not enforced theoretically, and it explains the good agreement of the MSM measurements through the coarse-grained levels.\n\nIn the lower panel, it is displayed the behavior of the network measurements as the degrees increase. From the left-most plot, one finds the complementary cumulative distribution function (CCDF) of the degrees, the ANND and the CC. Since the real CCDF is decreasing, the observed network has a higher presence of lower-degree nodes than the hubs whereas it is notscale-freeas its shape is not a straight line in log-log scale. Similarly, the ION (WTW) isdisassortativeandhierarchicalsince, respectively, the high-degree nodes are connected to low-degree ones, and they trade with loosely-interacting partners[35].\n\nThese plots underscore that the MSM-16, not only capture the CC (seeFigure\u00a02b), but also thelower-hopsmeasurements and behaviors. As expected, the LPCA-(8,8) provides a good fit only at the fitted scale as depicted by the blue points forand.\n\nSECTION: VI.3Reconstruction Accuracy and ROC-PR Curves\n\nInFigure\u00a05a, one can find the reconstruction accuracy (seeEquation\u00a033) for the DEG, ANND, CC across the available levels for the ING network, i.e.[26]. In particular, we reported thesummedLPCA with dimensionsand, and thesummedMSM with.\nSince we have fitted every model at, only most of the trends are peaked at the resolution scale, for e.g. the MSM-1 has a higher CC at levelthan at. In addition, out of the fitted level, the LPCA fails in generating an ensemble of networks that are consistent with the observed one. Contrarily, the MSM ensemble includes the measures at every level apart from resolutionwhere the topology change could not be grasped only by the0-parametersand therenormalizationrule. Especially for, the MSM overestimates the DEG, ANND, CC since the summation of the 0-parameters lead to bigger values than the 2-parameters fitted at level. As said, this is not per se a problem of the MSM since thescale-invarianceenforces theinnerconsistency of the MSM (seeEquation\u00a013) rather than recovering the fitted parameters at every level. Also, the choice of thepathologicalpartition leads to worse results than expected as discussed insubsubsection\u00a0VI.3.1.\nIn addition, by increasing the number of parameters, i.e., the reconstruction accuracy improves, but it leads to overfitting as highlighted by the higher BIC scores reported in Supp.Mat.Table\u00a01. Another way to tackle this deviation, could be to introduce a dyadic relationshipamong the nodes as done in[17]. However, this is out of the scope of this work.\n\nInFigure\u00a05b, one may arrive to similar conclusions but looking at different scores: the Area-Under the Curve (AUC) for the Receiver Operating Characteristic (ROC) and the Precision-Recall (PR) curves[33]. The illustrations underline thephase separationdue to their functional forms. In particular, even if LPCA-(8,8) outperforms all the other candidates at, its performances decreases with the scales; so, does LPCA-(1,1). On the contrary, the MSM displays growing scores since, due to the density increase, the TP are likely to grow in number. Hence, the ROC and PR curves are pushed towards theandupper boundaries.\n\nIn conclusion, the MSM can consistently model all the coarse-grained levels, whereas the LPCA outperforms the MSM only at the fitting scale. This implies that, by fine tuning the functional form, one can prioritize either thesingle-scale \u201coverfitting\u201dwith LPCA or thegeneralizationcapability with the MSM.\n\nThe agreement of the summed model and the observed (coarser) graph depends on the chosen partitions. In other words, by recalling the notation used in as inEquation\u00a01andEquation\u00a013, the relationship\n\ndepends on the partition sinceare functions of. Therefore, one can engineer a partitionthat spoils the latter approximation by requiring thataddresses the zeros of(cfr.Equation\u00a044) through\n\nAs said,would provide a partition leading to worse results than the ones observed for ION and WTW.\n\nSECTION: VI.4Expected Number of Triangles\n\nThe authors of[15]show that it is possible, by introducing LPCA, to reproduce the triangle density (TriDens) for an embedding dimensionlowerthat the number of nodes (cfr.[14]). Here, we show theexpectedTriDens as described byEquation\u00a041,Equation\u00a042.\n\nInFigure\u00a06, the filled azure dots depict the observed TriDens whereas the other markers identify each model: azure circles (LPCA-(1,1)), orange triangles (LPCA-(8,8)), green circles (MSM-1), red circles (MSM-2), violet crosses (MSM-8), brown triangles (MSM-16). InFigure\u00a06a, it is reported the levelwhere it is clear that even the lowest embedding dimension () well approximates the TriDens. By construction, asincreases, the difference among the TriDens vanishes until it coincides atsince the subgraph of hubs is likely not to contain triangles in adisassortativenetwork (seeFigure\u00a03). This result is not in contrast with the previous works[14,15]since we are computing theexpectedTriDens rather than theexactone. By coarse-graining the networkFigure\u00a06bat, the MSM models are in a good agreement with thecoarserTriDens. In particular, as seen inFigure\u00a03, asincreases, also the estimates improve. On the contrary, LPCA are underestimating the considered score being biased because, as said previously, it is not generalizable to lower resolutions.\n\nSECTION: VIIConclusions\n\nThe power of graphs relies on their ability to accommodate different kind of interactions by a suitable definition of nodes and edges.\nBy arbitrarily identifying a node as the aggregation of microscopic entities the resulting network is acoarse-grainedversion of the original one. By repeating this procedure several times, one obtains amulti-scaleunfolding of the observed graph. We have applied this procedure on the ION and the WTW (seesubsection\u00a0IV.1) showing how onegenerative processcould be represented at several resolutions.\n\nA relevant part of thenode-embeddingliterature[36]aims at find the optimal representation of the nodes at a single scale, neglecting these numerous ways of tracking down the generative process. To stress the point, weappliedeither asingle-scalemethod from the machine-learning field, i.e. LPCA[16], and thenewmulti-scale models enriched withnode embeddings(MSM)section\u00a0III. The key assumption is that, similarly to the generative process, the models must bescale-invariantthrough the scales. Since the MSM is built with this premise, it would naturally beself-consistentwhereas we forced LPCA to be self-consistent by applying the same renormalization rule of the MSM (Equation\u00a017). In particular, the renormalization rule (Equation\u00a015) states that the community vectors are thesumof their inner-node vectors. This allows for a principled interpretation of the sum of the node embeddings which is not possible with LPCA and, in general, with the single-scale models.\n\nAtfittingscale, LPCA outperforms MSM in every metric we have considered insection\u00a0V. At higher scales, the ranking is reversed (seeFigure\u00a05) as the predictions of LPCA highly deviates from the observed structure - this defined this change assingle-scale overfitting. More specifically, inFigure\u00a02a, we visualized at which extent imposing the self-consistency makes LPCA diverge from its coarse-graining probability - the MSM satisfies this identically.\nSecondly, we showed the agreement (disagreement) of the expected network measurement by using the summed MSM (LPCA). This implies that LPCA has to be fitted at every level as if every scale would be generated by a different generative process. Therefore, also the node embeddings would not be related to each other. On the contrary, the MSM can be fitted at the highest resolution, providing the fundamental vectors that can be summed to obtain the higher-level embeddings. This is a clear advantage of the MSM over the LPCA - even computationally as reported in the Supp.Mat.subsection\u00a0II.7.\n\nAs cross-comparison between the models, we visualized the Reconstruction Accuracy (Figure\u00a05a), the AUCs (Figure\u00a05b) and the expected number of triangles (Figure\u00a06). Specifically, the Reconstruction Accuracy highlights that the ensembles, generated by the two models, includes the observed quantities at level. However, by changing scale, LPCA generates graphs that are not related to the empirical one. Interestingly, also the MSM struggles to recover the observed properties at levelbecause the graph topology changes more than what expected by the model. In particular, without thedyadicparameters, the MSM overestimates the 0-parameters which lead to a higher density of edges at level(see the z-score in the legend ofFigure\u00a03b). As a consequence, all the measurements are overestimated and the dispersion interval can\u2019t include the observed values. InFigure\u00a05b, it has been depicted the clear sign of single-scale overfitting: the predictability of LPCA is restricted tosince the AUCs constantly decrease when it is applied to higher levels. Lastly, the analysis of the expected triangular density (Figure\u00a06) shows that it is possible to generate networks with comparable values as the observed triangular density. As said before, this possibility is spoiled under aggregation for the LPCA, but it is preserved for the MSM.\n\nIn conclusion, LPCA, developed using themaximum Shannon entropyprinciple, is the best model at its fitted level but struggles to accurately represent the network\u2019s structure at other scales. In contrast, the multi-scale model (MSM), based on thescale-invarianceprinciple, consistently captures coarser resolutions, such as the ION and WTW. The decline in LPCA\u2019s predictive performance at higher scales suggests that MSM provides a better balance for modeling the multi-scale structures. In addition, the MSM offers a meaningful interpretation of node embedding sums, as they naturally generate lower-resolution levels, making it a more versatile and comprehensive approach for analyzing networks.\n\nSECTION: VII.1Future Perspectives\n\nAlthough this work used undirected and binary models, it was a good starting point to extend the analysis to directed methods[37]and, hence, the interpretation of the resultingdirectednode embeddings within the economic theory. For the weighted part, there is still theoretical work to do to understand how the weights could be included in the MSM framework.\n\nSECTION: VIIIACKNOWLEDGMENTS\n\nWe thank ING Bank N.V. for their support and active collaboration. A special thanks to the whole DataScience team at ING Bank for their advice that helped shape this research.\n\nSUPPLEMENTARY MATERIALaccompanying the paper\u201cMulti-Scale Node Embeddings for Graph Modeling and Generation\u201dby R. Milocco, F. Jansen and D. Garlaschelli\n\nSECTION: INon-Negative Logistic PCA\n\nThe non-negative LPCA (LPCA)[15]aims to classify every edgeas existing (0) or non-existing (1). By treating every entryof the adjacency matrixAas a Bernoulli random variable (Equation\u00a05), this comes down to the factorization\n\nwhereis the logistic function depending on the scalar product of two embedding per nodes assumed to encode the role of each node in the network, namely.\nThe compact formulation on the LHS was written in terms of the matricesthat are created by stacking horizontally the vectorsrespectively.\n\nSimilarly, for the MSM (section\u00a0III), the LPCA vectors are fitted by means by maximizing the log-likelihood estimation\u201d[38]. In particular, the log-likelihood and its gradient read\n\nwhere the last passage is taken fromBCE-TensorFlowfor numerical stability. Note that the stationarity conditions\n\ncan\u2019t be split in a part dependent only by the adjacency matrix as for the Exponential Random Graph models[30]. Hence, LPCA hasn\u2019t asufficientstatistics and one has to use the whole adjacency matrix.\nThe motivation for having two vectors per node boils down to the framework studied in[15]. Specifically, the authors analyzed a dating graph reporting the messages exchanged among the male-female users living in two different cities.\nHence, they have introduced two vectors per node to grasp the heterophily (malefemale) and homophily (same city) \u201crole\u201d of each user. In the ION setting, we leave out this interpretation just considering them as parameters to be optimized.\n\nSECTION: I.1Renormalizing the LPCA\n\nThe LPCA does not have a recipe to renormalize the parameters and produce an \u201cup-scaled\u201d version of it. Nonetheless, it is possible to model the multi-level structureeither byfittingat every level or bycoarse-graining() as given by\n\nwhereare the block-nodes at level. However,spoils theself-consistency111111As said,provides multiple representation of the samegenerative process. Hence, the model should mimic this feature withself-consistency.of LPCA; a property that allows one functional form for all the levels. In particular,Equation\u00a08is a product of logistic functions which is not re-writtable as a logistic function.\n\nSince we want to test the capability of the model to remain self-consistent, we will renormalize the parameters by summing the microscopic parametersEquation\u00a017as done for the MSMEquation\u00a015and check for its agreement with the empirical quantities.\n\nAs pointed out insubsection\u00a0II.7, in order to compute, the computational complexity is higher than thesummed. Therefore, for the fairest comparison among the models, we will use.\n\nSECTION: I.2Inconsistency of the LPCA: a trivial example\n\nBy referring toFigure\u00a01, the microscopic network of 3 nodesmerges into the communitycontaining respectively the nodesand the node, namely.\n\nABlevel 1\n\nIn addition, to stress the point we will switch-off the dependence on. Therefore, the connection probability of the communitiesfrom theEquation\u00a018as\n\nwhich is different from the coarse-grained (Equation\u00a08)\n\nFrom the previous results, it is clear that fromone can\u2019t recover theby definingsimilarly toEquation\u00a015. Therefore, the model is not renormalizable.\n\nSECTION: IIDerivation of the multi-scale probability\n\nHere, we derive the multi-scale model formulation enhanced withvectors(MSM). As in the main text, we will consider a coarse-graining procedure fromtoeven though the treatment will hold for every pairwith. See[17], for further details even for the following passages.\n\nBefore introducing the model, it is worth to recall the problem settings to generate the observed multi-scale structure.\nConcretely, let us consider thebinaryundirected adjacency matrixat leveldescribing the microscopic interactions among the 0-nodes. Subsequently, ahierarchical and non-overlappingpartition of the microscopic nodesprescribing the community (block-nodes) membership of the lower-level nodes. Concretely, the block-nodeshosting all thenodes is obtained by\n\nwhere we have defined. Lastly, aruleto assign a link among blocks, namely\n\nwhereand. Therefore, by iterating the procedure it is possible to create the nested set of networks describing theoriginalphenomenon at different resolutions.\n\nIn order to model this architecture, one needs several assumptions. Thefirst onerequires that the MSM must describe the microscopic matrix. Similarly tosection\u00a0I,subject toandwhereis the ensemble of all the binary symmetric graphs withnodes.\n\nIn line with[17], we further assume that\n\nis given by a product (to-be-defined) between thedimensional vectorswith additional node-wise parametersonly active in the self-loop part ().\nIn particular,encodes the capability of nodeto connect to the other nodes; whereasits propensity for aself-interaction. Since the principles leading to an edge are different to the self-loop ones, we introduced two independent parameters.\nFurthermore, the matrix of parameterscould include also adyadicrelationship among the nodes. The higher order terms have been discarded since they will not be fully compatible with the hypothesis of \u201cindependent edges\u201d. For further details we refer to[17].\n\nTo highlight the parameter dependence of the model, in the following, we will use the notationwhere. Technically,and.\n\nFittedat the ground level, the ensemble generated by the MSM contains multiple configurations that, after coarse-graining, lead to the observed macroscopic[17], i.e.. In turns, this induces the probability of observingas\n\nTo enforce thescale-invarianceproperty, we require that thefunctional formof the MSM has to be independent from the chosen scale, i.e.. Furthermore, that the model can generate the possible-graphs in two equivalent wayshierarchicallyordirectly. The former one refers toEquation\u00a011, and it prescribes to generate the-graph ensemble with probabilityand, then, coarse-graining themtimes via the partitions. The other way around, the second one requires torenormalizethe parametersand, then, directly modelvia.\nImposing both requirements\n\nwhere we have defined the LHS and RHS of the first equation asrespectively.\nIn other words, the form of thewill depend on the scaleonly through the renormalized parameters. Moreover, by assuming that the links are statistically independent, the previous equation yields\n\nwhereand\n\ndepend, respectively, on the renormalized and the fitted parameters at level. Note that we didn\u2019t usebecause the functional form would bescale-invariantand the only dependence on the scale is through the parameters.\nThe interpretation is similar to theEquation\u00a09: the probabilitythat there is one among the block-nodesis given by the probability that there isat least one linkamong the microscopic nodes.\nSpecifically, it requires that the model remainsself-similarwhereas the parameters renormalize under renormalization (scale-variant).\n\nThe RHS returns the coarse-grained probability for every model, e.g. the MSM and LPCA. The crucial difference is that for the SSM(cfr.Equation\u00a014) because they are scale-invariant. For a concrete example, we refer to the sections where the models have been introduced.\n\nBy taking the logarithm of both sides of theEquation\u00a013, the only functional form compatible with that constraint\n\nwhereis a positive function such that. Proceeding as in the main reference, one may assume thatfor every level. In addition, the vectorial productshould be bilinear in order to allow forEquation\u00a013, namely. Still, since the connection probability is symmetric, the matrixMcould be set as the identity, i.e.(seesubsection\u00a0II.1). By taking the exponential of\n\none ends up with the (off-diagonal)scale-invariantprobabilityEquation\u00a016. For the self-loops part, the steps are similar to the ones described in the main reference.\n\nSECTION: II.1Bilinearity Requirement\n\nABlevel 1\n\nIn this subsection, we describe why theproduct must be bilinear and whyMcan be taken as the identity matrix given that the probability is symmetric. To start with, inFigure\u00a02, there have been represented 4 nodesat levelmerging, at level, intoand.\nHence, fromEquation\u00a013, the non-existence of a link (gray dashed lines) among the communitiesreads\n\nand the rightmost side enforces that thehas to be a bilinear function.\n\nAs said in the main text, the connection probability is symmetric, namely. In turns, this leads to. In addition, since,Mis also positive semidefined as. Therefore,has positive eigenvalues such that\n\nwhere(cfr. Cholesky decomposition). Briefly, choosing an arbitrary (symmetric)matrix will lead towithare optimized with. Hence, for simplicity, we rely on.\n\nLastly, fixing, allows recovering the product among scalarsfor. This was a successful way of modelling real world networks, e.g.[17,30].\n\nSECTION: II.2Loop parameters estimate\n\nThe log-likelihood regarding theself-loopsreads\n\nwhich depends either on vectorsbut also on. Since the probability is bounded,. Moreover, as for every nodethere would be only one of the two terms in the above likelihood,is going to take the values reported inEquation\u00a013. Hence, after having obtained the, we fixedto exactly reproduce the self-loops at levelas prescribed byEquation\u00a013.\n\nSECTION: II.3Gradient of the log-likelihood\n\nTo efficiently calculate the maximum of the MSM likelihood, one needs the analytical expression of the gradient. In particular, by differentiating with respect to the-th component of the-th embedding vector, one gets\n\nwhere we have used\n\nThus, by renaming the indexes,\n\nLastly, by leveraging on the gradient and the likelihood, we performed the optimization by means of three optimizers: Adam implemented from[39]; while Truncated-Conjugate Gradient and L-BFGS-B by means of the SciPy library[40].\n\nSECTION: II.4Structural Equivalence is not Statistical Equivalence for multidimensional-node embeddings\n\nCalculating the gradientEquation\u00a025for nodesat the maximum of the likelihood, one gets\n\nBy further assuminghave same neighbors (Structural Equivalence- StructE), i.e., it is possible to rearrange the above equations into\n\nwhere we have defined\n\nFor,\n\nis a monotonic function of, thus there exists an inverse functionsuch that. In other words, two nodesthat have the same neighbors, they arestatistically equivalent(StatE). The inverse implication is also true, i.e.by following the same steps backwards.\nIn conclusion, for, Structural Equivalence is equivalent to Statistical Equivalence:.\n\nThis result does not hold forsinceis not monotonic inas it depends on the scalar product. However, ifhave the same neighbors, they have the same role in the network, and so the model should have the same parameters for the two nodes. Formally,\n\nBy recalling the notation used before, we are imposing that StructE implies StatE.\n\nTo find the node embeddings in thereduceproblem, one starts by defining the set of nodes which share the same neighbors aswhereare the nodes with the same neighborhood with respect to the node. Referring toFigure\u00a02and considering the gray dashed lines as existing connection,and.\nSecondly, by defining the nodewith the lowest index among, as the representative of that StructE class, one obtains a set of pivotal nodes related to StructE. At this point, the number of parameters decreased fromtowhereis the number of elements the sethas.\nIn order to recover, the value of the representative parameteris copied to all the members of the class, formally\n\nwhereare the representative nodes. In this way, it is possible to use the likelihoodEquation\u00a011with fewer parameters than in the original formulation.\n\nSince the selected optimizers ([40]) are not \u201cstochastic\u201d, one may obtain the StatE between the embeddings by starting from the same initial conditions for StructE nodes. Indeed, the parameter updates will be the same as they depend on the neighborhood as shown byEquation\u00a04,25.\n\nIn conclusion, for, the StructE - StatE relied on the monotonic function; whereas it is not the case for. Therefore, to enforce StatE, one may reduce the problem and optimize only the representative of the StructE classes.\n\nSECTION: II.5Removal of deterministic nodes\n\nNetwork modelling assumes that the observedis a realization of a random process. However, it may happen that some nodes weredeterministic, i.e. fully-connected (FC) or disconnected (D) to the other nodes. Therefore, its behavior would betriviallyrecovered by setting the hidden variable of the FC nodes to infinity or the D nodes to zeros for theEquation\u00a010. That is, there is no need of fitting aprobabilisticmodel to grasp its role in the graph. For example, assumeand that the nodesis fully-connected whereasis disconnected. After the optimization, they will end up havingor. In turn, sinceimplying that they will not contribute on the ensemble fluctuation. Hence, one may hard code their variable, as seen before, to account for their roles in the graph. This way of finding the deterministic node parameters doesn\u2019t spoil the renormalization of the parameters. Indeed, looking atEquation\u00a015, a FC nodes would produce a FC block-node for every coarser level; whereas a vanishing parameter gives the freedom to the other terms in the summation.\n\nSECTION: II.6From Constrains to Bounds\n\nThe MSM probability requires a positive inner products\n\nfor every pair of nodes, in order to guarantee.\n\nHere, we will prove that the above constraints is equivalent of setting all vector components to be non-negative, namely. Roughly, the spanned region by the embeddings is enclosed in one quadrant of the space, and it is possible to rotate the vectors to lay in the positive quadrant.\n\nThe steps to show this are the following. First, since we are interested on the sign among the vectors, one may restrict to the set of unit vectorswhereand assume.\nTaking into consideration alsoEquation\u00a030, the considered set is\n\nwhich by construction has to property that\n\nIf one takes the most \u201cclockwise\u201d and \u201canticlockwise\u201d vectors in the setdefined as\n\nby construction they form an angle. Therefore, ifthe treatment is finished since the vectors lay in the positive quadrant. On the other hand, if, the vectors lay in the negative quadrant and they can be rigidly rotated to lie on the positive quadrant, i.e..\n\nSECTION: II.7Algorithmic Complexity\n\nAs described in thesubsection\u00a0I.2, in order to describe a coarser graph without refitting the parameters, one should to use the RHS ofEquation\u00a013121212This spoils its functional form, but this would be the only way to avoid refitting the same model at a higher scale. Specifically, the algorithmic complexity to obtain oneiswhere the evaluation ofis assumed to have a complexity. Hence, to compute the complexity of, one has to sum over all the pairs, i.e.\n\nwhereare the number of structural inequivalent nodes at.\nOn the other hand, assuming the \u201csum\u201d of vectors inEquation\u00a015of order, the complexity of(seeEquation\u00a016) reads\n\nwhere the summation over theparameters counts asoperations andare all pairs at the levelfor the full.\n\nThe improvement is reported by the ratio of the two complexities, namely\n\nFocusing on level,\n\none saves two order of magnitude by proceeding with the summed MSM rather than the coarse-grained LPCA. Hence, by means ofEquation\u00a017, LPCA recovers the same complexity of summed MSM; thereby enabling for a comparison of equal complexity.\n\nSECTION: II.8Principled Embedding Dimension via Information Criteria\n\n(a) AICs by model class for IONModelDimLevel 0Level 1Level 2Level 3LPCA(1, 1)0.57640.66610.54480.3733(8, 8)0.50720.57790.40252.6648e+17MSM10.57840.66580.54080.409720.55160.63410.51290.384130.54340.62420.50120.379140.53780.61690.49840.383150.53360.61270.49850.404660.53110.60880.49790.423270.53040.60740.49680.457480.52870.60520.49760.511890.530.60730.4980.5588100.53050.60640.50020.6091110.53110.61050.51170.6671160.53650.62260.53280.9697\n\n(a) BICs by model class for IONModelDimLevel 0Level 1Level 2Level 3LPCA(1, 1)0.6220.72950.66050.7189(8, 8)0.87181.08561.32762.6648e+17MSM10.60120.69750.59870.582520.59720.69760.62850.729630.61170.71940.67460.897440.62890.74390.72971.074150.64760.77140.78771.268460.66780.79920.84491.459770.690.82950.90151.666780.71110.85910.96021.893990.73520.89291.01842.1136100.75840.92371.07852.3367110.78180.95951.14772.5675160.90111.13031.45793.7339\n\nTo determine the \u201cbest\u201d embedding dimension for LPCA and MSM, we used the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC)[27],[28]. These scores are defined as\n\nwhereis the number of parameters,the number of observations andthe likelihood of the model. They encode the trade-off between the goodness-of-fitand the complexity of the model. Therefore, the \u201cbest model\u201d is the one with theminimumAIC or BIC; which one of the two remains a debated choice: the AIC is asymptotically equivalent to the Kullback-Leibler divergence among thegeneratingmodel and acandidateone[27]whereas the BIC to the Description Length (DL)[28]. As the DL is the only one embodying the \u201ctrade-off\u201d paradigm, we decided to select theminimumBIC criterion. InEquation\u00a038, the scores are not comparable across scale, therefore, wenormalizedthe AIC and BIC scores as\n\nwhereand. This doesn\u2019t affect the ranking, but it provides the AIC and BICper pair.\n\nThe results are summarized inTable\u00a01for the ION andTable\u00a02for the WTW where the best scores (minimum) are highlighted in green whereas the worst (maximum) in red. The comparison is provided among LPCA and MSM as they have a different functional forms especially in the combination of the parameters. We have considered only two levels for LPCA as the benchmark for lowestand \u201cmaximum\u201dwith respect to our computational facility. On the other hand, we spanned more dimensions for MSM to provide an extensive description of the model performances. Recall that, by increasing the level of coarse-graining, the network tends to beless complex: theEquation\u00a09likely densifies the network implying that the nodes will have more similar roles. Therefore, theidealdimensiondecreases. Lastly, theaverageBIC score is calculated to provide aglobalview of the model performances. Thus, the best model is the one with the lowestaverageBIC score across levels. Overall, the best models areLPCA-(1,1)andMSM-1as BIC penalizes more than AIC the complexity of the model. However, in the following we will display the behavior also of thefor MSM andto assess the model performances at higher dimensions.\n\nSECTION: II.9Statistical Impossibility into the application of Train and Test Split\n\nSince every pair is seen as a Bernoulli random variablenot identically distributed[41,8], this implies that every link has to be accounted in the training procedure since it isnot already modelledby other pairs.\n\nSECTION: IIIWorld Trade Web results\n\nIn this section, we report the same results presented in the main text, but for the World Trade Web. The conclusions are the same as for the ING network.\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.04354v1_content.txt"}, {"title": "FlashSloth: Lightning Multimodal Large Language Models via Embedded\n  Visual Compression", "authors": ["Bo Tong", "Bokai Lai", "Yiyi Zhou", "Gen Luo", "Yunhang Shen", "Ke Li", "Xiaoshuai Sun", "Rongrong Ji"], "published_date": "2024-12-05T16:34:07Z", "summary": "Despite a big leap forward in capability, multimodal large language models\n(MLLMs) tend to behave like a sloth in practical use, i.e., slow response and\nlarge latency. Recent efforts are devoted to building tiny MLLMs for better\nefficiency, but the plethora of visual tokens still used limit their actual\nspeedup. In this paper, we propose a powerful and fast tiny MLLM called\nFlashSloth. Different from previous efforts, FlashSloth focuses on improving\nthe descriptive power of visual tokens in the process of compressing their\nredundant semantics. In particular, FlashSloth introduces embedded visual\ncompression designs to capture both visually salient and instruction-related\nimage information, so as to achieving superior multimodal performance with\nfewer visual tokens. Extensive experiments are conducted to validate the\nproposed FlashSloth, and a bunch of tiny but strong MLLMs are also\ncomprehensively compared, e.g., InternVL2, MiniCPM-V2 and Qwen2-VL. The\nexperimental results show that compared with these advanced tiny MLLMs, our\nFlashSloth can greatly reduce the number of visual tokens, training memory and\ncomputation complexity while retaining high performance on various VL tasks.", "arxiv_id": "2412.04317v1", "html_link": "https://arxiv.org/html/2412.04317v1", "search_term": "ti:\"embeddings\"", "html_content": "SECTION: FlashSloth: Lightning Multimodal Large Language Models viaEmbedded Visual Compression\n\nDespite a big leap forward in capability,multimodal large language models(MLLMs) tend to behave like a sloth in practical use,i.e., slow response and large latency. Recent efforts are devoted to building tiny MLLMs for better efficiency, but the plethora of visual tokens still used limit their actual speedup. In this paper, we propose a powerful and fast tiny MLLM calledFlashSloth. Different from previous efforts, FlashSloth focuses on improving the descriptive power of visual tokens in the process of compressing their redundant semantics. In particular, FlashSloth introduces embedded visual compression designs to capture both visually salient and instruction-related image information, so as to achieving superior multimodal performance with fewer visual tokens. Extensive experiments are conducted to validate the proposed FlashSloth, and a bunch of tiny but strong MLLMs are also comprehensively compared, e.g., InternVL2, MiniCPM-V2 and Qwen2-VL. The experimental results show that compared with these advanced tiny MLLMs, our FlashSloth can greatly reduce the number of visual tokens, training memory and computation complexity while retaining high performance on various VL tasks. Our code is released at:https://github.com/codefanw/FlashSloth.\n\nSECTION: 1Introduction\n\nRecent years have witnessed the remarkable breakthroughs made by extendinglarge language models(LLMs)[56,20,71]to more modalities, e.g., buildingmultimodal large language models(MLLMs) for vision-language tasks[28,35,42]. Among these advancements, one main research focus is on enhancing the visual perception of MLLMs, and the widely recognized solution is to use a larger number of visual tokens[34,32,45]. For instance, LLaVA-NeXT[34]uses 5 times more visual tokens compared to LLaVA-1.5[33]by subdividing input images into multiple tiles. Similarly, recent MLLMs, such as InternVL1.5[10]and Qwen2-VL[58], can support up to thousands of visual tokens for high-resolution image understanding via dynamic-resolution encoding. Although effective, the excessive use of visual tokens further execrates already high computation of MLLMs, limiting practical use.\n\nIn this case, more and more efforts are devoted to the research of lightweight and efficient MLLMs[12,51,16,65]. In particular, with the emergence of small-scale LLMs,e.g., Phi[20]and Gemma[4], recent endeavors start to explore their use in building tiny MLLMs, such as MobileVLM[11,12], Imp[51]and Mini-Gemini[31]. Meanwhile, representative MLLM families also launch their slim versions for better mobile applications,e.g., Qwen2-VL[58], InternVL[10]and MiniCPM-V[16]. With a much smaller LLM structure, these tiny MLLMs typically scale to about 2-3 billion parameters, so their training expenditure as well as memory overhead are also much cheaper than previous MLLMs[33,28,42]. However, to retain general multimodal capability, most tiny MLLMs[51,65,10]still adopt a large number of visual tokens, making it hard to achieve actual speedup. As shown in Fig.1, with more visual tokens used, tiny MLLMs even have a slower response time111The time for the first answer token.than common MLLMs like LLaVA-1.5-7B[33].\n\nBy revisiting the development of vision-language research[59,73,60,2,43], we can see that the way to achieve better visual capability is not confined to a singular paradigm. In principle, the key to addressing visual shortcoming is to make \u201cvision\u201d matter in MLLMs[15,52], thereby helping them better understand visual information and also reduce the impact oflanguage bias[72,22]. From this perspective, the use of enough visual tokens does contribute more to self-attention modeling in MLLMs, but recent studies[7,62]also show that this paradigm is often inefficient and obviously redundant. In addition, various attempts have been successfully made in improving visual capability before the era of MLLMs. For instance, enriching the visual semantics[21,70,44]or refining complex image information based on visual saliency or question dependency through various attention-based approaches[64,2,73,41]. To this end, we believe that a good balance between the performance and efficiency of MLLMs is feasible.\n\nIn this paper, we propose a tiny and fast MLLM calledFlashSloth. The principle of FlashSloth is to improve the descriptive power of visual tokens in the process of refining and compressing their redundant semantics, thereby achieving actual speedup during inference. Concretely, FlashSloth first introduces aspatial-aware attentive poolingto compress the redundant image information while capturing visually salient semantics. Meanwhile, a novel and lightweightQuerymodule is equipped to grasp instruction-related image information, thereby compensating the loss of image details in the attention pooling process. Notably, this query module is embedded into the architecture of FlashSloth rather than as an independent bridge branch that requires another language modeling[74,13,61], e.g., Q-Former[28]. Thus, we term itEmbQ. In addition to the compact structure designs, EmbQ also consumes much lower training and inference costs, well facilitating the efficiency goal of FlashSloth. For instance, EmbQ does not require dedicated large-scale VL alignment pretraining[28]. With these intuitive designs, FlashSloth can not only greatly reduce the number of input visual tokens but also improve their discrimination for better multimodal reasoning.\n\nTo validate the proposed FlashSloth, we conduct extensive experiments on a set of highly-competitive VL and MLLM benchmarks[36,14,66,25,30], and compare it with a bunch of least tiny MLLMs, including Qwen2-VL-2B[58], Intern-VL-2[10], MiniCPM-V2[17], and MM1.5[68]. Experimental results demonstrate that compared to these advanced tiny MLLMs, our FlashSloth can reduce the number of visual tokens, training memory and inference computation by 80-89%, 61-80% and 70-98%, respectively, while shortening the actual response time by about 2to 5times. Retaining high efficiency, FlashSloth also exhibits competitive ness against these SOTA methods, and even perform slightly better on several common VL tasks,e.g., MMB[36]and MMMU[66], well confirming our motivation and the designs of FlashSloth.In summary, our contributions are three folds:\n\nWe propose a strong and fast tiny MLLM in this paper, coined asFlashSloth, showing that a good balance between performance and efficiency is feasible.\n\nIn FlashSloth, we introduce embedded visual compression designs to efficiently capture both visually salient and instruction-related semantics, namelySpatial Attention PoolingandEmbedded Querymodules.\n\nThe extensive experiments not only show the strong multimodal capability of FlashSloth, but also confirm its competitiveness with a set of advanced MLLMs while retaining higher efficiency.\n\nSECTION: 2Related Works\n\nSECTION: 2.1Multimodal Large Language Models\n\nBased on the rapid development oflarge language models(LLMs)[56,71,20]and visual encoders[50,48],multimodal large language models(MLLMs) also achieve significant strides in variousvision-language(VL) tasks. Numerous open-source MLLMs[27,li2023blip,42,33]emerges in recent years, some of which even achieve outstanding capability comparable to GPT-4[1]in specific fields. However, this advancement is often support with increasingly larger parameter sizes, which also results in heavy burden to the training and application of MLLMs. Therefore, more recent research resorts to smaller LLMs to build tiny MLLMs, such as Phi[20], Gemma[4], and Qwen2[3]. For instance, MobileVLM[11]first realize the attempt of extending tiny LLMs to multimodal tasks with a simple yet effective visual projection after image encoder. Additionally, more efforts are devoted to explore the design and training strategies of tiny MLLMs based on small LLMs, such as LLaVA-Phi[75], Imp[51], and PaliGemma[9]. Meanwhile, the influential MLLM families, such as MiniCPM-V[17], Qwen2-VL[58]and InternVL[10], also develop their slim but also powerful versions via exploring high-resolution image encoding and high-quality data collection. Overall, the advancement of tiny MLLMs well facilitate the real-world applications of MLLMs. However, a magnitude of visual tokens still used also slow down the response time of tiny MLLMs in addition to high expenditure[29,7],i.e., the first token prediction, hindering application.\n\nSECTION: 2.2Visual Token Compression\n\nMost existing MLLMs[53,58,31,10,45]usually rely on a large number of visual tokens for superior visual capability, whether they are large or tiny. However, this paradigm is often criticized for excessive computation and obvious visual redundancy, which also attracts an influx of interest in efficient visual learning of MLLMs[7,62,18].\nIn terms of network designs,Q-Former-like methods[28,13,74,61]uses learnable tokens to control the number of visual tokens, with a purpose of capturing instruction-related visual information via visual compression. However, they often use another language model like BERT[57]to interpret the text instruction and require dedicated vision-language pretraining. Some methods like Abstractor[6]and LDP[11,12]employ convolutional layers to learn local visual compression. Similarly, methods[10]like InternVL apply pixel shuffle to directly reduce the number of visual tokens. However, the information loss in these local compression methods are often not further compensated. The other main paradigm for visual efficiency is to apply external methods for effective visual token pruning during inference[7,62,18]. For example, FastV[7]determines each token\u2019s importance based on average attention,and FitPrune[62]selects retained features by minimizing the attention distribution difference before and after pruning. However, the contributions of token pruning methods are orthogonal to this paper, and we focus on improving the discrimination of visual tokens via investigating network structure design.\n\nSECTION: 3Method\n\nSECTION: 3.1Overall\n\nIn this paper, we propose a tiny and fast MLLM calledFlashSloth, of which framework is illustrated in Fig.1. FlashSloth aims to improve the descriptive power of visual tokens with embedded visual compressions,i.e., thespatial attention pooling(SAP) andembedded query(EmbQ) modules, thereby achieving superior multimodal capability with a shorter visual token sequence.\n\nConcretely, given an input image, FlashSloth uses the image encoder to extract its visual token features, denoted, wheredenotes the resolution andis the feature dimension. And the input text instructionis first truncated into a set of tokens, which are then vectorized by the corresponding word embeddings, denoted as. Here,is the length of text sequence.\n\nIn existing MLLMs[33,51], the number of directly output visual tokensis often large, especially for the high-resolution images[34,10,17]. Thus, we apply the SAP module to attentively capture the salient visual semantics while compressing the token redundancy. The processed visual tokens by SAP are denoted by, which has a much smaller number of tokens than.\n\nAfterwards, the compact visual tokensafter linear projection and the text tokensare fed to the MLLM structure, which are also padded withlearnable query tokens at the end of the sequence, denoted as.\n\nIn FlashSloth,serves to supplementin terms of the instruction-related image details. Particularly, to avoid another language modeling[13,74,61],will first attend the multimodal interaction in the MLLM, and then engage in EmbQ for visual querying at the-th layer:\n\nHere, the output of EmbQ is pure visual attention featureswith the same length of. Overall, the objective of FlashSloth\u2019s decoding can be defined by:\n\nwhereis the prediction probability distribution,is the answer sequence andis its length.denotes the answer tokens before the-th step.\n\nFrom the above introduction, we can see that FlashSloth is different from most MLLMs[33,10,58]in two main aspects. First, FlashSloth refine and compress visual tokens in terms of both visual saliency and instruction-related semantics, which can well collaborate with each other for different VL tasks. Second, all compression designs are lightweight and embedded in the architecture of FlashSloth without the requirement of specific tuning or pretraining[28]. In the following section, we will describe them in detail.\n\nSECTION: 3.2Embedded Visual Compression\n\nAs discussed above, the main principle of FlashSloth is to improve the discrimination of visual tokens while squeezing their length. To approach this target, we perform the visual compression in two aspects,i.e.,visual saliencyandinstruction dependency. Moreover, these designs are lightweight and can be embedded into the architecture of FlashSloth, serving the target of model efficiency.\n\nWe first introduce aspatial attention pooling(SAP) method to refine and compress the semantics in local visual tokens, borrowing the successful attention designs in previous VL research[73]. The intuition is that the visual tokens extracted by the encoders like ViT[67,49]already have a large receptive field as well as obviously overlapping information. Thus, an MLLM can mainly focus on the most visually salient information in each image regions.\n\nSpecifically, given the extracted visual tokens, we first spatially divide them into a set of region tokens with a size of, denoted. Thus, for each image region, SAP directly use a two-layerto predict its visual attention weights:\n\nThen, the visually salient feature of each image regionis directly obtained via weighed combinations:\n\nLastly, those salient features are tiled to form the new visual tokensand fed to FlashSloth.\n\nConsidering the varying difficulties of VL tasks[15,8,23], the salient semantics provided by SAP is prone to insufficient for multimodal reasoning. In this case, we further propose anembedded query(EmbQ) module towards instruction-aware visual compression.\nIn broad terms, EmbQ is similar to previous attempts like Q-Former[28],i.e., querying the text-related visual information, but it still exhibit obvious differences in design and operation.\n\nAbove all, our requirement for EmbQ is to accomplish coarse-grained visual grounding rather than accurate VL alignment. By revisiting previous VL research[59,60], we note that this requirement is easy to achieved without complex network structure and large-scale pretraining. Therefore, the design of EmbQ is neat and efficient, which is directly embedded into FlashSloth, as shown in Fig.2.\n\nConcretely, a set of learnable tokensare used asqueriesand padded in the input sequence of FlashSloth. Afterlayers of transformation,are fed to EmbQ for visual querying. In particular, we expect this operation will allowto obtain enough instruction information from the text tokens via self-attention. But during experiments, we note that more visual semantics are received since the length of visual tokens is much longer than that of the text ones, which contradicts the target of EmbQ.\n\nThus, we first interactandvia cross-attention:\n\nwhereare the obtained text queries, andare the projection weight matrices, anddenotes their dimension.\n\nThen, we can useto query visual information from the uncompressed visual tokens, defined by\n\nLastly,are up projected and then combined withfor the following multimodal inference of FlashSloth, as described in Sec.3.1. Notably, the process of EmbQ takes into account of the discrimination of well-learned visual tokens, so only one up-projection is used for visual tokens for scaling to the MLLM\u2019s dimension. Besides, we also use the embedding of \u2018dot\u2019 token to initialize queries, making them easier to accommodate to the semantic space of MLLM.\n\nSECTION: 3.3Training and Other Settings\n\nUnder the default setting, FlashSloth apply a two-stage training paradigm[35].The pretraining stage.Only the projector and spatial attention pooling are optimized for the alignment between visual and text tokens, while the LLM is fixed.The SFT tuning stage.Except vision encoder, FlashSloth are optimized, including LLM and EmbQ.\n\nTo tackle OCR tasks with a high demand on image resolution[34,26,10], we also propose a high-resolution version, termedFlashSloth-HD. In particular, FlashSloth-HD inputs images ofresolutions. In terms of image processing, we follow LLaVA-NeXT[34]to divide the images into four parts and a low-resolution thumbnail, of which visual tokens are extracted in parallel. Similarly, FlashSloth use SAP to squeeze their length greatly and EmbQ for visual querying. To save training expenditure, we only use high-resolution images in the SFT tuning of FlashSloth-HD, where the vision encoder is unfreezed for better accommodation. Details can refer to our project.\n\nSECTION: 4Experiment\n\nSECTION: 4.1Implementation Details\n\nIn terms of the default FlashSloth, we usesiglip-so400m[67]as the visual encoder with an input resolution of 384, andphi2-2.7B[20]as the LLM. The downsampling rate of SAP is set to 3, generating 81 visual tokens. The number of queries in EmbQ is 9, and an embedding query module with a dimension of 576 is inserted at the 8th layer of the MLLM by default. The model is trained byAdamW[37]optimizer and cosine learning rate scheduler for a total of 1 epoch. The initial learning rates for pre-training and instruction tuning are 1e-3 and 2e-5 with batch sizes of 256 and 128, respectively. All training is conducted on 8 NVIDIA A800 GPUs. For the FlashSloth-HD, the input image resolution is 768, and the image tokens are compressed to 405, while the other settings remain the same as FlashSloth.\n\nSECTION: 4.2Benchmarks and Metrics\n\nWe evaluate the model on seven multimodal benchmark datasets, including MMB[36], MME[14], mm-vet[63], Pope[30], SEED[25], MMMU[66], and MathVista[40]. And seven general visual-language datasets, including SQA[39], AI2D[24], GQA[19], TextVQA[54], ChartQA[46], DocVQA[47], and RealWorldQA. These benchmarks assess MLLMs from diverse perspectives, such as hallucination, multimodal perception and cognition, and multidisciplinary question answering. All evaluations are conducted using thelmms-eval[69].\n\nSECTION: 4.3Quantitative Analysis\n\nWe first compare the efficiency of FlashSloth with advanced tiny MLLMs, and also use the representative MLLM LLaVA-1.5-7B[33]as reference.Inference Efficiency.We first compare the inference efficiency of FlashSloth with three advanced tiny MLLMs[58,10,51]in Tab.1. For better comparisons, we use LLaVA-1.5-7B[33]as reference. From these statics, we can first observe that tiny MLLMs have a lower requirement of GPU memory than LLaVA due to the use of much smaller LLMs. Likewise, their theoretical computation (FLOPS) is also less than LLaVA. However, their actual inferences are not obviously faster,i.e., the response time or throughput. To explain, the large number of visual tokens will greatly slow down the decoding of first token,i.e., response time, making their advantages inKV caching[5]based decoding become not that obvious, especially that the answers of VL examples are often short[54,46,47]. We can also see that the dynamic resolution designs of Qwen2-VL and InternVL can help to adjust the number of visual tokens for different images, but still keeps a relatively large number, which also result in large latency. Lastly, we can see that with about 80-89% fewer tokens, FlashSloth exhibits obvious merits than these MLLMs in terms of all inference metrics. For instance, its response time is about 2 and 5 times faster than LLaVA and InternVL, respectively.\nIn terms of FlashSloth-HD, its overall efficiency is also superior than the compared MLLMs, even through it uses more visual tokens than FlashSloth. These results well confirm the advantages of FlashSloth in inference and visual compression.\n\nTraining Efficiency.We further report the training expenditures of FlashSloth and the other four MLLMs in Fig.3. For a quick comparison. we use the pretraining and tuning splits of LLaVA[33], and the per-GPU batch size is set to 32 for pretraining and 8 for instruction tuning. From these plots, we can first find that FlashSloth consumes much less training time than the other MLLMs, especially pretraining. In practice, its pretraining using the LLaVA split only takes 6.4 GPU hours, about 76% and 68% less than LLaVA and IMP, respectively. Its SFT tuning time (52 GPU hours) is longer due to more examples used, and it is also slightly affected by the input queries. However, the cost is still lower than than IMP (65.6 GPU hours) and MobileVLM (96 GPU hours). Similarly, with a well designed training scheme, FlashSloth-HD has a slightly longer training time (6.4+65.6 GPU hours), which is still cheaper than the other MLLMs. The other observation from Fig.3is that tiny MLLMs require GPU memories close to that of LLaVA except our FlashSloth. During pretraining, both FlashSloth and FlashSloth-HD only use about 81 visual tokens, making its memory overhead much lower than the other MLLMs. During instruction tuning, their GPU memories increase greatly. To explain, the queries are given for each instruction, and an SFT example is often a multi-round conversation, so the multiple paddings will bring in a larger sequence. With effective compression, this overhead is still lower than the compared methods. Overall, these results show the merits of FlashSloth in training efficiency.\n\nWe make performance comparisons between FlashSloth-HD and a bunch of advanced tiny MLLMs on 14 highly-competitive VL and MLLM benchmarks, as shown in Table2. From this table, we can first observe that FlashSloth is very competitive on common VL tasks with a much smaller number of visual tokens. For instance, its performance on MMB[36], GQA[19]and SQA[39]is much better than several previous tiny MLLMs with similar training data amount,e.g., MobileVLM[12], Mini-Gemini[31]and Imp[51]. Compared to the SOTA tiny MLLMs, such as InternVL[10]and Qwen2-VL[58], FlashSloth also exhibits good competitiveness on these tasks, but it still lags behind them on the OCR tasks like DocVQA[47], which requires high-resolution image inputs. In this case, we can see that its HD version,i.e., FlashSloth-HD, can well compensate this shortcoming. Overall, retaining better efficiency, FlashSloth-HD can generally reach the capability of InternVL2, and well shorten its gap to Qwen2-VL. Moreover, FlashSloth can even achieve new SOTA performance among tiny MLLMs on several benchmarks, such as MMB, GQA and AI2D. Considering the much smaller amount of training data for FlashSloth-HD, these results are in fact very notable, well confirming our motivation and designs.\n\nIn the first block of Tab.3, we first compare different visual compression designs. The most simple solution isaverage pooling, but it tends to lose key visual information, leading to obvious declines in most benchmarks,e.g., MME and MMB for multimodal perception and recognition. In contrast, our SAP can well keep the visual saliency, so as to obtaining better performance than simple pooling. In addition, we can also see that with the combination of EmbQ and SAP, FlashSloth can obtain very marginal performance drops compared to the baseline, while its efficiency is much better. This result confirms the supplement of EmbQ to SAP.\nIn the second and last blocks of Tab.3, we examine the settings of EmbQ. We can first see that the initialization of queries has impact on EmbQ. The random initialization can serves the target of EmbQ, but the initialization of textdottoken further improves performance slightly, suggesting their better interactions with other input tokens in MLLMs. Besides, we can also see that the number of queries required by EmbQ is very small, and 9 tokens are enough for visual querying. To explain, EmbQ serves to capture instruction-related information at a coarse granularity, as discussed above. As a supplement to SAP, this design only requires a few queries, especially considering the short questions in MLLM tasks. Overall, these results well confirm the designs of EmbQ.\n\nSECTION: 4.4Qualitative Analysis\n\nTo gain deeper insight into FlashSloth\u2019s process of enhancing visual feature description, we visualize the attention results of SAP and EmbQ, as shown in the figure4(a). As observed, SAP distributes attention more broadly, allowing the model to focus on salient information from different regions of the image. This helps the model capture salient details in images , such as the three small birds in the left image, seabirds in the middle image, and tiny text in the right image. In contrast, the embedded query focuses more narrowly on key, text-related information in the image, such as the elephant in the left image, the surfer in the middle image, and the hair in the right image. By combining these two attention mechanisms, the model can effectively prioritize the most important information in the image, enhancing the expressiveness of visual features. This demonstrates that the synergy between SAP and EmbQ allows FlashSloth to fully leverage visual information, resulting in improved performance across multi-task scenarios.\n\nIn the Figure4(b), We visualize the predictions of FlashSloth , FlashSloth-HD, Qwen2-VL and InternVL-2 for different VL examples. First, FlashSloth exhibits extremely fast response times, with latency significantly lower than the other two models, providing a good user experience. Second, for coarse-grained real-world QA and scientific QA tasks, FlashSloth\u2019s performance is on par with or even surpasses that of the other two models. In the top-left example, FlashSloth identifies grapes that the other models miss, and FlashSloth-HD answers in more detail. In the bottom example, FlashSloth provides the most accurate answer to a biological question. However, due to its lower resolution, FlashSloth underperforms on the OCR tasks. For instance, in the top-right example, FlashSloth fails to recognize correctly, but upon increasing the input resolution, FlashSloth-HD handles fine-grained OCR tasks effectively.\n\nSECTION: 5Conclusion\n\nIn this paper, we introduce FlashSloth, a powerful and fast tiny MLLM. By incorporating effective embedded visual compression designs, FlashSloth effectively captures both visual saliency and instruction-related semantics, achieving an optimal balance between performance and efficiency. Extensive comparisons with existing tiny MLLMs on various benchmarks demonstrate that FlashSloth significantly enhances both training and inference efficiency while maintaining competitive performance, which well validates its motivation and designs.\n\nSECTION: 6Acknowledgments\n\nThis work was supported by National Science and Technology Major Project (No. 2022ZD0118201), the National Science Fund for Distinguished Young Scholars (No. 62025603), the National Natural Science Foundation of China (No. U21B2037, No. U22B2051, No. 623B2088, No. U23A20383, No. U21A20472, No. 62176222, No. 62176223, No. 62176226, No. 62072386, No. 62072387, No. 62072389, No. 62002305 and No. 62272401), and the Natural Science Foundation of Fujian Province of China (No. 2021J06003, No. 2022J06001).\n\nSECTION: References\n\nSECTION: Appendix AQuantitative analysis\n\nSECTION: A.1Impact of EmbQ configuration\n\nTable4examines the impact of EmbQ\u2019s dimensions, number of layers, and insertion positions within the LLM on performance. The first and second blocks show that EmbQ achieves optimal results with a simple configuration of 576 dimensions and a single layer, aligning with FlashSloth\u2019s efficiency goals. The third block explores the effects of inserting EmbQ into shallow, middle, deep, or multiple layers. Excessively shallow insertions limit Query Tokens to self-interaction, restricting their ability to capture rich image and text features, while overly deep insertions hinder effective propagation of newly learned supplementary information to generated tokens.\n\nSECTION: A.2Impact of Feature Fusion Methods\n\nTable5compares methods for fusing features learned by EmbQ with Query Token features: direct replacement, addition, and gated fusion[45], which adjusts the contribution of each feature. Results show that direct addition achieves the best performance by effectively integrating EmbQ\u2019s features while retaining the original feature information.\n\nSECTION: A.3Comparison with Other Vision Compression Methods\n\nTable6compares our method with various visual feature compression approaches and evaluates the performance gains from incorporating EmbQ. The combination of SAP and EmbQ achieves the best results, validating the integration of SAP\u2019s saliency features with EmbQ\u2019s instruction-related features. Additionally, embedding EmbQ into any compression method consistently improves performance, highlighting its effectiveness.\n\nSECTION: Appendix BQualitative Analysis\n\nThis section showcases practical examples of FlashSloth and FlashSloth-HD, demonstrating their real-world performance in multidisciplinary question answering, code generation, real-world scene reasoning, fine-grained text information extraction, and chart analysis reasoning. FlashSloth delivers accurate responses and exceptional performance across these tasks.", "text_file": "data\\paper_texts\\2412.04317v1_content.txt"}, {"title": "Distance-Adaptive Quaternion Knowledge Graph Embedding with\n  Bidirectional Rotation", "authors": ["Weihua Wang", "Qiuyu Liang", "Feilong Bao", "Guanglai Gao"], "published_date": "2024-12-05T11:17:03Z", "summary": "Quaternion contains one real part and three imaginary parts, which provided a\nmore expressive hypercomplex space for learning knowledge graph. Existing\nquaternion embedding models measure the plausibility of a triplet either\nthrough semantic matching or geometric distance scoring functions. However, it\nappears that semantic matching diminishes the separability of entities, while\nthe distance scoring function weakens the semantics of entities. To address\nthis issue, we propose a novel quaternion knowledge graph embedding model. Our\nmodel combines semantic matching with entity's geometric distance to better\nmeasure the plausibility of triplets. Specifically, in the quaternion space, we\nperform a right rotation on head entity and a reverse rotation on tail entity\nto learn rich semantic features. Then, we utilize distance adaptive\ntranslations to learn geometric distance between entities. Furthermore, we\nprovide mathematical proofs to demonstrate our model can handle complex logical\nrelationships. Extensive experimental results and analyses show our model\nsignificantly outperforms previous models on well-known knowledge graph\ncompletion benchmark datasets. Our code is available at\nhttps://github.com/llqy123/DaBR.", "arxiv_id": "2412.04076v1", "html_link": "https://arxiv.org/html/2412.04076v1", "search_term": "ti:\"embeddings\"", "html_content": "SECTION: Distance-Adaptive Quaternion Knowledge Graph Embedding with Bidirectional Rotation\n\nQuaternion contains one real part and three imaginary parts, which provided a more expressive hypercomplex space for learning knowledge graph.\nExisting quaternion embedding models measure the plausibility of a triplet either through semantic matching or geometric distance scoring functions.\nHowever, it appears that semantic matching diminishes the separability of entities, while the distance scoring function weakens the semantics of entities.\nTo address this issue, we propose a novel quaternion knowledge graph embedding model.\nOur model combines semantic matching with entity\u2019s geometric distance to better measure the plausibility of triplets.\nSpecifically, in the quaternion space, we perform a right rotation on head entity and a reverse rotation on tail entity to learn rich semantic features.\nThen, we utilize distance adaptive translations to learn geometric distance between entities.\nFurthermore, we provide mathematical proofs to demonstrate our model can handle complex logical relationships.\nExtensive experimental results and analyses show our model significantly outperforms previous models on well-known knowledge graph completion benchmark datasets.\nOur code is available athttps://github.com/llqy123/DaBR.\n\nDistance-Adaptive Quaternion Knowledge Graph Embedding with Bidirectional Rotation\n\nWeihua Wang1,2,3,\u2020\u2020thanks:Corresponding Author. Email:wangwh@imu.edu.cn.,\nQiuyu Liang1,\nFeilong Bao1,2,3,\nGuanglai Gao1,2,31College of Computer Science, Inner Mongolia University, Hohhot, China2National and Local Joint Engineering Research Center of Intelligent\nInformationProcessing Technology for Mongolian, Hohhot, China3Inner Mongolia Key Laboratory of Multilingual Artificial Intelligence Technology, Hohhot, China\n\nSECTION: 1Introduction\n\nKnowledge graphs (KGs)Liang et\u00a0al. (2024a)are powerful tools for representing valid factual triplets by capturing entities and their relationships in a graphical format.\nOwing to the well-structured graph, KGs are often used for various Natural Language Processing tasks including but not limited to, question answeringMendes et\u00a0al. (2024); Faldu et\u00a0al. (2024), entity alignmentWang et\u00a0al. (2024), KG-based recommendationLiang et\u00a0al. (2024c)and KG enhanced Large Language ModelWen et\u00a0al. (2024).\n\nHowever, KGs are usually incomplete, and the incompleteness limits their application.\nAs an effective tool for completing missing facts, knowledge graph completion (KGC) has received much attention from researchers.\nTypically, researchers transform KGC tasks into knowledge graph embeddings (KGEs).\nKGE refers to learning representations of entities and relations in a low-dimensional space while preserving graph\u2019s inherent structure and semantic properties.\nIn this representation space, a scoring function can be defined to measure the plausibility of each triplet, where valid triplets should receive higher scores than these invalid ones.\n\nQuaternion contains one real part and three imaginary parts, which providing a more expressive space for learning embeddings of entities and relations.\nRotation in quaternion is often used to model the KGs.\nFor example, QuatEZhang et\u00a0al. (2019)learns semantic information about entities by treating relations as rotations from head entities to tail entities.\nTransERRLi et\u00a0al. (2024)encodes the KG by rotating the head and tail entities with their corresponding unit quaternions.\nThese models use either semantic matching or distance scoring functions to measure the plausibility of the triplet, respectively.\nHowever, it appears that semantic matching diminishes the separability of entities, while the distance scoring function weakens the semantics of entities from our visualization analysis in Figure1111For more information about queries, see Section6.4..\nSpecifically, as shown in Figure1, we can observe that QuatE using semantic matching as a scoring function overlaps in each queries.\nThe entities of TransERR using the distance scoring function are also indistinguishable from each query.\n\nTo address this issue, we propose aDistance-adaptive quaternion knowledge graph embedding withBidirectionalRotation model, named asDaBR.\nOur model combines semantic matching with entity\u2019s geometric distance to better measure the plausibility of triplets.\nSpecifically, in the quaternion space, we perform a right rotation on the head entity and a reverse rotation on the tail entity to learn the rich semantic features.\nThis process is called bidirectional rotation.\nWe conducted extensive experiments on multiple well-known benchmark datasets of knowledge graph completion task.\nThe experimental results and analyses demonstrated the effectiveness and robustness of our model.\n\nOur contribution is summarized as follows:\n\nWe propose to perform a right rotation on the head entity and a reverse rotation on the tail entity to learn the rich semantic features.\n\nWe propose to learn the embedding distance between entities by incorporating distance adaptive translations.\n\nWe provide mathematical proofs to demonstrate that our model can handle rich logical relationships.\n\nExtensive experiments show that our model provides consistent and significant improvements over previous models on the vast majority of metrics.\n\nSECTION: 2Related Work\n\nFor KGE models, the design of the scoring function directly affects these models\u2019 performance and effectiveness.\nBased on the calculation methods of scoring functions in previous models, KGE scoring functions can be mainly categorized into semantic matching- and geometric distance-based.\n\nSemantic matching.Semantic matching scoring functions capture the interactions between entities and relations through inner products on embedding vectors.\nThe hypothesis is that entities connected by relations are close to each other in the semantic space.\nFor example, QuatEZhang et\u00a0al. (2019)obtains entity semantic information through the Hamiltonian rotation of the head entity on the relation in quaternion space.\nDualECao et\u00a0al. (2021)further enhances QuatE to model knowledge graphs in dual quaternion space.\nQuatRENguyen et\u00a0al. (2022)associates each relation with two relation-aware rotations, which are used to rotate the quaternion embeddings of the head and tail entities, respectively.\n\nA common feature of these models is the computation of the inner product between the head entity and the tail entity after a relation transformation.\nHowever, these models overlook the geometric distance properties between entities in the knowledge graph, leading to distorted embeddings of the learned entities.\n\nGeometric distance.Geometric distance scoring functions assess the plausibility of triplets by calculating the distances between embedding vectors in the representation space.\nThe goal of this scoring function is keep the head/tail entity vector closer to the tail/head entity vector after being transformed through the relation vector.\nFor example, TransEBordes et\u00a0al. (2013), considered the first model to employ a geometric distance scoring function, assumes that tripletsin knowledge graphs should satisfy the expression.\nHowever, TransE struggles with more complex relation types, such as one to many (1-to-N), many to one (N-to-1) and many to many (N-to-N).\n\nTo address this limitation, several models using distance-based scoring functions have been proposed.\nFor example, Rotate3DGao et\u00a0al. (2020)maps entities to an 3D space, defining the relation as a rotation from the head entity to the tail entity.\nTrans4ENayyeri et\u00a0al. (2021)performs rotations and translations in a quaternion space.\nRotateCTDong et\u00a0al. (2022)transforms entity coordinates and represents each relation as a rotation in complex space.\nRotate4DLe et\u00a0al. (2023)employs two distinct rotational transformations to align the head embedding with the tail embedding.\nDCNEDong et\u00a0al. (2024)maps entities to dual complex number space, using rotations in 2D space via dual complex number multiplication to represent relations.\nTransERRLi et\u00a0al. (2024)encodes knowledge graphs by rotating the head and tail entities with their corresponding unit quaternions.\n\nA common feature of these models is that the plausibility of the triplets is evaluated by calculating the distance between the head entity and the tail entity after transformation.\nHowever, these models fail to consider information about entities within the semantic space, leading to performance degradation.\n\nSECTION: 3Preliminaries\n\nThis section begins with a definition of the knowledge graph completion task, followed by a brief background on quaternion algebra.\n\nSECTION: 3.1Knowledge Graph Completion\n\nKnowledge graph completion is the task of predicting missing elements in a triplet.\nThis task can be broken down into three sub-tasks: predicting the head entity, predicting the relation, and predicting the tail entity.\nFollowing previous research, our work focuses on predicting the headand tailentities.\nIt is because relation information is needed in the training process.\n\nSECTION: 3.2Quaternion Algebra\n\nThe quaternion extends complex number system to four dimensions.\nIn-dimensional quaternion space, a quaternionconsists of one real component and three imaginary components.\nIt can be formalized as:, whereare real numbers andare imaginary units.\nThe imaginary part satisfies the Hamilton\u2019s rulesHamilton (1844):.\n\nAddition.Given two quaternionsand, quaternion addition is defined as:\n\nNorm.The normalization of quaternionscan be defined by the following:\n\nInverse.The inverse of quaternionscan be defined by the following:\n\nwhereis the conjugate of.\n\nHamilton product.Given two quaternionsand.\nThe quaternion rotation of these two quaternions can be performed by the Hamilton product:\n\nwheredenotes the element-wise product.\n\nSECTION: 4Methodology\n\nIn this section, we describe our model in detail, which consists of two main parts:\n\nBidirectional rotation: Performing a right rotation on the head entity and a reverse rotation on the tail entity to learn the rich semantic features.\n\nDistance-adaptation: Incorporating a distance adaptive translation to learn the geometric distance between entity embeddings.\n\nSECTION: 4.1Symbol Description\n\nA knowledge graphis a collection of triplet, whereandare the entity set and relation set.andrepresent the number of entities and relations, respectively.\nGiven a triplet, the embeddings of head entity, relationand tail entitycan be represented by quaternions:\n\nSECTION: 4.2Part One: Bidirectional Rotation\n\nIn Figure2, we show the differences between our proposed bidirectional rotation and previous methods when modeling entity semantics.\nSpecifically, QuatE (Figure2(a)) performs a right rotation for head entity.\nQuatRE (Figure2(b)) performs two times right rotation for head entity and a right rotation for tail entity.\nOur model (Figure2(c)) performs a right rotation for head entity and a reverse rotation for tail entity.\n\nWe first normalize the relation quaternionto a unit quaternionto eliminate the scaling effect by dividing by its norm (Equation2):\n\nThen, the head entityis right rotated using the relation, i.e., the entity vector and the relation vector do a Hamilton product (Equation4):\n\nSimilarly, the inverse of the relation unit quaternionis used to make a reverse rotation of the tail entity:\n\nSinceis a unit quaternion, we have:\n\nwhereis the conjugate of.\n\nTherefore, the scoring functionfor the bidirectional rotation modeling entity semantics is defined by:\n\nSECTION: 4.3Part Two: Distance-Adaptation\n\nAs shown in Figure2, the previous QuatE (Figure2(a)) and QuatRE (Figure2(b)) can only learn the semantic information of an entity but ignore the geometric distance attribute of an entity.\nOur DaBR effectively addresses this limitation by adding a distance-adaptation (Figure2(c)).\n\nTherefore, to model the geometric distance information, we initialize a distance-adaptive relation embedding.\nFinally, the geometric distance part scoring functionis defined as:\n\nwhererepresents thenorm.\nDespite its simplicity, we find that the proposed method is effective enough in providing distance information for our model.\n\nSECTION: 4.4Scoring Function\n\nAfter obtaining the scoring functions for modeling entity semantics and entity geometric distances, respectively.\nWe fuse these scoring functions into a new scoring function for model training:\n\nwhererepresents the semantic matching scoring function,represents the geometric distance scoring function, andis an adaptive parameter that learned by our model.\n\nSECTION: 4.5Loss Function\n\nFollowingTrouillon et\u00a0al. (2016), we formulate the task as a classification problem, and the model parameters are learned by minimizing the following regularized logistic loss:\n\nwhereanddenote the embedding of all entities and relations.\nHere we use thenorm with regularization ratesandto regularizeand, respectively.is sampled from the unobserved setusing uniform sampling.represents the corresponding label of the triplet.\n\nSECTION: 4.6Discussion\n\nAs described inChami et\u00a0al. (2020), there are complex logical relationships (such as symmetry, antisymmetry, inversion and composition relationships) in the knowledge graph.\nIn this part, we analyze the ability of our DaBR to infer these relationships.\n\nLemma 1DaBR can infer the symmetry relationship pattern. (See proof in AppendixA.1)\n\nLemma 2DaBR can infer the antisymmetry relationship pattern. (See proof in AppendixA.2)\n\nLemma 3DaBR can infer the inversion relationship pattern. (See proof in AppendixA.3)\n\nLemma 4DaBR can infer the composition relationship pattern. (See proof in AppendixA.4)\n\nSECTION: 5Experiments\n\nIn this section, we first introduce the datasets, evaluation protocol, implementation details and baselines.\nSubsequently, we evaluate our model on four benchmark datasets.\n\nDatasets.\nTo verify the effectiveness and robustness our model, we conducted extensive experiments on four standard knowledge graph completion datasets including WN18RRDettmers et\u00a0al. (2018), FB15k-237Toutanova and Chen (2015), WN18Bordes et\u00a0al. (2013)and FB15kBordes et\u00a0al. (2013).\nThe WN18 and FB15k datasets are known to suffer from a data leakage problem, leading to models being easily inferred and consequently performing well on metrics.\nWN18RR and FB15k-237 were derived as subsets of WN18 and FB15k respectively.\nThese datasets are designed to address data leakage concerns and thereby present a more realistic prediction task.\nThe detailed statistics of the four standard datasets are shown in AppendixB.\n\nEvaluation protocol.\nSimilar to previous workZhang et\u00a0al. (2019); Li et\u00a0al. (2024), we employed the filtered evaluation setup described in referenceBordes et\u00a0al. (2013)to filter out real triplets during the evaluation process.\nThis was done to avoid flawed evaluations.\nWe used evaluation metrics encompassed Mean Rank (MR), Mean Reciprocity Rating (MRR) and Hits@n (n=1, 3 or 10).\nWhere a smaller value on the MR indicates a better model.\nThe final scoring model on the test set is derived from the model with the highest Hits@10 score on the validation set.\n\nImplementation details.\nWe conduct all our experiments on a single NVIDIA GeForce RTX 4090 with 24GB of memory.\nThe ranges of the hyper-parameters for the grid search are set as follows: the embedding dimension () is selected from {300, 400, 500}; the learning rate () is chosen from {0.01, 0.02, 0.05, 0.1}; and the number of negative triplets sampled () per training triplet is selected from {5, 10}.\nThe regularization ratesandare adjusted within {0.01, 0.05, 0.1, 0.5}.\nWe create 100 batches of training samples for different datasets.\nWe optimize the loss function by utilizing AdagradDuchi et\u00a0al. (2011).\nAll our hyper-parameters are provided in AppendixC.\n\nIt is worth noting that our modelsdo notemploy the training strategies of self-adversarial negative samplingSun et\u00a0al. (2019)or N3 regularization with reciprocal learningLacroix et\u00a0al. (2018).\n\nBaselines.\nTo verify the effectiveness of our model, we compared DaBR with several powerful baseline models, including both well-known and recently proposed ones with outstanding results.\nWe divide these models according to the scoring function as follows:\n\n1) Semantic Matching:TuckERBalazevic et\u00a0al. (2019), QuatEZhang et\u00a0al. (2019), DualECao et\u00a0al. (2021), QuatRENguyen et\u00a0al. (2022).\n\n2) Geometric Distance:ATTHChami et\u00a0al. (2020), Rotate3DGao et\u00a0al. (2020), Trans4ENayyeri et\u00a0al. (2021), RotateCTDong et\u00a0al. (2022), Rotate4DLe et\u00a0al. (2023), CompoundEGe et\u00a0al. (2023), HAQELiang et\u00a0al. (2024d), DCNEDong et\u00a0al. (2024), FHRELiang et\u00a0al. (2024b)and TransERRLi et\u00a0al. (2024).\n\nFor a fair comparison, we report the optimal results for these baselines from the original papers.\n\nSECTION: 5.1Main Results\n\nThe main results of our DaBR and the baselines for the WN18RR and FB15k-237 datasets are listed in Table1.\nWe categorize the baseline models into two main groups based on scoring functions, namely semantic matching and geometric distance scoring functions.\nThe models based onSemanticMatching are listed in the upper part of the table, while theGeometricDistance based methods are listed in the lower part of the table.\nIt is worth noting that our model\u2019s scoring function is the unique scoring function that simultaneously measures bothSemantic andGeometric distances.\n\nFrom Table1we can clearly see that our model achieves the best results on both datasets, except for the H@1 metric on the WN18RR dataset.\nSpecifically, compared to the best performing of the semantic matching model, QuatRE, our model drops from 1986 to 899 on the MR metric and absolutely improves 3.4%, 5.0%, 3.6% and 2.5% on the MRR, H@10, H@3 and H@1 metrics on the WN18RR dataset.\nOn the FB15k-237 dataset, our model decreases from 88 to 83 on the MR metrics, and absolutely improves on the MRR, H@10, H@3 and H@1 metrics by 1.6%, 1.5%, 1.4% and 1.8%.\n\nCompared to the latest and best performance of the geometric distance model, TransERR, our model decreases from 1167 to 899 on the MR metric and achieves an absolute improvement of 1.8%, 2.8%, and 3.4% on the MRR, H@10 and H@3 metrics on the WN18RR dataset.\nOn the FB15k-237 dataset, our model decreases from 125 to 83 on the MR metrics, and absolutely improves on the MRR, H@10, H@3 and H@1 metrics by 3.6%, 3.0%, 3.5% and 3.7%, respectively.\n\nThe KGC results on WN18 and FB15k datasets, as shown in Table2.\nThe Table2illustrates our model superiority over any previous model on the FB15k dataset.\nOn the WN18 dataset, our model achieves the best results on all metrics, except for the H@1 metric which achieves second place.\nIn conclusion, our model not only achieves optimal results compared to semantic matching models, but also achieves competitive results compared to geometric distance models.\n\nSECTION: 6Analysis\n\nTo demonstrate the superiority of our model, we have conducted in-depth analysis experiments from various aspects.\nThe obtained experimental results and analysis are as follows:\n\nSECTION: 6.1Ablation Analysis\n\nIn this section, we aim to evaluate the efficacy of bidirectional rotation and distance-adaptation within our DaBR.\nWe have designed the following model variants:\n\nVariant I: We remove the rotation of the tail entity and keep the rotation of the head entity.\n\nVariant II: We removed the distance-adaptation.\nThe DaBR degenerates into a semantic matching model.\n\nWe show the results of the ablation experiments in Table3.\nFrom the table, we can obtain the following conclusions:\n1) The rotation of the tail entity and distance-adaptation are important parts of our model.\n2) When our model removed the tail rotation, the model (i.e.,Variant I) still achieved the best results compared to the models in the Table1and Table2.\nWe attribute this to the fact that our model can measure both the semantics of entities and the embedding distance of entities.\n3) When our model removed distance-adaptation, the model (i.e.,Variant II) performance decreased dramatically on all datasets.\nIt is worth noting that our model still achieves optimal results on most datasets compared to the semantic matching model on most datasets.\n\nSECTION: 6.2Parameter Comparison Analysis\n\nTo analyze the number of parameters compared to other models, we compared our DaBR with the best semantic matching model (QuatRE) and the best geometric distance model (TransERR).\nGiven the same embedding dimension, QuatRE and TransERR haveparameters, while our DaBR hasparameters, whereandare the entity set and relation set.\nCompared to QuatRE and TransERR, our model achieves better results with fewer parameters.\n\nSECTION: 6.3Relationship Type Analysis\n\nTo explore the robustness of our model in the face of different relation types (one-to-many (1-to-N), many-to-one (N-to-1) and many-to-many (N-to-N)), we compared DaBR with QuatE and QuatRE in WN18R dataset.\nFor the results of the QuatE and QuatRE, we reproduce these models following the hyper-parameter settings of their paper.\n\nIn accordance with the calculation rules set out inBordes et\u00a0al. (2013), the test set of WN18RR has been divided into three categories: 1-to-N, N-to-1 and N-to-N.\nThe division results are shown in AppendixD, whereandrepresent the average degree of head and tail entities, respectively.\n\nWe show the MRR scores for the QuatE, QuatRE, and DaBR models for 0 to 5200 training epochs in Figure3.\nThis demonstrates the effectiveness of our model in modelling different types of relationships.\nIn particular, the model is superior in dealing with 1-to-N relationship.\n\u201c1-to-N\u201d means that a head entity can form a fact triplet with multiple tail entities.\nWe attribute this superior enhancement to the distance-adaptive embedding of our model.\n\nSECTION: 6.4Visualization Analysis\n\nIn this section, to explore the embedding results of our model after distance adaptive embedding, we visualize the the tail entity embeddings using t-SNEvan\u00a0der Maaten and Hinton (2008).\nSuppose (,) is a query whereandare the head entity and the relation, respectively.\nIf (,,) is valid, the entityis the answer to query (,).\nWe selected 9 queries in FB15k-237 dataset, each of which has 50 answers.\nFor more details about the 9 queries, please refer to the AppendixE.\n\nWe then use t-SNE to visualize the semantic matching models QuatE and QuatRE, the geometric distance model TransERR, and our combined semantic and geometric distance DaBR to generate the answer embeddings for epoch 1 and epoch 100, respectively.\nFigure4shows the visualization results222Refer to AppendixFfor more visualization results..\nEach entity is represented by a 2D point and points in the same color represent tail entities with the same (,) context (i.e. query).\n\nSpecifically, our model (Figure4(g)) in the first epoch have demonstrated better embedding compared to QuatE, QuatRE and TransERR.\nAt epoch 100, our model (Figure4(h)) show clear inter-cluster separability, with entities within each cluster (intra-cluster) being well-separated from one another.\n\nHowever, the semantic matching model QuatE (Figure4(b)) and QuatRE (Figure4(d)) heavily overlap entities within clusters despite inter-cluster separability.\nThe geometric distance model TransERR (Figure4(f)) clusters are indistinguishable from each other and entities within the clusters (intra-clusters) are distinguishable.\n\nTable4summarizes our analysis above, which we attribute to the fact that our model combines semantic matching with entity geometric distance to better measure the plausibility of triplets.\n\nSECTION: 6.5Visualization Ablation Analysis\n\nIn Figure5, we visualize that our model removes the distance adaptive embedding in the first epoch.\nWe can find that the visualization without the distance adaptive embedding (Figure5(b)) is worse than the with one (Figure5(a)).\nBy visualizing the ablation experiments, we can further illustrate the advantage of distance adaptive embedding.\n\nSECTION: 7Conclusion\n\nWe note that existing quaternion models based on semantic matching diminishes the separability of entities, while the distance scoring function weakens the semantics of entities.\nTo address this issue, we propose a novel quaternion knowledge graph embedding model.\nBy combining semantic matching with entity geometric distance, our model provides a robust and comprehensive framework for knowledge graph embedding.\nWe provide mathematical proofs to demonstrate our model can handle complex logical relationships.\nVisualization results show that our model can learn the geometric distance property between entities to achieve both inter-cluster and intra-cluster separability.\n\nSECTION: Limitations\n\nThe H@1 metric performance of our model on the WN18 and WN18RR datasets is not optimal.\nIn addition, like most knowledge graph embedding models, our model is unable to predict new entities that do not exist in the training data.\n\nSECTION: Acknowledgements\n\nThis work is supported by National Natural Science Foundation of China (No.62066033);\nInner Mongolia Natural Science Foundation (Nos.2024MS06013, 2022JQ05);\nInner Mongolia Autonomous Region Science and Technology Programme Project (Nos.2023YFSW0001, 2022YFDZ0059, 2021GG0158);\nWe also thank all anonymous reviewers for their insightful comments.\n\nSECTION: References\n\nSECTION: Appendix\n\nSECTION: Appendix AProof\n\nGiven, whereis a unit quaternion after normalization operation.\nWe can makeand then our scoring function can be simplified as follows:\n\nwhereis the Hamilton product,denotes the element-wise product, and \u201c\u201d is the inner product.\n\nSECTION: A.1Proof of Symmetry pattern\n\nIn order to prove the symmetry pattern, we need to prove the following equality:\n\nThe symmetry property of DaBR can be proved by setting the imaginary parts ofto zero.\n\nSECTION: A.2Proof of Antisymmetry pattern\n\nIn order to prove the antisymmetry pattern, we need to prove the following inequality when imaginary components are nonzero:\n\nWe expand the right term:\n\nWe can easily see that those two terms are not equal as the signs for some terms are not the same.\n\nSECTION: A.3Proof of Inversion pattern\n\nTo prove the inversion pattern, we need to prove that:\n\nWe expand the right term:\n\nWe can easily check the equality of these two terms.\nSinceis a unit quaternion, we have.\n\nSECTION: A.4Proof of Composition pattern\n\nFor composition relationships, we can get that:\n\nSECTION: Appendix BDataset statistics\n\nThe detailed statistics of the four standard datasets are shown in Table6.\n\nSECTION: Appendix COptimal hyper-parameters\n\nTable7shows the optimal hyperparameter settings for our model on the four benchmark datasets. The optimal parameters come from the highest scores of our model on the validation dataset.\n\nSECTION: Appendix DClassification rules\n\nThe classification rules and classification results for WN18RR dataset in the Table8.\n\nSECTION: Appendix EThe queries in t-SNE visualization\n\nIn Table5, we list the nine queries used in the t-SNE visualization (Section6.4in the main text).\nNote that a query is represented as, wheredenotes the head entity anddenotes the relation.\n\nSECTION: Appendix FMore visualization results\n\nFigure6shows more visualization results.", "text_file": "data\\paper_texts\\2412.04076v1_content.txt"}, {"title": "LuxEmbedder: A Cross-Lingual Approach to Enhanced Luxembourgish Sentence\n  Embeddings", "authors": ["Fred Philippy", "Siwen Guo", "Jacques Klein", "Tegawend\u00e9 F. Bissyand\u00e9"], "published_date": "2024-12-04T14:02:12Z", "summary": "Sentence embedding models play a key role in various Natural Language\nProcessing tasks, such as in Topic Modeling, Document Clustering and\nRecommendation Systems. However, these models rely heavily on parallel data,\nwhich can be scarce for many low-resource languages, including Luxembourgish.\nThis scarcity results in suboptimal performance of monolingual and\ncross-lingual sentence embedding models for these languages. To address this\nissue, we compile a relatively small but high-quality human-generated\ncross-lingual parallel dataset to train LuxEmbedder, an enhanced sentence\nembedding model for Luxembourgish with strong cross-lingual capabilities.\nAdditionally, we present evidence suggesting that including low-resource\nlanguages in parallel training datasets can be more advantageous for other\nlow-resource languages than relying solely on high-resource language pairs.\nFurthermore, recognizing the lack of sentence embedding benchmarks for\nlow-resource languages, we create a paraphrase detection benchmark specifically\nfor Luxembourgish, aiming to partially fill this gap and promote further\nresearch.", "arxiv_id": "2412.03331v2", "html_link": "https://arxiv.org/html/2412.03331v2", "search_term": "ti:\"embeddings\"", "html_content": "SECTION: LuxEmbedder: A Cross-Lingual Approach to Enhanced Luxembourgish Sentence Embeddings\n\nSentence embedding models play a key role in various Natural Language Processing tasks, such as in Topic Modeling, Document Clustering and Recommendation Systems. However, these models rely heavily on parallel data, which can be scarce for many low-resource languages, including Luxembourgish. This scarcity results in suboptimal performance of monolingual and cross-lingual sentence embedding models for these languages. To address this issue, we compile a relatively small but high-quality human-generated cross-lingual parallel dataset to trainLuxEmbedder, an enhanced sentence embedding model for Luxembourgish with strong cross-lingual capabilities. Additionally, we present evidence suggesting that including low-resource languages in parallel training datasets can be more advantageous for other low-resource languages than relying solely on high-resource language pairs. Furthermore, recognizing the lack of sentence embedding benchmarks for low-resource languages, we create a paraphrase detection benchmark specifically for Luxembourgish, aiming to partially fill this gap and promote further research.111https://github.com/fredxlpy/LuxEmbedder\n\nLuxEmbedder: A Cross-Lingual Approach to Enhanced Luxembourgish Sentence Embeddings\n\nFred Philippy1,2,\nSiwen Guo1,\nJacques Klein2,\nTegawend\u00e9 F. Bissyand\u00e921Zortify S.A., Luxembourg2University of Luxembourg, Luxembourg{fred, siwen}@zortify.com{jacques.klein, tegawende.bissyande}@uni.lu\n\nSECTION: 1Introduction\n\nThe development of sentence embedding models has been instrumental in applications such as Bitext Mining(Artetxe and Schwenk,2019), Information Retrieval(Thakur et\u00a0al.,2021), and most recently Retrieval Augmented Generation(Lewis et\u00a0al.,2020). Generative Large Language Models are not capable of handling these tasks as effectively, making sentence embedding models crucial in these areas. However, these models depend on large-scale parallel data to function effectively, a resource readily available for high-resource languages but sorely lacking for low-resource languages(Zhou et\u00a0al.,2018).\n\nOne way to address this issue is to apply cross-lingual sentence embedding models(Chidambaram et\u00a0al.,2019; Artetxe and Schwenk,2019; Reimers and Gurevych,2020; Yang et\u00a0al.,2020; Feng et\u00a0al.,2022; Wang et\u00a0al.,2022), which aim to embed various languages into a common shared representation space. This approach is intended to boost the performance of low-resource languages by leveraging cross-lingual transfer, where knowledge gained from high-resource languages contributes to the understanding and processing of low-resource languages. However, due to the significant differences in data availability, these models still exhibit a large performance gap between high-resource and low-resource languages.\n\nLuxembourgish, a West-Germanic language spoken by about 400\u2009000 people, is one of the many languages that face this challenge. While translation models for Luxembourgish exist(NLLB Team et\u00a0al.,2022; Song et\u00a0al.,2023), their performance remains significantly inferior to that of high-resource languages, hindering the creation of parallel data using methods like back-translation. This limitation also applies to general-purpose generative LLMs, making the direct creation of synthetic parallel data impractical as well. Our research aims to address this issue by collecting a comprehensive set of high-quality human-generated cross-lingual parallel data specifically for Luxembourgish. With this data, we train a sentence embedding model,LuxEmbedder, tailored specifically for Luxembourgish by leveraging cross-lingual transfer.\n\nAlthough cross-lingual sentence embedding models harness the strength of cross-lingual transfer to improve low-resource language performance, we argue that this does not eliminate the necessity for parallel data in these languages. Our findings demonstrate that incorporating these languages in parallel training datasets is essential, as it significantly improves alignment within cross-lingual models, particularly among other low-resource languages, in contrast to relying solely on high-resource language parallel data.\n\nAnother major challenge is the evaluation of sentence embedding models in low-resource languages, given that the primary benchmarks, such as MTEB(Muennighoff et\u00a0al.,2023)and BEIR(Thakur et\u00a0al.,2021), predominantly support English and a few other high-resource languages.\nTo address this, we establish a new paraphrase detection benchmark for Luxembourgish, facilitating future research and improving the language\u2019s representation in NLP. To thoroughly evaluate our enhanced model,LuxEmbedder, we use our own benchmark along with three additional evaluation tasks. The results indicate thatLuxEmbedderoutperforms not only other open-source models but also proprietary models in the majority of cases.\n\nSECTION: 2Dataset & Benchmark Construction\n\nWe create cross-lingual parallel data and a Luxembourgish paraphrase detection benchmark. See AppendixAfor details and Figure1for an overview.\n\nSECTION: 2.1Cross-Lingual Parallel Data (LuxAlign)\n\nWe collect news articles from RTL.lu, a Luxembourgish news platform that publishes in Luxembourgish (LB), English (EN), and French (FR). Due to the lack of explicit mapping between language versions, we use the OpenAI text embedding modeltext-embedding-3-small222https://platform.openai.com/docs/guides/embeddings/embedding-modelsto align articles across language pairs. LaBSE(Feng et\u00a0al.,2022)is then employed to extract parallel sentences from these aligned pairs for LB-FR and LB-EN.\n\nSECTION: 2.2Luxembourgish Paraphrase Detection (ParaLux) Benchmark\n\nThen, we repeat the same process but focusing exclusively on Luxembourgish articles. Within each article, using the same setup, we extract parallel sentences, which can be considered near-paraphrases, from which we hand-pick high-quality samples for our benchmark. From these paraphrased pairs, we promptGPT-4o333https://openai.com/index/hello-gpt-4o/to generate adversarial negative samples for each pair. Given its limited language capabilities in Luxembourgish, the generated adversarial negative samples are then checked and, if needed, corrected by a human annotator to ensure high quality and accuracy.\n\nThrough this methodology, we gather 25\u2009996 LB-EN, 86\u2009293 LB-FR samples forLuxAlign, and 312 samples forParaLux.\n\nSECTION: 3LuxEmbedder\n\nSECTION: 3.1Training\n\nGiven its cross-lingual capabilities and its already existing support of Luxembourgish, we use LaBSE(Feng et\u00a0al.,2022)as our base model, which we further train on both LB-EN & LB-FR parallel subsets fromLuxAlign.\n\nWe train the model using a batch size of 16 for 3 epochs with a constant learning rate ofusing a contrastive loss function. We reserve 1% of the data for evaluation, on which we evaluated every 500 steps, and retained the model with the best loss on the development set. The negative pairs for the loss function are created by randomly pairing each Luxembourgish sentence with the translation of another sentence from the dataset.\n\nSECTION: 3.2Evaluation\n\nWe comprehensively compareLuxEmbedder\u2019s performance across multiple tasks against a variety of open-source and proprietary baseline models.\n\nSECTION: 3.3Baselines\n\nWe provide more details on the used models in AppendixB.2.1.\n\nDeveloped by Cohere,embed-multilingual-light-v3.0andembed-multilingual-v3.0are multilingual embedding models, designed to handle over 100 languages, including Luxembourgish, producing embeddings of size 384 and 1\u2009024, respectively.\n\nOpenAI\u2019stext-embedding-3-smallandtext-embedding-3-largemodels generate embeddings with dimensions of 1\u2009536 and 3\u2009072, respectively. Despite the native API feature for embedding shortening, we use the full dimensions in our experiments. While these models have been assessed on the multilingual MIRACL benchmark(Zhang et\u00a0al.,2023), there is no official information on the number of supported languages.\n\nWe also compareLuxEmbedderagainst two open-source multilingual sentence embedding models that support Luxembourgish. These models are LaBSE(Feng et\u00a0al.,2022), which generates cross-lingual sentence embeddings for 109 languages, and LASER(Artetxe and Schwenk,2019; Heffernan et\u00a0al.,2022), which incorporates a multilingual teacher sentence embedding model and language-specific student models for 200 languages.\n\nWe further extend our evaluation to include mBERT, a multilingual BERT(Devlin et\u00a0al.,2019), variant pre-trained on 104 languages, and LuxemBERT(Lothritz et\u00a0al.,2022), a monolingual Luxembourgish BERT model. In our experiments, we leverage both CLS embeddings and MEAN-pooled embeddings from these models.\n\nSECTION: 3.4Evaluation Tasks\n\nAdditional details on the specific evaluation setup can be found in AppendixB.2.2.\n\nUsing SIB-200(Adelani et\u00a0al.,2024), a 7-class classification dataset, we perform similarity-based zero-shot classification. First, we fill each label into a pre-defined template sentence, and separately encode both the input document and all potential template-embedded labels. Then, the class with the most similar embedding to the input document is chosen, assessing the model\u2019s ability to generalize to new, unseen tasks without any task-specific training. To account for variability, we repeat this process for 5 different label templates and report the average performance.\n\nFor cross-lingual transfer performance, we use the embeddings generated by the respective model to fine-tune a classifier on the SIB-200 dataset in six different high-resource source languages and evaluate directly on the Luxembourgish test set.\n\nWe evaluate the model\u2019s proficiency in accurately retrieving or matching parallel sentence pairs from a bilingual corpus using the Tatoeba dataset. Since the original Tatoeba test set(Artetxe and Schwenk,2019)does not include Luxembourgish, we use the LB-EN, LB-NL, and LB-DE test sets developed by theTatoeba Translation Challenge(Tiedemann,2020).\n\nLastly, we evaluate the model on our newly created benchmark for paraphrase detection. This task involves determining which of two sentences is a paraphrase of a given anchor sentence. It tests the model\u2019s ability to discern nuanced semantic equivalence, which is critical for applications like plagiarism detection, question answering, and information retrieval.\n\nSECTION: 3.5Results\n\nLuxEmbedderdemonstrates superior performance among open-source models in all four tasks and even outperforms all tested proprietary models in 3 out of 4 tasks (Table1). Onlytext-embedding-3-largemodel shows superior cross-lingual transfer performance.\n\nIn particular, we observe considerable improvements inLuxEmbedder\u2019s performance on both monolingual tasks, Zero-Shot Classification and Paraphrase Detection, relative to its base model, LaBSE. This confirms the efficacy of our cross-lingual approach for Luxembourgish.\n\nSECTION: 4Cross-Lingual Alignment\n\nIn this section, we investigate the impact of fine-tuning models on parallel data for cross-lingual alignment between and within high-resource (HR) and low-resource (LR) languages.\n\nTo measure the cross-lingual alignment, we use Flores-200(NLLB Team et\u00a0al.,2022), which includes parallel sentences across 200 languages, making it an ideal resource for assessing cross-lingual alignment. We use the Centered Kernel Alignment (CKA) method(Kornblith et\u00a0al.,2019)to calculate the level of alignment by comparing the embeddings of parallel sentences from different languages.\n\nWe fine-tune LaBSE on three different language pairs: LB-EN, LB-FR, and EN-FR444Created using the same process as described in \u00a72.1., each time using 20\u2009000 parallel sentences from our newly compiled datasets. After fine-tuning, we assess cross-lingual alignment by comparing alignmentwithinHR languages and LR languages, as well asbetweenLR and HR languages555As HR and LR languages we select the 10 languages with the most and least training data in LaBSE which are also covered by Flores-200..\n\nOur observations (Figure2) reveal that when fine-tuning on parallel data, the alignment within the model generally increases. HR languages benefit equally from fine-tuning on any of the three language pairs. However, we observe that the alignment of LR languages benefits more when Luxembourgish is part of the training data compared to fine-tuning on HR language pairs alone.\n\nThese results indicate the critical importance of including LR languages, such as Luxembourgish, when collecting parallel data. Incorporating LR in the training process enhances cross-lingual alignment, not only for the respective language pair but also for other LR languages, more effectively than focusing solely on HR languages.\n\nSECTION: 5Conclusion\n\nSentence embedding models struggle with low-resource languages due to a shortage of parallel data. To address this problem, we collected high-quality, human-generated cross-lingual parallel data for Luxembourgish and developed an enhanced version of a cross-lingual sentence embedding model specifically adapted to Luxembourgish. This model outperforms open-source as well as proprietary models in almost all evaluations conducted in our study. Our findings also stress the importance of incorporating low-resource languages in parallel data collection, as evidence suggests that this enhances embedding alignment for both the target language and other low-resource languages within the same model more effectively than using high-resource language pairs alone. Therefore, we believe this research encourages further creation of parallel corpora for low-resource languages.\n\nSECTION: Limitations\n\nIt is important to note that we do not compare our embedding model against general-purpose generative LLMs. We acknowledge that some of these models, which are significantly larger in terms of parameter count, may outperformLuxEmbedderin certain tasks. Nonetheless, the primary objective of our paper is not to compete with generative models . Instead, our focus is on providing a robust sentence embedding model capable of solving specific tasks such as information retrieval, document clustering, and similar applications where generative language models may not be as effective.\n\nAdditionally, we acknowledge that our data is limited to the news domain, due to its availability. However, our goal is to use this data to boost the model\u2019s retrieval performance, facilitating future expansion into various other domains by mining a more diverse range of parallel data.\n\nSECTION: Ethical Statement\n\nIn the newly createdParaLuxbenchmark, the adversarial counterparts of the paraphrases have been edited in a way that some of the edited sentences may contain non-factual information. Therefore, we strongly recommend using this data solely, as designed, for evaluation purposes and not for training, to ensure the integrity of model development.\n\nFurthermore, our datasets, based on news articles, naturally include the names of individuals. As the text is publicly available and anonymization would greatly diminish data quality, we chose not to anonymize it. We believe that preserving the original context of publicly accessible information is essential for maintaining data integrity and the effectiveness of our research.\n\nSECTION: Acknowledgments\n\nWe are grateful to RTL Luxembourg for providing the raw data necessary for our research. Their support significantly facilitated our efforts.\n\nSECTION: References\n\nSECTION: Appendix AData Collection & Processing\n\nHere, we outline the method used to create cross-lingual training data and the paraphrase detection benchmark, providing examples in Tables2and3.\n\nWe gather news articles from the Luxembourgish news platform RTL666https://www.rtl.luwritten in Luxembourgish, French, and English, covering different time periods: from January 1, 1999 for Luxembourgish, from September 1, 2011 for French, and from January 1, 2018 for English, up until May 10, 2024. We first remove all URL tags and extraneous metadata, and filter out articles with fewer than 100 characters, as these are often just traffic or sports updates, which were not relevant for our study. To ensure linguistic accuracy, we use the OpenLID(Burchell et\u00a0al.,2023)to identify and exclude articles that are not in the intended language.\n\nSubsequently, we embed each article using the OpenAItext-embedding-3-smallmodel to facilitate cross-language article matching. To identify potential parallel articles in different languages, we first narrow down the candidates by considering only those articles published within a one-day window of the target article. Among these candidates, we select the one with the highest cosine similarity to the target article\u2019s embedding, provided the similarity score exceeds 0.65.\n\nIn parallel, we extract sentences from each article using the NLTK777https://www.nltk.orglibrary. For Luxembourgish, in the absence of a dedicated sentence tokenizer, we use the German tokenizer. After splitting the articles into sentences, we employ OpenLID once again to remove any sentences identified as being in the wrong language. Additionally, we filter out sentences with fewer than 10 characters or fewer than three words.\n\nNext, we embed each sentence using LaBSE, focusing on sentences from articles already matched with articles in another language. For each sentence, we restrict the candidates to sentences from the corresponding matched article, minimizing the risk of false positives. We then select the candidate sentence with the highest cosine similarity, provided it exceeds a similarity threshold of 0.7. After identifying all sentence pairs, we filter out pairs where the length difference is greater than 50%. To create a seed dataset forParaLux, we replicate this process within Luxembourgish articles alone.\n\nSECTION: Appendix BTraining and Evaluation Details forLuxEmbedder\n\nAll our training processes and experiments were run on 4 A100 GPUs within a few hours.\n\nSECTION: B.1Training\n\nGiven a sentence embedding modelwith parameters, for a sentence pairand its label(1 if positive pair, 0 if negative pair), the contrastive loss function is defined as:\n\nwhere\n\nis the margin value, defining the minimum distance that samples withing a negative pair should have\n\nwithandbeing the cosine distance in our experiments.\n\nSECTION: B.2Evaluation\n\nDue to the proprietary nature of Cohere\u2019s models,embed-multilingual-light-v3.0andembed-multilingual-v3.0, as well as OpenAI\u2019stext-embedding-3-smallandtext-embedding-3-large, detailed information about their training data and model architecture is not publicly available. We refer readers to their online documentation888https://cohere.com/blog/introducing-embed-v3999https://openai.com/index/new-embedding-models-and-api-updates/for any details.\n\nOur experiments with open-source models involve base multilingual BERT (cased)(Devlin et\u00a0al.,2019)and LuxemBERT(Lothritz et\u00a0al.,2022). These models feature identical architectures, including 12 attention heads and 12 transformer blocks, each with a hidden size of 768. mBERT\u2019s vocabulary size is 30\u2009000, whereas LuxemBERT\u2019s is 119\u2009547. Both models have about 110 million parameters.\n\nAdditionally, we incorporate LaBSE(Feng et\u00a0al.,2022), which also serves as the foundational model forLuxEmbedder. LaBSE is derived from the base multilingual BERT (cased) but features an expanded vocabulary of 501\u2009153 tokens. It has been trained using a combination of monolingual data and bilingual translation pairs.\n\nTo assess cross-lingual transfer performance, we use embeddings from the respective model to fine-tune a classifier on the SIB-200(Adelani et\u00a0al.,2024)dataset in several high-resource source languages, then evaluate it directly on the Luxembourgish test set.\n\nThe SIB-200 dataset includes over 200 languages, with 701 training, 99 development and 204 test samples per language.\n\nIn our experiments, however, we only train separately on French, English, German, Japanese, Chinese, and Russian. Additionally, we fine-tune on Luxembourgish, but this is not included in the average performance reported in Table1. The classifier is a simple linear layer with 7 output nodes, trained with the Adam optimizer and the cross-entropy loss function. Training is performed for 500 epochs with a constant learning rate of. We evaluate the classifier once per epoch and select the model with the best development loss. Each training process is repeated 4 times using different seeds to ensure robustness, and we report the average performance per source language in Table4.\n\nTo assess the zero-shot classification capabilities of different model, we again use the SIB-200 dataset(Adelani et\u00a0al.,2024). We independently encode the input and all potential labels, integrating each label within a prompt template. The class whose embedding has the highest cosine similarity to the input document is selected.\n\nWe use five different prompt templates to evaluate the classification performance and report the average performance per template in Table5. These templates are:\n\n[LABEL]\n\nAn d\u00ebsem Beispill geet et em [LABEL].This example is about [LABEL].\n\nD\u2019Thema vun d\u00ebsem Text ass [LABEL].The topic of this text is [LABEL].\n\nHei g\u00ebtt iwwer [LABEL] geschwat.Here we are talking about [LABEL].\n\nD\u00ebst Dokument besch\u00e4ftegt sech mat [LABEL].This document deals with [LABEL].\n\nThe labels in Luxembourgish we use in this classification task areTechnologie(technology),Reesen(travel),Politik(politics),Gesondheet(health),Ennerhalung(entertainment),Geographie(geography) andSport(sports).\n\nWe initially considered the Tatoeba dataset, but it lacks Luxembourgish in the original set. Instead, we used Luxembourgish-English, Luxembourgish-Dutch, and Luxembourgish-German test sets from theTatoeba Translation Challenge(Tiedemann,2020), which include 346 LB-EN, 291 LB-EN, and 292 LB-DE sample pairs.101010https://huggingface.co/datasets/Helsinki-NLP/tatoeba_mtWe conducted experiments in both retrieval directions and reported the full results in Table6.\n\nTo assess performance onParaLux, the model encoded the anchor sentence and both paraphrase candidates. The candidate with the greatest cosine similarity to the anchor was chosen as the predicted paraphrase.\n\nSECTION: Appendix CFull Results\n\nHere, we report the full experimental results from the evaluations on Cross-Lingual Transfer (Table4), Zero-Shot Classification (Table5) and Bitext Mining (Table6) conducted in Section3.5.\n\nSECTION: Appendix DDetails on the Cross-Lingual Alignment Experiments\n\nIn Section4, we measure the alignment of language-specific subspaces using the Centered Kernel Alignment (CKA) methodKornblith et\u00a0al. (2019). The CKA score of two representation matricesand, whereis the number of samples andis the embedding dimension of the model, when using a linear kernel, is given by\n\nwhereis the Frobenius norm.\n\nSince parallel cross-lingual data is essential for computing the CKA across various languages, we use the Flores-200 dataset(NLLB Team et\u00a0al.,2022), which includes human-curated translations between English and 204 other languages. Specifically, we use the devtest split, containing 1\u2009012 aligned sentences per language.\n\nWe choose the 10 languages with the highest and lowest amounts of training data in LaBSE, which are also included in Flores-200, to represent the HR and LR languages. As LR languages, we usebod,snd,tuk,ydd,wol,asm,smo,xho,nya, andsot. As HR languages, we useeng,rus,jpn,zho,fra,deu,por,nld,spa, andpol.\n\nThe exact CKA values across all language pairs are provided in Figure3.", "text_file": "data\\paper_texts\\2412.03331v2_content.txt"}, {"title": "Embedding formalism for ${\\mathcal N}$-extended AdS superspace in four\n  dimensions", "authors": ["Nowar E. Koning", "Sergei M. Kuzenko", "Emmanouil S. N. Raptakis"], "published_date": "2023-08-08T08:48:14Z", "summary": "The supertwistor and bi-supertwistor formulations for ${\\mathcal N}$-extended\nanti-de Sitter (AdS) superspace in four dimensions, ${\\rm AdS}^{4|4\\mathcal\nN}$, were derived two years ago in arXiv:2108.03907. In the present paper, we\nintroduce a novel realisation of the ${\\mathcal N}$-extended AdS supergroup\n$\\mathsf{OSp}(\\mathcal{N}|4;\\mathbb{R})$ and apply it to develop a coset\nconstruction for ${\\rm AdS}^{4|4\\mathcal N}$ and the corresponding differential\ngeometry. This realisation naturally leads to an atlas on ${\\rm\nAdS}^{4|4\\mathcal N}$ (that is a generalisation of the stereographic projection\nfor a sphere) that consists of two charts with chiral transition functions for\n${\\mathcal N}>0$. A manifestly $\\mathsf{OSp}(\\mathcal{N}|4;\\mathbb{R})$\ninvariant model for a superparticle in ${\\rm AdS}^{4|4\\mathcal N}$ is proposed.\nAdditionally, by employing a conformal superspace approach, we describe the\nmost general conformally flat $\\mathcal N$-extended supergeometry. This\nconstruction is then specialised to the case of ${\\rm AdS}^{4|4\\mathcal N}$.", "arxiv_id": "2308.04135v3", "html_link": "https://arxiv.org/html/2308.04135v3", "search_term": "ti:\"embeddings\"", "html_content": "SECTION: 1Introduction\n\nThe simplest AdS superspace in four dimensions,,\nwas introduced in the early years of supersymmetry by Keck[2]and Zumino[3]as the coset superspace111We remind the reader thatis the double covering group of the connected component of the Lorentz group,.\n\nand the thorough study of general superfield representations onwas given by Ivanov and Sorin[4]. It was also realised thatoriginates as a maximally supersymmetric solution in the following off-shell formulations forsupergravity:\n(i) the old minimal supergravity[5,6,7,8]with a cosmological term[9],\nsee[10,11]for a review; and (ii) the non-minimal AdS supergravity[12].\n\nThe group-theoretic realisation (1.1) ofAdS superspace has a natural extension to thecase (see, e.g.,[13])\n\nThe description ofas a maximally supersymmetric solution in the\nminimal off-shell formulation forsupergravity\nwith a cosmological term, developed by de Wit, Philippe and Van Proeyen[14],\nwas given in[15,16,17].222Puresupergravity in four dimensions was constructed by Ferrara and van Nieuwenhuizen in 1976[18], and puresupergravity with a cosmological term was constructed by Freedman and Das[19].\n\nThe conformal flatness of AdS4|4was first established in[4], and it was later re-derived in textbooks[10,11]within the supergravity framework.\nThe superconformal flatness ofwas demonstrated in[20]for arbitrary.\nAlternative proofs of the conformal flatness ofwere given in[16,21]using the off-shellsupergravity framework. Ref.[22]described different conformally flat realisations for AdS4|4and AdS4|8which are based on the use of Poincar\u00e9 coordinates.\n\nIn the non-supersymmetric case, there exist two different\nbut equivalent realisations of AdSd:\n(i) as the coset space; and (ii) as\na hypersurface in\n\nBoth realisations of AdSdhave found numerous applications in the literature.\nAs regards, only the coset superspace realisation (1.2)\nhad existed for many years.\nThe supertwistor and bi-supertwistor formulations forhave recently been developed[1].\nAnalogous results in three dimensions have been derived in[1,23].\nIn this paper we elaborate on the superembedding formalism333For a pedagogical review of superembeddings see[24,25].for.\n\nSince the work by Ferber[26], supertwistors have found numerous applications in theoretical and mathematical physics. In particular,\nsupertwistor realisations of compactified-extended Minkowski superspaces\nhave been developed in four[27,28]and three[29,30]dimensions, and their harmonic/projective extensions have been derived[31,32,29,33,34,35,36,30,37].444Similar ideas were applied in Ref.[38]to develop\nsupertwistor realisations of the-extended supersphere,\nwith, as a homogeneous space of the three-dimensional\nEuclidean superconformal group.Recently, supertwistor formulations for conformal supergravity theories in diverse dimensions have been proposed[39,40].\nTo the best of our knowledge, the supertwistor realisations of AdS superspaces in three and four dimensions have been given only in[1,23], although\n(super)twistor descriptions of (super)particles in AdS spaces had been studied in the literature earlier[41,42,43,44,45,46,47,48,49,50](see also[51]).555We are grateful to Alex Arvanitakis for\nbringing Ref.[51]to our attention.\n\nThis paper is organised as follows. In section2we give a brief review of the (bi)supertwistor description of AdS4|4Nand present a manifestlyinvariant model for a superparticle in.\nSection3is devoted to presenting a novel realisation of the AdS supergroup, which is then used in section4to develop a coset construction for. The coset construction is applied in section5to work out the differential geometry of.\nIn section6, by employing the framework of conformal superspace, we describe the most general conformally flat supergeometry and then specialise our construction to the case of.\nThe main body of the paper is accompanied by several technical appendices.\nAppendixAincludes essential definitions concerning the supergroupand corresponding supertwistors.\nAppendixBprovides a review of the conformally flat atlas for AdSd.\nIn appendixC, we spell out the-extended superconformal algebra.\n\nOur two-component spinor notation and conventions follow[11], and are similar to those adopted in[52]. The only difference is that the spinor Lorentz generatorsandused in[11]have an extra minus sign as compared with[52],\nspecificallyand.\n\nSECTION: 2The (bi)supertwistor description of AdS4|4N\n\nIn this section we give a brief review of the construction of[1]. The reader is referred to appendixAfor the technical details concerning the supergroupand supertwistors.\n\nAssociated with the space of even complex supertwistors,,\nis a Grassmannian of even two-planes.\nGiven such a two-plane, it is spanned by two even supertwistors,\n\nThe property ofbeing a two-plane means that the bosonic bodies ofandare linearly independent complex four-vectors.\nAn arbitrary elementis a linear combination, with the coefficientsbeing even elements of the Grassmann algebra.\nBy construction, the supertwistors (2.1) are defined modulo the equivalence relation\n\nsince bothanddefine the same two-plane.\n\nWe restrict our attention to the subset of those two-planes which satisfy the constraints\n\nHere (2.3a) refers to the body of thesupermatrix, anddenotes the conjugate of a pure supertwistor, see eq. (A.27).\nThe conditions (2.3) imply that the bodies of the four even supertwistorsform a basis for, in particular the supertwistors (2.1) generates a two-plane.\nWe emphasise that the conditions (2.3) are invariant under the equivalence transformations (2.2).\nIn what follows, the supertwistorwill be denoted.\n\nWe say that any pair of even supertwistors, eq. (2.1),\nconstrained by the conditions (2.3) constitutes a frame.\nThe space of frames will be denoted. The supergroupacts onby the rule\n\nThis group action is naturally extended to the quotient space. The latter proves to be a homogeneous space of, which was identified in[1]with the AdS superspace,\n\nGiven two frames,\none can construct the following-invariant two-point functions:\n\nwitha fixed positive parameter.\nThey do not change ifandare replaced by their equivalent frames (2.2), and therefore these-invariant two-point functions are well defined on. In the non-supersymmetric case,, the three two-point functions (2.6) coincide.\n\nGiven a point in, we associate with it the graded antisymmetric matrices\n\nThese supermatrices are invariant under arbitrary equivalence transformations (2.2),\nand therefore they may be used to parametrise AdS4|4N.\nThe bi-supertwistors (2.7) have the following properties:\n\nwheredenotes the graded antisymmetric\npart of.\nUsing the results of[36], the bi-supertwistor description of AdS4|4Ndefined by (2.8) may be shown to be equivalent to the supertwistor one described earlier.\n\nRestricting the above bi-supertwistor realisation of AdS4|4Nto thecase gives the bi-twistor formulation of AdS4, which in turn\nleads to a standard embedding formalism for AdS4. Building on the analysis given in section 3.3 of[1], it may be used to derive the reality condition\n\nHereanddenote\nthe-traceless parts ofand, respectively,\n\nAssociated withis areal5-vector\n\nHerearerealmatrices which obey the anti-commutation relations\n\nand are characterised by the property\n\nThe explicit realisation ofis given, e.g., in[30].\nMaking use of the completeness relation\n\nwe obtain\n\nIt may be shown that the bi-twistor description of AdS4is equivalent to the bi-spinor formalism introduced in[53].\n\nSince the two-point functions (2.6) are invariant under arbitrary equivalence transformations (2.2), they can be expressed in terms of the bi-supertwistors\n(2.7). In terms of the supermatricesanddefined by\n\nthese expressions have the form:\n\nWe point out that thetransformation (2.4) acts onandas follows\n\nThe bi-supertwistor realisation described above facilitates the construction of manifestlyinvariant models.\nIndeed, let us consider the following worldline action for a superparticle on AdS4|4N\n\nwhereparametrises the world line,denotes the einbein,is a real dimensionless parameter, andis a mass parameter. We can see that in the non-supersymmetric case,,\nthe-term is absent,\nsince the three two-point functions (2.6) coincide.\n\nSECTION: 3Isomorphic realisation of the AdS supergroup\n\nThe supergrouppossesses an alternative realisation,\nwhich we introduce below and which turns out to be useful for applications.\nThere is a simple motivation to look for such a realisation. To explain it, we consider the non-supersymmetric case,. It follows from (2.3a)\nthat for every frame\n\nthematricesandare non-zero. In the framework of the coset construction, however, it would be useful to deal with an isomorphic realisation ofthat would allow a frame such that eitheror.\n\nLet us consider a supergroup, denoted,\nconsisting of\nall evensupermatricessubject to the following constraints:\n\nHere we have introduced the graded antisymmetricsupermatrix\n\nwith\n\nIn what follows, we will denote the components of the matrices (3.12) asand, which is why we prefer to use\nthe notationinstead of.\n\nThe supergroupproves to be isomorphic to.\nThe proof is based on considering the following supermatrix correspondence:\n\nfor every supertwistor. Here the supermatrixis defined as\n\nIt obeys the useful properties:\n\nand\n\nThese conditions imply that\n\nAssociated withare two invariant inner products defined as\n\nfor arbitrary pure supertwistorsand.\nThe conditions (3.4) impose restrictions on the blocks of. For\n\nthese are:\n\nand\n\nIn the original realisation ofthe reality condition could be realised as the coincidence of the supertranspose and the Hermitian conjugate, eq. (A.26b).\nFor our new realisation of the supergroup,\n(A.26b) is replaced with the following condition\n\nFrom this we have the following conditions\n\nWe will now discuss involution for the supertwistors. Since the transformation (3.13b) applies to every supertwistor, we can also consider it applied to. We have\n\nThis acts explicitly on a supertwistoras\n\nThe components ofare given by\n\nLet us introduce a new operation, denoted by, by removing the factor ofin (3.37):\n\nThe components ofare given by\n\nWe therefore have the following reality condition with respect to the map\n\nThe mapis an involution, since it satisfies the property\n\nWe also observe that\n\nwhich, in conjunction with the properties (3.20), yields the following\n\nIt is useful to express the constraints (2.3), the two-point functions (2.6), and the bi-supertwistors (2.7) in terms of the new realisation of the supergroup.\nThe constraints can be expressed as\n\nFor the two-point functions we find\n\nThe bi-supertwistors (2.7) can be expressed in terms of transformed supertwistorsas follows\n\nThey satisfy the following properties\n\nFor the case, the-traceless parts of the bi-supertwistors take the form\n\nAs before, we can express the two-point functions (3.52) in terms of the supermatricesanddefined by\n\nThey then take the form\n\nSECTION: 4Coset construction\n\nThe alternative realisation of the AdS supergroup described in the previous section is ideal for developing a coset construction for.\nTo start with, it is worth recalling some basic definitions, see e.g.[54]for more details. Consider a homogeneous space, whereis a Lie group andis the isotropy subgroup (or stabiliser) of some point. A global coset representative is a bijective mapsuch that, wheredenotes the natural (canonical) projection. For many homogeneous spaces, no global coset representative exists.\nIn such a case, local coset representativeswith the propertycan be introduced on open chartsthat provide an atlas for. In the intersection of two chartsand,,\nthe corresponding coset representativesandare related by a little group transformation,, with.\n\nSECTION: 4.1Isotropy subgroup\n\nAs a marked (preferred) pointof AdS4|4N, we choose\n\nThe stabiliserofconsists of those elementsof the AdS supergroupwhich satisfy the conditions\n\nThese conditions imply that\n\nThus the stability subgroupis isomorphic to\n\nThe bi-supertwistors (3.53) corresponding to the preferred pointtake the form\n\nSECTION: 4.2Generalised coset representative\n\nThe freedom to perform arbitrary equivalence transformations (2.2)\ncan be used to fine-tune the conditions (3.51) to\n\nfor a fixed positive parameter. Such a frame is said to be normalised.\nUnder the condition (4.22a), the equivalence relation\n(2.2) turns into\n\nThe space of normalised frames will be denoted.\nAlong with the definition (2.5) given earlier,\nthe-extended AdS superspace can equivalently be defined as, where the equivalence relation is given by (4.23).\n\nThe conditions (4.22) can be recast in terms of the two-plane\n\nand imply the following constraints:\n\nRelation (4.28a) tells us that at least one of thematricesandis nonsingular.\n\nAssociated with the normalised two-planeis the following group element\n\nThe fundamental property ofis that, for any normalised two-plane.\nWe point out thatis symmetric,. The functional forms of the matricesandare fixed through the conditionand\nthe reality conditions (3.29a) and (3.29c).\nThe remaining blocks are then fixed by the group requirements (3.26) and (3.27).\nIt is possible to obtain alternate expressions for the blocksand, which may be more suited to performing calculations. They take the following form\n\nThese expressions can be seen to coincide with (4.29i) and (4.29j) by using the group requirements and the general form for the inverse of a supermatrix.\n\nThe group elementis characterised by the property\n\nwith. This relation means thatis not a genuine coset representative that is used in\nthe coset construction.\nHowever,will allow us to obtain a coset representative if we pick a single two-plane in each equivalence class.\nThis may be readily done in coordinate charts for.\n\nSECTION: 4.3AdS space ()\n\nAs noted above,\nat least one of thematricesand, see eq. (4.27), is nonsingular.\nTherefore we can naturally introduce two coordinate charts forthat provide an atlas. We define the north chart to consist of all normalised two-planes with. Similarly, the south chart is defined to consist of all normalised two-planes with.\n\nIn the north chart, we can use the freedom (4.23)\nto choose, and then\n\nwhereis a parameter, andare the Pauli matrices. The constraints (4.28a) and (4.28b) give, respectively,\n\nIt follows thatis real and. We also observe that. Since there is still a remnant of the equivalence relation\n(4.23),,\nit can be used to fix. Then we observe that the coordinate chart\nis specified by\n\nand the parameteris given by\n\nThe real coordinatesparametrise AdS4in the north chart.\nDirect calculation of the two-point function (3.52a) in this chart yields\n\nIn the south chart, the gauge freedom (4.23) can be used\nto choose,\nand then\n\nfor some parameter. Now, repeating the north-chart analysis\ntells us that the local coordinatesare real, and the following relations hold:\n\nThe two-point function (3.52a) in the south chart is\n\nIn the intersection of the two charts, the transition functions are\n\nIt follows that.\nComparing the above relations with those described in appendixB,\nwe find complete agreement except for the sign difference (4.47) and (B.10).\n\nSECTION: 4.4\n\nThe analysis of the previous subsection can be extended to the supersymmetric case in a similar fashion.\nLet us consider the north chart in which the matrixin (4.27)\nis nonsingular. The\nequivalence relation (4.23)\ncan once again be used to\nchoose, and then\n\nMaking use of (4.22) leads to the relations\n\nThe former is solved by\n\nWe see that the two-planes (4.51) are parametrised by the chiral coordinatesand, with.\n\nThe coset representative in the north chart is given by\n\nThe two-point function (3.52a) computed in the north chart yields\n\nwhere\n\nIn the non-supersymmetric case,, this reduces to\n(4.41).\n\nIn the south chart, the gauge freedom (4.23) can be used to fix. Repeating the analysis of the north chart leads to\n\nwith\n\nThe former is solved by\n\nWe see that the two-planes in the south chart (4.60) are parametrised by the chiral coordinatesand, with.\n\nThe coset representative in the south chart is given by\n\nThe two-point function (3.52a) computed in the south chart yields\n\nwhere\n\nIn the intersection of the two charts, the transition functions are given by\n\nIn addition, the two coset representatives (4.54c) and (4.63c) are related in the intersection by the point-dependent little group transformation\n\nExplicitly,is given by\n\nWe see thatis chiral, through the transition functions (4.66).\n\nSo far we have only considered the form of the two-planes in the north and south charts. It is also useful to describe the form of the bi-supertwistors (3.53) in an explicit coordinate system. In the north chart they take the form\n\nIt is of interest to compare this supermatrix with a similar result for compactified-extended Minkowski superspace, see eq. (3.17) in[36].\n\nSECTION: 5Superspace geometry\n\nIn this section we give explicit expressions for the vierbein, connection, torsion tensor and curvature tensor. From these expressions the graded commutation relations of the covariant derivatives can be derived.\n\nSECTION: 5.1Geometric structures in AdS4|4N\n\nLet us denote bythe superalgebra of the AdS supergroup, and bythe algebra of the stability group (4.14).\nLetbe the complement ofin,.\nThe superalgebraconsists of even supermatrices\n\nwith.\nThe elementstake the form\n\nAdditionally, the elementstake the form\n\nWith the following row-vector definition\n\nthe elements (5.12) take the form\n\nIt is straightforward to verify.\n\nWe may uniquely decompose the Maurer-Cartan one-formas a sum, whereis the vierbein taking its values in.\nThe Maurer-Cartan one-form is\n\nwhere the blocks are given by\n\nOne can make use of the group conditions (3.26) to recastin an equivalent form\n\nIn the above,is given by\n\nThe Maurer-Cartan one-form (5.22) can be decomposed into supermatrices of the form (5.8) and (5.19) to obtain the vierbein and connection.\nThe connection is\n\nwhere\n\nIt is possible that these expressions may be simplified by using an explicit form for, withgiven by (4.29f), however the above expressions appear most convenient for proving the required properties\n\nThe vierbein is\n\nwhere\n\nandis defined as in (5.23f) or (5.24).\nIt is straightforward to show that (5.42) is Hermitian, using (4.28b).\n\nUsing the above expressions we can now compute the torsionand curvature. In accordance with the coset construction, they are defined as follows:\n\nThere exists another simple expression for bothand, given by\n\nFollowing (5.43), the torsion is given by\n\nwhere\n\nThe curvature is given by\n\nwhere\n\nSECTION: 5.2Covariant derivatives\n\nThe vierbein and connection (as well as curvature and torsion) can be decomposed into the bases corresponding to the superalgebraand the algebra.\nAccordingly, we must introduce a basisfor the superalgebraand a basisfor the algebra.\nThe elementsofandof, given by (5.8) and (5.12), may be written as a linear combination of generators\n\nMaking use of (5.8) and (5.12)\nallows us to read off the graded commutation relations for\nthe generators ofand\n\nThese relations constitute the-extended AdS superalgebra.\n\nThe vierbein and the torsion two-form, as elements of, can be decomposed with respect to the basis as\n\nto obtain the one-formand the the torsion.\nA similar procedure follows for the curvature. We may further decompose the torsion and curvature components as\n\nBuilding on the approach used in[55], we can use (5.44) and the graded commutation relations (5.55) to determine the non-vanishing components of the torsion and curvature to be\n\nThese components can be used to construct the graded commutation relations of the covariant derivatives\n\nThe algebra of covariant derivatives is thus given by\n\nSECTION: 5.3AdS superspace\n\nMany of the expressions in subsection5.1containand. These are, in principle, expressible in terms of,, and. These expressions are, however,-dependent. Below, we will discuss both of these in thecase.\n\nUsing the group requirements (3.26a) we can rearrange for\n\nwhich in thecase yields the following expression\n\nwhere\n\nFurthermore,has the explicit solution\n\nWe can use these expressions to computefrom the vierbein (5.23f). Forit is\n\nwhere\n\nThis expression coincides with (5.27) when considered in thecase.\n\nSECTION: 5.4North and south charts\n\nThe results of subsections5.1,5.2and5.3did not make use of the freedom (4.23) to fix a coordinate system. In this section we will use these results to describe the geometry in thecase for the north and south charts, given by two-planes of the form (4.51) and (4.60).\n\nIn the north chart, the vierbein (5.41) reads\n\nwhereis computed using (5.69) as\n\nIn the above,andare the flatsuperspace vielbeins.\nThe general forms for the vielbeins of a superspace with superconformally flat geometry are\n\nwhereis chiral (antichiral).\nIn our case it is straightforward to compute the coefficients in (5.75), which yields the following expression\n\nIndeed, (5.73) can be shown to take the form\n\nwithandgiven by (5.75). The connection is given by\n\nwhere the components of the connection read\n\nWe\nintroduce the inverseof the vielbein supermatrix,\n\nLet us then define the vector fields\n\nHereare theflat superspace covariant derivatives.\nWe find\n\nThe components of the connectionwere given with respect to the basisin (5.85).\nUsing the inverse vierbein defined by (5.86), the connection can be decomposed into the basis, with which we can then construct explicit expressions for the covariant derivatives\n\nThey take the following form\n\nThe expressions (5.90) can be seen to coincide with the general form for the covariant derivatives of a conformally flat superspace.\n\nIn the south chart, the vierbein (5.41) is given by\n\nwhereis\n\nWe showed in section4.4that the coset representatives in the north and south charts were related by a little group transformation, see (4.67).\nUnder such a transformation, the vierbein and connection transform as follows\n\nWe can see then that the vierbein supermatrix in the north chart is related to that in the south chart by\n\nwhich yields\n\nwithgiven by (4.68e).\nThe vector fieldsare also related in the intersection of the two charts. We find\n\nSECTION: 6Conformally flat supergeometry\n\nThis section is devoted to a description of the most general four-dimensional conformally flat supergeometry. Our approach will be to begin with a general conformally flat superspace whose local structure group is the superconformal group.666Such a supergeometry is known as a conformal superspace. They may also be employed to describe non-conformally flat supergeometries, see e.g.[56,57]for more details.Then, by performing a series of gauge fixings, and passing through the conventionalandsuperspaces, we realise thesupergeometry within this framework.\n\nSECTION: 6.1Conformal superspace: conformally flat geometry\n\nWe consider a conformally flat-extended\nsuperspace, parametrised by local coordinates, where,,and.\nThe structure group is chosen to be, the-extended superconformal group.\nIts corresponding Lie superalgebra,, is spanned by the translation, Lorentz,-symmetryand, dilatation, and the special conformalgenerators, see appendixCfor more details.\nThe geometry of this superspace is encoded within the conformally covariant derivatives, which take the form:\n\nwheredenotes the inverse supervielbein and the remaining superfields are connections associated with the non-translational generators of the superconformal group.\n\nBy definition, the gauge group of conformal supergravity is generated by local transformations of the form\n\nwhere the gauge parameters satisfy natural reality conditions. Given a conformally covariant tensor superfield(with its indices suppressed), it transforms under such transformations as follows:\n\nIn general, the algebra of covariant derivativesshould be constrained such that it: (i) has a super Yang-Mills structure; and (ii) is expressed solely in terms of a single superfield, the super-Weyl tensor. In this section, we will restrict our attention to conformally flat backgrounds, which are characterised by vanishing super-Weyl tensor. As a result, the only non-vanishing sector ofis\n\nSECTION: 6.2Degauging (i):superspace\n\nAccording to eq. (6.2), under an infinitesimal special superconformal gauge transformation, the dilatation connection transforms as follows\n\nThus, it is possible to impose the gauge, which completely fixes\nthe special superconformal gauge freedom.777There is a class of residual gauge transformations which preserve this gauge. They generate the super-Weyl transformations of the degauged geometry.As a result, the corresponding connection is no longer required for the covariance ofunder the residual gauge freedom and\nmay be\nextracted from,\n\nHere the operatorinvolves only the Lorentz and-symmetry connections\n\nThe next step is to relate the special superconformal connectionto the torsion tensor associated with. To do this, one can make use of the relation\n\nIn conjunction with (6.4), this relation leads to a set of consistency conditions that are equivalent to the Bianchi identities of (conformally flat)superspace[58].\nTheir solution expresses the components ofin terms of the torsion\ntensor ofsuperspace and completely determines the algebra.\n\nWe begin by solving eq. (6.8) in thecase. The outcome of this analysis is:\n\nHereis a chiral scalar superfield\n\nWe now pause and comment on the geometry described by. In particular, by employing (6.8) one arrives at the following anti-commutation relation\n\nIt follows that if one performs the shift\n\nthen the-dependent terms in (6.11) vanish.\nThe resulting algebra of covariant derivatives, up to dimension-, takes the form\n\nwhich describes asuperspace[58,10]with vanishing super-Weyl tensor.\n\nAbove we made use of the special conformal gauge freedom to degauge from conformal tosuperspace. Now, we will show that the residual dilatation symmetry manifests in the latter as super-Weyl transformations. To preserve the gauge, every local dilatation transformation should be accompanied by a compensating special conformal one\n\nThis is the case only if the special conformal parameter is\n\nWe now determine what transformation ofand the torsions ofsuperspace this induces. They may be determined by making use of the following relation\n\nSpecifically, one finds that the super-Weyl transformations of the degauged geometry are:\n\nwhich are in agreement with the ones presented in[59]. Additionally, for infinitesimal, these transformations may be obtained from the ones presented in[58].\n\nWe now extend the analysis presented above to thecase. A routine calculation leads to the following expressions for the degauged special conformal connection:\n\nThe dimension-1 superfields introduced above have the following symmetry properties:\n\nand satisfy the reality conditions\n\nThecharges of the complex fields are:\n\nNow, by employing (6.8), we find that the anti-commutation relations for the spinor covariant derivatives are:\n\nAt the same time, the consistency conditions arising from solving (6.8) lead to the Bianchi identities:\n\nNow, in complete analogy with thestory described above, we show how the residual dilatation symmetry of conformal superspace manifests in the present geometry as super-Weyl transformations. It may be shown that the following combined dilatation and special conformal transformation, parametrised by a dimensionless real scalar superfield=, preserves the gauge:\n\nAt the level of the degauged geometry, this induces the following super-Weyl transformations\n\nwhere we have made the definitions:\n\nIn the infinitesimal case, these transformations are a special case of the ones presented in[58].888Recently, the super-Weyl transformations of-extended superspace have been computed within a local supertwistor formulation approach, see[40]for more details.Further, for, these may be read off from the finite super-Weyl transformations presented in[60].\n\nSECTION: 6.3Degauging (ii):superspace\n\nIn the preceeding subsection we have shown that the degauging of the-extended conformally flat supergeometry described in section6.1leads to (conformally flat)superspace. The latter is characterised by the property that its local structure group is. In the present section we will further degauge this geometry by breaking the local-symmetry group down to.\n\nThis procedure consists of the following steps. First, one must eliminate thecurvature. This involves redefiningto absorb such terms in the algebra of covariant derivatives and employing super-Weyl transformations to set the remaining contributions, which describe purely gauge degrees of freedom, to zero. For, this role is played by the chiral spinor, while in thecase,should be gauged away. Next, by performing some localtransformation one may always set, and so the local-symmetry group has been reduced to. Finally, one must identify the class of residual combined super-Weyl and localtransformations preserving this geometry. As will be shown below, such transformations are parametrised by a dimensionless chiral scalar(and its conjugate).\n\nAs pointed out above, the spinoris the chiral field strength\nof an Abelian vector multiplet and describes purely gauge degrees of freedom. By employing the super-Weyl transformatons (6.17f)\nit is possible to fix the gauge\n\nBy inspecting the algebra of covariant derivatives (6.13), it is clear this leads to vanishingcurvature. Hence, in this gauge theconnectionmay also be gauged away\n\nThen, the algebra of covariant derivatives (6.13) reduces to\n\nwhich describes a conformally flat GWZ geometry[61]. This algebra should be accompanied by the constraints (6.10), provided one sets.\n\nEquation (6.17f) tells us that imposing the conditiondoes not fix completely the super-Weyl freedom. The residual transformations are generated\nby parameters of the form\n\nHowever, in order to preserve thegauge,\nevery residual super-Weyl transformation (6.30) must be accompanied by the following compensatingtransformation\n\nThis leads to the transformations:\n\nIn the infinitesimal limit, these transformations may be obtained from the ones given in[62].\n\nAs discussed above, in thecase, the torsiondescribes purely gauge degrees of freedom.\nThus, by employing the super-Weyl freedom described by eq. (6.25), it may be gauged away\n\nIn this gauge, it is natural to shiftas follows:\n\nThen, by making use of (6.22), we find that these covariant derivatives obey the algebra:\n\nIn thecase this algebra of covariant derivatives coincides with conformally flat limit of the one derived by Grimm[63]. It should be pointed out, however, that no discussion of super-Weyl transformations was given in[63]. As a result, the setup of[63]is insufficient to describe conformal supergravity. These transformations were later computed in[15].\n\nThe geometric superfields appearing above obey the Bianchi identities (6.23) (upon imposing (6.33)). Now, by examining equations (6.35), we see that thecurvature has been eliminated and therefore the corresponding connection\nis flat. Consequently, it may be set to zero via an appropriate localtransformation;. As a result, the gauge group reduces to. Hence, we will refer to this supergeometry as conformally flatsuperspace.\n\nIt turns out that the gauge conditions (6.33) andallow for residual super-Weyl transformations, which are described\nby a parameterconstrained by\n\nThe general solution of this condition is\n\nwhere the parameteris covariantly chiral, with zerocharge, but otherwise arbitrary.\nTo preserve the gauge condition, every super-Weyl transformation, eq. (6.25), must be accompanied by the following compensatingtransformation\n\nAs a result, the algebra of covariant derivatives of (conformally flat)superspace is preserved by the following set of super-Weyl transformations:\n\nForcase these transformations are a special case of the ones given in[15]. It is important to point out that forthe chiral parameterand its conjugateappear in (6.39) only in the real combination.\n\nIn the case that, the covariant derivatives of-extended Minkowski superspace, the relations (6.39)\nprovide a conformally flat realisation for an arbitrary conformally flat superspace.\n\nSECTION: 6.4Degauging (iii):-extended AdS superspace\n\nAs an application of the superspace geometries sketched above, we now show how the-extended AdS supergeometry may be described withinsuperspace.\nSuch a supergeometry is characterised by the following conditions:\n\n(i) the torsion and curvature tensors are Lorentz invariant;\n\n(ii) the torsion and curvature tensors are covariantly constant.\n\nThese conditions imply the following relations:\n\nKeeping in mind these constraints, the algebra obeyed byreduces to the following:\n\nwith the identificationwhen. Additionally, in thecase, one may impose the reality conditionby performing some rigidphase transformation.\nThen, the resulting geometry coincides with the one of[16]. We will not impose this reality condition below.\n\nWhen, the constraintimplies the following integrability condition999In thecase, a solution to eq. (6.42) is the reality condition.\n\nThis means that, by performing a localtransformation,101010Strictly speaking, this should be performed within thesuperspace of section6.2, though it is also sufficient to introduce a flatconnection.one can bringto the form\n\nthough it should be emphasised that our frame will no longer be conformally flat. Asis theinvariant tensor, it follows that the-symmetry group reduces to. The former may then be utilised to raise and lower indices in accordance with the rule\n\nFurther, upon inspection of (6.41), the-symmetry generators only appear in the algebra of covariant derivatives via the combination\n\nThegeneratormay be shown to act on isospinors as follows\n\nThe resulting algebra of covariant derivatives is as follows:\n\nThis algebra coincides with the one presented in eq. (5.60) provided one fixes, which indicates that, for, the latter also does not describe a conformally flat frame. This will be elaborated on in a forthcoming work.\n\nWe now relax the constraint (6.43) and provide a manifestly conformally flat realisation of AdS superspace. By definition, a conformally flat supergeometry may be related to a flat one by performing some super-Weyl transformation. In the case of AdS superspace, this means that the curved covariant derivativesare related to those of Minkowski superspace, see eq. (6.39), as follows:\n\nAs compared with[20], our work provides an alternative proof of the conformal flatness of-extended AdS superspace. It should also be pointed out that the logarithm of the chiral parameter, which was defined in equations (4.51) and (4.52), is proportional to;. Further, incase, they are related via eq. (5.76).\n\nSECTION: 7Conclusion\n\nThis work has completed the construction of the embedding formalism forinitiated in[1]. In the original realisation[1], superspace Poincar\u00e9 coordinates forare naturally introduced, and therefore that realisation is well suited for AdS/CFT calculations in the spirit of[53].\nThe novel realisation of the-extended AdS supergroup, which has been introduced in this paper, is more suitable for the coset construction,\n\nThe AdS superparticle model (2.19) is one of the main results of this paper. Settingin (2.19) gives a unique AdS extension of the model for a massive superparticle in Minkowski superspace. In terms of the local coordinates in the north chart described in subsection4.4, the kinetic terms have the form\n\nwhere the one-formis defined in (4.56).\nIn the non-supersymmetric case,, the-term is absent.\nTherefore, forthe-term does not contain purely bosonic contributions.\nIt may be checked that the-term contains a higher-derivative contribution\nproportional to.\nThus our superparticle model (2.19) may be viewed as an AdS analogue of the Volkov-Pashnev model[64].111111We are grateful to Dmitri Sorokin for bringing the references[64,65]to our attention.In-extended Minkowski superspace, forit was possible to add a fermionic WZ-like term to the superparticle action[65]. Such structures are more difficult to generate in the AdS case.\n\nIn this paper we have also provided descriptions of the most general conformally flat-extended supergeometry in four dimensions. Specifically, we have realised this geometry in three different superspace frameworks: (i) conformal superspace; (ii)superspace; and (iii)superspace. Additionally, we computed the finite super-Weyl transformations within theandsuperspaces. As an application of this construction, we utilised it to obtain a new realisation forand describe the specific super-Weyl transformation (6.48) required to \u2018boost\u2019 to this superspace from a flat one.\n\nAcknowledgements:We are grateful to Alex Arvanitakis, Dmitri Sorokin and Gabriele Tartaglino-Mazzucchelli for discussions. SMK is grateful to the organisers of the CQUeST-APCTP Workshop \u201cGravity beyond Riemannian Paradigm\u201d (Jeju Island, South Korea)\nwhere part of this work was completed, for the fantastic scientific atmosphere and generous support. He also acknowledges kind hospitality and generous support extended to him during his research stay at KIAS, Seoul. The work of SMK and ESNR is supported in part by the Australian Research Council, projects DP200101944 and DP230101629.\nThe work of NEK is supported by the Australian Government Research Training Program Scholarship.\n\nSECTION: Appendix AThe supergroupand supertwistors\n\nIn this appendix we collect essential definitions concerning the supergroupwhich has two different but related origins in supersymmetric field theory:\n(i) as\nthe-extended superconformal group in three dimensions; and\n(ii) as\nthe-extended AdS supergroup in four dimensions. We start by discussing the complex supergroupof whichis a real form.\n\nThe supergroupnaturally acts on the space ofevensupertwistors and on\nthe space ofoddsupertwistors.\nAn arbitrary supertwistor is a column vector\n\nIn the case of even supertwistors,is bosonic\nandis fermionic.\nIn the case of odd supertwistors,is fermionic whileis bosonic.\nThe even and odd supertwistors are called pure.\nIt is useful to introduce the parity functiondefined as:ifis even, andifis odd.\nIt is also useful to define\n\nThen the componentsof a pure supertwistor\nhave the following Grassmann parities\n\nThe space of even supertwistors is naturally identified with,\nwhile the space of odd supertwistors may be identified with.\n\nLet us introduce the following graded antisymmetricsupermatrix\n\nwheredenotes the unitmatrix.\nMaking use ofallows us to define a graded symplectic inner product on the space of pure supertwistors by the rule: for arbitrary pure supertwistorsand,\nthe inner product is\n\nwhere the row vectoris defined by\n\nand is called the super-transpose of.\nThe above inner product is graded anti-symmetric:\n\nThe supergroupis defined to\nconsist of those evensupermatrices\n\nwhich preserve the inner product (A.13) under the action\n\nSuch a transformation maps the space of even (odd) supertwistors onto itself.\nThe condition of invariance of the inner product (A.13)\nunder (A.17) is\n\nIt is useful to recast the definition ofin an equivalent form by writingas a block supermatrix:\n\nA pure supertwistor is said to be real if its components obey the reality condition\n\nThe space of real even supertwistors is naturally identified with,\nwhile the space of real odd supertwistors may be identified with.\nGiven two real supertwistorsand,\nit holds that\n\nThe reality condition (A.24) is not preserved under the action (A.17) of.\n\nBy definition, the real subgroupconsists of those transformations which preserve the reality condition\n(A.24),\n\nIn the case of complex supertwistors, the following involution can be defined\n\nIts crucial property is thatis a supertwistor with respect to,\n\nWe also observe that\n\nGiven a real supertwistorsatisfying the reality condition (A.24), it holds that.\n\nSECTION: Appendix BConformally flat atlas for AdSd\n\nA-dimensional AdS space, AdSd,\ncan be identified with a hypersurface in pseudo-Euclidean spacedefined by\n\nwheredenotes the Cartesian coordinates of,\nwithand.\nOne can cover AdSdby two charts:\n(i) the north chart in which; and\n(ii) the south chart in which.121212The north chart and the south chart are everywhere dense open subsets of AdSd. In particular, those points of AdSd, which do not belong to the north chart, are characterised by the conditionsand, and therefore they span a light cone in.Each chart may be parametrised using a natural generalisation of the stereographic projection for a-dimensional sphere.\nGiven a pointin the north chart, its local coordinateswill be chosen to correspond to the intersection of the hyperplanewith the straight lineconnectingand the \u201cnorth pole\u201d.\nSimilarly, given a pointin the south chart, its local coordinateswill be\nchosen to correspond to the intersection of the hyperplanewith the straight lineconnectingand the \u201csouth pole\u201d.\n\nIn the north chart, the straight linecan be parametrised\nas\n\nandcorresponds to some valueof the evolution parameter,. We then derive\n\nThe embedding coordinatescan be expressed in terms of the local ones,\n\nFor the induced metric we obtain\n\nIn the south chart, the straight linecan be parametrised\nas\n\nandcorresponds to some valueof the evolution parameter,. We obtain\n\nThe embedding coordinatesare expressed in terms of the local onesas follows:\n\nThe induced metric has the form\n\nIt remains to consider the intersection of the north and south charts, which is characterised by. A short calculation of the transition functions gives\n\nIt may also be seen thatin the intersection of the charts.\n\nSECTION: Appendix CThe-extended superconformal algebra\n\nIn this appendix, we spell out our conventions for the-extended superconformal algebra of Minkowski superspace,. It was initially described in the literature by Park[66], see also[67]. We emphasise that the appropriate relations differ by an overall sign as compared with those of eq. (5.55). This distinction arises from our adoption of the convention where generators act on fields and operators in a consistent manner.\n\nThe conformal algebra,, consists of the translation, Lorentz, special conformaland dilatationgenerators. Amongst themselves, they obey the algebra\n\nThe-symmetry groupis generated by theandgenerators, which commute with all elements of the conformal algebra. Amongst themselves, they obey the commutation relations\n\nThe superconformal algebra is then obtained by extending the translation generator toand the special conformal generator to. The commutation relations involving the-supersymmetry generators with the bosonic ones are:\n\nAt the same time, the commutation relations involving the-supersymmetry generators\nwith the bosonic operators are:\n\nFinally, the anti-commutation relations of the fermionic generators are:\n\nWe emphasise that all (anti-)commutators not listed above vanish identically.\n\nSECTION: References", "text_file": "data\\paper_texts\\2308.04135v3_content.txt"}, {"title": "Twisted right-angled Artin groups embedded in knot groups", "authors": ["Keisuke Himeno", "Masakazu Teragaito"], "published_date": "2024-12-05T03:33:36Z", "summary": "Twisted right-angled Artin groups are defined through presentation based on\nmixed graphs. Each vertex corresponds to a generator, each undirected edge\nyields a commuting relation and each directed edge gives a Klein bottle\nrelation. If there is no directed edge, then this reduces to an ordinary\nright-angled Artin group.\n  There is a characterization of right-angled Artin groups which can be\nembedded in knot groups by Katayama. In this paper, we completely determine\ntwisted right-angled Artin groups embedded in knot groups.", "arxiv_id": "2412.03849v1", "html_link": "https://arxiv.org/html/2412.03849v1", "search_term": "ti:\"embeddings\"", "html_content": "SECTION: Twisted right-angled Artin groups embedded in knot groups\n\nTwisted right-angled Artin groups are defined through presentation based on mixed graphs.\nEach vertex corresponds to a generator, each undirected edge yields a commuting relation and\neach directed edge gives a Klein bottle relation.\nIf there is no directed edge, then this reduces to an ordinary right-angled Artin group.\n\nThere is a characterization of right-angled Artin groups which can be embedded in knot groups by Katayama.\nIn this paper,\nwe completely determine twisted right-angled Artin groups embedded in knot groups.\n\nSECTION: 1.Introduction\n\nTwisted right-angled Artin groups (abbreviated as TRAAGs) are introduced in[4](also[21]) as a natural generalization of right-angled Artin groups (RAAGs).\nFor TRAAGs, there are a few of recent results[1,7].\nIn particular, there is a characterization of TRAAGs with torsion, or left ordering.\n\nRecall the definition of TRAAG (see[1,7]).\nLetbe a mixed graph, whereis a (finite) simple graph, andis a subset of.\nAn element ofis called a directed edge.\nIf a directed edge has a tailand a head, then we denote it by.\nAn undirected edge connectingandis written as.\nThen thetwisted right-angled Artin group based onis\ndefined as\n\nIf, thenis the ordinary right-angled Artin group based on.\nThe second type of relationis called theKlein relation([7]).\n\nDroms[6]gave a complete characterization of RAAGs which are-manifold groups, that is, the fundamental groups\nof connected-manifolds.\nPrecisely, a RAAGis a-manifold group if and only if each connected component ofis a tree or a triangle.\nThis leads us to a natural question.\n\nWhich TRAAGs are-manifold groups?\n\nAs the simplest example, letbe a single arc.\nMore precisely, it consists of two vertices and a single directed edge.\nThenis isomorphic to the fundamental group of the Klein bottle.\n(The Klein relation originates from this.)\nHencecan be realized as the fundamental group of the twisted-bundle over the Klein bottle.\nIn general, the answer to Question1.1is widely open.\n\nKatayama[15]gives a complete characterization of RAAGs which embed into a knot group, the fundamental group\nof the complement of a knot in the-sphere.\nFor readers\u2019 convenience, we state his result below (Theorem1.2).\n\nIn general, if a grouphas a subgroup isomorphic to a group, then\nwe say thatembeds into.\nIf there is no confusion, then we often write.\n\nLetbe a knot in the-sphere.\nThe knot exterioradmits the torus decomposition, or the JSJ decomposition (see[13,14,22]), where\neach piece is hyperbolic or Seifert fibered.\nWe remark that a Seifert fibered piece is either a composing space, a torus knot exterior or a cable space.\nAny of these admits the unique Seifert fibration.\nIf there are two Seifert fibered pieces glued along a common boundary torus, then\nthe pair is called aSeifert-Seifert gluing.\n\nA complete bipartite graphwithis called a star.\nA path withvertices is.\nIn particular,is a single vertex.\nA forest is a graph containing no cycles.\nHence every connected component of a forest is a tree.\nThese are undirected graphs.\nThe disjoint union of two graphsandis denoted by.\nAlso,denotes the disjoint union ofcopies of.\n\nThus.\nWe note that, a free group of rank,,\nand.\n\nLetbe a non-trivial knot inwith exteriorand knot group.\nLetbe an undirected graph, andthe associated RAAG.\n\nIfconsists of only hyperbolic pieces, thenembeds intoif and only iffor.\n\nIfis Seifert fibered, that is,is a torus knot, thenembeds intoif and only ifis eitheror a single starfor.\n\nIfcontains both of a Seifert fibered piece and a hyperbolic piece, and there is no Seifert-Seifert gluing, thenembeds intoif and only ifwith. (Possibly, there is no star.)\n\nIfcontains a Seifert-Seifert gluing, thenembeds intoif and only ifis a forest.\n\nFor the unknot, the knot group is an infinite cyclic group.\nHence onlycan embed there.\n\nIn this paper, we focus on TRAAGs embedded in a knot group, and give a complete characterization of such TRAAGs.\nTo state our main theorem, we add some terminologies.\n\nFor an integer, asink staris a star digraph withvertices anddirected edges which share the same head.\nSee Figure1.\n\nIfhas the central vertex(of degree) and end vertices, then\nthe associated TRAAGhas a presentation\n\nA (non-trivial) torus knot of typeis said to beof even type, and\nits exterior is referred to as atorus knot exterior of even type.\nSimilarly, the exterior of a torus knot of type, which lies on the boundary of a smaller solid torusin a solid toruswithand runstimes along, is called acable space of even type.\n(In this case,is possible.)\nASeifert fibered piece of even typemeans either of them.\n\nLetbe a mixed graph with at least one directed edge, and letbe the TRAAG based on.\nLetbe a non-trivial knot inwith exteriorand knot group.\n\nIfconsists of only hyperbolic pieces, thencannot containas a subgroup.\n\nIfis Seifert fibered, that is,is a torus knot, thenembeds intoif and only ifis a (non-trivial) torus knot of even type andis a single sink starwith.\n\nIfcontains both of a Seifert fibered piece and a hyperbolic piece, and there is no Seifert-Seifert gluing, thenembeds intoif and only if\nthere is at least one Seifert fibered piece of even type, andfor,andhas at least one sink star.\n\nIfcontains a Seifert-Seifert gluing, thenembeds intoif and only if\nthere is at least one Seifert fibered piece of even type, andfor, whereis a forest andhas at least one sink star. (Possibly,is empty.)\n\nWe remark that there is no upper bound for the number of stars or sink stars in cases (3) and (4) above.\nSince we assume thathas at least one directed edge,\nno TRAAG can embed into the knot group of the unknot (see Section2).\n\nThroughout the paper, we use the following notation.\nIn a group, the commutator betweenandis(although the symbol is the same as an undirected edge,\nit may not cause a confusion), and\nthe conjugate ofwithis.\nFor a subgroupand,denotes the conjugate subgroup.\n\nIf an elementsatisfies the relationfor some, and,\nthenis called ageneralized torsion element of order two[10](or a reversible element[5]).\nIf both ofandbelong to a subgroupof, then\nthe pairis called ageneralized torsion pair in[10].\n\nSECTION: 2.Preliminaries\n\nWe put the following assumption on mixed graphs throughout the paper.\n\nAssumption.Any mixed graphhas at least one directed edge.\nThat is,.\n\nLet, and letbe\nthe induced subgraph spanned byin.\nThenis isomorphic to the subgroupgenerated byin.\n\nThe corresponding claim for RAAGs is also well known (see[15]).\n\nThe next lemma is crucial to our argument, and it will be used repeatedly.\n\nLetbe a mixed graph consisting of a single directed edge.\nIfembeds into a knot group,\nthencontains a Seifert fibered pieceof even type such thatis conjugate into.\nMore precisely,for some, and, whereis the (unique) exceptional fiber of indexin.\n\nUnder the assumption of Lemma2.2,is the fundamental group of the Klein bottle.\nTheorem 1.4 of[10]shows thatfor some Seifert fibered pieceof,\nand Theorem 1.10 of[10](and its proof) describes.\nThusgives a regular fiber of, which is central in.\n\nBy Lemmas2.1and2.2, we see that the knot group of the unknot does not admit a TRAAG.\nAlso, any TRAAG itself cannot be a knot group, since the abelianization of a TRAAG (with a directed edge)\ncontains a-torsion[7].\n\nTheorem1.3(1) immediately follows from these lemmas.\n\nIf the knot exteriorconsists of only hyperbolic pieces,\nthendoes not admit a TRAAG as a subgroup.\n\nIn particular, the knot group of a hyperbolic knot cannot contain a TRAAG.\n\nSuppose thatfor a mixed graph.\nBy our assumption,has a directed edge, which gives an induced subgraph.\nThuscontains a Seifert fibered piece by Lemmas2.1and2.2.\n\u220e\n\nFor a mixed graph, theunderlying graphis just the graph\nobtained fromby forgetting the orientation of all directed edges.\n\nFirst, we want to exclude triangles in a mixed graph when the corresponding TRAAG embeds into a knot group.\nThere are 7 possibilities of triangles as shown in Figure2.\n\nLetbe a mixed graph.\nAssume thatembeds into a knot group.\nThen the underlying graphofcannot contain a triangle.\n\nAssume that the underlying graphcontains a triangle, and\nletbe the corresponding subgraph of.\nLetbe the vertices.\nNote thatis the induced subgraph of.\nHenceby Lemma2.1.\nWe eliminate all possible types as shown in Figure2.\n\n(a) corresponds to, which is impossible by[19, Theorem 5.4.2].\n(b) corresponds to, whereis the fundamental group of the Klein bottle.\nSince, we havein a knot group, impossible again.\n\nConsider a triangleof type (c).\nLet,andbe the edges.\nThen, andcommutes withand.\nWe claim.\nSuppose not.\nThenfor some integers.\nRecall that we have relationsand.\nThis means thatand.\nBy taking a conjugate ofwith, we have.\nThus, which implies.\nSo,, which contradicts the fact that any knot group is torsion-free.\nWe have thus shown that.\nThis is impossible as before.\n\nFor (d), let,andbe the edges.\nThen.\nConsider the centralizerofin the knot group.\nIt contains.\nSince,also belongs to.\n\nWe claim that.\nAssume.\nThenfor some integers.\nBy conjugating with,.\nOn the other hand,implies.\nHence, so, a contradiction.\n\nThus we have shown thatis bigger than.\nThis implies that there exists a Seifert fibered pieceof the knot exterior(with respect to the torus decomposition)\nandsuch thatandis equal to the centralizer ofin([2,13]).\n\nFor simplicity of notation, we keep using the same symbols after taking conjugations with.\nThus,, andis bigger than.\nHenceis a power of a regular fiber of(see the paragraph after Theorem 2.5.2 of[2]), and.\nThere are only three possibilities of; a torus knot exterior, a cable space or a composing space.\n\nNote thatcontains the Klein bottle group.\n(has a subgroup, which contains\na subgroup.\nBy taking a conjugate with, we find the above subgroup of.)\nThis means thatcontains a generalized torsion element of order two.\nHenceis not a composing space[10], and has an (unique) exceptional fiberof even index.\n\nHowever,[10, Theorem 1.10]claims thatis conjugate to a power of the exceptional fiberin.\nIn fact,for some.\nRecall thatis a power of a regular fiber. So,for some integer, andis central in.\nSincehas index, we have.\nThus\n\nThis contradicts thatin.\n\nConsider (e).\nSimilarly, let,andbe the edges.\nSince,\nit has a subgroup.\n\nWe claim.\nSuppose not. Thenfor some integers,and.\nRecall.\nBy taking a conjugate with, we have, so.\nThis is impossible.\nSincecommutes withand,, a contradiction.\n\n(f) leads to a torsion by[1], impossible.\n\nFinally, consider (g).\nThere are three edges,and.\nThenis the Klein bottle group, which has an index two subgroup.\nConsider the centralizer. It containsand.\n\nWe claim.\nSuppose not.\nThenfor some integersand.\nTaking a conjugate withgives.\nHence, which gives.\nThus.\nHowever, this contradicts thatis the Klein bottle group.\n\nThusis bigger than.\nAs in (d), there exists a Seifert fibered pieceof, and we can assume that, after conjugations.\nThen,is a power of a regular fiber, and.\n\nOn the other hand,contains the Klein bottle group.\nHencehas a generalized torsion element of order two.\nAs in (d),is conjugate to a power of an exceptional fiber in.\nThis leads to a contradiction thatinas in (d).\n\u220e\n\nFor a mixed graph, assume thatembeds into a knot group.\nThen\nthe RAAGbased on the underlying graphis a subgroup of.\n\nThis follows from[21].\nLetbe the generators of.\nThensatisfies the conditionsandof[21], by Lemma2.4.\nTheorem 2(iv) of[21]claims that the squaresgenerate\na subgroup ofwhose the only defining relations areif.\nClearly, this subgroup is isomorphic to.\n\u220e\n\nAs remarked before, Droms[6]shows that\na RAAGis a-manifold group if and only if each connected component ofis\na tree or.\nSince we consider only knot groups, we can say more as in[15].\n\nIf a RAAGembeds into a knot group, then each connected component ofis a tree.\nHenceis a forest.\n\nNote.\nSincecannot embed into a knot group (see[19, Theorem 5.4.2]),\nthe conclusion immediately follows from the above claim of Droms[6].\n\u220e\n\nWe can prove an analogous result for TRAAGs owing to Lemma2.5.\n\nIf a TRAAGembeds into a knot group, then the underlying graphofis a forest.\n\nAssume thatcontains a cycle for a contradiction.\nWe can assume that the vertices of the cycle span an induced cycle.\nThenhas a subgroup.\nBy Lemma2.5,, which is impossible by Lemma2.6.\n\u220e\n\nSECTION: 3.Torus knots\n\nAmong torus knots, we can restrict ourselves to torus knots of even type by Lemma2.2.\nIn this section, we prove the following.\n\nLetbe a non-trivial torus knot of even type.\nThen a TRAAGembeds intoif and only ifis a sink starfor some.\n\nClearly, a sink starhas an induced subgraphfor any.\nHence, ifembeds into, then so doesby Lemma2.1.\nButhas no directed edge, so we exclude this possibility from the conclusion.\n(Compare Theorem1.2(2).)\n\nFirst, we confirm that the TRAAG based on a sink star can embed intofor a non-trivial torus knotof even type.\n\nLet, and letbe a sink star withvertices.\nIfis a non-trivial torus knot of even type, thencan embed into.\n\nLetbe the standard presentation of.\nNote thatis central.\nSetfor.\nThen.\nFor,\n\nWe show thatgenerates a free group of rankin.\nFirst,.\nThis follows from the observation\n\nin the amalgamated free product.\n(Note that.)\nSecond, letsuch that eachandfor.\nThen we see. For, if bothor, then there is no cancellation in the productas shown above.\nAlso, if, then each of\n\nyields a non-trivial reduced word.\nThusis an alternate sequence ofand, which impliesin.\n\nHence the setgenerates a free subgroup of rankin(see[17, Proposition 1.9]).\nThus.\n\u220e\n\nA similar argument shows that if, thencan embed into the fundamental group of a cable space of even type.\n\nLetbe a cable space of type.\nThen, whererepresents the exceptional fiber of index.\nSetforas before.\nThenis central in, and.\n\nNote thatis the amalgamated free product,\nandis reduced there. The remaining argument is the same as in the proof of Proposition3.2.\n\nConversely, we suppose that a TRAAGembeds into a torus knot group.\nWe will show thatis a sink star.\n\nis a star.\n\nBy Lemma2.5, the RAAGcan embed into.\nThen Theorem1.2([15]) shows thatis a star.\n(Since we are assuming thathas at least one directed edge, we can exclude the case whereis an empty graph.)\n\u220e\n\nIfhas order two, thenis a sink star.\nHence we assume thathas at least three vertices.\nLetbe the central vertex, and letbe the end vertices.\n\nThere is no directed edge with tail.\n\nAssume that there is a directed edge.\nThenfor some, whereis the exceptional fiber of indexby Lemma2.2.\nHence, whereis a regular fiber.\nThen, becauseis central in.\nThusis also central.\nHowever, this contradicts the fact thatwithis a free subgroup of rank two inby Lemma2.1.\n\u220e\n\nis a sink star.\n\nBy our assumption thathas at least one directed edge and Lemma3.5,\nthere is a directed edge.\n\nSuppose that there is an undirected edgewithfor a contradiction.\nAgain,for some, whereis the exceptional fiber of index,\nand, a regular fiber, as in the proof of Lemma3.5.\n\nLet.\nThen.\nConsider the natural projection\n\nNote thatandcommute in, so doand.\nHere,.\nThuslies in a conjugate subgroup, orandare both powers of the same element (see[18, Corollary 4.1.6]).\n\nIn the former,for some integers.\nThen, socommutes with.\nThis contradicts thatandgenerate a free group of rank two.\n\nIn the latter, setandfor some integers.\nThen, which implies thatfor some integer.\nFurther,, sois a power of.\nThuscommutes with, which is impossible as above.\n\u220e\n\nThis follows from Propositions3.2and3.6.\n\u220e\n\nA similar argument gives the next.\n\nLetbe a cable space of even type, and let.\nThen a TRAAGembeds intoif and only ifis a sink starfor some.\n\nThe \u201cif\u201d part is given in Remark3.3.\nSuppose thatembeds into.\nLemmas3.4and3.5hold again.\n\nAs in Remark3.3,\nlet, whererepresents the exceptional fiber of index.\nSet, which is a regular fiber.\nBy using the natural projection\n\nthe remaining argument goes on as in the proof of Proposition3.6.\n\u220e\n\nSECTION: 4.Group theoretic results\n\nIn this section, we prove some technical results for the remaining sections.\n\nSECTION: 4.1.Amalgamated free product\n\nFor a knot exterior, choose an essential torusfrom the torus decomposition.\nThis torus decomposesintoand.\n(They are not necessarily pieces arisen from the torus decomposition.)\nHence.\n\nKeeping this in mind, consider an amalgamated free product.\nSelect the right coset representative systemsformodandmod(see[18]).\n\nFor, ifis conjugate into, thenfor some.\n\nBy the assumption,for some.\nIf, then we are done.\nIf, then, a contradiction.\nHence we assume.\n\nSet, and letbe the reduced form of, where(possibly,) andare representatives.\nIn particular,,or, andanddo not belong to the same factor.\n\nThen the proof of[18, Theorem 4.6(i)]claims that\nin the sequence\n\nany element except the last belongs to.\n\nLet.\n(When, set.)\nThen.\nIf, then\n\nhas representative length, becauseand.\nThus.\nSince, we are done.\n\u220e\n\nLet. Suppose thatis not conjugate into.\nIffor, then.\n\nLetbe the reduced form of.\nSupposefor a contradiction.\nThen either, orand.\n\nSuppose. Then\n\nSince, both sides have distinct representative lengths, a contradiction.\n\nSuppose thatand.\nThen we havein the right hand side of (4.1).\nHence both side have distinct representative lengths again.\n\u220e\n\nSECTION: 4.2.Generalized torsion pair\n\nWe consider the situation where a knot groupcontains\na generalized torsion pair.\nEquivalently, we have a subgroup([9]).\nBy Lemma2.2, the knot exteriorcontains\na Seifert fibered pieceof even type, which is either\na torus knot exterior or a cable space.\nWe exclude the case where, because the torus knots are done in Section3.\nAfter a conjugation, we may assume that[10].\nThat is, the pairis a generalized torsion pair in.\n\nIn this subsection, we prove that neithernorlies infor any boundary torus\ncomponentof.\n\nLetbe a boundary component of a Seifert fibered pieceof the knot exterior.\nIfis a generalized torsion pair in,\nthen.\n\nSupposefor a contradiction.\nWe divide the argument into two cases.\n\nCase 1.is a torus knot exterior of type.\n\nOn, we have a standard meridian-longitude pair, whereis null-homologous in.\nLet.\nThenimplies.\nSince, we have.\n\nWe may assume. Here, we make use of stable commutator length.\nFor,(see[3,11,12]).\nThis contradicts that any generalized torsion element of order two has scl([11]).\n\nCase 2.is a cable space of type.\n\nFor a solid torus, take a concentric smaller solid toruswith, and acurveon, runningtimes along.\nThenis homeomorphic to the exterior ofin.\nLetandbe a meridian-longitude pair on,\nand letbe the core of(and).\nThen.\n\nFirst, set. Then,\nand we can putfor some integersand.\nSince,,\nwhereis a meridian of the curve.\nHowever,implies, so, a contradiction.\n\nSecond, letbe the boundary of the tubular neighborhood of.\nLetbe a regular fiber of.\nThen, and\nset.\nAs before,implies.\nHowever,\n\nHence, soagain.\n\u220e\n\nIfis a generalized torsion pair, then so isfor any.\nFor,implies.\n(Of course, we need, which follows from the fact thatis torsion-free.)\nThus Lemma4.3shows thatfor any.\n\nLetbe a boundary component of a Seifert fibered pieceof the knot exterior.\nIfis a generalized torsion pair in,\nthen.\n\nBy Lemma2.2,for someand the (unique) exceptional fiberwith index.\nIn particular,gives a regular fiber.\n\nCase 1.is a torus knot exterior of type.\n\nOn, choose a standard meridian-longitude pair, whereis null-homologous in.\nThen.\nSince, we have.\n\nSuppose. Thenfor some integersand, and.\nHence.\nFor,\nconsider.\nIn,, so.\nThen, which contradicts thatis torsion-free.\n\nCase 2.is a cable space of type.\n\nAs in the proof of Lemma4.3,\nwe have a presentation, where.\n\nFirst, set.\nAssume, and set.\nThen.\nSince, we have.\nHence\n\nAs before,in, so.\nThenand.\nHowever, this is impossible, becauseis odd.\n\nSecond, letbe the boundary of the neighborhood of the curve.\nThen.\nAssume. Then, so.\nAgain, consider.\nThen\n\nbecause.\nThis contradicts.\n\u220e\n\nSECTION: 5.Directed edges in a mixed graph\n\nBy Corollary2.3and Theorem3.1,\nthe remaining case is whenadmits a Seifert fibered pieceof even type.\nWe suppose this situation throughout this section.\n\nLetbe a mixed graph with directed edge.\nSuppose thatembeds into a knot group.\nBy Lemma2.2, we may assume that.\nThusgives a generalized torsion pair in, that is,, equivalently.\n\nThere are two possibilities for.\nIfis a torus knot exterior, thensplitsintoand, say.\nThat is,, so.\nIfis cable space, then there are two cases.\nIfcontains, then chooseas another boundary component.\nOtherwise, we can choose either boundary component as.\nThussplitsinto, which contains(possibly,), and.\nThen.\nHence the torus knot exterior case can also be regarded as the latter when.\nSince, the pairis also a generalized torsion pair in.\n\nNeithernoris conjugate intoin.\n\nConsider the element.\nBy Lemma4.3,.\nAssume thatfor some.\nBy Lemma4.1, we can assume that.\n\nSuppose thatis a torus knot exterior. Then.\nBy taking a conjugation with,yields.\nThus a new pairis another generalized torsion pair in.\nThis contradicts Lemma4.3.\n\nSuppose thatis a cable space.\nIf, then we are done as above.\nOtherwise, letbe the other boundary component of.\nThen, so.\nWe remark thatby Lemma4.3.\n\nIfis not conjugate intoin, then Lemma4.2implies.\nAgain,gives a generalized torsion pair in, contradicting Lemma4.3.\n\nThusfor some.\nHowever, we can assume thatby Lemma4.1.\nThengives a generalized torsion pair in,\ncontradicting Lemma4.3again.\n\nThe argument for the elementis the same. Use Lemma4.5instead of Lemma4.3.\n\u220e\n\nLetbe a mixed graph with directed edge.\nAssume thatembeds into a knot group.\nThen\n\nthere is neither an undirected edgenor a directed edgewith; and\n\nthere is neither an undirected edgenor a directed edge.\n\nBy Lemma2.7, each connected component ofis a tree.\n\n(1)\nIf there is an edgeorwith, then\nwe have a relationin.\n\nConsideras above.\nWe have(Lemma4.3) andis not conjugate intoin(Lemma5.1).\nLemma4.2implies.\n\nIf, then we have.\nThis is impossible by Proposition3.6and Theorem3.7.\n\nIf, then consider a splittingas in the proof of Lemma5.1.\nThen.\nSinceis not conjugate intoinby Lemma5.1(we can choose any boundary component ofas),\nLemma4.2implies, so, impossible again.\n\n(2)\nSinceis simple (no multiple edges),automatically.\nThe rest of the argument is the same as (1). Use Lemma4.5instead of Lemma4.3.\n\u220e\n\nLetbe a mixed graph.\nIfembeds into a knot group, then each connected component ofhaving\na directed edge is a sink star.\n\nThis immediately follows from Proposition5.2.\u220e\n\nSECTION: 6.No Seifert-Seifert gluing\n\nIn this section, we consider the case wherecontains both a Seifert piece and a hyperbolic piece, and there is no Seifert-Seifert gluing.\n\nSuppose thatcontains both a Seifert piece and a hyperbolic piece, and that there is no Seifert-Seifert gluing.\nThen a TRAAGembeds intoif and only if\nthere is at least one Seifert fibered piece of even type, andforand. (By our assumption,has at least one sink star.)\n\nFirst, we prove the \u201cif\u201d part.\n\nSuppose thatcontains both a Seifert piece and a hyperbolic piece, and that there is no Seifert-Seifert gluing.\nLetforand, where there is at least one sink star.\nIfcontains at least one Seifert fibered piece of even type, thenembeds into.\n\nLetbe a Seifert fibered piece of even type, anda hyperbolic piece which\nshares a torus boundary.\nThen.\nWe know thatfor anyby Proposition3.2and\nRemark3.3.\n\nTakeandsuch thatfor any non-zero integer.\n(The choice ofis attained by taking a loxodromic element in.) We remark thatis malnormal in[8].\n\nForwith,gives a reduced word whose first and last lie in.\n\nIf, thenis reduced.\nOtherwise.\nThenby the malnormality.\nHenceis reduced.\n\u220e\n\nIn,.\n\nLetbe a non-empty reduced word.\nAssume.\nWe can assume that\n\nwhere,andafter conjugation.\nThen we haveandis a non-zero power of.\nBy Claim6.3and, this gives a non-empty reduced word, a contradiction.\n\u220e\n\nThus we have a free product subgroup.\n\nFor a given mixed graph, let.\nFor any finite sumof,, so.\n(This can be seen by taking a covering space of a wedge of a-complexwithand a circle.)\n\nIf we take sufficiently many copies of, thenby Lemmas2.1and2.5.\nThus we have.\n\u220e\n\nThe \u201cif\u201d part is shown in Proposition6.2.\n\nConversely, suppose thatembeds into.\nBy Lemma2.5and Theorem1.2,\nwe know that.\nBy Corollary5.3, each connected component ofhaving a directed edge is a sink star.\nHence the conclusion follows.\n\u220e\n\nSECTION: 7.Seifert-Seifert gluing\n\nFinally, we prove the next to complete the proof of Theorem1.3.\n\nAssume thatcontains a Seifert-Seifert gluing.\nThen a TRAAGembeds intoif and only if\nthere is at least one Seifert fibered piece of even type,\nandfor, whereis a forest andhas at least one sink star. (Possibly,is empty.)\n\nSECTION: 7.1.Katayama\u2019s embedding of\n\nWe assume thatcontains a Seifert-Seifert gluing in this section.\nLetandbe such Seifert fibered pieces with a common torus boundary.\nThen Katayama[15](also[20]) observes.\nSincefor any forest[16],\nit implies.\n\nLet.\nWe briefly recall the constructionof[15].\n\nLetbeor.\nWhenis a torus knot space,has a subgroup,\nwhere the commutator subgroup is a free group of rank.\nNote that this subgroup has finite index.\nIfis a composing space, whereis the-holed disk,\nthen.\nFinally, ifis a cable space, then it has a finite cyclic cover homeomorphic to\na composing space.\nIn any case,has a finite index subgroupfor some, where thefactor is generated by a regular fiber.\nFor, this subgroup is denoted by.\n\nLetbe a regular fiber of. Then.\nNote thaton, becauseandare adjacent distinct pieces.\nWe do not know whetherand.\nBut, sincehas finite index in,\na sufficiently high power of(resp.) lies in(resp.).\nLetbe such a power.\nFinally, takeso thatis free.\n(Although we do not know whether, we can take.)\n\nThus we haveand, which are isomorphic to, and.\nHencegives.\n\nUnder this situation, we prove a technical result.\nWe keep using the above notation.\n\nLet.\nThere exists an elementsuch thatfor anyand thatis a free group of rank three.\n\nIn fact,.\n\nWe divide the argument into three cases.\n\nCase 1.is a torus knot exterior.\n\nRecall that, whereis the commutator subgroup of.\nSince,for any.\nHence we can chooseso thatis free.\nThis implies, otherwiseandwould commute.\n\nWe verify thatfor any.\n\nLetbe the meridian and longitude pair on.\nFor a givenwith, suppose.\nThenfor some integers.\nHence.\nThen, so.\nThis means thatcommutes with.\nSince the centralizer ofis([2]),.\nBy homological reason,is a power of.\nHowever, such a relationis impossible in the commutator subgroup which is free,\nbecauseis not a power of.\n\nCase 2.is a composing space.\n\nThen, and.\nHence.\n(Possibly,is a power of.)\nIn this case, we can chooseso thatis a free group of rank three in.\nIn particular,.\n\nAgain,\nwe see thatfor.\nFor, choose the generators, whereis represented by a component of.\nLetwith.\nSuppose.\nThenfor some integersand.\nSinceis central,, so.\nSince,.\nThus, becauseis a regular fiber representing the-factor of.\nThen we have a relationin.\nThis is impossible, becauseis not a power of.\n\nCase 3.is a cable space.\n\nThe cable spaceis obtained from a solid torusby removing\na tubular neighborhood of a torus knot of type, lying on a smaller concentric torus.\nA punctured meridian diskcorresponds to the-factor of.\nThe-factor comes from a regular fiber.\nHence we can chooseso thatis a free group of rank three.\n\nWe may assume thatis the inner boundary component by the symmetry of.\nAgain, we know.\nLetbe the meridian and longitude pair on.\nFor, assume that.\nIfwith, then.\nThus, so.\nAs in Case 1, we have, which impliesfor somefrom homological reason.\n\nConsider the finite cyclic coverof, corresponding\nto the kernel of,\nsendingand, whereon.\nThen, and the relationyieldsfor suitable liftsofand. This implies, because.\nHence we havein.\nHowever, this is impossible, because the centralizer ofisand\u220e\n\nSECTION: 7.2.Embedding of\n\nFrom now, we assume thatis a Seifert fibered piece of even type, and.\nBy Lemma7.2, we take an element.\nAlso, applying Lemma7.2to, we have an elementwith a similar property to.\nRecall thatis a regular fiber of, and set.\n\nForwith,gives a reduced word in the amalgamated free productwhose first and last lie in.\n\nIf, thensatisfies the conclusion.\nIf, then.\nThusgives the reduced word.\nFinally, if, thenand, sois reduced.\n\u220e\n\nIfwith, thengives a reduced word in\nthe amalgamated free productwhose first and last lie in.\n\nRecall that.\nFirst, we deal with the case wheredoes not contain a power of.\nThen.\nIf, thenis reduced.\nSuppose.\nIf, then.\nThusis reduced.\nIf, then, and.\nThereforeis reduced.\n\nThus we may assume thatcontains a non-zero power of.\nWe write\n\nwherecontains no,,, and possiblyor.\nFurthermore, we may assume thatis not a power of, since.\nIfis a non-zero power of, thenis merged into, or.\nFor the latter, since,, then we are done.\nSimilarly, ifis a power of, thenis merged into, or.\nThe latter can be treated as above.\nHence, we may assume that anyis not a non-zero power of.\n\nNow,\n\nIf, then we are done.\nAssume.\nSincedoes not contain, andis central in,\nwe can write, whereinvolves onlyand.\nPossibly,, and either letter does not appear.\n\nThen.\nNote thatand thatis not a power of,\nbecauseis a free group of rank three.\n\nIflies in, then.\nHowever,in, which\ncontradicts thatis a free group of rank three.\n\nThe argument foris similar.\n\u220e\n\nHere, we introduce a symbol.\nFor example,is a non-zero power of,is a word consisting\nof only a non-zero power ofand, both of which appear.\nWe can suppose that such a word has the shortest length among the words representing the same element in.\n\nFordefined in (7.1),\nthere are five possibilities:\n\n,,\n\n,\n\n,\n\n,\n\n.\n\nThe remaining is to show that the middle segment of (7.2),\n\ngives a reduced word whose first and last lie in.\n\nFirst, ifhas a form of (2), that is, a power of, then.\n(4) is similar.\nFor (3), if, then.\nSinceis not a power of, this contradicts thatis a free\ngroup of rank two.\nFor (5), if, then, so, a contradiction again.\nThus wheneverhas a form of (2)\u2013(5),.\n\nFinally, among, we look at them of type (1).\nIfis of type (1) butis not of type (1), then\nset.\nClearly,.\nAssumefor a contradiction.\nWe can writewith.\nThen, so, which\ncontradicts thatis a free group of rank two.\nThus.\nThis argument works whenis of type (1), but neithernoris of type (1).\n\nIf there are successive\u2019s of type (1),, say, then\nset\n\n(Here,andare not of type (1).)\nThen the same argument shows.\n\u220e\n\nWe prove thatcan embed into.\n\nIn,\n\nThe argument is similar to that of Claim6.4. (Use Lemmas7.3and7.4.) \u220e\n\nSECTION: 7.3.Final argument\n\nAssume thatcontains a Seifert-Seifert gluing, and there is at least one Seifert fibered piece\nof even type.\nLetfor, whereis a forest andhas at least one sink star.\nThencan embed into.\n\nFirst, assume thatcontains a Seifert fibered pieceof even type which\nis glued to a Seifert fibered piece.\nLet.\nBy Proposition7.6, we can embedinto.\nRecall that([16]), sinceis still a forest.\nThus.\nHere, we can embedas in the proof of Proposition6.2.\nHence\n\nSecond, we assume that there is no Seifert fibered piece of even type which is adjacent to a Seifert fibered piece.\nLetbe a Seifert fibered piece of even type. We havefor any.\nBy the assumption, there is a Seifert-Seifert gluing.\nLetandbe such Seifert fibered pieces.\nWe know that.\n\nLet us choose a boundary componentofsuch thatsplitsintoand, and,.\nIn particular,is a boundary component of a hyperbolic piece.\nThen.\nWe remark thatis malnormal in[8].\n\nTakeand.\n\nForwith,gives a reduced word whose first and last lie in.\n\nForwith,gives a reduced word whose first and last lie in.\n\n(1)\nThe argument is the same as (1) of Claim6.3.\n\n(2) If, thengives a reduced word.\nSuppose.\nIf, thenis reduced.\nOtherwise,by the malnomality.\nThusitself is reduced.\n\u220e\n\nAs in Proposition7.6,\nwe can show thatin.\nThe rest of the argument is the same as the first situation.\n\u220e\n\nThe \u201cif\u201d part is Theorem7.7.\nWe prove the \u201conly if\u2019 part.\nBy Lemma2.5and Theorem1.2,is a forest.\nOn the other hand, each connected component ofhaving a directed edge is a sink star by Corollary5.3.\n\u220e\n\nThis immediately follows from\nCorollary2.3, Theorems3.1,6.1and7.1.\n\u220e\n\nSECTION: Acknowledgment\n\nWe would like to thank Ryoya Kai\nfor helpful conversations.\n\nSECTION: References", "text_file": "data\\paper_texts\\2412.03849v1_content.txt"}, {"title": "Detecting Redundant Health Survey Questions Using Language-agnostic BERT\n  Sentence Embedding (LaBSE)", "authors": ["Sunghoon Kang", "Hyeoneui Kim", "Hyewon Park", "Ricky Taira"], "published_date": "2024-12-05T02:18:35Z", "summary": "The goal of this work was to compute the semantic similarity among publicly\navailable health survey questions in order to facilitate the standardization of\nsurvey-based Person-Generated Health Data (PGHD). We compiled various health\nsurvey questions authored in both English and Korean from the NIH CDE\nRepository, PROMIS, Korean public health agencies, and academic publications.\nQuestions were drawn from various health lifelog domains. A randomized question\npairing scheme was used to generate a Semantic Text Similarity (STS) dataset\nconsisting of 1758 question pairs. Similarity scores between each question pair\nwere assigned by two human experts. The tagged dataset was then used to build\nthree classifiers featuring: Bag-of-Words, SBERT with BERT-based embeddings,\nand SBRET with LaBSE embeddings. The algorithms were evaluated using\ntraditional contingency statistics. Among the three algorithms, SBERT-LaBSE\ndemonstrated the highest performance in assessing question similarity across\nboth languages, achieving an Area Under the Receiver Operating Characteristic\n(ROC) and Precision-Recall Curves of over 0.99. Additionally, it proved\neffective in identifying cross-lingual semantic similarities.The SBERT-LaBSE\nalgorithm excelled at aligning semantically equivalent sentences across both\nlanguages but encountered challenges in capturing subtle nuances and\nmaintaining computational efficiency. Future research should focus on testing\nwith larger multilingual datasets and on calibrating and normalizing scores\nacross the health lifelog domains to improve consistency. This study introduces\nthe SBERT-LaBSE algorithm for calculating semantic similarity across two\nlanguages, showing it outperforms BERT-based models and the Bag of Words\napproach, highlighting its potential to improve semantic interoperability of\nsurvey-based PGHD across language barriers.", "arxiv_id": "2412.03817v1", "html_link": "https://arxiv.org/html/2412.03817v1", "search_term": "ti:\"embeddings\"", "html_error": "Error processing HTML: HTTP Error 404: Not Found"}, {"title": "Phase transitions of correlated systems from graph neural networks with\n  quantum embedding techniques", "authors": ["Rishi Rao", "Li Zhu"], "published_date": "2024-04-12T19:09:56Z", "summary": "Correlated systems represent a class of materials that are difficult to\ndescribe through traditional electronic structure methods. The computational\ndemand to simulate the structural dynamics of such systems, with correlation\neffects considered, is substantial. Here, we investigate the structural\ndynamics of $f$- and $d$-electron correlated systems by integrating quantum\nembedding techniques with interatomic potentials derived from graph neural\nnetworks. For Cerium, a prototypical correlated $f$-electron system, we use\nDensity Functional Theory with the Gutzwiller approximation to generate\ntraining data due to efficiency with which correlations effects are included\nfor large multi-orbital systems. For Nickel Oxide, a prototypical correlated\n$d$-electron system, advancements in computational capabilities now permit the\nuse of full Dynamical Mean Field Theory to obtain energies and forces. We train\nneural networks on this data to create a model of the potential energy surface,\nenabling rapid and effective exploration of structural dynamics. Utilizing\nthese potentials, we delineate transition pathways between the $\\alpha$,\n$\\alpha'$, and $\\alpha''$ phases of Cerium and predict the melting curve of\nNickel Oxide. Our results demonstrate the potential of machine learning\npotentials to accelerate the study of strongly correlated systems, offering a\nscalable approach to explore and understand the complex physics governing these\nmaterials.", "arxiv_id": "2404.08782v3", "html_link": "https://arxiv.org/html/2404.08782v3", "search_term": "ti:\"embeddings\"", "html_content": "SECTION: Phase transitions of correlated systems from graph neural networks with quantum embedding techniques\n\nCorrelated systems represent a class of materials that are difficult to describe through traditional electronic structure methods. The computational cost of simulating the structural dynamics of such systems, with correlation effects considered, is substantial. Here, we investigate the structural dynamics of- and-electron correlated systems by integrating quantum embedding techniques with interatomic potentials derived from graph neural networks. For Cerium, a prototypical correlated-electron system, we use Density Functional Theory with the Gutzwiller approximation to generate training data due to efficiency with which correlations effects are included for large multi-orbital systems. For Nickel Oxide, a prototypical correlated-electron system, advancements in computational capabilities now permit the use of full Dynamical Mean Field Theory to obtain energies and forces. We train neural networks on this data to create a model of the potential energy surface, enabling rapid and effective exploration of structural dynamics. Utilizing these potentials, we delineate transition pathways between the,, andphases of Cerium and predict the melting curve of Nickel Oxide. Our results demonstrate the potential of machine learning potentials to accelerate the study of strongly correlated systems, offering a scalable approach to explore and understand the complex physics governing these materials. [This is a preprint of Phys. Rev. B 110, 245111 (2024)]\n\nSECTION: IIntroduction\n\nStrongly correlated systems, characterized by significant electron-electron interactions, present a frontier in materials science and condensed matter physics. These interactions lead to phenomena like Mott transitions[1,2], heavy fermion behavior[3,4,5], spin-charge separation[6,7]and other correlation induced effects that can be technologically useful and physically interesting[8,9,10,11,12].\nConventional computational methods for dealing with interactions, such as Dynamical Mean Field Theory (DMFT)[13,14], Density Matrix Renormalization Groups (DMRG)[15,16], Gutzwiller wavefunction techniques[17,18,19], and Hubbard U corrections to Density Functional Theory (DFT)[20,21], have advanced our understanding of the electronic structures of such materials. However, simulating the structural dynamics and thermodynamics of strongly correlated materials remains a significant challenge due to the computational cost associated with the large Hilbert spaces that arise from many-body electron-electron interactions[22,23]. As the number of orbitals increase, the Hilbert space of commonly used models, such as the multi-orbital Hubbard model or the Anderson impurity model, rapidly increases as well. This expansion makes the calculation of dynamics particularly tedious for correlated systems, as dynamics generally require extensive sampling over many structural configurations.\n\nThis computational bottleneck becomes particularly prohibitive when exploring structural phase transitions and conducting molecular dynamics (MD) simulations. Although methods like DMFT and DMRG can offer nearly quantitative accuracy, their scalability issues and the technical difficulties in applying these approaches to multi-dimensional and large multi-orbital systems limit their practicality for direct studies of structural dynamics. For example, the exact method for solving the DMFT impurity problem, the continuous time quantum monte carlo (CTQMC) algorithm[24,25,26], samples Feynman diagrams to arbitrary order in imaginary time. This suffers from a large number of Monte Carlo steps required to accurately gauge the self energy as temperature is lowered or when considering a greater number of orbitals.\nSimilarly, DMRG becomes technically difficult to implement in more than two dimensions[27].\nTechniques like the Hubbard U correction to DFT Hamiltonians (DFT+U), while useful, often lack the quantitative accuracy needed for predicting complex behaviors under varying conditions such as high pressure[28], and are less quantitatively accurate than DMFT[29,30].\nThe Gutzwiller approximation technique, equivalent to a mean field approximation of slave boson techniques in the limit of infinite spatial dimensions[31,32,33], has been successfully applied to numerous correlated systems[34,35,36,37,38,39,40]and provides a relatively cheap way method for tackling large multi-orbital systems. However, even this method is significantly more computationally expensive than traditional DFT as it provides iterative corrections to a tight-binding Hamiltonian generated from DFT, and is primarily a zero-temperature technique.\n\nThese computational challenges are significant in studying materials like Cerium (Ce) and Nickel Oxide (NiO), which exhibit interesting properties under extreme conditions. Cerium, known for its complex structural phase transitions under intermediate pressure, exemplifies the challenges associated with accurately determining the lowest energy phase under varying pressure and temperature conditions. These phases are critical for understanding its properties and potential applications, yet first-principles study is hampered by the prohibitive computational cost of simulating strong correlation effects. Similarly, determining the melting point of compounds like NiO, an important component of the lower mantle of Earth[41,42,43], under high pressures is crucial for understanding the dynamics and properties near the Earth\u2019s core. Traditional DFT methods fall short of accurately predicting the electronic structure of NiO and Ce due to strong correlation effects, underscoring the need for more advanced simulation techniques.\n\nIn response to these challenges, this study introduces a novel approach that leverages machine learning (ML) to develop interatomic potentials from quantum embedding methods, aiming to transform the study of strongly correlated systems. By combining these beyond-DFT techniques with the robust interpolation capabilities of graph neural networks, we propose a method to significantly reduce the time associated with simulating the structural properties of these complex materials while taking into account dynamical correlation effects. Specifically, we focus on the multi-orbital Gutzwiller approximation for-electron materials and DFT+DMFT for-electron materials as computationally tractable methods to generate the initial training data for our ML models. This approach enables us to capture the essential physics of correlated systems with distributed computational cost, offering a promising pathway to accurately simulate phase transitions in Ce and the melting behavior of NiO under extreme conditions. Through the innovative use of machine learning interatomic potentials (MLIPs), we offer a scalable approach to explore and understand the complex structural dynamics of strongly correlated materials, contributing to the accelerated development of novel materials with desirable properties.\n\nSECTION: IIMethods\n\nLocal Density Approximation (LDA) calculations were carried out using the augmented plane wave plus local orbital method, as implemented in the WIEN2K package[44]. For Ce, a muffin tin radius of 2.5 Bohr was utilized, while for Ni and O, the radii were set to 1.8 Bohr and 1.5 Bohr, respectively. The smaller muffin tin radii for the NiO system accommodated high-pressure calculations. RKMax was set to 8.5 for the Ce system and 7.5 for the NiO system. For the Cerium calculations, 500 k-points were used while the NiO calculations used 1000 k-points. LDA calculations were converged to a charge density withinand energy withinRydberg. DMFT calculations for both systems were carried out using the eDMFT package[45,46], with the impurity problem solved through a Continuous Time Quantum Monte Carlo solver[47]. Forces were obtained as a derivative of the Luttinger-Ward functional with respect to atomic positions[48].\n\nFor the Cerium system, charge self-consistent Gutzwiller calculations were carried out for every training and testing structure using the CyGutz package as implemented in[49,33]until energy change was less thanRydberg. The Hubbard U parameter chosen was 6.0 eV with a Hund\u2019s coupling of 0.7 eV, since previous studies indicate these values provide good agreement with experimental lattice parameters[50,51].\n\nTraining structures were generated by producing 50 intermediate structures between the face-centered cubic and each of the 2 intermediate pressure stable phases. The intermediates were created by linearly interpolating the lattice vectors and the atomic positions. Those intermediates were then each perturbed 10 times, leading to 500 random structures generated for each pathway. The face-centered cubic phase was also perturbed 499 times leading to an additional 500 structures. Perturbations to a structure include displacement of the unit cell basis vector lengths by a random amount between 0.4\u00c5and -0.4\u00c5, a change in the angles by a random amount between 20 to -20 degrees, and the atomic positions randomly displaced from their original positions with a displacement chosen from a normal distribution with a standard deviation of 0.15\u00c5and a mean value of 0. Each of the perturbations are performed one after the other in the order stated. Charge self-consistent LDA+Gutzwiller calculations were then carried out on each structure to obtain values for internal energy.\n\nThe resulting energies were used to train a graph neural network using the M3GNET package[52]. 1350 structures were randomly selected as training data, with the remaining 150 structures used for validation. Since force data is not yet easily obtainable within the Gutzwiller approximation, the finite difference method was used to calculate the forces and unit cell stresses after generation of the MLIP.\n\nThe solid state nudged elastic band (NEB) method was then used to discover transition pathways between stable structures[53]. This method involves fixing the starting and ending structures, attaching springs to the atoms and unit cell parameters of the intermediate structures, and optimizing the intermediate structures until a stable pathway is found. The MLIP was used as the mapping between the crystal structure and the energies, forces and stresses. The pathways found were checked using charge self-consistent DMFT. We checked the pathways at temperatures of 116 K and 400 K for the impurity solver withMonte Carlo steps split across 16 processors. Results converged on average at 10 charge-self consistency steps defined as the point when variation of the energy dropped beloweV. The same values of Coulomb repulsion and Hund\u2019s coupling used for the Gutzwiller solver were used for the DMFT calculations. We employed nominal double counting as described in Refs.[54,55,45]to correct for the double contribution to the energy from both the LDA and DMFT solvers with a nominal value of 1.0 for Ce. Nominal double counting has been shown to perform better than the fully localized limit method used in many other studies[55].\n\nFor Nickel Oxide, full DMFT calculations were carried out to compute energies and forces. A Coulomb repulsion value of 8.0 eV and Hund coupling of 0.9 eV were chosen as they have been found to provide a good description of the electronic structure of NiO within DMFT at high pressures[56]. A double counting value of 8.0 was chosen and calculations were carried out at a temperature of 611 K, which is above the Neel temperature of NiO. ApproximatelyMonte Carlo steps were sampled, split across 8 processors.\n\nThe training data was generated by performing 500 random perturbations of the atomic positions and lattice constants for the trigonal, face-centered cubic and body centered cubic phases each for a total of 1500 training structures. For each structure, the volume was also randomly perturbed, down to 65% of the equilibrium volume, in order to capture behavior at high pressures. After the volume change, perturbations to the structure include displacement of the unit cell basis vector lengths by a random amount between 0.3\u00c5and -0.3\u00c5, then a change in the angles by a random amount between 22.5 to -22.5 degrees, and displacement of the atoms in the unit cell from their original positions by a random value between -10% to 10% of each of the newly changed unit cell vectors. The same training and validation split of 1350 to 150 was used for NiO as for Ce.\n\nMolecular dynamics simulations were carried out using the atomic simulation environment package (ASE)[57]in the micro-canonical (NVE) ensemble to investigate the evolution of the melting curve under pressure using the Z-method[58,59]. The NVE ensemble fixes particle number (N), volume (V) and conserves the internal energy (E). This is suitable for the Z-method as it involves heating a structure at a constant volume until it melts, and observing the pressure and temperature where the melt occurs.\n\nExperimental volume-pressure relations were compared to DFT calculations from the VASP package[60,61,62]using a plane-wave basis set with Planar augmented wave pseudopotentials[63,64,65]and an exchange-correlation functional based on a revised Perdew-Burke-Ernzerhof for solids (PBESol) version of the generalized gradient approximation[66]. Additional calculations were also carried out using the standard Perdew-Burke-Ernzerhof (PBE) functional[67]when appropriate.\n\nThe M3GNET Graph neural networks take as input the atomic numbers, positions and lattice constants for a given unit cell of a crystal structure. The atomic positions are then used to derive edge bonds between neighboring positions, as well as three body interactions, such as bond angles. Atomic numbers are converted to a vector of dimension 64 using the built in graph featurizer. Edge vectors between 2 atoms were generated using basis functions proposed by Ref.[68], with the distance between the atoms as input and a cutoff of 5\u00c5. Three-body features were restricted to a cutoff radius of 4\u00c5. These features were then fed into a graph convolution layer 4 times, with the embeddings being adjusted at each pass, before being passed to a gated multi-layer perceptron. The multi-layer perceptron then maps the graph convoluted input to energies, and can be differentiated to obtain forces and stresses. It consists of 3 layers with 64 weights for the first 2 hidden layers, and 1 output weight for the last layer. Further details on the construction of the interatomic potentials from graph features, as well as details of the model architecture can be found in Ref.[52]. The same model architecture as described above was used for both Cerium and Nickel Oxide. The weights of the MLIP were adjusted using the AdamW optimizer[69]until the mean average error of the energies was less than 50 meV/atom for Cerium and 20 meV/atom for NiO. The weights of the neural network can also be refined by force data. In the case of Cerium, we have trained a single neural network to predict the energies by comparing the predicted energies by the model to the actual energies, then updating the weights based on the difference. For Nickel Oxide, we have trained two separate models, one to predict energy while the other predicts forces. This was achieved by reducing the adjustment of the weights based on the difference in predicted and actual energies to 1/10thof the weight the difference in predicted and actual forces in the force model, and similarly reducing the impact of the predicted to actual forces in the energy model. The force model was trained until errors were below 0.2 eV/\u00c5.\n\nSECTION: IIIResults and discussion\n\nAs a prototypical correlated-electron material, elemental Ce has been extensively studied for decades[70,71,72], with computational focus directed at the isostructuraltransition, which has been described as a collapse due the large decrease in volume (17%) as pressure increases past 0.8 GPa[73].\nEfforts have also been made to explore the intermediate pressure phases of Ce within the 5-12 GPa range, where it typically undergoes a structural phase transition from a face-centered cubic phase (,-Ce) to either a monoclinic (,-Ce) or orthorhombic (,-Ce) phase, depending on sample preparation conditions[74,75,76]. However, the stability and synthesis conditions for these intermediate-pressure pathways remain ambiguous. Some studies advocate for the monoclinic phase as most stable[77,78]while others argue that the orthorhombic phase[79,80]is most stable at these conditions. Recent investigations suggest that the orthorhombic phase is favored at higher temperatures, whereas cold-working tends to stabilize the monoclinic phase[75]. There is a notable scarcity of computational studies investigating the transition pathways for these phase transformations.\nTraditional DFT struggles to account for the correlation effects present in Ce, and beyond DFT methods are required for accurate descriptions of the potential energy surface. In addressing the computational complexity associated with exploring these transition pathways, our study employs a MLIP, trained with data from LDA+Gutzwiller calculations, to investigate the transition pathways from thephase to these intermediate pressure phases.\n\nOne limitation of the Gutzwiller solver is the neglect of electronic entropy. Since the Gutzwiller solver runs at T = 0 K, it does not account for the mixing of higher energy states, and the ground state calculated is simply the lowest energy state. While this limitation is not expected to significantly affect the final pathway calculated, it is important to note that finite temperature extensions of the Gutzwiller solver[81]may be employed in the future to increase the accuracy of results.\n\nPredicted energies from our interatomic potential are in agreement with those obtained from the Gutzwiller solver, as demonstrated in our results (Fig.1(a)). While higher energy phases show more errors, the accuracy for low energy structures (critical for transition pathways) is quite high, which lends confidence to our computational predictions, as transition pathways are typically comprised of structures from the low energy regime. Shown in Fig.1(b) is a histogram of the errors for structures with a Gutzwiller energy within 200 meV/atom of thephase. For these low energy structures, the error is clustered more tightly around 0 than for higher energy structures. The energies observed along the transition pathway are expected to stay within 100-150 meV/atom of thephase, as typical barrier heights generally do not exceed this value[82,83]. The mean average error for all the structures is 50 meV/atom. However, for structures with Gutzwiller energies within 200 meV/atom of thephase energy, the mean average error is 11.4 meV/atom.\n\nFurthermore, since we are calculating the final free-energy values using a DMFT solver, the relative energies,andphases should be highly accurate and not susceptible to the error produced within the MLIP. That error will show itself within the transition barrier height as the MLIP may choose a less optimal transition pathway. However, the barrier height difference is greater than the 11.4 meV/atom error, and the free energies of the final transition pathway are calculated using full DMFT, which should provide accurate free energies independent of the method used to generate the structures along the transition pathway. With DMFT free energies typically showing an error of 1 meV, this corresponds to a temperature error of approximately 11.6 K, which is negligible compared to the temperature range investigated here.\n\nUsing the solid-state NEB method, we explored transitions under 7.5 GPa of pressure and checked the results using a full DMFT treatment calculated at 116 K and 400 K. We calculated transition pathways between each of the two phases using the MLIP as a solver for the solid state NEB method. Since the end points are fixed, we do not expect to see overshoots in the beginning and final structures. As our goal is to investigate potential experimental transition pathways to theandphases from thephase, overshoots are not necessary in this case.\n\nThe initial and final structures were chosen to minimize the Gibbs Free Energy at 7.5 GPa. For thephase, structural parameters were obtained from Ref.[78]with data taken from the measurements at a pressure of 0.94 GPa. For theandphases, structural parameters were taken from Ref.[77]for the data at 7.5 GPa. For all phases, a Gibbs free energy versus volume graph was constructed and the volume that minimized the Gibbs free energy at 116K was chosen for the endpoints.\n\nSince the MLIP is trained on structures near the phase space of linearly interpolated pathways between each of the stable phases, the transition pathway found will be comprised of structures close to the linearly interpolated pathway as well. It is unlikely that the transition pathway would take the structures very far away from this regime as the energetic cost of such a transition would be quite high. However, the possibility exists and can be further investigated with a more accurate generalized potential comprised of training data from regimes outside the scope of this study. Since the solid state NEB method discovers only one pathway for each phase transition, other techniques such as umbrella sampling[84], evolutionary algorithms[85], or swarm intelligence methods[86]may be employed in the future to increase the accuracy of results. Training the MLIP on finite temperature data may also lead to greater accuracy for specific temperatures.\n\nThe free energies of the structures along the predicted transition pathways were then calculated using DMFT at 116 K and 400 K to obtain the shifts in energetics at different temperatures. DMFT includes electronic entropic contributions at finite temperatures, making it more suitable for checking the energetics of the predicted pathways than the Gutzwiller method. In addition, using DMFT allows us to mitigate the errors introduced by the MLIP by providing a full re-calculation of the free energy at a finite temperature. However, generating training data for-electron materials with many DMFT calculations is extremely computationally cumbersome. Therefore, the Gutzwiller solver is preferable for training data generation, as it captures many of the shifts in electronic density produced by strong correlation effects while allowing for rapid calculations.\n\nAt the lower temperature of 116 K (Fig.2a), thephase is predicted to transition to the monoclinicphase, which has the lowest Gibbs free energy. The transition to the orthorhombic-Ce phase, while still exothermic, involves a higher reaction barrier and higher Gibbs free energy at 116 K, suggesting a less favored phase transition under these conditions. These findings agree with the experimental observations and support the hypothesis that cold-worked samples prefer the monoclinic phase, as it has the lowest Gibbs free energy at 116 K. The difference in the height of the reaction barriers is around 80 meV/atom, which is quite a bit higher than the error of the MLIP for structures within 100 meV of thephase and larger than the energy scale of the temperatures under consideration. While the exact value may differ, the qualitative behavior of thepathway having a higher reaction barrier at this temperature should hold. The differences in free energies for the two phases should also be accurate as those are calculated using DMFT with no error introduced by the MLIP.\n\nAs the temperature increases to 400 K, the energy landscape shifts (Fig.2b). The reaction barriers stay at roughly the same height. However, the orthorhombic-Ce phase becomes the lowest energy state by around 3 meV/atom, indicating an increased likelihood of obtaining-Ce at higher temperatures.\nThis observation aligns with the established phase diagram and supports the hypothesis that thephase is a metastable state at higher temperatures[75]. The reduction in observation of thephase at such temperatures can be attributed to the role of thermal energy in favoring the orthorhombic phase, even with a higher reaction barrier.\n\nIt should be noted that the equilibrium structures may not appear to be saddle points in the pathway as the flattening is not complete near the endpoints. This is due to the different methods used to calculate the pathway. Specifically, the equilibrium phase volumes are optimized using the Gibbs free energy from DMFT, while the intermediate states are calculated using the solid-state NEB method with the MLIP. This causes the curvature near the endpoints to be slightly steeper than if one method was used throughout. The MLIP also predicted slightly lower barrier heights, around 20 meV lower for thephase and 10 meV lower for thephase compared to DMFT, although the general shape of the curve remained the same.\n\nWe also track entropy across the transition pathway as shown in Fig.2c at a temperature of 400 K. Since spin-orbit coupling splits the Cerium-orbitals into 6 degenerateand 8 degeneratestates, we represent the entropy of the single particle local space dimension of the 6-fold degeneratestates byline and the entropy of the full orbital by theline. Entropy calculations further elucidate the phase stability, revealing higher entropy for thephase, which corroborates its thermal stabilization.\n\nWe report only the 400 K run results for the entropy calculations as CTQMC cannot accurately sample the parts of the partition function required to reliably estimate electronic entropy as low temperatures. This is due to the sampling of terms in the hybridization expansion of the action being pushed to higher orders as temperature is lowered. Since the technique for estimating electronic entropy involves the zeroth order term, this leads to problems at low temperatures where insufficient sampling of the low order diagrams makes data unreliable[46].\n\nA large coexistence region of these two phases is reasonable in light of the competing mechanisms. Based on our calculations, the entropic contributions are strong for thephase, which competes against the lower reaction barrier for thepathway. Given that fluctuations in temperature during sample preparation are inevitable, either phase may be produced depending very heavily on the conditions of preparation.\nIt is also worth noting that reaction barriers may be substantially larger when elastic effects are taken into account, as indicated by Bustingorry et al.[87]. In our current study, the reaction barriers are calculated without explicit elastic interactions between domains or grains, which may lead to an underestimation of the true barrier height. Accounting for elastic effects in future calculations could provide a more comprehensive view of the phase stability and transition pathways, particularly under non-uniform stress conditions.\n\nBuilding upon our insights from elemental Ce, we extend our investigation to NiO, a prototypical Mott insulator emblematic of strongly correlated compounds. First studied in the 1930s[89,90,91], the unique properties of NiO are the result of the strong correlation among its 3electrons[92,93]. Experimentally, it has been established that NiO remains insulating under extremely high pressures[56], and NiO can be considered as a representative compound for the behavior of correlated materials under the extreme conditions found within the Earth. Accurate predictions of large-scale thermal properties, such as melting curves, are essential for understanding the structure inside the Earth. These properties, however, are challenging to calculate with conventional DFT due to its failure to accurately describe the ground state of strongly correlated materials[94,95].\n\nAddressing these challenges, we use graph neural networks trained on energies and forces from LDA+DMFT calculations. DMFT inherently includes the dynamical correlation effects in the many-body Hamiltonian, offering an accurate prediction of electronic properties. DMFT is also less sensitive to the particular Hubbard U value used. Under pressure, the U value is liable to change, but this effect affects final results less when performing DMFT calculations as compared to techniques like DFT+U. This approach also benefits from the inclusion of many-body effects in force calculations, potentially leading to significant structural insights when contrasted with standard DFT predictions[96].\nTo ensure sufficient transferability, the MLIP is trained\non a variety of data generated by random structure generation as well as energy versus volume data generated from the face centered cubic (), trigonal (), and body centered cubic () structures. LDA+DMFT calculations were run at 611 K, above the Neel temperature for NiO. Since our goal is to investigate melting curves, NiO will be within the paramagnetic phase near the melting temperature and the training data should reflect the target electronic structures. Higher temperatures for the DMFT solver may be chosen, but this does not affect the electronic structure significantly. Calculations done at 611 K and 2000 K using DMFT for the FCC structure show only a 0.004 difference in the occupation number of the impurity site. The full Coulomb interaction was taken into account when solving the impurity problem.\n\nThe accuracy of the machine learning network is depicted in Fig.3a, showing close alignment between the predicted and actual energies and forces. The error for the 150 validation structures is around 19 meV/atom, which is far below the energy scale for the temperatures of interest. Since we are looking at temperatures of 2000 K and greater, this corresponds to an energy scale of at least 172 meV. The error of 19 meV/atom is fairly uniform across the whole energy scale, increasing slightly at energies more than 1 eV/atom above the lowest energy structure.\n\nFigure3b shows the equilibrium volume predictions under varying pressures for the face-centered cubic phase of NiO, as derived from various computational methods and techniques. The LDA+DMFT calculations were run at a temperature of 611 K so that NiO is within the paramagnetic phase. Wien2k[44]and VASP[44]were both run with the PBESol[66]GGA exchange correlation functional at zero temperature. Experimental results were gathered at room temperature, which is around 300 K. The MLIP shows good agreement with LDA+DMFT calculations in a wide pressure range, indicating a high degree of accuracy. Notably, both the MLIP and LDA+DMFT calculations surpass the traditional DFT predictions from Wien2k and VASP when compared to the experimental data. The MLIP provides an energy versus volume curve in good agreement with that from LDA+DMFT, yet it achieves this at a significantly reduced computational cost.\n\nWe have also performed Quasi-Harmonic Approximation (QHA) calculations using the PBE functional within VASP as well as the Phonopy package[97,98,99]in order to investigate thermal expansion effects. The QHA allows the inclusion of lattice vibration entropy as well as energy contributions from lattice vibrations in order to investigate the Gibbs free energy under varying temperature and pressure. The teal left-pointing arrows in Figure3(b) show the equilibrium volume at a temperature of 611 K for QHA calculations run using the PBE GGA functional[67]. The PBESol functional was not used as it indicated that volume decreases at high temperatures. A previous study has found PBE to be closer to experimental lattice constants in AFMII NiO, which PBESol significantly underestimates[94]. We have also found PBESol to underestimate lattice constants in the paramagnetic cubic phase. The temperature of 611 K was chosen in order to give a fair comparison with the ML potential and the LDA+DMFT calculations. The agreement with experiment is quite good for the QHA calculations and show that lattice vibrational entropy is quite significant in NiO. However, since zero temperature DFT calculations are used for these calculations, results may be quite different if the ground state charge density at zero temperature differs significantly from the charge density at finite temperatures. DMFT should provide more consistent results at finite temperature across a wider range of structures.\n\nNext, we performed molecular dynamics calculations to estimate the melting curve of NiO under high pressures. Theoretical studies on the melting curves of strongly correlated materials are rare, due to the challenges associated with accurately simulating these phenomena across different pressure conditions. Leveraging a neural network that directly predicts energies and forces enables us to perform supercell calculations more cost-effectively than traditional DFT, while still capturing some of the force renormalization effects.\n\nFor our melting curve investigation, we used the Z method owing to its efficiency and proven reliability in experimental comparisons[100,101,102,103]. Focusing on high-temperature conditions, we constructedsupercells derived from the face-centered cubic structure for our molecular dynamics simulations in the NVE ensemble with a spread of initial temperatures from a Maxwell-Boltzmann distribution.\nOur simulations were performed near the melting point, which is well above the Neel temperature of 525 K. Therefore, the starting structure for the molecular dynamics simulation was chosen to be the face-centered cubic structure (), as this is the structure that NiO adopts above its Neel temperature[104,88].\nThe system was allowed to equilibrate over 10 picoseconds with a timestep of 1 femtosecond, with thermal averages of relevant observables taken over the final 2 picoseconds. The melting curve was modeled using the Simon-Glatzel equation, a method that has been successful with other transition metal compounds[105,106,107,108], represented as:\n\nwhereis the melting temperature at a pressure. The parameters of the Simon-Glatzel equation were obtained by a non-linear least squares fit to the pressure-temperature points along the isochores where the full melt occurred. This is indicated by an increase in pressure and a simultaneous drop in temperature.\nFig.4presents the melting curve of NiO in Pressure-Temperature (PT) space, mapped using isochores of 70, 65, and 60. The MLIP effectively captures the melting behavior of NiO, with the predicted melting curve aligning well with experimental data at atmospheric pressure[109]. The alignment of the MLIP with experimental data suggests a reliable model for predicting the behavior of strongly correlated materials under extreme conditions akin to those deep within the Earth. Therefore, this approach, integrating data from DMFT into graph neural networks, offers a novel and efficient pathway to explore the complex melting behavior of strongly correlated materials like NiO.\n\nSECTION: IVconclusion\n\nIn summary, we have demonstrated the utility of MLIPs in accelerating investigations of dynamics of correlated systems. For Cerium, training data for the MLIP was generated using the LDA+Gutzwiller method, as generation of force data from DMFT for-electron compounds with significant spin-orbit coupling is beyond our computational capabilities. The finite difference method was used to calculate forces and stresses, and these in turn were used to predict viable transition pathways between intermediate pressure phases of Cerium. The resulting transition pathway supports the claim that thephase is more stable than-Ce at low temperatures. The transition barrier to thephase decreases with increasing temperature while the Gibbs free energy of thephase decreases below that of thephase at high temperatures. Given the larger reaction barrier to thephase at both low and high temperatures, but lower Gibbs energy at high temperatures, it is no surprise a large coexistence region for these two phases exists. For NiO, full DMFT calculations became feasible due to the smaller orbital size and the reduced effects of spin-orbit coupling. While still quite expensive to converge, training data of both energies and forces were obtained and an ML interatomic potential was trained on this data. Molecular dynamics simulations were carried out to determine the melting point of this material, with good agreement to the experimental ambient-pressure melting temperature. We hope in the future that more MLIPs can be trained for correlated systems, unlocking new avenues of transition state searching, theoretical thermodynamic predictions, and eventually crystal structure prediction of strongly correlated systems.\n\nSECTION: References", "text_file": "data\\paper_texts\\2404.08782v3_content.txt"}, {"title": "Navigating Perplexity: A linear relationship with the data set size in\n  t-SNE embeddings", "authors": ["Martin Skrodzki", "Nicolas F. Chaves-de-Plaza", "Thomas H\u00f6llt", "Elmar Eisemann", "Klaus Hildebrandt"], "published_date": "2023-08-29T16:24:11Z", "summary": "Widely used pipelines for analyzing high-dimensional data utilize\ntwo-dimensional visualizations. These are created, for instance, via\nt-distributed stochastic neighbor embedding (t-SNE). A crucial element of the\nt-SNE embedding procedure is the perplexity hyperparameter. That is because the\nembedding structure varies when perplexity is changed. A suitable perplexity\nchoice depends on the data set and the intended usage for the embedding.\nTherefore, perplexity is often chosen based on heuristics, intuition, and prior\nexperience. This paper uncovers a linear relationship between perplexity and\nthe data set size. Namely, we show that embeddings remain structurally\nconsistent across data set samples when perplexity is adjusted accordingly.\nQualitative and quantitative experimental results support these findings. This\ninforms the visualization process, guiding the user when choosing a perplexity\nvalue. Finally, we outline several applications for the visualization of\nhigh-dimensional data via t-SNE based on this linear relationship.", "arxiv_id": "2308.15513v2", "html_link": "https://arxiv.org/html/2308.15513v2", "search_term": "ti:\"embeddings\"", "html_content": "SECTION: LaTeXAuthor Guidelines for EUROGRAPHICS Proceedings Manuscripts\n\nThe ABSTRACT is to be in fully-justified italicized text,\nbetween two horizontal lines,\nin one-column format,\nbelow the author and affiliation information.\nUse the word \u201cAbstract\u201d as the title, in 9-point Times, boldface type,\nleft-aligned to the text, initially capitalized.\nThe abstract is to be in 9-point, single-spaced type.\nThe abstract may be up to 3 inches (7.62 cm) long.Leave one blank line after the abstract,\nthen add the subject categories according to the ACM Classification Index\n(see https://www.acm.org/publications/class-2012){CCSXML}<ccs2012>\n<concept>\n<concept_id>10010147.10010371.10010352.10010381</concept_id>\n<concept_desc>Computing methodologies\u00a0Collision detection</concept_desc>\n<concept_significance>300</concept_significance>\n</concept>\n<concept>\n<concept_id>10010583.10010588.10010559</concept_id>\n<concept_desc>Hardware\u00a0Sensors and actuators</concept_desc>\n<concept_significance>300</concept_significance>\n</concept>\n<concept>\n<concept_id>10010583.10010584.10010587</concept_id>\n<concept_desc>Hardware\u00a0PCB design and layout</concept_desc>\n<concept_significance>100</concept_significance>\n</concept>\n</ccs2012>\n\n[300]Computing methodologies\u00a0Collision detection\\ccsdesc[300]Hardware\u00a0Sensors and actuators\\ccsdesc[100]Hardware\u00a0PCB design and layout\n\nSECTION: 1Introduction\n\nPlease follow the steps outlined in this document very carefully when\nsubmitting your manuscript to Eurographics.\n\nYou may as well use theLaTeXsource as a template to typeset your own\npaper. In this case we encourage you to also read theLaTeXcomments\nembedded in the document.\n\nSECTION: 2Instructions\n\nPlease read the following carefully.\n\nSECTION: 2.1Language\n\nAll manuscripts must be in English.\n\nSECTION: 2.2Margins and page numbering\n\nAll printed material, including text, illustrations, and charts,\nmust be kept within a print area 7 inches (17.7 cm) wide by\n9.44 inches (24 cm) high. Do not write or print anything\noutside the print area. Number your pages on odd sites right\nabove, on even sites left above, no page number on the first site.\nPage numbers should be local to each paper in the formatwiththe current page number andthe total number of pages of the respective paper.\n\nSECTION: 2.3Formatting your paper\n\nAll text with the exception of the abstract must be in a two-column format.\nThe total allowable width of the text area \u2013 including header and footer\nlines \u2013 is 177\u2009mm (7 inch) wide by 245\u2009mm (9.64 inch) high.\n\nColumns are to be 84\u2009mm (3.3 inch) wide, with a 8\u2009mm (0.315 inch) space\nbetween them.\n\nThe space between the header line and the first line of the text body and\nbetween the last line of the text body and the footer line is 5\u2009mm\n(0.196 inch) each.\n\nSECTION: 2.4Type-style and fonts\n\nWherever Times is specified, Times Roman may also be used. If\nneither is available on your word processor, please use the font\nclosest in appearance to Times that you have access to. Only\nType-1 fonts will be accepted.\n\nMAIN TITLE. The title should be in Times 17-point, boldface type and\ncentered. Capitalize the first letter of nouns, pronouns, verbs, adjectives,\nand adverbs; do not capitalize articles, coordinate conjunctions, or\nprepositions (unless the title begins with such a word). Leave two blank\nlines after the title.\n\nAUTHOR NAME(s) and AFFILIATION(s) are to be centered beneath the title and\nprinted in Times 9-point, non-boldface type. This information is to be\nfollowed by two blank lines.\n\nThe ABSTRACT ist to be in a one-column format. The MAIN TEXT is to be in a\ntwo-column format.\n\nMAIN TEXT. Type main text in 9-point Times, single-spaced. Donotuse\ndouble-spacing. All paragraphs should be indented 1 em (the length of the\ndash in the actual font). Make sure your text is fully justified \u2013 that is,\nflush left and flush right. Please do not place any additional blank lines\nbetween paragraphs. Figure and table captions should be 9-point Times\nboldface type as in Figure2.\n\nLong captions should be set as in Figure1or\nFigure3.\n\nFigures which need the full textwidth can be typeset as Figure3.\n\nCallouts should be 9-point Times, non-boldface type. Initially\ncapitalize only the first word of section titles and first-, second-, and\nthird-order headings.\n\nFIRST-ORDER HEADINGS. (For example,1. Introduction) should be Times\n9-point boldface, initially capitalized, flush left, with one blank line\nbefore, and one blank line after.\n\nSECOND-ORDER HEADINGS. (For example,2.1. Language) should be Times\n9-point boldface, initially capitalized, flush left, with one blank line\nbefore, and one after. If you require a third-order heading (we discourage\nit), use 9-point Times, boldface, initially capitalized, flush left, preceded\nby one blank line, followed by a period and your text on the same line.\n\nThe headline(authors / title)must be shortened if it uses the full\ntwo column width of the main text.\nThere must be enough space for the page numbers. Please use \u201cet al.\u201d if\nthere are more than three authors and specify a shortened version for your title.\n\nSECTION: 2.5Footnotes\n\nPlease donotuse footnotes at all!\n\nSECTION: 2.6References\n\nList all bibliographical references in 9-point Times, single-spaced, at the\nend of your paper in alphabetical order. When referenced in the text, enclose\nthe citation index in square brackets, for example[Lous90]. Where\nappropriate, include the name(s) of editors of referenced books.\n\nFor your references please use the following algorithm:\n\noneauthor: first 3 chars plus year \u2013\ne.g.[Lous90]\n\ntwo,threeorfourauthors: first char\nof each family name plus year \u2013 e.g.[Fellner-Helmberg93]or[Kobbelt97-USHDR]or[Lafortune97-NARF]\n\nmore than 4authors: first char of family name from\nfirst 3 authors followed by a \u2019*\u2019 followed by the year \u2013\ne.g.[Buhmann:1998:DCQ]or[FolDamFeiHug.etal93]\n\nFor BibTeX users a style fileeg-alpha.bstandeg-alpha-doi.bstis available which uses the above algorithm.\n\nFor Biber users a style fileEG.bbxis available which uses the above algorithm.\n\nSECTION: 2.7Illustrations, graphs, and photographs\n\nAll graphics should be centered.\n\nFor all figures please keep in mind that youmust notuse images with transparent background!\n\nIf your paper includes images, it is very important that they are of\nsufficient resolution to be faithfully reproduced.\n\nTo determine the optimum size (width and height) of an image, measure\nthe image\u2019s size as it appears in your document (in millimeters), and\nthen multiply those two values by 12. The resulting values are the\noptimumandresolution, in pixels, of the image. Image quality\nwill suffer if these guidelines are not followed.\n\nExample 1:\nAn image measures 50\u2009mm by 75\u2009mm when placed in a document. This\nimage should have a resolution of no less than 600 pixels by 900\npixels in order to be reproduced faithfully.\n\nExample 2:\nCapturing a screenshot of your entirepixel display\nmonitor may be useful in illustrating a concept from your research. In\norder to be reproduced faithfully, thatimage should\nbe no larger than 85 mm by 64 mm (approximately) when placed in your\ndocument.\n\nSECTION: 2.8Color\n\nPlease observe:as of 2003 publications in the proceedings of the\nEurographics Conference can use color images throughout the paper. No\nseparate color tables are necessary.\n\nHowever, workshop proceedings might have different agreements!\nFigure3is an example for creating color plates.\n\nSECTION: 2.9Embedding of Hyperlinks / Typesetting of URLs\n\nDue to the use of the packagehyperrefthe original behavior\nof the commandurlfrom the packageurlis not available. To circumvent this problem we either recommend to\nuse the commandhttpAddrfrom the\nincluded packageegweblnk(see below) or to replace the\ncommandurlby the commandwebLink\u2013 e.g. in cases whereurlhas been used\nwidely in BibTeX-References. In the latter case we suggest to run\nBibTeX as usual and then replace all occurences ofurlbywebLink\n\nThe provided commands for hyperlinks, in a nutshell, are:\n\ne.g.http://diglib.eg.org/handle/10.2312/306\n\ne.g.https://diglib.eg.org/handle/10.2312/306\n\ne.g.ftp://www.eg.org/EG/DL/ftpupload\n\ne.g.http://diglib.eg.org/handle/10.2312/306\n\ne.g.publishing@eg.org\n\ne.g.publishing@eg.org\n\ne.g.http://www.eg.org/some_arbitrary_long/but_useless/URL\n\nSECTION: 2.10PDF Generation\n\nYour final paper should be delivered as a PDF document with all typefaces\nembedded.LaTeXusers should usedvipsandps2pdfto\ncreate this PDF document. Adobe Acrobat Distiller may be used in place ofps2pdf.\n\nAdobe PDFWriter isnotacceptable for use. Documents created with\nPDFWriter will be returned to the author for revision.pdftexandpdflatex(and its variants) can be used only if the author can\nmake certain that all typefaces are embedded and images are not downsampled\nor subsampled during the PDF creation process.\n\nUsers with no access to these PDF creation tools should make available a\nPostScript file and we will make a PDF document from it.\n\nThe PDF filemust notbe change protected.\n\ndvipsshould be invoked with the-Ppdfand-G0flags in order to use Type 1 PostScript typefaces:\n\nIf you are using version 7.x of GhostScript, please use the following method of invokingps2pdf, in\norder to embed all typefaces and ensure that images are not downsampled or subsampled in the PDF\ncreation process:\n\nIf you are using version 8.x of GhostScript, please use this method in place of the example above:\n\nConfiguration of these tools to embed all typefaces can be accomplished by editing theupdmap.cfgfile\nto enable inclusion of the standard (or base) 14 typefaces.\n\nLinux users can run theupdmapscript to do this:\n\nWindows users should edit theupdmap.cfgfiles found in their TeX installation directories (one or both\nof the following may be present):\n\nEnsure the value forpdftexDownloadBase14is \"true,\" and then follow the instructions found here:http://docs.miktex.org/manual/to update your MikTeX installation.\n\nWe recommend to use a Distiller job options file that embeds\nall typefaces and does not downsample or subsample images when creating the PDF document.\n\nSECTION: 2.11License Form\n\nYou must include your signed Eurographics License Form\nwhen you submit your finished paper. We MUST have this form before\nyour paper can be published in the proceedings.\n\nThere is a separate workflow for signing licenses for Computer Graphics Forum\n(regular issues, conference issue, special issues).\n\nSECTION: 2.12Conclusions\n\nPlease direct any questions to the production editor in charge of\nthese proceedings.", "text_file": "data\\paper_texts\\2308.15513v2_content.txt"}]]]