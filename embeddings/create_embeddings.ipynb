{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "from utils import create_document_embeddings, get_document_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_EMBEDDINGS = r\"../data/embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.09693518e-02,  5.88474944e-02,  2.82677300e-02,\n",
       "         2.62138695e-02, -9.06362943e-03, -7.64047354e-02,\n",
       "         5.53913191e-02, -3.68839912e-02, -7.84823671e-04,\n",
       "        -1.27433864e-02,  1.58711728e-02,  2.14681029e-02,\n",
       "         6.23353664e-03,  8.90833605e-03, -7.68442303e-02,\n",
       "         8.39350000e-03, -1.81107409e-02,  1.22036822e-02,\n",
       "        -1.23330548e-01,  2.18525846e-02, -2.26291306e-02,\n",
       "         4.76487502e-02, -2.50382796e-02,  3.09831463e-02,\n",
       "        -5.96614145e-02,  3.93032730e-02,  4.28193845e-02,\n",
       "         2.59722732e-02, -1.50488382e-02, -6.17700703e-02,\n",
       "         2.94973440e-02,  4.22532260e-02,  3.13867591e-02,\n",
       "        -2.40303874e-02,  9.24553722e-03,  6.24796227e-02,\n",
       "        -6.07810020e-02, -7.88905621e-02,  2.15569939e-02,\n",
       "         7.95919355e-03,  1.69848315e-02, -1.63327418e-02,\n",
       "        -4.67240810e-02, -1.32707031e-02,  2.02383064e-02,\n",
       "         2.97268182e-02, -2.98732631e-02, -1.25523005e-02,\n",
       "         8.27551261e-02,  5.37742898e-02, -1.58221647e-02,\n",
       "        -2.77961344e-02, -3.48346941e-02,  3.86657640e-02,\n",
       "         6.19013198e-02,  9.06479731e-03,  6.26904368e-02,\n",
       "        -4.83573601e-02,  3.07563995e-03, -1.19147366e-02,\n",
       "        -1.77879781e-02,  3.64505798e-02, -5.54778427e-02,\n",
       "         5.75296357e-02,  8.02568644e-02, -4.62392159e-02,\n",
       "        -1.56093743e-02,  1.58683881e-02, -3.75822559e-02,\n",
       "         3.69687751e-03, -3.70853357e-02,  6.15685247e-04,\n",
       "         1.84067208e-02,  4.34003212e-02, -4.08551022e-02,\n",
       "         4.78404649e-02, -3.18526011e-03, -3.84088159e-02,\n",
       "         1.38297556e-02,  2.15528943e-02,  7.82871842e-02,\n",
       "        -8.25728625e-02, -6.10168092e-03,  2.29489058e-04,\n",
       "        -2.16912571e-02, -5.86359855e-03, -8.30585975e-03,\n",
       "        -1.35945100e-02,  2.34500878e-02, -2.82312557e-02,\n",
       "        -9.57145840e-02,  3.01771387e-02,  3.08088865e-02,\n",
       "         1.15250219e-02, -7.85432011e-02, -7.28690773e-02,\n",
       "         2.80596949e-02, -1.86854880e-03, -1.10536583e-01,\n",
       "         2.26134032e-01,  1.41833033e-02,  1.94919612e-02,\n",
       "         1.00536766e-02,  2.05720589e-03,  3.60470265e-02,\n",
       "         3.69646819e-03, -1.50754163e-02,  2.61372514e-02,\n",
       "        -2.33011730e-02, -2.76932679e-02, -3.59048545e-02,\n",
       "        -1.01148896e-02, -9.19284299e-03,  1.15039367e-02,\n",
       "         4.86870334e-02, -1.28888674e-02, -1.55881997e-02,\n",
       "         5.75491749e-02, -1.22397523e-02, -9.98167600e-03,\n",
       "         4.77973297e-02, -1.55187016e-02,  1.03720473e-02,\n",
       "         2.76245549e-02, -3.54209803e-02, -1.82255416e-03,\n",
       "         5.68489581e-02, -6.34204255e-33,  3.46260518e-02,\n",
       "        -4.15359139e-02,  1.83909293e-02,  1.38616905e-01,\n",
       "         3.58069912e-02,  3.74557488e-02, -5.97899146e-02,\n",
       "        -1.34772649e-02,  2.33585723e-02,  1.61528960e-03,\n",
       "         4.17664871e-02,  3.83370221e-02, -2.50702687e-02,\n",
       "        -2.70933062e-02,  1.57864802e-02,  5.27380258e-02,\n",
       "        -1.90344471e-02, -2.13990966e-03,  4.63560112e-02,\n",
       "         4.63843867e-02, -1.21530797e-02,  2.87792590e-02,\n",
       "         2.24013217e-02,  7.71038979e-02, -2.05152296e-03,\n",
       "         1.43935881e-03, -8.94951634e-04, -6.40047714e-02,\n",
       "         6.71395585e-02,  9.37636849e-03, -1.70833189e-02,\n",
       "        -2.57587004e-02,  1.90387778e-02,  1.95822306e-02,\n",
       "        -1.72508843e-02, -9.88639612e-03, -4.34764326e-02,\n",
       "        -7.87290372e-03, -6.37012348e-02, -3.41084078e-02,\n",
       "        -3.73809971e-03,  1.91361103e-02, -2.13702302e-02,\n",
       "        -1.75026748e-02, -6.71249535e-03, -2.85104103e-02,\n",
       "         8.35429132e-02,  1.79861728e-02,  2.09534243e-02,\n",
       "         2.19074227e-02, -5.78254275e-02, -1.70850381e-02,\n",
       "        -4.78337780e-02,  1.54403690e-02, -2.56244931e-02,\n",
       "        -6.96799532e-03,  4.23160754e-03, -4.45373356e-04,\n",
       "        -2.87175365e-02, -1.88719667e-03,  3.75486426e-02,\n",
       "         7.55455196e-02, -3.27283777e-02, -4.25241105e-02,\n",
       "        -1.21297929e-02, -2.73582917e-02,  9.21540987e-03,\n",
       "         3.38814920e-03,  1.73714161e-02, -2.21022009e-03,\n",
       "        -2.66367346e-02,  1.53022539e-03,  2.07471270e-02,\n",
       "         2.32728533e-02, -1.09091243e-02,  2.09847279e-02,\n",
       "         5.32023832e-02,  1.59894512e-03,  5.78964613e-02,\n",
       "        -1.93536505e-02,  4.74384502e-02,  5.49511462e-02,\n",
       "        -2.96085924e-02, -4.60637584e-02,  1.18923351e-01,\n",
       "         1.78839900e-02, -2.29240824e-02, -1.10155702e-01,\n",
       "        -2.35426389e-02,  2.52600871e-02, -1.10670835e-01,\n",
       "         4.39351127e-02,  5.23444265e-02, -2.41233557e-02,\n",
       "        -6.72687218e-02,  4.47845201e-33,  8.35904479e-02,\n",
       "         1.09522101e-02, -8.85711797e-03, -3.04099359e-02,\n",
       "        -7.50769526e-02, -3.43341008e-02, -3.73862423e-02,\n",
       "         1.39759108e-01, -6.07286096e-02,  3.65861878e-02,\n",
       "         4.05698679e-02, -1.06162848e-02,  1.03162065e-01,\n",
       "        -8.60946253e-04,  1.77603625e-02,  3.82372886e-02,\n",
       "         1.08310498e-01,  1.61976814e-02, -6.41521672e-03,\n",
       "        -2.21789908e-03, -3.36271748e-02, -3.39412130e-03,\n",
       "        -5.60246184e-02, -2.76991557e-02, -2.68587321e-02,\n",
       "         5.06005157e-03,  4.32401896e-02,  1.14609879e-02,\n",
       "        -7.58414790e-02,  6.94714487e-04,  2.21074075e-02,\n",
       "         2.46916153e-02,  5.58101572e-03, -7.30403326e-03,\n",
       "        -2.74671614e-02,  6.90159947e-02, -5.47379144e-02,\n",
       "        -1.67864747e-02, -3.54237705e-02, -1.21407751e-02,\n",
       "        -5.92066497e-02, -3.87065019e-03,  4.91185077e-02,\n",
       "         3.57839763e-02, -5.42306677e-02, -8.29543173e-03,\n",
       "        -6.00189604e-02,  3.90802883e-02, -6.65877014e-05,\n",
       "         6.60723820e-03, -4.08752002e-02, -2.97073387e-02,\n",
       "         2.97125168e-02, -3.95721756e-04, -2.18459144e-02,\n",
       "         1.31609477e-03, -2.22333297e-02,  2.42898823e-03,\n",
       "         1.45017337e-02, -2.74800062e-02,  2.02264693e-02,\n",
       "         1.68333016e-02,  2.35508308e-02,  7.79650509e-02,\n",
       "         2.46541435e-03, -2.36136876e-02,  8.22580914e-05,\n",
       "         2.89417524e-02, -6.15908206e-03, -4.11971509e-02,\n",
       "        -1.01314010e-02, -1.30252875e-02, -6.12647235e-02,\n",
       "         3.70552088e-03, -3.84945683e-02,  5.39595038e-02,\n",
       "        -2.84797624e-02, -4.01221141e-02, -1.00330906e-02,\n",
       "         8.41140933e-03, -2.17131246e-02,  3.06524113e-02,\n",
       "        -7.93640688e-03, -5.01234410e-03,  1.36795249e-02,\n",
       "        -1.37860365e-02,  1.46634700e-02,  3.80556844e-02,\n",
       "         6.03998546e-03, -3.18470970e-03,  5.45486622e-03,\n",
       "        -2.31345743e-02,  1.98837016e-02,  7.68553978e-03,\n",
       "        -4.78334241e-02, -1.65706950e-08, -3.62674743e-02,\n",
       "         1.96403656e-02,  8.41286127e-03, -1.88794360e-03,\n",
       "         1.79894175e-03,  7.45434873e-03, -8.34205002e-03,\n",
       "        -1.02810059e-02, -2.82035470e-02,  4.74875346e-02,\n",
       "         3.63293625e-02,  7.06043914e-02, -4.98814508e-02,\n",
       "         3.52778807e-02,  3.60813029e-02,  5.07130399e-02,\n",
       "        -7.35512655e-03, -1.20986504e-02,  9.69251618e-04,\n",
       "         5.38794370e-03, -6.14887401e-02, -3.66657749e-02,\n",
       "         1.14868488e-02,  4.36547026e-03,  2.10815389e-02,\n",
       "        -2.85507478e-02, -2.32179780e-02,  1.87186152e-02,\n",
       "         1.68767888e-02, -4.33162972e-02,  2.73876227e-02,\n",
       "         8.03870186e-02, -3.92223597e-02,  2.22178921e-02,\n",
       "        -9.79230851e-02, -3.62527482e-02,  8.21389258e-03,\n",
       "         7.37951696e-02,  3.88250537e-02, -2.63880566e-03,\n",
       "        -6.00122288e-02,  6.24546558e-02, -2.44296119e-02,\n",
       "        -8.15433189e-02, -1.48682352e-02, -4.53550741e-03,\n",
       "         1.00343771e-01, -1.05755879e-02, -3.03916670e-02,\n",
       "        -4.04958799e-02, -1.14619713e-02,  7.05078198e-03,\n",
       "         2.50345208e-02,  3.00265756e-03,  1.07809633e-01,\n",
       "         4.03891690e-02,  1.40235797e-02,  3.02011315e-02,\n",
       "        -5.69148622e-02,  3.21446396e-02,  8.69862363e-02,\n",
       "        -2.98644789e-03,  2.06387788e-02, -7.54485093e-03],\n",
       "       [-9.81531944e-03,  2.35766619e-02, -1.74512994e-02,\n",
       "         2.15991512e-02,  3.00013926e-04, -6.78647757e-02,\n",
       "         4.15223613e-02, -5.70120476e-03,  2.97212135e-02,\n",
       "         6.97847456e-04, -2.87760440e-02,  1.21285245e-02,\n",
       "        -1.33539131e-03,  2.20762175e-02, -4.56222594e-02,\n",
       "        -2.52246913e-02, -2.48372927e-03, -1.75552014e-02,\n",
       "        -7.48171806e-02,  3.82562354e-02, -1.82060916e-02,\n",
       "         2.43612863e-02, -5.31831086e-02,  9.40748118e-03,\n",
       "         1.38034308e-02,  1.43620204e-02,  3.36679257e-03,\n",
       "         1.25367707e-02, -2.61731800e-02, -3.30181629e-03,\n",
       "        -5.84460795e-02,  3.10840737e-02,  6.24040104e-02,\n",
       "         1.91703252e-02, -3.29122692e-02, -3.25652994e-02,\n",
       "         2.26149540e-02,  1.47987148e-02,  2.19508428e-02,\n",
       "         2.69419737e-02,  7.14395335e-03, -8.50974470e-02,\n",
       "        -3.01573034e-02,  2.51561701e-02,  1.57140419e-02,\n",
       "         3.97453755e-02,  3.27494740e-03,  1.21566616e-02,\n",
       "        -3.77554782e-02,  2.55566929e-02, -2.56551132e-02,\n",
       "        -5.45889279e-03, -7.39361495e-02, -4.19519283e-02,\n",
       "         3.61523628e-02,  3.60052213e-02,  2.71617156e-02,\n",
       "         1.48499217e-02,  5.73183820e-02,  3.37625034e-02,\n",
       "         1.55306337e-02,  2.59899888e-02, -3.02649066e-02,\n",
       "         2.72251535e-02,  6.64996207e-02,  3.23333293e-02,\n",
       "        -5.32774627e-02, -8.57716203e-02, -4.09347564e-02,\n",
       "         1.49978511e-03,  9.74346709e-04, -4.25444962e-03,\n",
       "         4.88164537e-02,  4.19479646e-02,  7.80033693e-03,\n",
       "        -4.95092478e-03, -1.13165714e-02, -4.98970337e-02,\n",
       "         8.29941332e-02,  4.26015854e-02, -4.70788926e-02,\n",
       "        -6.01256453e-02, -1.29455952e-02,  1.48110595e-02,\n",
       "        -1.78139172e-02,  2.84999721e-02,  7.04844072e-02,\n",
       "         1.62472632e-02, -4.22706306e-02, -2.92385388e-02,\n",
       "         1.51000861e-02,  1.01519655e-02, -4.96966168e-02,\n",
       "        -5.46463812e-03, -3.47588360e-02, -4.79987031e-03,\n",
       "        -2.08817422e-03,  1.45287989e-02, -5.40667325e-02,\n",
       "         1.20056108e-01, -1.02231111e-02,  3.29581760e-02,\n",
       "         4.17533144e-03, -8.86182114e-03, -3.78152765e-02,\n",
       "        -2.85845250e-02, -3.13826017e-02,  6.85686618e-03,\n",
       "         2.17862763e-02, -1.85801517e-02, -2.87364665e-02,\n",
       "        -6.99801184e-03,  3.17430198e-02,  6.99755326e-02,\n",
       "         2.80958824e-02,  2.96430383e-02, -4.70575243e-02,\n",
       "         4.49985936e-02,  2.35868916e-02, -8.02430883e-03,\n",
       "         3.28444429e-02, -3.12867351e-02, -2.86202668e-03,\n",
       "        -3.99081595e-03, -3.67209464e-02, -2.79789269e-02,\n",
       "         4.73879743e-03, -2.43629690e-33, -8.40735901e-03,\n",
       "        -2.24443898e-03,  2.68099997e-02,  4.59298566e-02,\n",
       "         3.76649052e-02,  1.77158825e-02, -2.78782211e-02,\n",
       "         1.83412060e-03, -5.70693761e-02, -4.14139554e-02,\n",
       "        -4.04895991e-02, -4.60329913e-02, -4.60844301e-02,\n",
       "        -3.41532379e-03,  8.54000896e-02,  5.61898500e-02,\n",
       "        -3.86896692e-02,  9.26372707e-02, -7.04161525e-02,\n",
       "         2.47562658e-02, -6.59145117e-02,  5.08970208e-03,\n",
       "        -2.19468754e-02, -5.39304055e-02, -6.09288737e-02,\n",
       "         3.61464918e-03, -2.27505118e-02,  1.06438319e-03,\n",
       "        -2.91253962e-02,  6.49873167e-02, -2.39239130e-02,\n",
       "         5.49400635e-02, -6.38103932e-02,  7.96178635e-03,\n",
       "         1.29781757e-03,  2.55999528e-03,  3.80792655e-02,\n",
       "        -8.40036571e-02,  8.27674940e-03,  3.61243151e-02,\n",
       "        -1.27555020e-02, -8.62624124e-03, -2.17521619e-02,\n",
       "         1.64141208e-02,  2.73049399e-02,  9.57359001e-03,\n",
       "        -3.27730365e-02, -9.41463932e-03,  7.13664740e-02,\n",
       "        -1.25231780e-02, -2.11092960e-02,  4.55834996e-03,\n",
       "        -2.31096111e-02,  4.47456259e-03, -3.81362662e-02,\n",
       "         2.58888267e-02,  3.85401808e-02, -2.86011770e-02,\n",
       "        -4.32884470e-02,  6.59750253e-02,  5.66387177e-02,\n",
       "         5.95408082e-02, -3.64518873e-02, -9.03695635e-03,\n",
       "        -4.43746597e-02, -3.96755710e-02, -2.56538279e-02,\n",
       "        -8.42630025e-03, -3.70112956e-02, -3.16315889e-03,\n",
       "         1.05076563e-03, -2.46801488e-02,  2.12235563e-03,\n",
       "        -5.46784513e-03, -1.50476955e-03, -3.81497405e-02,\n",
       "        -4.53373119e-02,  4.73721959e-02,  3.35454009e-03,\n",
       "        -4.05582786e-03,  2.89142877e-02, -9.10957307e-02,\n",
       "         2.69215629e-02, -3.28836180e-02, -3.43099236e-05,\n",
       "         4.50647343e-03, -2.20886357e-02, -9.09062400e-02,\n",
       "        -4.01523616e-03, -5.52195869e-02, -5.75608537e-02,\n",
       "         1.84851326e-02,  2.06807330e-02, -1.52529404e-02,\n",
       "         2.00782884e-02, -5.10819519e-34, -5.22218645e-02,\n",
       "         4.38273102e-02, -6.24163561e-02,  3.26986462e-02,\n",
       "         7.67677426e-02, -2.77729519e-02,  1.05737463e-01,\n",
       "         6.96126893e-02, -3.13257128e-02,  1.06653385e-01,\n",
       "        -3.42886373e-02,  7.16262497e-03,  2.75407583e-02,\n",
       "         3.08209099e-03,  5.41421436e-02,  7.99794309e-03,\n",
       "         4.78961542e-02, -8.52882490e-03, -1.71045028e-02,\n",
       "        -5.76161034e-02, -4.68345955e-02,  4.64934483e-02,\n",
       "         5.37120625e-02,  3.35917510e-02, -1.65120885e-03,\n",
       "         7.91192520e-03,  1.03726517e-03, -1.96308810e-02,\n",
       "        -8.04591477e-02, -2.84956675e-03, -2.11297534e-03,\n",
       "        -3.25726867e-02, -1.12441659e-01, -4.82017770e-02,\n",
       "         4.49524596e-02,  1.72458775e-02,  4.30243090e-02,\n",
       "        -1.67064015e-02, -3.28069441e-02,  3.29428539e-03,\n",
       "        -8.96587502e-04,  9.08863917e-03,  3.21591347e-02,\n",
       "         1.00762382e-01,  9.60712321e-04,  1.39057115e-02,\n",
       "         2.60168649e-02, -7.19390512e-02,  1.23809222e-02,\n",
       "         6.40313253e-02, -7.69515336e-02, -1.01586841e-02,\n",
       "         1.09277200e-03,  4.01517637e-02, -2.73590051e-02,\n",
       "         1.69200674e-02,  4.23681550e-03, -4.41330345e-03,\n",
       "        -1.45987840e-02,  9.81366448e-03, -3.34900133e-02,\n",
       "         7.24321008e-02, -2.20198929e-02,  2.89235078e-02,\n",
       "        -3.24554145e-02, -3.46073462e-03, -4.00877967e-02,\n",
       "         4.95970547e-02, -1.08843204e-02,  1.19285267e-02,\n",
       "         4.25702482e-02,  4.18502316e-02, -1.10789135e-01,\n",
       "        -4.78191748e-02,  4.52564359e-02, -9.70671475e-02,\n",
       "        -1.64944511e-02,  4.43106927e-02,  2.80844681e-02,\n",
       "         4.45745885e-03, -3.96864600e-02, -3.55673209e-02,\n",
       "        -1.13706356e-02,  3.12827379e-02,  5.54262474e-03,\n",
       "         2.64555123e-02,  3.97457927e-03,  4.92657572e-02,\n",
       "        -6.53052703e-04, -7.97695480e-04,  3.01496182e-02,\n",
       "        -8.77415016e-03,  1.71782393e-02, -6.79932907e-02,\n",
       "         1.92474872e-02, -2.01429486e-08, -6.26077037e-03,\n",
       "         4.11674976e-02,  1.15182064e-02, -2.43706666e-02,\n",
       "         3.61209996e-02,  8.32115263e-02,  1.41878035e-02,\n",
       "        -3.86324860e-02, -2.38370132e-02,  2.38107108e-02,\n",
       "         1.02750197e-01,  3.41439657e-02, -4.33816686e-02,\n",
       "         1.05027221e-02,  6.16383962e-02, -6.66104257e-02,\n",
       "         2.98655722e-02,  1.09820543e-02,  1.40552688e-02,\n",
       "         9.89068486e-03, -5.36550991e-02,  7.13824108e-02,\n",
       "         1.02524376e-02,  8.85976665e-03, -3.26385088e-02,\n",
       "         5.65895811e-02,  5.20986170e-02,  2.42251959e-02,\n",
       "         1.81319881e-02, -2.23980006e-02,  2.37501394e-02,\n",
       "         6.86925948e-02, -3.41147110e-02, -5.13056815e-02,\n",
       "         3.28299962e-03,  2.32969131e-02, -2.65135635e-02,\n",
       "        -6.15591183e-04,  2.21928842e-02,  7.05658570e-02,\n",
       "        -4.81309220e-02, -1.42728649e-02, -2.52283905e-02,\n",
       "         4.29543480e-02, -4.69558388e-02, -1.47561515e-02,\n",
       "        -1.18394606e-02, -3.02900840e-02, -2.63937283e-03,\n",
       "        -2.67870184e-02, -6.57145754e-02, -4.64006048e-03,\n",
       "         2.80130114e-02,  3.96172591e-02,  5.63908480e-02,\n",
       "        -1.53904501e-02, -9.05199070e-03, -1.11009879e-02,\n",
       "        -5.25389686e-02,  1.69054475e-02,  9.11533460e-02,\n",
       "         6.22401386e-03,  6.95541576e-02, -1.67300552e-03]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_document_embeddings(model, [[\"Hello, world!\", \"Goodbye. This is the end.\"], [\"This is a test\", \"This is not a test\"]], PATH_TO_EMBEDDINGS, \"demo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_embs = get_document_embeddings(r\"../data/embeddings/demo.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing REGEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = 'compression quality, or color which are capable enough to dramatically modify or restructure the underlying image properties111https://www.nytimes.com/interactive/2023/06/28/technology/ai-detection-midjourney-stable-diffusion-dalle.html?auth=register-google&utm_source=pocket-newtab-intl-en. Image forgeries are almost always followed by these operations like adding some noise or applying JPEG compression to obscure any traces of image generation or forgeries and thereby'\n",
    "\n",
    "# content = re.sub(r'\\[(\\d+[,]?)+\\]([^\\w\\t])', r'\\2', content) # matches when punctuation follows the citation\n",
    "# content = re.sub(r'\\[(\\d+[,]?)+\\](\\w+)', r' \\2', content)\n",
    "content = re.sub(r'(\\d+)?http[s]?:\\/\\/[^\\s]+([^\\.\\?\\!\\,:; ])', r'', content)\n",
    "# content = re.sub(r'(\\d+)?(http[s]?|file):\\/\\/(^.\\s)*([\\.,]+)?', r'\\3', content)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = r'''Due\n",
    "to the efficiency of hierarchical architectures, U-Net-based Diffusion Models (DMs)[1]have achieved remarkable performance in visual generation tasks[2,3,4].\n",
    "Recently, inspired by the success of Transformers[5,6,7], Transformer-based DMs dubbed Diffusion Transformers (DiTs) have been developed[8]and exhibit great scalability on more complex generation tasks. Particularly, the state-of-the-art (SOTA) generation framework Sora[9]is built upon DiTs, highlighting their great potential and effectiveness.\n",
    "However, their large model size and intensive computations involved in the iterative denoising process result in slow inference speeds[10], calling for effective model compression methods.'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import logging\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "- Skip poorly fomarted papers\n",
    "- Split paper by SECTION rather than '\\n' char\n",
    "- Remove other citation types (e.g. (Name et. al., year))\n",
    "- Obtain Keywords from first section if possible\n",
    "- Make section embedding Hierarchical (combine the embeddings of subections!)\n",
    "- Add better ending condition (The end of the paper is not always denoted by a SECTION)\n",
    "- Conver ending_words matching to regex\n",
    "- Adjust multi_sec_embed to create an list of ndarrays before concatenating\n",
    "- Change how the `preprocess` and `is_paper_good` methods are used (e.g. change them to be class methods)\n",
    "\n",
    "**Potential for cleaning via HTML Parsing**\n",
    "- Remove Code\n",
    "- See when paper content ends?\n",
    "- Remove citations\n",
    "- Remove links\n",
    "\n",
    "**Problem Papers**\n",
    "- 2411.13913v1_content (Unusable)\n",
    "- 2403.11585v3_content (Should end earlier)\n",
    "\n",
    "\n",
    "**Embeddings**\n",
    "- Each embedding file contains a collection of embeddings with shape = (num_of_secs, emb_size). NOTE THIS IS SUBJECT TO CHANGE!\n",
    "- The last embedding in the file is the papers mean embedding.\n",
    "\n",
    "\n",
    "**Model Info**\n",
    "- https://huggingface.co/sentence-transformers/sentence-t5-large\n",
    "\n",
    "**Things to Read**\n",
    "- https://huggingface.co/docs/transformers/model_memory_anatomy\n",
    "- https://huggingface.co/docs/transformers/perf_train_gpu_one\n",
    "- https://huggingface.co/docs/transformers/llm_tutorial_optimization\n",
    "\n",
    "**Training End Dates**\n",
    "- LLAMA : Dec. 2023\n",
    "- sentence-t5-large : ??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_papers = Path(r'../papers')\n",
    "path_to_embs = Path(r'../data/embeddings')\n",
    "\n",
    "END_OF_PAPER_WORDS = [\n",
    "    'references',\n",
    "    'acknowledgements', \n",
    "    'acknowledgement',\n",
    "]\n",
    "\n",
    "papers_to_skip = [\n",
    "    '2311.11329v2_content.txt',\n",
    "    '2411.09324v2_content.txt',\n",
    "    '2411.14259v1_content.txt'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/sentence-t5-large', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_lib = EmbeddingLibrary(\n",
    "    path_to_papers=path_to_papers,\n",
    "    path_to_embs=path_to_embs,\n",
    "    model=model,\n",
    "    end_of_paper_words=END_OF_PAPER_WORDS,\n",
    "    papers_to_skip=papers_to_skip,\n",
    "    name=\"Prototype\"\n",
    ")\n",
    "\n",
    "emb_lib.update_full_paper_embs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model.encode(['RF jamming transmits radio signals indiscriminately across a range of frequencies, causing interference and disrupting communication. Jamming can be broadly categorized as active jamming and reactive jamming. Active jamming continuously emits powerful interference signals, but its continuous operation leaves detectable traces, making it vulnerable to defensive techniques. Reactive jamming adjusts its jamming behavior according to observed signals in the environment. It remains silent when the channel is idle but initiates high-power signal transmission upon detecting activity on the channel. The drawback of these approaches is that spectrum owners may promptly detect the presence of an attack and respond accordingly.'])\n",
    "\n",
    "y = model.encode(['The cat was fat. So was the dog.', 'He ran so fast. But not fast enough. Sad fat dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 768)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7425, 0.7420, 0.7429, 0.7443, 0.7437, 0.7426, 0.7426, 0.7418, 0.7421,\n",
      "         0.7428, 0.7431, 0.7434, 0.7432, 0.7447, 0.7426, 0.7425, 0.7430, 0.7425,\n",
      "         0.7437, 0.7419, 0.7440, 0.7423, 0.7412, 0.7420, 0.7433, 0.7431, 0.7438,\n",
      "         0.7427, 0.7441, 0.7427, 0.7429, 0.7430, 0.7430, 0.7424, 0.7423, 0.7447,\n",
      "         0.7436, 0.7422, 0.7425, 0.7439, 0.7442, 0.7432, 0.7438]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2411.09598v1',\n",
       " '2403.12778v2',\n",
       " '2411.09101v1',\n",
       " '2411.09702v1',\n",
       " '2309.01837v3']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = ' the research provides an innovative and efficient solution for code generation from ML task descriptions, showcasing the capabilities of Linguacodus. By capitalizing on the Code4ML dataset’s wealth of resources and introducing a structured approach to instruction synthesis and code generation, we bridge the gap between natural language task descriptions and executable code, making machine learning development more accessible and efficient'\n",
    "\n",
    "emb_lib.search_papers(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, SimilarityFunction\n",
    "from embeddings.embedding_library import EmbeddingLibrary\n",
    "import logging\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_papers = Path(r'papers')\n",
    "path_to_embs = Path(r'data/embeddings')\n",
    "\n",
    "# should all be lower case\n",
    "end_of_paper_words = [\n",
    "    'references',\n",
    "    'acknowledgements', \n",
    "    'acknowledgement',\n",
    "]\n",
    "\n",
    "papers_to_skip = [\n",
    "    '2311.11329v2_content.txt',\n",
    "    '2411.09324v2_content.txt',\n",
    "    '2411.14259v1_content.txt'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/allenai-specter', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_lib = EmbeddingLibrary(\n",
    "    path_to_papers=path_to_papers,\n",
    "    path_to_embs=path_to_embs,\n",
    "    model=model,\n",
    "    end_of_paper_words=end_of_paper_words,\n",
    "    papers_to_skip=papers_to_skip,\n",
    "    name=\"Prototype\",\n",
    "    path_to_log=Path(\"./log\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.39it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.85it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.72it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.93it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 56.92it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 109.88it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 80.12it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 37.47it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.95it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.45it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 69.05it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.16it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 90.63it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 52.81it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 56.88it/s]\n",
      "Batches:  50%|█████     | 674/1335 [00:05<00:05, 117.07it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n",
      "\u001b[1;32m----> 1\u001b[0m \u001b[43memb_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_sec_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mskip_existing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\scholarly-search\\embedding_library.py:112\u001b[0m, in \u001b[0;36mEmbeddingLibrary.multi_sec_embed\u001b[1;34m(self, skip_existing, encoding)\u001b[0m\n",
      "\u001b[0;32m    109\u001b[0m             logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPREPROCESSED CHUNK:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mchunk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;32m    111\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEND OF FILE - SAVING EMBEDDINGS FOR \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;32m--> 112\u001b[0m full_paper_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_sents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb_size)\n",
      "\u001b[0;32m    113\u001b[0m paper_embs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([paper_embs, full_paper_emb], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;32m    114\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_to_embs \u001b[38;5;241m/\u001b[39m file\u001b[38;5;241m.\u001b[39mstem\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_content\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m), paper_embs)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:621\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    618\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n",
      "\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;32m--> 621\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;32m    623\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:688\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[1;34m(self, input, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    686\u001b[0m     module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n",
      "\u001b[0;32m    687\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n",
      "\u001b[1;32m--> 688\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs)\n",
      "\u001b[0;32m    689\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:350\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n",
      "\u001b[0;32m    348\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;32m--> 350\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;32m    351\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;32m    353\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n",
      "\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n",
      "\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n",
      "\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n",
      "\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n",
      "\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n",
      "\u001b[0;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n",
      "\u001b[1;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n",
      "\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n",
      "\u001b[0;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n",
      "\u001b[0;32m    686\u001b[0m         hidden_states,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    692\u001b[0m         output_attentions,\n",
      "\u001b[0;32m    693\u001b[0m     )\n",
      "\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:627\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n",
      "\u001b[0;32m    624\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;32m    625\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n",
      "\u001b[1;32m--> 627\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    628\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n",
      "\u001b[0;32m    629\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    630\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n",
      "\u001b[0;32m    632\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\transformers\\pytorch_utils.py:248\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n",
      "\u001b[0;32m    245\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n",
      "\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n",
      "\u001b[1;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:640\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n",
      "\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n",
      "\u001b[0;32m    639\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n",
      "\u001b[1;32m--> 640\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    641\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n",
      "\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n",
      "\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n",
      "\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:554\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n",
      "\u001b[0;32m    552\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n",
      "\u001b[0;32m    553\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n",
      "\u001b[1;32m--> 554\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLayerNorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\rooty\\UWEC\\Fall 24\\CS 426\\GroupProject\\GroupProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1732\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m   1729\u001b[0m             tracing_state\u001b[38;5;241m.\u001b[39mpop_scope()\n",
      "\u001b[0;32m   1730\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;32m-> 1732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapped_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;32m   1734\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "emb_lib.multi_sec_embed(skip_existing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_lib.update_full_paper_embs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.similarity_fn_name = SimilarityFunction.COSINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.9992, 0.9996,  ..., 0.9993, 0.9994, 0.9991],\n",
       "        [0.9992, 1.0000, 0.9995,  ..., 0.9993, 0.9994, 0.9994],\n",
       "        [0.9996, 0.9995, 1.0000,  ..., 0.9994, 0.9995, 0.9991],\n",
       "        ...,\n",
       "        [0.9993, 0.9993, 0.9994,  ..., 1.0000, 0.9994, 0.9994],\n",
       "        [0.9994, 0.9994, 0.9995,  ..., 0.9994, 1.0000, 0.9989],\n",
       "        [0.9991, 0.9994, 0.9991,  ..., 0.9994, 0.9989, 1.0000]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = emb_lib.full_paper_embs\n",
    "\n",
    "model.similarity(x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "import umap.plot\n",
    "\n",
    "\n",
    "mapper = umap.UMAP().fit()\n",
    "\n",
    "p = umap.plot.points(mapper, labels=data.Cat1)\n",
    "umap.plot.show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2410.23279v3',\n",
       " '2411.14354v1',\n",
       " '2410.14845v2',\n",
       " '2312.09440v2',\n",
       " '2308.07279v2']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emb_lib.search_papers(\n",
    "    '',\n",
    "    'Inspired by the capacity of self-attention to capture patch-level interactions, we propose a 4D interaction encoder for efficient human-scene interaction estimation. In Fig.3, we visualize the attention map of the token overlapped with the head of a person and the feature map output by the last block of a DINOv2dinov2pre-trained ViT.Rich semantic information in the feature map enables effective distinction between objects. However, it cannot represent interactions between image regions. In contrast, a specific token’s attention map is capable of acquiring interaction information between it and other regions. Therefore, we propose the interaction encoder to leverage attention maps, which are referred to as 4D interaction features, from pre-trained ViTs with a simple adaptation (Fig.4(a)). Compared with the encoders used in the previous tasks (Fig.4(b)) such as object detectionvitdetand image mattingvitmattethat utilize feature maps to distinguish objects, the interaction feature-based encoder inherently facilitates the explicit capture of patch relations, which is required by gaze following. Given an image with a resolution of. The interaction encoder extracts 4D features, which explicitly describe the interactions between image patches. The correlations among them are represented as a 4D tensor with a size of, whereand, anddenotes the patch-size of the ViT. Different from the final feature map leveraged in other regionsvitdet;vitpose;vitmatte, this 4D representation reflects the inner-token relations which are more effective in gaze following.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emb_lib.paper_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GroupProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
