SECTION: A Robust Image Forensic Framework Utilizing Multi-Colorspace Enriched Vision Transformer for Distinguishing Natural and Computer-Generated Images

The digital image forensics based research works in literature classifying natural and computer generated images primarily focuses on binary tasks. These tasks typically involve the classification ofnatural images versus computer graphics imagesonly ornatural images versus GAN generated imagesonly, but not natural images versus both types of generated images simultaneously. Furthermore, despite the support of advanced convolutional neural networks and transformer based architectures that can achieve impressive classification accuracies for this forensic classification task of distinguishing natural and computer generated images, these models are seen to fail over the images that have undergone post-processing operations intended to deceive forensic algorithms, such as JPEG compression, Gaussian noise addition, etc. In this digital image forensic based work to distinguish between natural and computer-generated images encompassing both computer graphics and GAN generated images, we propose a robust forensic classifier framework leveraging enriched vision transformers. By employing a fusion approach for the networks operating in RGB and YCbCr color spaces, we achieve higher classification accuracy and robustness against the post-processing operations of JPEG compression and addition of Gaussian noise. Our approach outperforms baselines, demonstrating 94.25% test accuracy with significant performance gains in individual class accuracies. Visualizations of feature representations and attention maps reveal improved separability as well as improved information capture relevant to the forensic task. This work advances the state-of-the-art in image forensics by providing a generalized and resilient solution to distinguish between natural and generated images.

SECTION: 1Introduction

The widespread availability of image acquiring devices has led to images being captured easily. Apart from the camera-captured‘natural or real images’, there exists‘computer graphics images’, i.e., images generated through computer graphics software. The other category of computer generated images includes the Generative Adversarial Network‘(GAN) images’. Beyond the artistic and entertaining purposes of these computer generated images, they also have a significant possibility to easily propagate misinformation via online platforms, especially when associated with fake news[1]. Hence there exists a high necessity for computational aids to distinguish natural and computer generated images. Most works in the literature that distinguish natural and computer generated images consider either computer graphics images only or GAN images only in the category of computer generated images[2,3,4,5]. That is, these works focus only on one category of computer generated images and not both. However, these forensic algorithms do not suit the real-world scenario where the process of image generation is unknown. Also, there could be cases if a generated image is not computer graphics then maybe it could be a GAN image or vice-versa. Hence a single forensic classification system is required to investigate images and understand whether they are natural/real photographs or the ones created by computer graphics or GAN algorithms, with high accuracy.

With the advent of convolutional neural network (CNN) architectures and transformer based architectures, the image classification systems are achieving high classification accuracies[6,7]. Utilizing these techniques the forensic algorithms or tools to distinguish natural and computer generated images while moving to attain some success have been mostly vulnerable to different variations in images caused due to image quality, resolution, compression quality, or color which are capable enough to dramatically modify or restructure the underlying image properties111https://www.nytimes.com/interactive/2023/06/28/technology/ai-detection-midjourney-stable-diffusion-dalle.html?auth=register-google&utm_source=pocket-newtab-intl-en. Image forgeries are almost always followed by these operations like adding some noise or applying JPEG compression to obscure any traces of image generation or forgeries and thereby deceiving image forensic algorithms and tools[8]. Therefore, besides producing a high performance system for classifying photographs and computer generated images of both categories (computer graphics and GAN), it is equally essential to review the robustness of these forensic systems towards various post-processing operations. Hence, unlike the works in the literature that deals the forensic task of distinguishing natural and computer generated images as a binary-class classification task either by considering only computer graphics or only GAN images, our work proposes a generalized forensic algorithm, i.e., a three-class classification task classifying real, computer graphics and GAN images. Our methodology focuses on building a classification model such that it achieves high accuracy and is highly robust against post-processing operations.The major contributions of our work are:

We propose a robust image forensic classification framework for distinguishing natural and computer generated images including both computer graphics and GAN images, utilizing a vision transformer based approach and employing a fusion of two color space transformations RGB and YCbCr.

We devise a novel methodology by exploiting the vulnerability of the decrease in classification accuracies of image classes with the increase in compression factors, to majorly focus on building a forensic classifier system that is highly robust against the post-processing operations.

We compare the performance of our proposed model with a set of baselines and could observe that the performance of our approach outperforms the baselines with significant performance gains in total and individual class accuracies, is highly robust against post-processing operations of image compression and addition of gaussian noise, and is also generalizable to unseen test data.

We visualize the feature representations of our model and compare with the input image features and feature representations of the baseline method, where we could observe that our model provides much better separability between the different classes, helping towards improved classification performance.

We also analyze the attention maps of the networks of our fused model which shows that our novel methodology has the capability of improved information capture relevant to the forensic task of classifying natural and generated images.

The rest of this paper is organized as follows. Section2presents a brief survey of related works and delineates our proposed work from the related works in the literature. Section3discusses the methodology of the proposed work along with the dataset used in this study, motivation and detailed description of the proposed model. Section4presents the experimental settings and details of the baseline models used for comparing our proposed model. Section5presents the results and discussion including the results of the experiments for robustness, generalizability, feature visualization, and the analysis of attention maps. Section6finally draws the conclusions.

SECTION: 2Related work

Image processing has a wide range of applicability in various research areas, including computer vision[9,10], computer graphics[11], medical image processing[12,13,14], digital image forensics[15,16,17,18,19,20], etc. Digital Image Forensics is a research area that examines digital images using scientific approaches to provide evidence of their authenticity in different contexts, such as criminal cases, civil cases, etc. There are various different sub-tasks, which this area of research focuses on. These mainly include the development of computational algorithms to detect computer-generated images such as graphics and GAN images[2,3,4,5], detecting recaptured images[21], forged images[22], etc.

Image forensic based works that distinguish natural and computer graphics images can be seen since the 1990s. These include statistical feature based works that utilize features based on local binary patterns, co-occurrence matrices, histogram features, etc.[23,24], transform domain features based works utilizing wavelet features, quaternion wavelet features, etc.[25,26], device based features exploiting features based on intrinsic fingerprints, demosaicing, etc.[27,28]. Also, there have been many works utilizing deep learning architectures such as CNNs[29,30,3,5].Works that distinguish natural and GAN images are comparatively much more recent than the category described above of natural versus computer graphics images. These works in the literature that attempt natural versus GAN images, mostly employ deep learning based approaches such as CNNs, XceptionNet, InceptionNet, etc.[4,31,32,33], than the handcrafted feature based approaches[34]. Few works are also seen to utilize transformer based architectures to detect GAN generated videos[18], but these works also only consider the single category of generation, that is natural versus GAN only. Works detecting GAN images generally consider image content based feature analysis[31,35,36]or some works are also found to consider residual features[34], biometric features[37], etc. GAN image detection algorithms have applicability in numerous categories such as detection of generated faces[32,38,37], generated obscene images[36], generated medical images[35], etc. Works in the literature classifying natural and computer-generated images are also found to study and conduct robustness experimentations towards post-processing operations, mainly JPEG compression[24,31,37]. Table1shows a snapshot of works classifying natural and computer-generated images.

SECTION: 2.1Our work in context

To the best of our knowledge, there is only a single work that distinguishes natural versus computer generated images by considering both computer graphics and GAN images in the category of computer generated images[16], and utilizes an EfficientNet CNN architecture for the three-class classification task. But, to the best of our knowledge, no works are seen to be reported using the high performance recent deep learning architectures i.e., transformer based architectures for classifying natural images and both categories of computer generated images, i.e., Graphics and GAN generated images. Our digital image forensic analysis based work utilizes vision transformers and proposes a fusion based approach to distinguish between natural versus computer generated images of both categories, i.e., Graphics and GAN images. Our methodology is based on the novel idea of exploiting the vulnerability of decline in classification accuracies of different classes with the increase in compression, that helps to primarily focus on developing a forensic classifier system that is highly robust against post-processing operations such as JPEG compression, addition of gaussian noise, etc. Despite other works in the literature that utilize certain color space transformations for the task of distinguishing natural images from computer-generated images of either of the category such as[38,34], our work utilizes two color spaces in our methodology, based on our rigorous set of experiments, one for improving the overall accuracy of the task of distinguishing natural images from computer-generated images of both categories and other color space that deals with improving the robustness of the proposed model against post-processing operations. Our methodology follows a transfer learning approach which helps to improve the accuracy of the task without involving the burdensome process of pre-training and increasing the training complexity.

SECTION: 3Methodology

In this work, the forensic task of distinguishing natural and computer generated images is formulated as a three-class classification task, where the three classes are GAN, Graphics, and Real. GAN and Graphics images, even though both being computer generated, are maintained as separate classes as they have different generation processes. For this three-class classification problem, we utilize transformer based deep neural networks to find the best-fit mapping functionfor the train datawhere for each imagein the training set,, indicating either of the classes GAN, Graphics or Real, respectively.

SECTION: 3.1Dataset

In this work, we utilize the GAN, Graphics, and Real images from the dataset used in the study[16]. The dataset comprises 12000 images in total, where each of the classes contains 4000 images. The class GAN consists of images collected from a variety of generative algorithms such as ProgressiveGAN[39], StyleGAN[40], StyleGAN2[41], and StyleGAN2-ADA[42]. Whereas, the images for the classes Graphics and Real are obtained from the Computer Graphics versus Photographs dataset[2]. The entire dataset is challenging and contains a diverse category of images in terms of image content (indoor and outdoor scenes, animals, objects, etc.), origin (different generative/graphics algorithms or cameras, etc), and quality. Also, the computer generated images i.e., both GAN and graphics are photorealistic, i.e., they are not manually easy to predict as computer generated ones. There are no any pre-processing operations performed on the images in the dataset. For the training, validation, and testing the dataset is proportionally split in the ratio 60:20:20 respectively.

SECTION: 3.2Motivation

The images transmitted through social media and online platforms are subject to image compression, mostly JPEG compression, where different online platforms follow different JPEG compression standards[43,44,45]. Also in the cases of intentionally propagating fake images to support some piece of fake news to convince the audience, these images are seen to be mostly subject to multiple image compressions[8,1]. Hence, any model proposed for distinguishing natural and computer generated images, apart from obtaining high classification accuracy must also be highly robust towards compression. But most of the computational models in the literature proposed for the forensic task of distinguishing natural and computer generated images, are less robust to post-processing operations, particularly the JPEG compression[46]. That is, as the compression factor of the images increases (or the quality factor of the images decreases), the accuracy of the computational models to distinguish natural and computer generated images decreases. We analyze the classification accuracies of different computational models to distinguish GAN, graphics, and real images, at varying levels of JPEG compression, including the state-of-the-art vision transformer based architectures ViT-Base and ViT-Large[47], state-of-the-art CNN based architecture InceptionResNet[48]and a few other baselines that classify natural and computer generated images[16,3,30,4]. Figure1shows the classification accuracies of these models for original images (uncompressed images or zero compression factor) and for various compression quality factors in descending order. As can be seen from the figure, the accuracy of all the models decreases from the original or uncompressed scenario, as the compression increases.

To propose a robust model for the task, hence, we further analyze the class accuracies of these models. Figure2shows the individual class accuracies of the models for varying levels of JPEG compression i.e., accuracies of the class GAN generated images at varying levels of compression factors in figure2a, graphics generated images in figure2b, and real images in figure2c. We could observe that, for original or uncompressed images, class accuracies of GAN generated images are always higher than the graphics and real images, for all the models. But at the same time, the class GAN is the most affected as the compression increases (as shown in figure2a). That is, there is a very high drop/decay in the class GAN accuracies of the models with even small increments in JPEG compression. For the class Real in figure2c, the drop in class accuracies of the models as the compression increases are comparatively very less and for some compression rates the accuracies even gets an increase in values. Whereas, for the class Graphics in figure2b, there is negligible drop in class accuracies of the models, it has almost stable accuracies as the compression increases. Altogether, analyzing the class accuracies shows that the rate of decrease in class accuracies with the increase in compression differs for different classes. And hence, how we can exploit this vulnerability of the decrease in accuracy with an increase in compression, which differs for each class, is the motivation for our proposed approach of a robust classification model for the forensic task of distinguishing natural and computer generated images.

SECTION: 3.3Network architecture

We devise our methodology in such a way that the proposed model should obtain high classification accuracy and should also be highly robust. We propose a Multi-Colorspace fused and Enriched Vision Transformer (MCE-ViT) model by parallely combining two transformer based networks that operates in two different color spaces, one for obtaining high classification accuracy and the other network dedicated to improve the robustness of classification. The entire architecture of our proposed model is shown in figure3. For obtaining high classification accuracies we choose as the first network of the fused model, one of the recent state-of-the-art transformer based models ViT[47]. This network takes as input the RGB images, and we name this first ViT network asRGB ViT network. We follow a transfer learning strategy where we choose the ViT network pre-trained on the Imagenet 21-k dataset[49]and fine-tune the network using the task specific GAN, Graphics and Real images dataset (detailed in section3.1). Even though this firstRGB ViT networkis found to obtain high classification accuracies for uncompressed images, it also has a performance decay when the images are JPEG compressed, as already seen in figure1. Since the rate of performance decay is different for different classes (as detailed in section3.2), allowing the model to also learn about these differences in performance decay among the classes, would help to better identify these classes in compressed scenarios. Therefore we design a strategy for the second network in our fused model in such a way as to make the model learn these differences and thereby improve the robustness.

As the firstRGB ViT networkof the proposed fused model primarily focuses on high classification accuracies, the second network of the fused model is designed primarily to focus on the task of improving the robustness of the entire classification. Hence our strategy for the second network of the fused model is motivated by the fact that, since the rate of differences in accuracies between original and compressed versions of images are different for different classes, enriching the images in each class with this information on their corresponding rate of differences would be highly beneficial to distinguish natural and generated images with high classification accuracies, even for images in post-processed scenarios thereby improving robustness of the model. Accordingly, in the second network of our fused model, the input RGB images from the dataset are initially converted to the YCbCr color space. The motivation for choosing YCbCr color space is that it is the color space commonly used in the process of JPEG compression[50]. The process of JPEG compression mainly downsamples the chrominance planes of this colorspace, i.e., Cb and Cr, because the changes brought up in these planes are less visually discernible[50,51]. Hence enriching the network with the error of chrominance planes between the original images and their corresponding JPEG compressed versions, would enable the network to learn the distinguishable feature of the rate of differences in classification accuracies based on compression that differs for different classes. On this basis, our secondEnriched YCbCr ViT networkenriches the YCbCr images with the information on the error of chrominance planes between the original images and their corresponding JPEG compressed versions as given in the following equation, and this second pre-trained ViT network is fine-tuned using these enriched YCbCr images.

Both the firstRGB ViT networkand the secondEnriched YCbCr ViT networkproduce an output feature vector. Even though the feature vector from the first RGB ViT network is very powerful to classify gan, graphics, and real images with high accuracy in uncompressed scenarios, in order to decrease its effect towards performance decay in compressed scenarios, we perform average pooling on this feature vector to obtain only the representative feature of the RGB ViT network. Thus, pooling the feature vector from the RGB ViT network helps to not affect the robustness of the model in compressed scenarios, as well as to maintain high model accuracies. Later, to build the fused model, the feature vectors from both the ViT networks are concatenated to form a single feature vector which is passed to a fully connected neural network with a dense layer and a 3-class output layer that determines the class label of the images.

Our entire methodology including the conversion to YCbCr color space and enriching the YCbCr images with the error of chrominance planes between the original and their corresponding JPEG compressed versions can be applied irrespective of any image formats such as .png, .jpeg, etc. For example, for a .png image format, the firstRGB ViT networktakes as input the RGB image as such. For the secondEnriched YCbCr ViT network, the .png image would be converted from RGB to YCbCr, which is later enriched with the error of chrominance planes of this YCbCr image and its corresponding JPEG compressed version. Furthermore, our novel methodology can also be applied to compressed input images as well (the major aim of this work being to develop a robust model towards post-processed images). This is because our methodology focuses on the rate of difference in accuracies as compression increases, where we could observe from the figure2that these rates are different for different classes. This rate of difference not only occurs between an original/uncompressed version of an image and its corresponding compressed version with a compression quality factor of 90, but also between compressed images with quality factors of 90 and 80, 80 and 70, etc. Accordingly, when a compressed image is presented to our proposed model, the secondEnriched YCbCr ViT networkwould perform its action of conversion to YCbCr, again compressing it, and enriching the YCbCr with the error of chrominance planes of these two. That is, since our methodology is centered on the rate of difference in classification accuracies as compression increases which differs for different classes, our methodology can handle uncompressed or compressed input images of different formats, thereby making the proposed model highly robust.

SECTION: 4Experimental Settings

The proposed study utilizes the vision transformer ‘vit-large’ with patch size 16 and input image size 224 x 224, for bothRGBand theEnriched YCbCrnetworks of the fused model. TheEnriched YCbCr ViT networkutilizes JPEG compression with a quality factor of 90. The study follows transfer learning strategy where both the ‘vit-large’ networks are pre-trained on ImageNet-21k[49]and fine-tuned on the task specific dataset used in the study. Each ViT network produces an output feature vector of size 1024. After manually analyzing the results of average pooling of the feature vector from the firstRGB ViT networkwith various kernel sizes and strides, we use a kernel size of 16 and a stride of 16, as a representative setting, and the resultant feature vector of size 64 is concatenated with the feature vector from the secondEnriched YCbCr ViT network. The entire concatenated feature vector of size 1088 is fed to a dense layer of 512 neurons withReLUactivation function, followed by an output dense layer of 3 neurons withsoftmaxactivation function, making 559,107 number of trainable parameters. The other hyperparameters are batch size 16,categorical crossentropyloss function,Adamoptimizer, and 50 epochs. The experiments are conducted on the deep learning workstation equipped with Intel Xeon Silver 4208 CPU at 2.10 GHz, 256 GB RAM, and two GPUs of NVIDIA Quadro RTX 5000 (16GB each), using the libraries Tensorflow (version 2.8.0), Keras (version 2.8.0), Torch (version 1.13.1+cu116), PyTorch Lightning (version 1.9.0), Transformer (version 4.17.0), and Albumentations (version 1.3.1).

The model performance analysis experiments in this work focus on analyzing the performance of the proposed model based on classification accuracy and comparing it with the baselines. The classification accuracy222https://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-scoreof a multi-class (3-class in this work) classifier model is computed as follows:

where,andindicate the original/ground-truth class and predicted class of an ithimage respectively, andindicates the total number of images.

SECTION: 4.1Baselines

For the purpose of model performance comparison with baselines, we consider state-of-the-art works in the literature that classify natural and generated images. To the best of our knowledge, there is only a single work[16]classifying GAN, graphics, and real images and therefore, we choose this work for our baseline comparison experiments. We also perform model performance comparison with works that classify natural and either of the computer generated images (i.e., GAN or graphics only)[3,4,30], and an off-the-shelf deep neural network architecture[48]. We detail these baselines below.

MC-EffNet: Multi-colorspace fused EfficientNet networks proposed in[16]. This baseline work proposes a fused model by parallely combining three EfficientNet-B0 networks that operate in three different color spaces RGB, LCH, and HSV. After converting the images into LCH and HSV color spaces, the values are rescaled into the range 0 to 255 in order to supply into the EfficientNet network. The pre-trained EfficientNet-B0 networks are fine-tuned using the GAN, Graphics, and Real images of the dataset used in[16]. The feature vectors from all three EfficientNet networks are then concatenated and supplied to an output dense layer with 3 neurons and asoftmaxactivation function. This forms the MC-EffNet-1 network. Whereas, the MC-EffNet-2 network includes an additional preprocessing layer of laplacian of gaussian operation, before performing the color conversions to LCH and HSV. For both the networks the hyperparameters are,categorical cross-entropyas loss function,Adamoptimizer with learning rate 0.001, 256 as batch size, and 100 epochs.

The other baselines chosen for this study include InceptionResNet[48], Quan et al.[3], Nataraj et al.[4]and Rezende et al.[30]that were chosen as the baselines in the work of MC-EffNet[16]. InceptionResNet[48]is one of the high-accuracy models in the ImageNet classification task. We follow a transfer learning approach over the InceptionResNet-V2 architecture where the final output layer of 1000 neurons is replaced with 3 neurons to suit this 3-class classification task. The state-of-the-art works[3,4,30]in the literature classify natural images and either of the types of computer generated images (i.e., either only GAN images or only computer graphics images). Quan et al.[3]propose a CNN based model to classify natural and computer graphics images. They predict results through local-to-global strategy where classification results of local patches are computed initially, and later the global prediction results of the whole image is acquired via majority voting. Nataraj et al.[4]proposes a CNN based model to classify natural and GAN images by incorporating a co-occurrence feature of the RGB colorspace. Rezende et al.[30]classify natural and graphics images using a transfer learning approach on the ResNet-50 model in combination with an SVM classifier. All these baselines[3,4,30]are observed obtaining accuracy gains in their paper over many other related works in the literature. The experimental settings of all the baselines are followed in the same way as mentioned in the work of MC-EffNet333https://github.com/manjaryp/GANvsGraphicsvsReal/tree/main/Baselines[16]. Since all these baselines are not originally designed as 3-class classification tasks, for performance comparison experiments, the top dense layer of all these baselines is replaced using a final dense layer of 3 neurons withsoftmaxactivation function, to conform to the 3-class classification task. Later, we fine-tune all the baselines with the dataset used in this study. This provides a general setting to analyze and compare the potential of the models in detecting generated images including both GAN and graphics.

SECTION: 5Results and Discussions

Our proposed model achieves a test accuracy of 94.25 percent. The confusion matrix and detection error trade-off (DET) curve of the test results of our proposed model is shown in figure4. Table2presents a comparison of the test results of our model and the baselines in terms of total accuracy and class-wise accuracy. The test results indicate that our proposed model outperforms the baselines in terms of overall test accuracy and even in the case of individual class-wise accuracy. Our model obtains a gain of 4.87 percentage points in terms of overall accuracy when compared to MC-EffNet-2, the best performing model among the baselines.

Among the three classes, GAN obtains the highest individual class accuracy of 98.75 percent, achieving a gain of 2 percentage points when compared to the highest class accuracy of 96.75 percent obtained by MC-EffNet-2 amongst the baselines. Class Graphics achieves an accuracy of 91.00 percent, a very high gain of 7.25 percentage points when compared to the highest class accuracy of 83.75 percent for the MC-EffNet-2 model amongst the baselines. Class Real achieves 93.00 percentage accuracy, i.e., a gain of 5.37 percentage points when compared to the highest class accuracy of 87.63 percent obtained by MC-EffNet-2 amongst the baselines.

SECTION: 5.1Robustness against Post-processing

Besides achieving high classification accuracies on original images or images that are not being post-processed, an efficient image forensic algorithm should also provide robust classification output over post-processed images. Hence, the robustness of the proposed modelMCE-ViTis evaluated towards the typical post-processing operation of JPEG compression and is compared against the baselines. Apart from the baseline models discussed in section4.1, we also analyze the robustness of the proposed model over the off-the-shelf pre-trained vision transformer networks, ViT-B/16 and ViT-L/16 that are fine-tuned using the dataset used in this study. This is because we have observed in section3.2that despite improved accuracies achievable by ViT they are prone to decline in performance with an increase in compression. So here, we also inspect whether our novel methodology of exploiting the vulnerability of the decrease in classification accuracies of image classes with the increase in compression, has helped to improve robustness. Here, the results of ViT-L/16 will serve as an ablation study for our methodology that incorporates colorspace fusion and enrichment of YCbCr with the error of chrominance planes information using ViT-L/16.

Every model trained on uncompressed data (or original train data in the dataset) is tested separately over ten different test data created using various JPEG compression quality factors within the range 100 to 10, in steps of 10. Figure5shows the robustness test results, where it can be observed that compared to the baselines, the proposed model achieves improved robustness towards post-processing based on JPEG compression, for all the compression quality factors. Here also, similar to the classification results of original uncompressed images (table2), MC-EffNet-2 is the baseline model that shows the next improved robustness among all the baselines for different compression quality factors. When looking at the results of uncompressed images the accuracy of the proposed model is less than the ViT-L (ablation) and the ViT-B networks. We provide in table3, the accuracies of our model and the ViT-L and ViT-B networks, for a better comparison. From the table, we can observe that for uncompressed images the proposed model has accuracy less than both ViT-L and ViT-B networks. This could be because, the feature vector from the ViT-L network even though is very powerful in classifying gan, graphics, and real images with high accuracy in uncompressed scenarios, since they fail in compressed scenarios our model has only taken a representative feature of this network through average pooling so that it provides better accuracy and also decrease its effect towards performance decay in compressed scenarios. But even then, our model has very significant robustness starting from a very low compression (quality factor 90) to a very high compression (quality factor 10). That is, even being built with a backbone of the ViT network, our methodology of exploiting the vulnerability of accuracy decline in image classes with an increase in compression, plays a vital role in providing significant robustness.

Besides analyzing robustness against JPEG compression in terms of overall accuracy, we also analyze the robustness of our proposed model specifically for the class GAN, since it is the class that is most affected by post-processing operations and also the major contributing factor towards the decrease in overall accuracy, as already discussed in section3.2. Figure6shows the robustness test results of our proposed model and the baselines, specifically for the class GAN. We can observe that our proposed model achieves very high class accuracy for all the compression factors. The class accuracy obtained for our model while classifying original images (no compression) is 98.75 percent. At the same time, for the images compressed at a compression factor of 10 (very high compression), the class accuracy of our model is 85.13 percent. That is, for a very high compression factor of 10, our model obtains a very high gain of 41.32 percentage points when compared to the next higher accuracy of 43.81 percent at compression factor 10 for the baseline Rezende et al.[30].

The class accuracy of the baseline ViT-L network (ablation) at a compression factor 10 is only 34.00 percent. That is, our proposed model even being built using the ViT-L network, but our methodology of color space fusion and the enrichment of YCbCr with the error of chrominance planes information between original and compressed images has proven to be highly advantageous in improving the robustness of the proposed model thereby achieving a class accuracy of 85.13 percent even at a compression factor of 10, i.e., a very huge gain of 51.13 percentage points when compared to the ViT-L network (ablation).

We also find the robustness of our model against another post-processing operation of addition of Gaussian noise and compare it with the baselines. The accuracy of our proposed model and the baselines at various standard deviation () values of Gaussian noise is shown in figure7. We can observe that compared to the baselines our model achieves better robustness against Gaussian noising also. Thus, this shows that, even if the images are post-processed with the compression or addition of gaussian noise, our novel methodology of exploiting the rate of difference in accuracies as compression increases, is powerful enough to distinguish between GAN, graphics, and real images.

SECTION: 5.2Generalizability

This study also analyzes the generalizability of our proposed modelMCE-ViTby testing over unseen data. The proposed modelMCE-ViTwhich is fine-tuned on the dataset used in this study is tested over three other combinations of unseen GAN, Graphics, Real images, i.e., PG2versus PRCG versus PIM-Google[52,53], PG2versus PRCG versus PIM-Personal[52,53], and Cycle GAN versus Raise versus Level-Design[54,55,56]datasets, with 160 images in each of the class of the three datasets, as experimented in the baseline work[16]. The generalizability test results are shown in table4, where it can be observed that compared to the baselines our proposed model is able to obtain higher test accuracies. The results hence prove that our model has better generalizability, which can even help towards the future challenges in generated image categories.

SECTION: 5.3Feature Visualization

The dimension of images that are input to the proposedMCE-ViTmodel is 2242243, i.e., the length of the raw image features is 150528.MCE-ViTprojects these raw image features into a smaller dimension of length 1088, with an aim to provide better separability between the three classes. We analyze the separability potential of the feature vector of our model by comparing against the feature vectors of raw input image pixels and the best baseline model MC-EffNet-2 that obtains the next higher classification accuracy among all the baselines. The t-Distributed Stochastic Neighbor Embedding (t-SNE)[57]dimensionality reduction technique can visualize high dimensional features into a two-dimensional plane and thereby helps to easily compare the separability potential of the feature vectors. Using the t-SNE technique we project the raw image features, feature vector output from the baseline model MC-EffNet-2 and feature vector output from our proposed modelMCE-ViT, into two-dimensional plots, shown in figure8. In each of the plots, three different colors are used to indicate three different classes, green circles are used to represent GAN image class, blue squares are used to represent Graphics images class, and pink diamonds are used to represent the class of Real images. From the t-SNE visualizations, it is clearly understandable that the raw image features do not have the potential to separate the classes, all the classes are clustered together, more particularly towards the center of the plot (figure8a). The output features projected from the baseline model MC-EffNet-2 (figure8b) seem to produce separability between the classes better than the raw image pixels. The output features projected from our proposed modelMCE-ViT(figure8c) seem to produce much better separability between the three classes when compared to both raw pixels and the best baseline MC-EffNet-2. This demonstrates that our proposed modelMCE-ViThas better capability to transform the raw image pixels to a much better separable feature space for the forensic task of distinguishing natural images from computer-generated images including computer graphics and GAN images.

SECTION: 5.4Attention visualization

While visualizing the attention maps of both the networks of our fused model i.e., theRGB networkand theEnriched YCbCr network, we could observe that compared to theRGB network, theEnriched YCbCr networkcould potentially identify more regions or information in an image that are helpful for classifying images into the classes GAN, Graphics or Real. Table5shows a comparison of the attention map visualizations of both theRGB networkand theEnriched YCbCr networkfor a set of GAN, Graphics and Real images. For example in the first generated image of a cat in the set of GAN images, the RGB network mainly captures the edges of the cat, whereas theEnriched YCbCr networkcaptures some of the information captured by theRGB networkand even more including from the background, irrespective of the regions of the object in the image. Similarly, in the second generated face image, the forehead, right eye, and the image background are given more attention by theEnriched YCbCr networkthan theRGB network.

Similarly in the set of graphics images also, we can observe that theEnriched YCbCr networkis able to capture more relevant image information in detail than theRGB network, such as the regions of uneven illuminations in the image. Also, mostly, the attention given to the captured image regions by theEnriched YCbCr networkare higher than the attention given by theRGB network.

In the real class of images also we can observe that theEnriched YCbCr networkcould capture more image information than theRGB network. For example in the first image of the dog and the second image of the cow, the attention given to the image regions are higher forthe Enriched YCbCr networkthan theRGB network. Also, more image regions relevant for classification are seen to be captured by the attention maps of theEnriched YCbCr network.

Thus for all the classes we can observe that the image information captured by theEnriched YCbCr networkfor this forensic task is comparatively much more relevant than theRGB network. This, in fact, shows the powerful nature of our methodology incorporating the YCbCr color space transformation and enriching the color space with the information on the rate of differences between original and corresponding compressed versions of images in different categories, to capture image information that is highly relevant to the forensic task of classifying natural and generated images.

SECTION: 6Conclusion

In this work a robust approach towards distinguishing natural and computer generated images including both, computer graphics and GAN generated images is proposed, unlike the works in the literature to distinguish natural and computer generated images that consider only either of the generated images category. Our novel methodology focuses on exploiting the vulnerability of decrease in classification accuracies of image classes with the increase in compression factors, to majorly focus on building a forensic classifier system that is highly robust. The proposed work utilized a fusion of two vision transformers one of them operating in RGB color space and the other in YCbCr color space. Transformer based architectures can provide good classification performance, while our methodology focuses on improving the robustness of the proposed model against post-processing operations such as JPEG compression by maintaining a high model accuracy because usually the forensic algorithms and tools even with high performance accuracies are fooled using these post-processed images. Experiments are conducted to analyze the performance of the model and are compared against a set of baselines. The proposed model achieves higher accuracy than the baselines and is found to be highly robust and generalizable. Visualizing the features showed better separability capability of the proposed model than the baseline. The work also studied the attention map visualizations of the networks of the fused model and observed that the proposed methodology could capture more image information relevant to the forensic task of classifying natural and generated images. To aid future research, the relevant materials of this study including the source code are made publicly available athttps://github.com/manjaryp/MCE-ViTandhttps://dcs.uoc.ac.in/cida/projects/dif/mcevit.html. In the future, we plan to use and integrate our methodology to develop a web tool where users can test in real-time any image doubted for the authenticity of whether it is natural or computer generated. Additionally, we plan to explore other challenges in the area of digital image forensics, such as identifying recaptured images and computer generated videos.

SECTION: Acknowledgment

This work was supported by the Women Scientist Scheme-A (WOS-A) for Research in Basic/Applied Science from the Department of Science and Technology (DST) of the Government of India under the Grant SR/WOS-A/PM-62/2018.

SECTION: References