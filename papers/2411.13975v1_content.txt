SECTION: Transforming Static Images Using Generative Modelsfor Video Salient Object Detection

In many video processing tasks, leveraging large-scale image datasets is a common strategy, as image data is more abundant and facilitates comprehensive knowledge transfer. A typical approach for simulating video from static images involves applying spatial transformations, such as affine transformations and spline warping, to create sequences that mimic temporal progression. However, in tasks like video salient object detection, where both appearance and motion cues are critical, these basic image-to-video techniques fail to produce realistic optical flows that capture the independent motion properties of each object. In this study, we show that image-to-video diffusion models can generate realistic transformations of static images while understanding the contextual relationships between image components. This ability allows the model to generate plausible optical flows, preserving semantic integrity while reflecting the independent motion of scene elements. By augmenting individual images in this way, we create large-scale image-flow pairs that significantly enhance model training. Our approach achieves state-of-the-art performance across all public benchmark datasets, outperforming existing approaches. Code and models are available athttps://github.com/suhwan-cho/RealFlow.

SECTION: 1Introduction

In deep learning-based image and video processing tasks, the quantity and quality of training data are crucial factors influencing model performance. However, video data introduces additional complexities due to its temporal dimension, making annotation far more labor-intensive and costly compared to image data. To address this challenge, researchers often turn to large-scale, annotated image datasets, which are more accessible and diverse, to support model learning for video tasks. A common strategy for adapting image data to video tasks involves transforming static images and their corresponding annotations (e.g., segmentation masks) to simulate sequences that mimic the temporal progression of video content. Spatial transformations, such as affine transformations and spline warping, are applied to static images to introduce motion cues, thereby generating image sequences that resemble actual video data. This approach has proven effective in video processing, allowing models to learn temporal patterns and structural variations from diverse image data, which helps mitigate the limited availability of video data.

However, this approach falls short in tasks where motion cues are critical. For instance, video salient object detection (VSOD) requires both appearance and motion cues to accurately identify and segment prominent objects in video sequences. VSOD models typically adopt a two-stream approach, incorporating both RGB images and optical flow maps to capture appearance and motion properties simultaneously. The flow information provides context for the dynamics of each object, allowing models to distinguish between independent object movement and background motion. Unfortunately, existing image-to-video simulation methods, which rely on simple spatial transformations, fail to generate realistic optical flows, as shown in Figure1(a). These transformations neglect the contextual relationships between objects in the scene, resulting in flows that fail to accurately reflect the independent motion properties of individual elements. As a result, these methods fall short in tasks like VSOD, where both appearance and motion cues are essential for accurate segmentation.

To overcome this limitation, we propose a novel solution using image-to-video diffusion models to generate realistic transformations of static images. Diffusion models possess the ability to generate novel images while fully understanding the contextual relationships within them. This enables the generation of plausible optical flows by transforming source images while preserving the semantic integrity of the scene and reflecting the independent motion of each object, as depicted in Figure1(b). Unlike traditional spatial transformations, which distort images without accounting for object relationships, our approach maintains these relationships to generate high-quality image-flow pairs. These pairs significantly enhance model training for motion-guided tasks such as VSOD. By augmenting static images with diffusion-based transformations, we can simulate realistic video sequences and generate the corresponding optical flows necessary for effective learning in two-stream models.

We evaluate our method on several public VSOD benchmarks. Unlike previous approaches that rely on complex network architectures, we adopt a simple encoder-decoder framework to demonstrate the effectiveness of our training strategy. Experimental results show that augmenting training data with large-scale image-flow pairs significantly improves model performance. Specifically, we achieve new state-of-the-artmeasure[15]scores of 94.5%, 92.6%, 80.3%, and 96.2% on the DAVIS 2016[39]validation set, FBMS[34]test set, DAVSOD[16]test set, and ViSal[52]dataset, respectively.

Our main contributions can be summarized as follows:

We identify the limitations of current image-to-video simulation methods for motion-guided tasks, emphasizing their inability to produce realistic optical flows.

We demonstrate that image-to-video diffusion models can generate realistic image transformations that preserve contextual relationships, enabling the synthesis of plausible optical flows.

We show that these image-flow pairs significantly improve VSOD model performance, achieving state-of-the-art results across public benchmark datasets.

SECTION: 2Related Work

Utilizing image data for video models.To leverage the extensive knowledge embedded in large-scale image datasets, video processing tasks commonly incorporate image data. A straightforward approach is to augment image data with spatial transformations and treat the original and newly generated images as a video sequence.

In video object segmentation (VOS), for instance, MaskTrack[40]learns a semi-supervised VOS network by applying affine transformations and thin-plate spline[4]deformations to the segmentation mask of the query frame. The transformed mask is treated as the mask of the previous frame and used along with the query frame image as a training sample. RGMP[35]adopts a similar approach to MaskTrack, simulating the previous frameâ€™s masks through transformations, while further enriching the diversity of training samples by overlaying foreground objects from salient object detection datasets[45,9]onto background images from PASCAL-VOC[14,21]. To create varied training samples, spatial transformations are separately applied to each image, simulating diverse scenarios. Recent methods leverage image data even more directly. Rather than simply approximating conditions encountered during inference, as MaskTrack and RGVI do, these methods generate video-like samples by independently transforming images and their corresponding object masks, then sequencing them temporally. This approach has been shown to be effective across various VOS settings, including both semi-supervised and unsupervised VOS methods[36,43,44,8,10,7,33].

Beyond VOS, using image data is also common in other video tasks. STEm-Seg[2]trains a video instance segmentation model using image instance segmentation datasets, synthesizing training clips with random affine transformations and motion blur effects applied on the fly. FCNS[53]introduces a novel data augmentation technique that simulates video training data from images to learn varied saliency information and mitigate overfitting given the limited availability of training videos.

While these methods effectively implement an image-to-video simulation approach, they rely solely on spatial transformations without accounting for contextual information within each image. As a result, the optical flows generated between the original and transformed images lack meaningful motion cues that could enhance object-level semantics.

Motion-guided architecture.Leveraging motion cues is essential in many video tasks, as it provides crucial insights into the dynamics of each element within images. These cues are typically captured by estimating movement between consecutive frames, incorporating short-term temporal information. This approach is particularly effective in segmentation tasks, as the spatial details preserved in flow maps improve model accuracy in pixel-level prediction.

MATNet[63]integrates motion information with appearance features through an asymmetric attention block within each encoding layer, transforming appearance features into motion-attentive representations. MGA[31]employs separate modules for salient object detection in still images and motion saliency detection in optical flow maps, adapting these modules to each other through end-to-end training. AMC-Net[57]dynamically balances appearance and motion cues when fusing features from each modality, adjusting feature scaling via gating functions. FSNet[24]models deep relationships between appearance and motion cues within feature embedding layers, enforcing mutual constraints that enable robust encoding of each modality. PMN[29], GSA-Net[30], and DPA[12]leverage attention mechanisms to fuse appearance and motion cues while capturing temporal relationships along the sequence.

Despite these advancements, motion-guided methods often face limitations due to the scarcity of video training data. While image-to-video simulation techniques strive to produce realistic videos, the optical flows generated from this simulated data lack authentic motion characteristics, reducing their effectiveness for training motion-guided models.

Image-to-video generation.Recently, video generation models have gained significant attention due to their wide-ranging applications in content creation and editing. Among these, image-to-video generation models, which produce videos from single images, have demonstrated superior generation quality and controllability compared to general video generation approaches. By using the given image as an explicit first-frame guide, these models ensure high-quality generation aligned with the desired content.

The advent of diffusion models[23,46,41]has greatly advanced image-to-video generation, harnessing the powerful generative capabilities of diffusion. Notable examples of such approaches include Stable Video Diffusion[3], Gen-2[42], I2VGen-XL[60], PikaLabs[28], SparseCtrl[20], and SORA[5]. Extending these works, some recent methods add more control by explicitly guiding generation with optical flow or trajectory information[59,32].

Our objective, however, is purely to transform given images into plausible optical flows. For this purpose, we adopt a straightforward image-to-video architecture that generates natural videos starting from a single image as the first frame, without explicit guidance in the video generation process.

SECTION: 3Approach

SECTION: 3.1Transforming Static Images

As shown in Figure1, spatially distorting the original source image to generate a target frame for optical flow estimation does not yield realistic optical flow maps. To obtain high-quality optical flow maps with video-like properties from static images, we use an image-to-video generation model that generates new frames directly, rather than relying on the distortion of the source image.

Network overview.In the image-to-video generation process, we employ Stable Video Diffusion[3]based on a 3D-UNet[13]architecture. Following recent approaches, a pixel-to-latent conversion is applied before the denoising model, with a latent-to-pixel conversion following it, using a pre-trained VAE[26]encoder and decoder, respectively. Notably, the VAE decoder incorporates 3D convolutional layers to account for the time dimension, thereby ensuring temporal consistency during the decoding process.

Starting from Gaussian noiseand the source image latent, the denoised latentis obtained as

wheredenotes the denoising 3D-UNet model. The output denoised latentis then decoded with the VAE decoder on a per-frame basis, with each frame representing a transformed image generated from the source image. Note that the number of video framesis set to, following the default setting in Stable Video Diffusion.

Sampling details.During the sampling process, we usesteps of the deterministic DDIM sampler[47]. The classifier guidance scale[22]is set tofor the first frame andfor the last frame. The resolution and frame rate are set toand, in line with the default setting. The decoding chunk size is set toto balance generation quality and computational cost.

SECTION: 3.2Hallucinating Flows

After generating target images from each source image using the image-to-video generation model, we obtain sets of source-target pairs for network training. Formally, the temporary paired datacan be represented as

whererepresents the source image anddenotes the generated target image at each frame. From these temporary pairs, we construct training samples for network training, as illustrated in Figure2.

Data creation.In order to obtain the final image-flow pairs to learn the two-stream network for VSOD, we need to estimate flows from the temporary paired data. For optical flow estimation, we use a pre-trained RAFT model[50]while maintaining the original image resolution. Formally, the final paired datacan be obtained as

whereindicates the estimated optical flow map from frameto frame. Note that the paired datais obtained for each static image separately, and therefore, fromsource images, we can obtainpaired data samples.

Dataset construction.We adopt the DUTS[51]dataset as the source for applying our data creation protocol, which consists of 10,553 training images and 5,019 testing images. Since these image samples are used solely for network training, we utilize the entire training and testing sets to construct our training dataset. For each static image from DUTS,image-flow samples are extracted, resulting in a total of 15,572 data pairs for training. Note that, as the flows are calculated using the source image as the starting frame for optical flow estimation, the RGB images, segmentation masks, and optical flows are all spatially aligned. These data pairs are then combined with conventional training pairs to enhance data diversity during network training.

SECTION: 3.3Segmentation Model

With the aid of our simulated training samples, we train a VSOD network to detect primary objects at the pixel level. Following conventional flow-guided object segmentation approaches, we design our model using a two-stream architecture, as shown in Figure3.

Network architecture.Our network includes dual encoders designed to capture appearance cues from RGB images and motion cues from optical flow maps. Both encoders are based on MiT-b2[56], utilizing only its encoder component. After embedding each modality, multi-level features are fused through simple attention blocks[55]. The fused features are then passed through the decoder, where they are progressively decoded by combining feature interpolation with skip connections. Following the decoding process, a binary segmentation mask representing the primary objects is obtained.

Network training.The segmentation network is trained using a straightforward supervised learning protocol. We employ cross-entropy loss and the Adam optimizer[27], with a fixed learning rate of 1e-5, a batch size of 16, and an input resolution of. The training is conducted on two GeForce RTX TITAN GPUs, each utilizing less than 16GB of GPU memory. The entire network training process takes approximately two days on this hardware. During training, we combine both actual video data and simulated data derived from static images. Specifically, the actual data consists of the DAVIS 2016[39]and DAVSOD[16]training sets. The data samples are randomly shuffled, with the mixture ratio set to 2:1:1 for simulated data, DAVIS 2016 data, and DAVSOD data, respectively.