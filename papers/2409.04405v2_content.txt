SECTION: Asymptotic-state prediction for fast flavor transformation in neutron star mergers

Neutrino flavor instabilities appear to be omnipresent in dense astrophysical environments, thus presenting a challenge to large-scale simulations of core-collapse supernovae and neutron star mergers (NSMs). Subgrid models offer a path forward, but require an accurate determination of the local outcome of such conversion phenomena. Focusing on “fast” instabilities, related to the existence of a crossing between neutrino and antineutrino angular distributions, we consider a range of analytical mixing schemes, including a new, fully three-dimensional one, and also introduce a new machine learning (ML) model. We compare the accuracy of these models with the results of several thousands of local dynamical calculations of neutrino evolution from the conditions extracted from classical NSM simulations. Our ML model shows good overall performance, but struggles to generalize to conditions from a NSM simulation not used for training. The multidimensional analytic model performs and generalizes even better, while other analytic models (which assume axisymmetric neutrino distributions) do not have reliably high performances, as they notably fail as expected to account for effects resulting from strong anisotropies. The ML and analytic subgrid models extensively tested here are both promising, with different computational requirements and sources of systematic errors.

SECTION: IIntroduction

Neutrino flavor transformation, an intrinsically quantum phenomenon first discovered with atmospheric[1]and solar[2]neutrinos, is now well-established. The mixing parameters in the three-flavor scenario have been measured with increasing accuracy in the last decades[3], or are within reach of existing and future neutrino experiments (CP-phase,octant, mass ordering). In dense astrophysical environments like core-collapse supernovae (CCSNe) or neutron star mergers (NSMs), the very high matter and neutrino densities source effective potentials which necessitate a quantum description of neutrino kinetics[4,5,6,7]. This wrinkle results in a rich and complex phenomenology of neutrino flavor conversion (see[8,9,10]for recent reviews).

In particular, “fast” flavor instabilities (FFIs), which are associated to the existence of an (electronheavy-leptonic flavor) neutrino lepton number (ELNXLN) angular crossing, have been shown to be very common in dense environments (see e.g.,[11,12,13,14,15,16,17,18]). The neutrino distributions in existing NSM simulations imply unstable modes that grow on timescales of nanoseconds and lengthscales of millimeters. Multidimensional direct simulation of neutrino quantum kinetics on large scales is computationally challenging[19,20,21]even if such strong instabilities are not fully realized (e.g.,[22]).

One path forward consists in designing an appropriate subgrid model, which can be incorporated with limited cost in hydrodynamic codes (see, for instance,[23,24,25,26,27,28]). These models must first identify the locations and growth rates of instabilities, which can be done with linear stability analysis[29,30,31,32]. They must then determine the asymptotic state reached after the FFI, for which a general analytic solution is not yet known.
Local, dynamical simulations of neutrino evolution in NSM-like environments can be used to estimate the asymptotic state on small scales. Simulation codes performing such a task are widely available with varying numerical techniques and approximations (e.g.,[33,34,35,36,37]). Although a number of approximate mixing schemes have been proposed, the most sophisticated of them are restricted to axially symmetric situations (e.g.,[24,38,39,19,40,41,42,43]).

An attractive alternative to analytic subgrid models is a machine learning (ML) model trained to reproduce the results of local simulations of flavor instability.
ML technology applied to neutrino flavor instabilities has been pioneered by Abbar et al., with models dedicated to the detection of instabilities, trained with artificial data[44], more physical distributions[45]and extended to non-axisymmetric crossings in[46]. The capability of ML models to predict the outcome of FFIs, based on an axisymmetric analytical prescription developed in[42], was discussed in[47], with a multi-energy generalization in[48]. These works have demonstrated that ML can in principle perform very well under the constraints imposed, but cannot be directly applied to fully relativistic and multidimensional hydrodynamic simulations without further improvements, notably removing symmetry simplifications and expanding the parameter space of the training set. In addition, it is presently unclear whether ML models in general circumstances actually have superior performance over other analytic subgrid models.

It is also important to note that the majority of multidimensional simulations of CCSNe and NSMs evolve the neutrino radiation field based on angular moments[49,50,51]. While some subgrid models have been proposed to predict the angular structure of the post-instability distribution[39,41,40,42], others are built to predict post-instability moments using only pre-instability moments as inputs (e.g.,[24,47,27]). The latter are most directly applicable to current state-of-the-art global simulations.

There are nevertheless a few caveats to take into account. First, the previously mentioned models all start from a “classical” configuration, i.e., without flavor mixing, which consists in the distributions (or the angular moments) of each flavor of (anti)neutrinos. The model then predicts the final distributions or moments, once again for each flavor. For instance, () become (). This approach neglects any quantum coherence in both the initial and final states. In the density matrix formalism, this amounts to considering that the final density matrix is essentially flavor-diagonal. There are nevertheless examples of systems with maintain some flavor coherence, for which an “effectively classical” description is not adequate (see e.g.,[52,53]).

Second, the use oflocalasymptotic-state subgrid models must be considered with caution. As discussed in[54], the underlying idea of such subgrid models in a large-scale simulation is to instantaneously impose the final state that we predict here, whenever the flavor configuration is unstable, using the fact that the timescale of fast flavor conversionis much smaller than the time steps of a simulation. This procedure is then reiterated after a simulation time step, and so on. However, there is an internal inconsistency as the evolution between two simulation time steps is entirely classical, although fast flavor conversions should be, by assumption, potentially taking place during. Since the FFI is expected to erase ELN-XLN crossings, one might expect that in an actual system neutrinos always stay at the edge of instability, a regime where flavor conversion might be different from the cases we consider here, as recently explored in[22]. In addition, gradients allow for an asymptotic state that is qualitatively different from one predicted by simulations with periodic boundary conditions (e.g.,[55,41]), even including completely swapping flavors instead of relaxing to a mixed equilibrium under certain boundary conditions[21,56]. Nevertheless, asymptotic-state subgrid models present the tremendous advantage of being relatively easy to implement in existing large-scale simulations. Such models can provide insight into the implications of local instabilities on the evolution of CCSNe and NSMs when a fully consistent treatment of flavor transformation is still not yet achievable.

Third, it is well known that representing neutrino distributions with a small number of angular moments can induce sizeable errors (e.g.,[57]for classical transport, and[58,59]in the quantum case). In the context of NSMs, where strong anisotropies and inhomogeneities can break the assumptions that go into constructing an analytic closure, the maximum entropy closure used in the following cannot represent the true angular structure of the radiation field[60,61]. The FFI generally involves strong angular dependence in the amount of flavor transformation, so the post-instability distributions are far from the maximum-entropy distributions assumed here (e.g.,[35,62,63,42]). Just as with classical radiation transport, a fully angle-dependent treatment of the FFI is needed to capture all of the details, though such treatments are naturally much more expensive. That being said, moment methods for quantum kinetics work surprisingly well, both in terms of analytically predicting flavor instabilities[32]and directly simulating them[64,37], especially in the context of recent improvements to analytic treatments of quantum closures[65], cementing moment methods as an attractive, inexpensive alternative to fully multi-dimensional quantum kinetics.

In this paper, we perform a systematic assessment of the performance of existing mixing schemes in actual NSM environments, based on tens of thousands of local, three-dimensional simulations of FFIs. We go beyond the standard axisymmetric assumption by generalizing mixing prescriptions based on flavor equilibration in a particular angular domain to a generic three-dimensional system, which is a necessary step forward to maintain a reasonable accuracy on the direction of (anti)neutrino fluxes. We also introduce a new ML model, specifically designed for relativistic settings, and evaluate its performance on different NSM simulation snapshots.

The most directly comparable ML model in the literature is that of[47], in which the authors develop a neural network to predict the outcome of the FFI assuming axisymmetric neutrino distributions. In this work the authors use a collection of data points both generated analytically via the approximate “Power-1/2” scheme[42]and from direct simulation, and train a neural network with a single hidden layer to reproduce the post-instability moments with accuracies of a few percent. We build on this work in a few important ways. We consider general anisotropies, allowing distributions from all neutrino flavors to have fluxes in arbitrary directions. We construct our model to take as input only relativistically invariant quantities, produce results that are also invariant under rotations and boosts, and do not assume knowledge of ELN conservation, crossing location, or relative crossing depth. Finally, our training, test, and validation datasets are drawn from a combination of randomly-generated neutrino distributions and distributions extracted from 3D general-relativistic simulations of NSMs.

This paper is organized as follows. In Sec.II, we present our machine learning model and the datasets used for its training and testing. In Sec.III, we review existing mixing schemes previously used in the literature and generalize them to a multidimensional environment, including introducing a straightforward and performant three-dimensional, non-axisymmetric model. Our main results, comparing the performance of all these subgrid models on various datasets, are gathered in Sec.IV. We summarize and conclude in Sec.V.

SECTION: IIMachine Learning Model

SECTION: II.1Design

The objective of the ML model is to take the 24 moments of the neutrino radiation field (four-flux of each species) as input and predict the same 24 moments after the fast flavor instability. Throughout this work, it is assumed that the input moments are approximately homogeneous, and the output moments are averaged over the spatial volume of the simulation (which allows to safely overlook the microscopic chaoticity of the system[66]). We label the energy-integrated number four-flux moments as, whereis a spacetime index,indexes whether the moment belongs to neutrinos or antineutrinos, andindexes the neutrino flavor:111Although we only consider in this paper the 3-flavor case, we will keep some definitions generic by introducing the number of flavors.

withthe classical distribution function ofneutrinos, and similarly for antineutrinos. Using the standard notationsfor the number density andfor the spatial number flux density, we have for instance, in the comoving frame,,or. The moments are usually chosen to be expressed in the frame comoving with the fluid, since that is where neutrino-matter interactions are most easily defined, but the number four-flux is a relativistic vector.

One of the main design criteria of the ML model is to ensure that it is invariant under rotations and clear to interpret in relativistic settings. To achieve this, we restrict the inputs to be dot products between the pairs of number four-fluxes and between each four-flux and the four-velocity of the fluid, assuming a metric signature of. That is, our array of inputs is a set of dot products for each unique pair of:

where the spacetime index is lowered via the metric and. For 6 species of neutrinos, these are 27 unique quantities.

We use a neural network (NN) whose structure is shown on Fig.1. Between the input and output layers are three fully-connected hidden layers with leaky rectified linear unit (ReLU) activation functions and a leaky slope parameter of 0.01. We find no improvement but increased training duration for deeper networks, and network width is much more effective at improving prediction quality. Each hidden layer has a width of 128, since with our given dataset size, larger widths did not significantly increase prediction quality and were more prone to over-training.

In order to maintain rotational invariance in the output as well, we express the post-instability moments as a linear combination of the input moments

The values of the coefficientsare themselves a function of the set of 36 numbersthat are the direct outputs of the neural network. This extra layer (“Conserve Layer” on Fig.1) enforces conservation of energy and particle number:

When contracted with, the first term in parentheses separately sums over all neutrino and antineutrino flavors. The second term does the same for, such that the term in parentheses yields the net deficit number current for neutrinos () and antineutrinos (. We evenly distribute the deficit over theflavors, which forces particle number conservation without preferring any single flavor. The overall action of the ML model can be seen as a complicated non-linear function.

We do not explicitly enforce conservation of lepton number, so the model serves as an honest assessment of the prospects of future models to deal with a more complete set of phenomena that do not respect this symmetry. However, since this particular model is trained on FFI simulations, it should learn to approximately preserve the lepton number of each flavor. This will be tested in the results section.
ELN conservation can of course be enforced after the model’s predictions. However, doing so by modifying thetensor leads to conservation of ELN flux, which is not a property of the FFI under periodic boundary conditions, and leads to significant errors (see AppendixA.1).

Although the ML model is fundamentally built for three flavors, we restrict our inputs to have equal distributions ofand(and separately equal distributions ofand), reflecting the approximately equal interaction rates of heavy lepton neutrinos in NSM environments. The ML model does learn to predict approximately equal distributions for these neutrino species, but we explicitly average the heavy lepton distributions output by the ML model. This significantly reduces the effective size of the parameter space, allowing the model to converge with many fewer training data points than would be required in general for a three-flavor model. The inputs and outputs of the model could be simplified in this light, but we choose to leave it as a general three-flavor process so it can be more readily extended to other transformation phenomena in the future.

The ML training software is publicly available at Ref.[67].

SECTION: II.2LocalEmuSimulations

Training and testing the ML model requires accurate dynamical simulations of neutrino evolution to determine the asymptotic state of an unstable distribution. Although different choices of boundary conditions and the inclusion of collisional processes can significantly alter the spatial and temporal structure of the FFI (e.g.,[68,20,41,21,69,56]), we operate under the assumption that the asymptotic state is determined locally. We use local, centimeter-scale simulations with periodic boundary conditions to estimate the asymptotic state of the FFI. To that end, we useEmu[35], a particle-in-cell (PIC) code which simulates a large number of computational particles each associated with a position, momentum, and aflavor density matrix(for antineutrinos). We perform simulations of the energy-integrated distributions to explore the self-interaction-dominated limit relevant to the FFI.

The PIC implementation of the Quantum Kinetic Equations (QKEs) describing (anti)neutrino dynamics[4,7,70]reads, in the limit adapted to fast flavor conversions, for each particle:

whereis the particle’s direction,andare the number of neutrinos and antineutrinos contained in each computational particle,is the self-interaction mean-field Hamiltonian reconstructed at (), and. The solver uses second-order shape functions and a global fourth-order time integration scheme. The initial flavor diagonal components of the density matrices are attributed to individual particles such that the initial number four-fluxis reproduced for each flavor, with an angular distribution determined by the classical maximum entropy (ME) closure[71], consistently with the choice of closure used in the NSM simulations we will consider. The angular distribution associated to the momentsis, in the direction of the unit vector:

where, andis determined by solving:

Therefore, the initial values ofandare set such thatand, whereis the number of computational particles per cell andis the spatial volume of the cell. This allows the momentsto be reconstructed with enough angular resolution.

The fast flavor instability is associated with the development of flavor coherence from an initially almost flavor-diagonal density matrix. In other words, the quantity

withthe domain volume, grows exponentially fromto(the growth rate and associated lengthscale of the instability can be determined by linear stability analysis[32]), before entering the non-linear decoherence phase. Examples are shown on Fig.2, and the datasets represented by each color are introduced in Sec.II.3.

All flavor transformation simulations in this work are one-dimensional. Each dataset has different choices regarding the particular simulation resolution, spatial extent, and number of grid cells (see specifics in Sec.II.3). However, in all cases we first rotate the (anti)neutrino fluxes such that the net ELN-XLN flux lies in the-direction, and also discretize space only in this direction. This artificially enforces homogeneity in the- and-directions, but since the fastest growing unstable mode closely aligns with this direction, we are able to capture the dominant dynamical features. Note that although we assume homogeneity in two directions, we do not assume isotropy or axial symmetry in momentum space.

For the simulations used in the paper, we impose random perturbations to the flavor off-diagonal components of the particle density matrices with magnitudes on the order ofrelative to the diagonals. We guarantee that each simulation represents a robust FFI by only accepting simulations in whichhas a local maxima (at) separated from the initial value by at least 3 orders of magnitude. This criterion allows us to distinguish between small fluctuating perturbations and genuine FFIs. We then average the final distributions fromto the end of the simulation. We ensured that the duration of each simulation is at least 3 times the time required to reach the saturation of the FFI (end of the exponential growth of), as seen in Fig.2— this ensures that we are able to average over a time duration of at least.

SECTION: II.3Training Data

The ML model is trained on several datasets composed of both stable and unstable input distributions. These include poloidal slices of neutron star merger simulations and randomly generated distributions, which we describe in the following subsections. The data used in training our ML models is publicly available at Ref.[72].

We use the data from a general relativistic simulation of the merger of two neutron stars with component masses ofand[73].
The merger simulations are performed with the general relativistic radiation hydrodynamics code SpEC[74,75], using the SFHo equation of state for dense matter[76]and a gray two-moment scheme for neutrino transport[77,78]. Neutrino-matter interaction rates are computed using the NuLib library[79], including charged current reactions for electron type (anti)neutrinos, electron-positron annihilations and nucleon-nucleon Bremmstrahlung for heavy-lepton neutrinos, and scattering on neutrons, protons, alpha particles and heavy nuclei for all species (see M1-NuLib simulation in[73]). The remnant is a hypermassive neutron star that collapses to a black hole after. We consider two pre-collapse snapshots of this simulation here, atandpost-merger (see Fig.3for two transverse slices).

We also consider the 5-ms post-merger snapshot from a completely independent simulation of a neutron star merger with component masses ofand[78], shown on Fig.4. This simulation (subsequently referred as “M1NuLib-2016”) is performed with the same two-moment radiation transport code, but with the LS220 equation of state[80]. In this low mass symmetric system, a larger fraction of the matter is in the neutron star remnant, rather than in the accretion disk or matter outflows. This second system also has a more stable neutron star remnant: the higher mass, asymmetric system previously discussed collapses after, while this simulation shows no sign of collapse to a black hole for itsof evolution. This snapshot is used as test data to assess the performance of the ML model in a different environment.

The M1 scheme used in these simulations only evolves the first angular moments of the neutrino distributions (number density, flux density) for three species:,and(in a three-flavor framework, this corresponds to assuming). The system of equations is closed by specifying an analytic form for the pressure tensor as a function of, namely, the “Minerbo” closure[71,81,82,57]. Without detailed angular distributions, we determine which locations are unstable to the FFI by searching for electron lepton number (ELN) crossings in the maximum entropy distributions following the procedure derived in[16].

The training data from these snapshots consists of both stable and unstable datasets. In the stable dataset, we take all of the FFI-stable distributions on theslice from the refinement level described in Fig.3, and assume that the final distribution is equal to the initial distribution for each neutrino species. This amounts to 21,360 stable data points in the 3-ms M1-NuLib snapshot, 18,886 stable data points in the 7-ms M1-NuLib snapshot. We also use FFI-stable distributions from all four refinement levels for the sameslice, amounting to 160,647 and 114,895 stable data points from the 3-ms and 7-ms snapshots respectively, together with 10,970 stable data points from the 5-ms M1NuLib-2016 snapshot to train an additional ML model that we call “ML_ext” in the following.

In the unstable training dataset, we take all of the FFI-unstable distributions in the refinement level described in Fig.3and simulate their evolution as described in Sec.II.2. The domain size in the-direction was chosen such that, whereis the wavelength of the fastest growing mode as determined by linear stability analysis[32]. The grid for each simulation was composed of 1,024 spatial scales, such that the wavelength of the fastest growing mode also spanned approximatelygrid cells. The resulting data were only used if the duration of the simulation was at least three times the time required for the FFI to saturate to ensure sufficient time to average over the asymptotic state as described in Fig.2. After this selection process, the 3-ms snapshot yielded 9,199 data points and the 7-ms snapshot yielded 7,849 data points. In the ML_ext model, we also use 7,579 data points similarly simulated based on unstable distributions in the 5-ms M1NuLib-2016 snapshot. It is clear from Fig.2that all of the simulations indeed show a clear signature of linear growth and saturation at a stable final state, as indicated by flat curves for.

We use a number of randomly generated distributions to expand the parameter space explored by the model beyond the conditions present in a NSM simulation evolved assuming classical transport. In all of these distributions, we once again assume that muon and tau flavor neutrinos have the same distributions. However, unlike the NSM-based data, neutrinos and antineutrinos do in general have different distributions — this is notably expected to be the case in general after fast flavor conversion, a situation that will arise if the ML model is embedded in a large-scale simulation.

We first run a set of 4,028 1DEmusimulations starting from randomly-generated initial conditions. In all of these simulations, we choose a domain size ofand resolve the domain with 8,192 grid cells. This ensures that the smallest unstable wavelengths are resolvable by the grid, and all but extremely large wavelengths fit in the domain. In all of these simulations the net number densities of neutrinos and antineutrinos were set to, such that the smallest unstable wavelength (in the limit of oppositely-directed neutrino beams) is[83]. The number densities of each species was uniformly sampled between 0 and 1, the densities of tau neutrinos were set to be equal to those of muon neutrinos, and all densities were then scaled by a constant factor to achieve the above net density. The directions of the flux vectors for each species was sampled as, whereis a uniformly sampled random number between 0 and 1, such that all distributions have a positivecomponent of the number flux (since one expects all distributions to be moving in roughly the same direction locally in a NSM). The azimuthal direction of each flux was sampled uniformly, and the flux factor was sampled as. Only unstable distributions were simulated, and after the selection process described for the NSM simulations and shown in Fig.2, this process yielded 2,074 data points.

In addition to randomized unstable distributions, we also train with a large number of randomly-generated distributions that are trivially stable, such that we can assume the asymptotic state is equal to the initial one. At the beginning of training each model, we generatedistributions with number densities sampled as above, but all flux factors set to zero (labelled “0 flux factor” in plots). In addition, we sampledistributions where only a single species has nonzero density, and has a flux direction uniformly sampled in solid angle and a flux factor uniformly sampled between 0 and 1 (called “1 species” in plots). In this case, if the muon or tau neutrino is selected to have nonzero density, both flavors are set with the same distribution. Finally, we generate an additionaldistributions with number densities uniformly sampled as above, flux directions uniformly sampled in solid angle, and flux factors sampled as(called “random” in plots). This exponent is used to cause only about half of the sampled points to be stable (and hence used in training), in order to more fully sample the line between stable and unstable distributions. In this case, we test for stability by constructing a maximum entropy distribution for each species, discretizing into 378 directions, and checking for the presence of an ELN crossing. Finally, we generatedistributionsat every training epochto be used to probe for unphysical predictions. These are generated using the same method as the “random” stable generated points above, but leaving in both the stable and unstable points (labelled “unphysical” in plots).

SECTION: II.4Training Process

The ML model is trained to minimize the loss function given by

The first three loss functions are applied to all of the unstable distributions described in Sec.II.3. Each of the other terms is applied to one of the unique data sets described in Sec.II.3.

The directional loss term is designed to contribute nothing to the loss when the predicted and true fluxes lie in the same direction:

where “pred” refers to the moments predicted by the ML model, and “true” to the value obtained in theEmusimulations. To lighten the notation, the quantities in the sum are implicitly evaluated for the data pointand the species(for instance, if,is). In this work, we assume three neutrino flavors, or six neutrino species including neutrinos and antineutrinos.

The loss from the predicted flux factor is

and the number density loss function reads

The unphysical loss function penalizes any predictions that produce negative number densities or flux factors larger than one. Specifically,

Finally, the remaining loss functions are computed as

We randomly select 10% of each dataset to serve as test data, and the remaining 90% is the data on which the model is optimized. We train the model to minimize the loss function using the AdamW optimizer[84]with an initial learning rate ofand a weight decay of 0.01. We also employ a plateau learning rate scheduler with a patience of 500 iterations, a cooldown of 500 iterations, and a reduction factor of 0.5. The reduction in learning rate mitigates large training performance oscillations that arise otherwise. We also use a dropout probability of 0.1 to prevent over-training and improve model generalizability. Larger dropout probabilities tended to add significant noise to the training process that resulted in larger final loss values, and models trained without dropout were easily over-trained. These hyperparameters produced the highest quality models among many iterations of different hyperparameters, and were trained until performance stopped improving. The models are likely constrained by the amount and quality of the input data, but increasing the amount of data generally requires more training time, and despite our data cleaning procedures, the results of theEmusimulations are accurate only to about 1%. Training higher-quality models could benefit from a larger amount of more precise data.

Figure5shows how the training and test errors evolve with training epochs. The first thing to note is that the training error is usually above the test error as a result of using dropout. With a dropout probability of 0.1, during each training epoch 10% of the nodes are removed from the network to force redundancy in the model, so the predictive capabilities are worse than the full model employed to calculate the test loss. The loss decreases very quickly at first, but tapers (notice the logarithmic horizontal axis). After about 20,000 epochs, the model is unable to improve, driving the learning rate down to essentially zero. The model learns to avoid unphysical results within the first thousand epochs or so (black curve in the “Unphysical” panel), although the reduced network used during each training step is not as good at doing so (blue curve in the same panel). We plot the square root of the loss function so that the plotted values roughly represent the magnitude of the error one can expect in each quantity. By the end of the training process, all of the metrics are accurate to within a few percent.

It is reasonable to expect that the final state predicted by the ML model be stable to further transformation, since the FFI works to remove ELN-XLN crossings[19,20,40]. However, this is not in general the case in our model due to a combination of model imperfection and the generally faulty assumption that the input distributions follow maximum entropy profiles [Eq. (6)]. Although the final state of theEmusimulations indeed exhibits no ELN-XLN crossing, if this distribution is integrated into two moments and redrawn as a maximum entropy distribution, there is no guarantee that the ELN-XLN is still zero in the “crossed” region. Thus, if the output of the ML model (which reflects the arbitrary angular distribution simulated byEmu) is mapped back into the input (which assumes a maximum-entropy distribution), the model is unable to enforce that this new distribution is stable even if the training were perfect. The stability of the final state is demonstrated in the “Stable (final state)” panel of Fig.5, using Eq. (14) between the moments resulting from the ML model being applied once and twice. As the model learns, it indeed predicts that successive predictions exhibit smaller changes, but it never goes to zero. We come back to this issue in Sec.IV.2on a particular example.

Overall, the ML model is able to learn trends reasonably well from a wide range of stable and unstable distributions using only their angular moments. We will spend much of the remaining text quantifying this performance on unseen data with respect to other proposed models.

SECTION: IIIApproximate mixing schemes for fast-flavor conversion

Since the computational cost of local QKE calculations is prohibitive in large-scale simulations, finding other ways to determine the asymptotic state reached by the system after the FFI is a necessity.

ML models are an attractive and efficient way to determine this final state: once trained, the use of an ML model is very fast, and the training can be straightforwardly adapted to other flavor conversion mechanisms. But there have also been many other studies focused on finding analytical prescriptions for the asymptotic state that can also be efficiently implemented in a large-scale simulation. In this section, we introduce a few of the analytical schemes previously designed in the literature, along with a new, fully three-dimensional scheme extended from[40,41], so we can compare their performance with the ML model in the next section.

SECTION: III.1Existing Mixing Schemes

The approximate schemes used in the literature are usually built assuming identical distributions ofandflavors, called in the followingfor neutrinos andfor antineutrinos. As previously, we will denote with a prime the quantities in the post-FFI asymptotic state.

A range of prescriptions are designed directly at the level of the angular moments, and amount to some sort of flavor equilibration constrained by the symmetries of the problem (e.g., ELN conservation). In the following, we consider several such schemes from Ref.[24].

The “Mix1” case assumes explicit conservation of the ELN and XLN (the latter being zero in the NSM points we consider), with equipartition between the species with the lowest number densities. The equipartition value is

and the final number densities are set to,, the remaining number densities being determined by ELN and XLN conservation. Note that the total number of neutrinos and antineutrinos is, as expected, conserved with this prescription. These conditions can be, in practice, rewritten in a form similar to Eq. (3):

The fluxes are modified in the same way, that is,for.

The “Mix1f” case assumes the same transformation for the number densities as Mix1, but conserves flux factors instead of applying the same transformation matrix to the fluxes:

A similar scheme was considered in Ref.[26], where the flux factors are conserved for the species whose population actually decrease with flavor conversion, the remaining fluxes being set to conserve the total flux, but in this work we use the version in[24].

The equipartition treatment of Ref.[23](called “Mix2” in[24]) does not conserve separately the ELN and XLN, but instead the total lepton number (ELN + XLN). It assumes separate equipartition of neutrinos and antineutrinos:

In terms of thetransformation matrix, it reads for 3 flavors:

The fluxes are modified with the same mixing coefficients. This scheme breaks the symmetry, and in general violates net ELN conservation that should be enforced by the QKEs in the FFI limit. This scheme is thus expected to give less accurate results compared to the Mix1(f) cases.

Other works have not aimed at determining directly the angular moments post-FFI, but rather the full angular distributions of neutrinos. Detailed studies have thus led to the design of a prescription for the direction-dependent survival probability[38,39,19,40,41,42,47], i.e., the probability for an electron neutrino to still be an electron neutrino after the FFI. For axially symmetric distributions, and assuming identical initial distributions of(that is,), these prescriptions generally enforce full flavor equipartition on the “small side” of the angular ELN crossing, i.e., the angular range where the difference betweenanddistributions is smaller in absolute value. The survival probability on the “large side” is then set by ELN conservation with a prescribed angular dependence. This probability is for instance described as a box-like function (which creates artificial discontinuities in the final angular distributions)[40,41]or with a continuous transition[42,47,85]. Since it was shown to be superior to the box-like form in axisymmetric simulations, we consider the latter scheme, with the so-called “Power-1/2” analytic form (see details in[42]).

These prescriptions require knowledge of the angular distributions of (anti)neutrinos. Since we are applying these schemes to distributions described only by moments, we assume angular distributions given by the classical maximum entropy closure (6), assuming axisymmetry around the direction of the net ELN-XLN flux. In practice, we setafter rotating the system to place the net ELN-XLN flux in thedirection.
The final distributions calculated accordingly to the Power-1/2 mixing scheme are then integrated following Eq. (1) to obtain the predicted moments.

SECTION: III.2Three-Dimensional, Non-Axisymmetric Mixing Scheme

The box-like scheme of[40,41]can be straightforwardly generalized for non-axisymmetric distributions in the following way.222We assume once again equal distributions for heavy-lepton flavor neutrinos (), and heavy-lepton flavor antineutrinos ().We define the ELN-XLN distribution in a given directionby.
The fast flavor instability is associated to the existence of a crossing in[86,87], which means that the angular space can be divided in two regions:

whereis the Heaviside distribution. Let us denote by() the smallest (largest) value betweenand, and() the associated domain of integration. We then define the survival probability:

The final angular distributions are set to:333Since we are considering pairwise conversions betweenand, we have transitionswith probability, transitions() with probability, and similarly for antineutrinos. Accounting forandleads to (22).

withand likewise for antineutrinos. We dub this procedure “Box3D.” As in the Power-1/2 scheme, the final state is discretely integrated to obtain the final four-flux. Note that reproducing the initial moments with higher precision require more angular bins, hence a higher computation time (compared to the ML model, for instance). Although the Power-1/2 scheme is justified by simulations of axisymmetric systems, there has not been to date systematic studies of non-axisymmetric configurations that would support the Box3D generalization we propose here. The results of Sec.IVwill nevertheless show that this model offers the best overall performance in a range of environments, making it a strong candidate in non-axisymmetric cases.

The solution of the QKEs (5) in the fast flavor limit must preserve a number of symmetries. With periodic boundary conditions, the following quantities (averaged over the simulation box) must be conserved:,,,,,. We summarize in Table1how the different subgrid models satisfy these conservation requirements.

The Box3D scheme conserves by construction all these quantities, but its actual implementation relies on an angular discretization of momentum space. As a consequence, for any given angular resolution, the initial four-fluxis reconstructed with a given precision. The conserved quantities, etc. are then conserved, but with the systematic error associated to the initial discretization. In this paper, we use 100 angular bins forand likewise for, which leads to negligible resolution errors.

SECTION: IVSubgrid Model Performance

Given theEmusimulations introduced in Sec.II.3, we have at our disposal several datasets on which we can assess the performance of the previous subgrid models. We recall that the ML model is trained on unstable points from the 3-ms and 7-ms snapshots only ; the snapshot of a previous merger model (“M1NuLib-2016”) thus allows to check its capabilities in a different NSM environment.

SECTION: IV.1Performance Metrics

We assess the accuracy of a prediction of the final four-flux, compared to the true valueobtained from a resolvedEmusimulation, via the following metrics. Each metric yields a number for a single data point (e.g., one number per location in the NSM snapshot).

First, we assess the accuracy of number density predictions through the maximal normalized difference of number densities,

Then, the direction of (anti)neutrino fluxes is measured via the maximal misalignment of the spatial flux densities, weighted by the flux factors,

where inside the brackets, we used the shorthand notationsand. Note thatis weighted by the flux factor of each species. This is to account for the fact that the smaller the flux factor, the less relevant its precise direction.

Finally, we measure the absolute difference of flux factors,

with the same shorthand notations as before.

With these definitions, each of these metrics can have values between 0 and 1.

SECTION: IV.2Single Prediction Example

We first focus on an example point (“NSM1”), identified on Fig.4and previously studied by particle-in-cell and moment methods in[64,37]. The classical moments for this NSM1 point from[78]are given in Table2.

In Fig.6we show the evolution of the domain-averaged angular moments calculated byEmu, together with asymptotic state predictions from the subgrid models for the NSM1 point. The four columns show results for four different (anti)neutrino species, and each of the four rows corresponds to a spacetime component of the number flux. The gray curves show the evolution of each moment from anEmusimulation performed with the resolution used in our training dataset (378 particles per cell), and the black curve shows the same using a much higher angular resolution (1506 particles per cell). The proximity of the curves demonstrate that resolution in theEmusimulation is not a significant factor. The solid black line shows the averaged asymptotic state. Recall that this simulation is from a dataset that is not used during training, and the point is selected as a well-studied example, and not for one with particularly good or poor performance. Note that in Sec.IV.3we demonstrate that there is a distribution of errors, such that all sub-grid models exhibit in certain cases larger errors than those shown in Fig.6.

The predicted final state from the ML model is shown in dashed blue. While it is similar to the simulated final state, it is clear that the amount of flavor transformation apparent in the number density (bottom row) is under-predicted, at comparable level with the Box3D scheme. The Power-1/2 scheme performs better, but the Mix1 and Mix1f schemes over-predict the amount of transformation. The Mix2 scheme performs very poorly, notably for antineutrinos (a consequence of the imposed equilibration ofand, which is not at all the case in the actual evolution).

The ML and Box3D perform reliably well in predicting the asymptotic values of the flux components. The Mix1 scheme produces reasonable results, but the Mix1f scheme induces significant errors, at times with the opposite trend compared to the true solution. For the same reason as for the number densities, the Mix2 scheme induces large errors on antineutrinos. The Power-1/2 scheme does a very good job of predicting the final flux in the-direction, but the scheme is not able to predict the change in theandflux components since it only has an axisymmetric interpretation. Note also that the changes in theandfluxes are small compared to theflux and especially the number density, so errors appear enhanced.

Overall, the ML and Box3D methods show the most consistently good performance across all moments. The performances of all subgrid models applied to the NSM1 data point, as measured by the metrics of Eqs. (23)–(25), are reported in Table3.

As emphasized before, the final angular distributions obtained inEmuare not maximum entropy distributions[64,37]. However, embedding our results (through the ML or Box3D model, typically) in a global M1 simulation would, for a given set of final angular moments, a priori implicitly assume that they correspond to a maximum entropy distribution (or another type if a different closure is chosen). We show on Fig.7how the ELN-XLN distribution is affected by this maximum entropy reconstruction.

In the initial state (top panel), we clearly identify the regions of positive (red) and negative (gray) ELN-XLN. The black curve shows the contour of zero ELN-XLN. The finalEmuresults are in very good agreement with the Box3D scheme: the net ELN-XLN is negative, such that the FFI washes away the ELN-XLN in the angular region where it was initially positive. However, looking at the third row (“Maximum Entropy”), where the final moments from theEmusimulation are used to form the equivalent maximum entropy distributions (6), we see that the cancellation of the ELN-XLN in the top hemisphere is lost, which highlights the strong non maximum-entropy-like features of the asymptotic state distributions. Yet, by construction of the M1 scheme with closure, “Maximum Entropy” is the underlying distribution that would be embedded in a global M1 simulation employing any such subgrid model. Solving this inconsistency is beyond the scope of this paper, and is a significant task ahead for quantum moment methods (see also[64,37,32,65]).

SECTION: IV.3Ensemble Performance

In order to characterize the performance of the models, we apply them to the datasets listed in Sec.II.3to demonstrate the average behavior and the presence of any outliers. We do not plot results for training and test data separately, since the distributions look very similar. This is in line with the training and test losses plotted in Fig.5and indicates that the ML model is not over-trained. Although the model generalizes well to unseen data randomly selected from the same dataset as the training data, we test its ability to generalize to truly new data by testing the model predictions on simulations from an entirely separate simulation of a NSM (“M1NuLib-2016”). We also show the changes in performance if this new simulation is included in the training dataset (“ML_ext” model, see Sec.II.3).

We first demonstrate the ability of each model to correctly predict the outcome of the FFI inunstabledistributions. In Fig.8we show cumulative distribution functions (CDFs) for the amount of error for each performance metric and each set ofEmusimulations. The gray dotted curves and the shaded gray area show how incorrect it is to assume that neutrinos do not change flavor. In the 3 ms snapshot, the number densities in unstable locations only change by a few percent (top left panel), indicated by the gray dotted curve quickly rising close to a value of one. The Mix1 and Mix1f schemes over-predict the amount of flavor transformation, but the ML, Power-1/2, and Mix3D schemes perform better. The Mix2 scheme does the poorest job, with large number density errors, which is expected given the incorrect assumptions of equipartition embedded in this model (already discussed in the example of Sec.IV.2). Looking at the number density errors in the other datasets (i.e., other panels in the left column), this trend is persistent, noting, moreover, that the amount of flavor transformation generally increases when comparing datasets from top to bottom. Power-1/2 does the best job at predicting number densities in the NSM datasets. In the random dataset, which contains highly non-axisymmetric distributions, the assumption of axisymmetry made in deriving the approach is more strongly violated and it performs much more poorly (bottom left panel). In the NSM datasets, the difference in performance onbetween the Power-1/2 and Box3D models is largely due to the discontinuous nature of the Box3D model: the axisymmetric version of the box-like scheme (not shown) is very close to the Box3D curve for, indicating that an advantage of Power-1/2 comes from the smooth transition between angular regions in its design. Developing a continuous version of the Box3D model is an open question that we do not seek to address in this paper.

In fact, all of the methods that assume axisymmetry do comparatively poorly in both the flux direction and magnitude metrics for the random dataset (bottom center and bottom right panels). The poor performance of the “No conversion” approach demonstrates that in these cases the flux vectors undergo significant changes. Both the ML and Box3D models do surprisingly good at predicting these changes, but it should be pointed out that many of these samples were used to train the ML model, whereas the Box3D model is not informed by any of these simulations.

In the NSM snapshots, the Power-1/2 scheme has reliably small errors for the predicted flux factors (right column), but reliably large errors for the predicted change in flux direction (center column). Once again, the Box3D and ML models provide the most robust performance, but when looking at data the ML model was not trained with (third row), the Box3D model is clearly superior. The dashed light blue line on Fig.8shows the performance of the ML_ext model, which is also trained on the M1NuLib-2016 data. As expected, the results are better compared to the fiducial ML model, in particular for the flux direction (center column). The performance remains inferior to the Box3D model, which might indicate the need for a more intricate ML model than the most simple relativistically covariant form of Eq. (3) to accommodate various NSM environments. All in all, the Box3D mixing scheme seems to describe the outcome of FFI in a large range of situations with a percent-level accuracy.

The most egregious errors committed by subgrid models are predictions that violate basic symmetries of the problem and realizability constraints. In these regards, apart from the Mix2 scheme, ML is the worst offender. The ML model is not constructed or trained to conserve ELN (see the complication described in AppendixA.1), and any ability to conserve ELN is learned from the training datasets themselves, which do conserve ELN to machine precision. Figure9shows CDFs of the violation of ELN conservation predicted by the ML model in each of the four unstable datasets. As in the previous results, the model struggles more with distributions that undergo more flavor transformation. In the NSM snapshots ELN is violated up to about 2.5 %.

The ML model very rarely makes unrealizable predictions. Namely, out of all the unstable datasets, no point leads to a negative number density, and 0.29 % of the Random-unstable points lead to flux factors larger than one. The ML_ext model has the same features, except that it predicts negative number densities in 0.5 % of the M1NuLib-2016 unstable points probed, and flux factors larger than 1 in also 0.5 % of these points (one third of these problematic points have both species with a negative number density and species with a flux factor larger than 1). If these issues do arise in a dynamical calculation, we suggest a minimal patch to the final prediction in AppendixA.2.

In Fig.10we plot distributions of prediction errors from applying the models to stable distributions. Ideally, the models should predict no flavor transformation in those cases. The Mix3D scheme is not plotted because, by construction, it predicts exactly no change when the distribution is stable. The ML model predicts number densities that change by at most a few percent on the training data (first, second, and fourth panels in the left column), but when the model is applied to unseen distributions (third panel in the left column) it performs much worse, with number density errors above 5 % for a quarter of the data points. A similar trend is seen in predictions of the flux directions and flux factors. The ML_ext model, which is also trained on stable points from the M1NuLib-2016 simulation, performs significantly better on this dataset as expected, with errors similar to those in the 3-ms and 7-ms datasets. Oddly, the ML_ext model performs worse in predicting flux factors, despite being exposed to much of the M1NuLib-2016 dataset, as it seems to prioritize performance in predicting number density. Given the additional training data, the ML_ext model may benefit from a larger neural network or from loss function tuning to prioritize performance appropriately.

The Mix1f scheme perfectly preserves the flux directions and flux factors, thus working as expected for stable distributions, but results in number density errors that are even larger than those predicted by the ML model. The Mix1 model predicts number densities identical to those predicted by Mix1f, but predicts fluxes that are much more wrong than those predicted by ML on average. Mix2 predictably performs very poorly in determining final number densities, since it predicts significant transformation regardless of the stability of the distributions. These issues are not a significant problem in the merger simulations where they were employed[23,24]because there was a binary flag determining whether a distribution is unstable, and flavor was only transformed if the distribution was deemed unstable. Thus, the ML model would benefit from using such a stability determination as well, and should produce results more accurate than the Mix models.

The Power-1/2 scheme correctly detects the stable points in the NSM datasets and hence predicts no change in number density. The errors in the flux metrics in the NSM datasets are then a result of removing theandcomponents of the flux in the axisymmetrizing process. This results in flux alignment predictions that are generally worse, but flux factor predictions that are generally better than other schemes. However, it performs moderately worse in the (highly non-axisymmetric) random dataset, as it artificially predicts that some points are unstable in the axisymmetrized distributions (see bottom row, left panel of Fig.10).

Several previous studies have indicated that the FFI occurs deep within the disk close to the energy-dependent neutrino decoupling surface. While the simplest models suggest ubiquity of the FFI[11], investigation of the neutrino radiation fields in neutron star merger models based on initial accretion disk configurations suggest that there is quite ubiquitous instability at early times (i.e., tens of milliseconds)[88], but the FFI ceases in the polar regions with increasing time[24,89], though other models show the opposite trend[23]. The complex trend of unstable regions following the distribution of tidal arms is only apparent in other calculations based on simulations starting from two distinct neutron stars instead of an initial equilibrium disk configuration[16]. The largest errors in the ML model track these matter inhomogeneities, which suggests that fully three-dimensional, relativistic simulations of NSMs are a stronger testbed for flavor transformation subgrid models.

In this work, we only consider early-time snapshots due to the high cost of relativistic multidimensional NSM simulations. In order to quantify where the model predictions suffer most, we show the spatial distribution of errors in the M1NuLib-2016 snapshot induced by the ML and Box3D models in Fig.11. We also include the errors incurred if no flavor conversion is considered (bottom row), which provides a measure of the actual amount of flavor transformation in the unstable points.

The blue color corresponds to the stable points while purple marks the unstable points with the “true” final value given byEmusimulations. The gray dots correspond to unstable configurations where theEmusimulations did not provide sufficient quality result, and largely populate regions of marginal instability. At 5 ms, the compact object at the center and the surrounding region out to aboutdo not show the presence of FFI (with the exception of a small number of marginally unstable locations). The dense parts of the tidal arms in the equatorial regions at aboutandare also sources of neutrino emission and do not show instability, but in the space between the tidal arms there is a nontrivial interplay between radiation coming from the central compact object and from the nearby tidal arms. It is in these regions that the ML model has the most difficulty. As expected, the ML_ext model performs better than the fiducial model on unstable points (consistently with Fig.8), and likewise in stable regions, with the exception of larger flux factor errors at the edge of instability (second row, right panel), once again in agreement with the CDFs in Fig.10. As discussed before, the Box3D mixing scheme offers the best overall performance.

Recall that we apply the ML model to stable regions to thoroughly test its performance even though there should be no changed predicted in these distributions. The ML model predicts small changes in the central stable area, but incorrectly predicts large changes in the flux factors in the stable regions of the tidal arms. This area is the origin of the large errors seen in the right column of Fig.10. Once again, the ML model would be improved by combining with a binary stability indicator as in[23,24].

SECTION: VConclusion

While simulations of core-collapse supernovae and neutron star mergers (NSMs) are expected to exhibit neutrino fast flavor instability, it is not yet understood how the instability modifies the dynamics and ejecta composition, since the requisite scales are impossible to resolve in global simulations. While pioneering calculations have bounded the magnitude of the FFI’s impacts using a variety of subgrid models with qualitatively correct behavior, there are currently no subgrid models available that provide quantitatively correct predictions of how the FFI changes the densities and fluxes of each species of neutrino in a general multidimensional environment. In this work, we assess the accuracy of proposed subgrid models, extend a promising subgrid model to multiple dimensions, and create a machine learning (ML) model anchored in detailed small-scale simulations of the FFI.

To do so, we aggregated over 26,000 simulations of the FFI based on both randomized initial conditions and conditions extracted from global NSM simulations. In addition, we incorporated more than 286,000 neutrino distributions that arestableto the FFI into the training and evaluation of our models.

The main objective of all of these models is to predict the first two angular moments (density and flux) of each neutrino species after saturation of the FFI using only the same angular moments as inputs. We constructed a ML model to do this based on a fully-connected neural network, but with additional structures to ensure conservation of particle number, to make the result rotationally invariant, and to make interpretation in the context of a fully general-relativistic NSM or supernova simulation straightforward. The code for constructing and training the ML model is open-source[67], as are the training data and the final trained models[72].

We also extend the box-like model of[40]to treat the FFI in non-axisymmetric neutrino distributions. Specifically, based on the pre-instability angular moments we construct a multidimensional distribution for each species, discretize each distribution into 10,000 directions, and apply a prescription to mix flavor at each direction based on the location of the ELN-XLN crossing.

The main result is that this “Box3D” model is particularly compelling as a subgrid model. It compares very favorably to the results of individual quantum kinetics simulations, both in terms of the final states of the angular moments of each species (red dashed line in Fig.6), and indeed the full angular distribution (bottom panel in Fig.7). Of the models studied, it most robustly results in small errors for predictions of the final states of unstable distributions (Fig.8), correctly predicts exactly zero change for stable distributions, and exactly conserves net lepton number.444Note that during the refereeing process of this paper,[90]presented an alternative generalization of the Power-1/2 scheme that appears more accurate than the Box3D scheme in their non-axisymmetric simulations.

However, there are potential limitations to the Box3D model. First, it requires reconstructing and discretizing each distribution, and the resolution of the discretization can impact the results, especially if the distribution is highly forward-peaked. That is, even if the neutrino distribution is stable, the act of discretizing and re-integrating results in a change to the moments. This can be addressed in the case of stable distributions by only modifying neutrino distributions if they are unstable, but there are still resolution-dependent errors in unstable distributions. In addition, the model is specific to the FFI, and it is not clear if the model can be extended to more general situations outside of the approximations made in this work.

In this sense, the ML model is attractive, as it could a priori be trained on a wider range of flavor conversion mechanisms. It is, in addition, especially efficient on GPU-based systems. It exactly preserves net neutrino number, is rotationally invariant, and does not depend on a choice of discretization. Although all of the data we use to train the model conserves net lepton number, the model itself does not conserve lepton number, nor is additional loss imposed during training to drive conservation. Although the result is that the ML model incorrectly allows for violation of lepton number conservation in the FFI limit (Fig.9), it does approximately learn to conserve lepton number. In addition, various schemes can be employed to enforce lepton number conservation following the ML model’s prediction (e.g., AppendixA.1). The ML model does, however, rarely make unrealizable predictions in the form of negative densities or flux factors larger than 1. This, too, can be accounted for after the fact with minimal modifications to the predictions (AppendixA.2). The ML model is particularly worrying in that it predicts changes to both number densities and fluxes when applied to FFI-stable distributions (Figure10). In real applications, it would benefit from a stability classifier (e.g.,[91,92,15,23,24,16,44,45]) to enforce no change in stable distributions.

For both the ML and Box3D models, errors seem to be largest for distributions that are marginally unstable (Fig.11), while still being at the percent level for the Box3D scheme. This suggests the need for more dedicated studies of the outcome of the FFI for configurations at the edge of instability, as recently pointed out by[22]. We leave this question for future work.

It is important to caution that good performance on training and test datasets drawn from the same statistical distribution (which is the case for the datasets taken from the NSM simulation in[73]) does not imply generalizability. This can be most clearly seen by comparing the results of our fiducial ML model to our ML_ext model on the 5 ms M1NuLib-2016 NSM snapshot (third row of Figs.8and10). The fiducial ML model, which never sees these datasets, performs well below what one would predict based on its performance on test data during training. Not surprisingly, the ML_ext model, which is also trained on data from this snapshot, shows much better performance. However, if applied to another completely independent NSM simulation, the ML and ML_ext models would both deal with situations outside of the parameter space spanned by their training data, and could offer poorer performances.
It should also be pointed out that FFIs can occur in other dense environments like core-collapse supernovae (see[93]and references therein). Notably, a number of the previous studies mentioned in this work were carried in spherically-symmetric CCSN environments (e.g.[38,26,63,43]). It was nevertheless shown that non-axisymmetric ELN crossings can appear in CCSNe (e.g.,[94]), making the case for three-dimensional models like the Box3D prescription. While we expect this model should provide reasonable results in the context of neutrino distributions in supernovae, it is unclear how well the ML models will generalize from the NSM training sets to CCSN settings.

There are a number of further developments that could enhance the viability of subgrid models. First and foremost, the performance of the models needs to be compared with more global simulations of flavor instability to map out where the assumptions of instantaneous flavor change based on results from periodic box simulations is relevant, potentially requiring extensions to subgrid models to account for gradients. Even the models present here should be extended to account for other flavor transformation phenomena, although this will require a large number of additional calculations to use for training or assessment purposes. Although the Box3D model has a straightforward interpretation in the context of multi-energy distributions (the survival probability is the same for every neutrino energy along a given direction), the ML model would need to be verified or extended in this context. The simultaneous treatment of both lepton number and energy redistribution between neutrino flavors is not trivial, and care must be taken to treat both consistently. These are of course left for future work.

SECTION: Appendix AEnforcing Additional Constraints in the ML Model

In this appendix, we introduce various “fixes” that can be used to explicitly enforce some constraints or physical realizability conditions in the ML model.

SECTION: A.1Conservation of Electron Lepton Number

We could further modify thevalues in order to explicitly conserve net lepton number for each neutrino flavor. The initial and predicted net lepton numberfor each flavoris

The excess of lepton number in the ML model prediction is then. If we subtract half of this excess from the neutrino distribution and add half to the antineutrino distribution, the model perfectly preserves lepton number. Specifically, we use a new transformation matrix given by

where

However, this causes all spacetime components ofto show this property, when in reality only the time component should be conserved. We refer to the model trained with this scheme as model ML_yELN.

In addition, one can simply adjust the number densities directly to enforce ELN conservation. We refer to the model trained with this scheme as ML_directELN. This is logically equivalent to imposing the above modification to thematrix only for the time component of the four-flux, as defined in the frame comoving with the background fluid, thus breaking the Lorentz invariance of our fiducial model. Although it may in fact be more correct to allow for a special reference frame, the use of this model is a bit more subtle in relativistic codes. In addition, care must be taken to transform energy density and number density consistently, since net lepton number is conserved, but the same is not true for neutrino energy. We explore this option as a curiosity here without proposing a generally consistent way to use it as a subgrid model.

As a proof of principle, we indicate in Table4the error metrics of models trained with the same datasets as the fiducial ML model, but either using thematrix of Eq. (27) (“ML_yELN” model), or with the direct modification of number densities (“ML_directELN”). We only give the values for the M1NuLib-2016 snapshot, as they are representative of the overall trend. We introduced the notation, which corresponds to the quantity whose histogram is plotted on Fig.9.

First, as shown in the last column of Table4, the “adjusted” models work as expected in that they assure ELN conservation. They offer, on average, better predictions for the output number densities, which is reasonable as ELN conservationissatisfied by the true. Regarding fluxes, the ML_directELN model performs similarly to the fiducial model, once again predictably since the fluxes are not affected by enforcing ELN conservation only in the number density moment. The change in thematrix leads, as explained above, to better predictions for the number densities, but worse ones for the fluxes, as they are incorrectly forced to parallel the change in number densities. This is particularly true in the “Random” dataset where axisymmetry is not satisfied and where fluxes change much more than in NSM environments (see Fig.8). Applying the ML_yELN model to the Random dataset (not shown) results inanderrors one order of magnitude larger than the other models. It would therefore seem that directly changing the number densities to satisfy ELN conservation is the best strategy. However, this is not a viable path forward in general. First, we mentioned above the difficulties associated to relativistic invariance and the treatment ofenergydensities on top of number densities. Then, ELN conservation is a special feature of fast flavor instabilities, such that a more general ML model must be able to not conserve ELN in some cases.

SECTION: A.2Enforcing Realizability

In the event that the ML model makes an unphysical prediction (i.e., flux factor larger than 1 or negative number densities), it is possible to minimally modify the solution to bring it back to a realizable state. This is done by setting the final four fluxes to a weighted average between the complete mixing solution and the result of the ML model and choose the weighting such thatfor all species (i.e., such that all four-fluxes are timelike). A flux factor larger than 1 would manifest as a spacelike four-flux, though this algorithm also works to address negative number densities as well. Specifically,

where

with,is the flavor-averaged four-flux,,, and, with primes and species indices suppressed. The piecewise nature of Eq. (30), with the arbitrary threshold of, avoids floating-point precision errors near values of zero by neglecting the quadratic term when solving for. In addition, the fix is only applied if, in which caseis added tobefore use in Eq. (29), which ensures the realizability of the final state in spite of potential round-off errors. We maximize over species such that we can use the same interpolation factor for all species to preserve net neutrino number conservation. The use ofselects the branch that leads to the most positive value of.

SECTION: References