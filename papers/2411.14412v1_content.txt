SECTION: Adversarial Poisoning Attack on Quantum Machine Learning Models

With the growing interest in Quantum Machine Learning (QML) and the increasing availability of quantum computers through cloud providers, addressing the potential security risks associated with QML has become an urgent priority. One key concern in the QML domain is the threat of data poisoning attacks in the current quantum cloud setting. Adversarial access to training data could severely compromise the integrity and availability of QML models. Classical data poisoning techniques require significant knowledge and training to generate poisoned data, and lack noise resilience, making them ineffective for QML models in the Noisy Intermediate Scale Quantum (NISQ) era. In this work, we first propose a simple yet effective technique to measure intra-class encoder state similarity (ESS) by analyzing the outputs of encoding circuits. Leveraging this approach, we introduce a quantum indiscriminate data poisoning attack, QUID. Through extensive experiments conducted in both noiseless and noisy environments (e.g., IBM_Brisbane’s noise), across various architectures and datasets, QUID achieves up toaccuracy degradation in model performance compared to baseline models and up toaccuracy degradation compared to random label-flipping. We also tested QUID against state-of-the-art classical defenses, with accuracy degradation still exceeding, demonstrating its effectiveness. This work represents the first attempt to reevaluate data poisoning attacks in the context of QML.

SECTION: IIntroduction

Quantum computing is rapidly progressing, with companies like Atom Computing and IBM recently unveiling the largest quantum processors ever developed, boasting 1,225 and 1,121 qubits, respectively[1,2]. The significant interest in quantum computing stems from its potential to offer substantial computational speedups over classical computers for certain problems. Researchers have already begun leveraging current noisy intermediate-scale quantum (NISQ) machines to demonstrate practical utility in this pre-fault-tolerant era[3]. Within this emerging field, quantum machine learning (QML) has also gained considerable attention, merging the power of quantum computing with classical machine learning algorithms. QML explores the potential to improve learning algorithms by leveraging unique quantum properties like superposition and entanglement, opening new horizons in computational speed and capability. Several QML models have been explored, with quantum neural networks (QNNs)[4,5,6]standing out as the most popular model, mirroring the structure and function of classical neural networks within a quantum framework.

However, the rapidly evolving field of quantum computing brings not only unique computational capabilities but also a variety of new security challenges. Several recent works have explored the security aspects of quantum circuits in current NISQ-era.
Authors in[7]introduced a power side-channel attack on quantum computer controllers, demonstrating how power trace analysis could be used to reconstruct quantum circuits. Researchers have also begun analyzing the effectiveness of adversarial attacks on QML models, such as model stealing attacks[8], where an adversary can replicate the functionality of a black-box QNN using only input-output queries. Novel defenses have been proposed to mitigate such attack scenarios[9]. Additionally, recent works have highlighted the threat of backdoor attacks[10,11]on QNNs, revealing them to be a significant concern for QML models. Another critical threat to QML models in current cloud settings, considered the greatest concern for classical ML deployment[12,13], is data poisoning attacks, which have not yet been explored in the context of QML.

Data poisoning attacks, first proposed in[14], are a class of adversarial attacks aimed at manipulating a victim model by either injecting poisoned samples into the dataset or perturbing a subset of the training data. A detailed description of data poisoning attacks and their sub-classes can be found in SectionII-B. These attacks have been extensively studied in the classical domain, where most approaches rely on a deep understanding of the victim’s training procedure (unrealistic) and gradient optimization techniques to generate minimally perturbed poisoned samples (costly). Furthermore, such methods often lack robustness to the noise present in modern NISQ devices, highlighting the need for a fresh perspective on data poisoning attacks in QML. Building on how data poisoning initially focused predominantly on label-flipping attacks, we take a first step in exploring indiscriminate data poisoning in QML by proposing a novel label-flipping technique based on intra-class state similarity in quantum Hilbert space.

In this work, we propose a quantum indiscriminate data poisoning attack, QUID, using a novel label-flipping strategy. QUID assigns poisoned labels to a subset of the training data such that the encoded state is furthest from its true class. Unlike other classical label-flipping[15,16]that require separately training the victim model to find adversarial labels, QUID avoids this costly training process while being noise-robust, as it can effectively determine poisoned labels by executing the encoding circuit on noisy hardware. Our main contributions in this work are:

Introduced intra-class encoder state similarity (ESS) in the quantum Hilbert space using density matrices derived solely from the encoding circuit. ESS can be extensively used to estimate the performance of state preparation circuits for any given dataset without the need to train QML models from scratch.

Leveraged ESS to propose a novel label-flipping attack, QUID, which determines optimal poisoned labels without requiring knowledge of the victim model’s training procedure or the need for any additional training.

Evaluated the performance of QUID against random flipping and randomly poisoned samples across various QNN architectures and datasets in both noiseless and noisy environments, along with testing against a state-of-the-art defense technique, demonstrating its effectiveness.

To the best of our knowledge, this is the first work to evaluate data poisoning attacks in the context of QML.

SECTION: IIBackground

SECTION: II-AQuantum Neural Network (QNN)

A QNN comprises three key components: (i) a classical-to-quantum data encoding circuit, (ii) a parameterized quantum circuit (PQC) optimized for specific tasks, and (iii) measurement operations. For continuous variables, angle encoding is widely used, mapping classical features to qubit rotations along a chosen axis[5]. Since qubit states repeat every, features are typically scaled toto(orto) during preprocessing. In this study, we usegates for encoding classical features into quantum states.

A PQC consists of parameterized quantum gates designed to solve specific problems and serves as the only trainable block in a QNN for pattern recognition. It includes entangling operations to generate correlated multi-qubit states and parameterized single-qubit rotations for exploring the solution space. Measurement collapses qubit states to ‘0’ or ‘1’, and we use the expectation value of Pauli-Z to determine the average qubit states. These values are fed into a classical neuron layer (equal to the dataset’s class count) for final classification in our hybrid QNN, as shown in Fig.1. Other QML architectures may apply softmax directly or use additional classical layers for processing.

SECTION: II-BData Poisoning Attack

Data poisoning attacks[14,17]are a class of adversarial attacks aimed at manipulating data by either tampering the current samples/labels or injecting poisoned samples to compromise the integrity and/or availability of the model. Data poisoning attacks can be broadly classified into three main categories: i)Targetedattacks, which preserve the functionality and behavior of the system for general users while causing misclassification for specific target samples; ii)Indiscriminateattacks, which aim to degrade model performance on test data; and iii)Backdoorattacks, which, for any test sample containing a specific pattern, known as the backdoor trigger, induce misclassification without affecting the classification of clean test samples. In this work, we focus on indiscriminate data poisoning attacks on QNNs which can further be divided into;a) Label-flippoisoning, where adversary does not perturb the features but only flips label of a subset of the dataset,b) Bilevelpoisoning, where attacker can obfuscate both the training samples and their labels andc) Clean-labelpoisoning where attackers only slightly perturb each sample to preserve it’s class i.e. performing a clean-label attack. Here, we propose a quantum state proximity based label-poisoning attack.

SECTION: II-CRelated Works and Limitations

Although there has been work on backdoor attacks in QNNs[11,10], some of which modify QNN architecture to perform indiscriminate attacks, no research has yet explored the viability of data poisoning attacks on QNNs. Current state-of-the-art (SOTA) classical indiscriminate data poisoning attacks[19,17]are gradient-based and assume that the adversary has access to the training data (), the target model (white-box access), the training procedure (loss function, optimization process, hyperparameters, etc.), and sometimes even the test data[18]. QUID, on the other hand, only requires access toand gray-box access to the QNN circuit, i.e., only the encoding circuit of the QNN. This is a more realistic and efficient framework considering the high cost associated with trainings QNN on NISQ devices.

Apart from relying on extensive knowledge and expensive optimization procedures to poison the dataset, most classical SOTA attacks ensure that the poisoned samples, i.e.,, are minimally perturbed to avoid detection. For example, the Gradient Canceling (GC) attack[19]aims to manipulate the model’s training process by injecting slightly perturbed poisoned data into, with the goal of disrupting learning by “canceling out” the gradients that guide parameter updates during training. However, due to the noisy nature of current quantum hardware, these small perturbations, effective in classical deep learning, are masked by the inherent quantum device noise, rendering them ineffective (TableI). Studies have even shown that inherent noise makes QNNs adversarially robust[20,21]. Consequently, there is a need to reevaluate data poisoning attacks in the context of QML.

SECTION: IIIAdversarial Framework

SECTION: III-AAttack Model

We consider a scenario where the victim sends both the pre-processed and normalized data,, and the QNN () to the quantum cloud provider for training. Although the current pipeline involves sequentially encoding data into the QNN one sample at a time and sending it to the cloud for execution, in the near future, a more efficient pipeline will likely be adopted to avoid network latency delays.
However, we assume the presence of an adversary, unknown to the victim, within the quantum cloud. This adversary could be a malicious insider managing the quantum cloud, or an external attacker breaching its defenses.
Our primary concern is the integrity and the availability risk posed by these adversaries. Specifically, the adversary can modify, to degrade the QNN’s performance, making it ineffective and unusable for the victim.

SECTION: III-BAdversary Objective

The primary goal of the adversary is to poison a portion () of the training dataset () in a way that maximally degrades the QNN’s performance on the entire test set (). The adversary may be motivated by various factors, such as gaining a competitive advantage in the market or deceiving the victim into altering the QNN training specifications (e.g., PQC architecture, or hyperparameters). Such actions could require either retraining the QNN or training for more epochs, ultimately resulting in greater profit for the cloud-based adversary. Here, we consider that the adversary only poisons a portion () ofand does not inject additional poisoned samples to it. Thus, total size of the training dataset i.e.is fixed.

SECTION: III-CAdversary Knowledge

We assume that the cloud-based adversary has access to the training data,, and gray-box access to the QNN circuit () executed on the quantum hardware. This means the adversary has knowledge of the data encoding circuit of the QNN but does not necessarily have access to the rest of the circuit or the training procedure. This is a reasonable assumption since the victim needs to define how the training data is encoded into the QNN, thereby giving the adversary direct access to the state preparation (encoding) circuit.

SECTION: IVProposed Methodology

Fig.2provides a high-level overview of our poisoning technique. Before delving into the details, we first define the notion of Intra-Class Encoder State Similarity (ESS), which is leveraged to poison the training data. Subsequently, we outline the label-poisoning strategy employed by QUID.

SECTION: IV-AIntra-Class Encoder State Similarity (ESS)

In quantum machine learning models, classical data is mapped into the quantum Hilbert space using an encoding circuit (). A variety of encoding techniques exist, but the main purpose of all of them is to embed the classical data in a way that leads to minimal loss of information, i.e., the separability of the encoded states, represented by the density matrix, belonging to the same class (intra-class) is low, while that between different classes (inter-class) is high. Theoretically, we can say that given any input space, corresponding labels, an encoding circuitthat maps classical data to density matrices in the quantum Hilbert space, and a distance metric, the quantum encoding circuitpreserves the class structure in the sense that:

wherebelong to the same class, andbelongs to a different class. Thus, for any input data, we can easily estimate the corresponding labelby calculating the matrix distance between encoded state and all the other encoded states as,, average them based on the class and assign it the label corresponding to the class with.

We take an empirical approach to validate intra-class state similarity by assigning the encoded state’s density matrix the label of its nearest class in the quantum Hilbert spaceand verifying how accurately it matches the true label. We conducted experiments using the angle encoding technique (withandgates) across different datasets and employing various distance metrics. Specifically, given two density matrices,and, we use the following Frobenius norm, trace norm and a normalized distance metric derived from the Hilbert-Schmidt inner productto calculate the distance between the two states in.
We avoided fidelity-based distance metrics like the Hellinger distance due to it’s high computation time (almostcompared to the Frobenius norm) while offering no performance improvement.

Fig.3illustrates how accurately we can identify classes in a portion (50%) of a reduced MNIST-10 dataset using intra-class state similarity. Under both noiseless and noisy scenarios, the Frobenius norm outperforms the HS and Trace norms in determining the correct classes. TableIIfurther presents the performance of different 4-class datasets under noisy conditions. Here too, we observe that, on average, using the Frobenius norm to calculate the distance between density matrices results in better performance while requiring considerably less computation time, making it the optimal choice of distance metric. Furthermore, we believe ESS can be used to estimate the performance of encoding methods and to estimate the separability of datasets when mapped towithout the need for training the entire QNN from scratch.

SECTION: IV-BQUID Poisoning Strategy

The goal of an adversary in data poisoning attacks is to create a poisoning datasetwhich leads to maximum performance degradation of QNN () onor,

whererepresents the set of parameters of.
We use the intra-class quantum state similarity concept to find adversarial labels which leads to high performance degradation. Given that for any dataset, intra-class distances and inter-class distances inshould be low and high respectively, we modify the labels of a subset () of the training datawhich leads to high intra-class distances. Specifically, given the training dataset, we first split it into cleanand poisoned datasetbased on the poison ratio. For samples, find density matrix of the encoded states, calculate distance between the encoded statesand assign the class label withdistance. An algorithmic description of QUID’s label poisoning technique is described in Algorithm1. Essentially, we propose a greedy algorithm to determine optimal labels, in contrast to[16]which relies on a brute force training method involving iterating through all possible labels for each sample and selecting the label that results in the maximum loss.

SECTION: VEvaluation

SECTION: V-ASetup

Device:We conducted all noiseless experiments on Pennylane’s[22]lightning.qubitdevice using the “Adjoint” differentiation method to calculate gradients. For noisy training, we employed thedefault.mixeddevice and introducedAmplitude DampingandDepolarizing Channelnoise after each gate in the circuit, with a high error probability of(unless stated otherwise) to effectively simulate the noisy nature of current hardware. Considering that generic methods for calculating gradients, such as backpropagation, are not viable on actual quantum devices, we used the Simultaneous Perturbation Stochastic Approximation (SPSA) method for gradient calculation in our QNN training. This approach generates noisy gradients, providing a more realistic representation of the performance expected from real quantum hardware and ensuring that our experimental results align more closely with practical quantum computing environments.

Training:For evaluation, we used PQC-1, 6 and 8 to build our QNNs (PQC-represents circuit-in[23]), initialized with random weights. In most of the experiments we used a 4-qubit or 8-qubit QNN and angle encoding to encode classical features, consistent with recent QML works[24,25]. For training the QNNs we used CrossEntropyLoss() as our classification loss function. The hyperparameters used for training are; Epochs: 30, learning_rate (): 0.01, batch_size = 32 and optimizer: Adam. All training are done on an Intel Core-i9 13900K CPU with 64GB of RAM.

Dataset:Since NISQ devices struggle with large images due to limited qubits, high error rates and complex data encoding, similar to recent works[26,25], we conduct all experiments using a reduced feature set of MNIST, Fashion, Kuzushiji and Letters datasets with latent dimension(from original 2828 image) generated using a convolutional autoencoder[27]. Thus, for each dataset, we create a smaller 4-class dataset from these reduced feature sets i.e., MNIST-4 (class 0, 1, 2, 3), Fashion-4 (class 6, 7, 8, 9), Kuzushiji-4 (class 3, 5, 6, 9) and Letters-4 (class 1, 2, 3, 4) with each having 1000 samples (700 for training and 300 for testing). Since each of these datasets is of dimension, we encode 2 features per qubit. Number of shots/trials is set to 1000 for all experiments.

SECTION: V-BAnalysis

Angle vs. Amplitude Encoding:To evaluate how effectively the proposed ESS (SectionIV-A) can be used to measure the performance of state preparation circuits on any given dataset, we compare the intra-class state similarity for angle and amplitude encoding under varying noise levels (: error probability) on a reduced MNIST-10 dataset with a latent dimension of 16. Considering that amplitude encoding can encodefeatures inqubits, for fairness, we used a 4-qubit circuit for angle encoding, with each qubit having four rotation gates (,,,), to provide a better comparison against amplitude encoding. Fig.4shows the performance comparison of both encoding techniques. The results are interesting, as amplitude encoding performs worse under noiseless conditions. However, it is evident that amplitude encoding outperforms angle encoding when encoding larger datasets, likely due to the loss of information caused by applying a high number of sequential rotation gates in angle encoding which is further amplified by noise. Conversely, angle encoding performs well for smaller datasets, as demonstrated in Fig.3, where two features are encoded per qubit.

Label-flip vs. Bi-level:Even though QUID is primarily a label-flipping (LF) technique, we compare its performance against random bi-level poisoning. In this method, apart from flipping labels, we also replace samples of the poisoned data () i.e.(), with random features and calculate the optimal poisoned labels for these random features. TableIIIshows the performance comparison of label-flipping versus bi-level poisoning when training a 4-qubit QNN on various datasets under noise with error rate. Our experiments indicate that, in most cases, adding randomly generated poisoned samples (PS) actually leads to better performance (i.e. less accuracy & loss degradation) compared to the scenario of using only LF with QUID. For the remaining analysis, we use only the label-flipping technique.

Noiseless vs. Noisy Simulations:We compare the performance of QUID when training a QNN under both noiseless and noisy conditions. Table.IVpresents the performance comparison of QUID across various poison ratios, denoted as. From the figure, it is evident that for the same poison ratio, QUID demonstrates significantly better performance under noisy conditions compared to noiseless conditions. For example, at a poison ratio of, QUID reduces performance byunder noisy conditions compared to 53% under noiseless conditions on Fashion-4 dataset. This result highlights the robustness of QUID against noise.

Real Hardware Noise:Given the limited availability of real quantum devices, we directly integrated noise models from actual quantum hardware. Specifically, we employed the noise models of IBM_Kyiv and IBM_Brisbane, and incorporated them into theqiskit.aerdevice to accurately simulate the noise characteristics of these devices. TableVlists the performance of QUID and random label-flipping on real hardware noises. We observe that even in this scenario, QUID is able to significantly degrade the performance of the baseline model byon IBM_Brisbane’s noise andon IBM_Kyiv’s noise.

Poison Ratio ():We also evaluate the performance of QUID using different poison ratios, which helps determine the threshold beyond which the model’s performance decreases drastically. All training in this evaluation is conducted under noise () using the SPSA gradient calculation method. TableVIlists the test accuracies of various QNNs trained across different datasets and poison ratios (). As expected, we observe from the table that baseline models of more complex PQCs like PQC-6 perform worse compared to PQC-8 under hardware noise, while the circuit with the least gate count and depth significantly outperforms both. Consequently, we selected PQC-1 for noisy training on the other datasets. Another important observation is that with a small poison ratio (), there appears to be minimal performance degradation. However, for, the performance suffers diminishing returns, as the trained model on QUID achieves a test accuracy less than, worse than random prediction (for 4-class dataset), rendering the model completely unusable for the victim. In all cases, the proposed QUID technique performs significantly better than the random label-flipping technique.

Defense Evaluation:Although label-flipping attacks are relatively easy to defend against in the classical domain using data sanitization techniques[15,28], implementing such defenses in the current quantum cloud environment poses significant challenges, as the victim cannot intervene once the quantum circuit and training data () are submitted for training. Therefore, we evaluate the effectiveness of QUID against a practical training-based defense technique, SS-DPA[29], which is specifically designed to mitigate label-flipping attacks. SS-DPA leverages subset aggregation and ensemble learning together with semi-supervised learning. TableVIIillustrates the performance of QUID with and without SS-DPA, trained in a noiseless setting. The hyperparameterin SS-DPA is set to 3 due to the smaller dataset sizes. While the application of SS-DPA improves performance by a small amount in most cases, it is not very effective for large. Moreover, it is extremely expensive in terms of computation overhead, as it requires the training and retraining of multiple QNNs. Future research should focus on the development of effective and efficient defense techniques tailored specifically to QMLs.

SECTION: VIDiscussions

Scalability:To demonstrate the effectiveness of QUID for larger QNNs trained on larger datasets, we designed an 8-qubit QNN with two layers of PQC-1[23]. We trained this model on the MNIST-10 dataset, reduced to a latent dimension of, using 2000 samples with a 70:30 train-test split. Fig.5shows the test loss and test accuracy of the victim model when trained on a poisoned dataset with a poison ratio, using both random label flipping and QUID’s label poisoning technique. The results clearly illustrate that QUID outperforms random label flipping techniques, even for in this scenario. Specifically, QUID results in additionalaccuracy degradation and incurshigher test loss compared to random flipping, further validating its effectiveness.

Applications:QUID is not only effective against indiscriminate poisoning attacks but can also be adapted for targeted attacks, where the adversary selectively flips labels of a specific class. This tactic allows the QNN to perform well on most test data while deliberately misclassifying the targeted class, serving the adversary’s objective. Additionally, QUID can also be used for secure dataset releases, as demonstrated in[30]. By injecting poisoned samples with labels determined by QUID, legitimate users can prevent unauthorized usage. Attackers unknowingly training on such datasets would produce suboptimal models, rendering their efforts ineffective.

Limitations:The primary limitation of QUID lies in its reliance on the density matrix to assign optimal labels. While density matrices can be reconstructed on real hardware using techniques such as quantum state tomography, the measurement cost scales exponentially with the number of qubits. Recent studies[31]have proposed more efficient simulation techniques, but these approaches remain substantially more expensive compared to generic expectation-value-based measurement techniques. However, the latter approach comes at the cost of major information loss, which limits the effectiveness of QUID.

SECTION: VIIConclusion

In recent years, there has been significant research evaluating the security of quantum computers in cloud settings. In this work, for the first time, we focus on one such threat to QML models, specifically the threat of data poisoning attacks. An adversary in the cloud can significantly manipulate the performance of QNN models by tampering with the dataset. To this end, we propose a novel label poisoning attack, QUID, based on intra-class encoder state similarity (ESS), which can degrade the performance of QML models. Our extensive experiments under noise reveal that QUID significantly outperforms random poisoning and can degrade QNNs accuracy by.

SECTION: Acknowledgment

The work is supported in parts by NSF (CNS-2129675, CCF-2210963, OIA-2040667, DGE-1821766 and DGE-2113839), gifts from Intel and IBM Quantum Credits.

SECTION: References