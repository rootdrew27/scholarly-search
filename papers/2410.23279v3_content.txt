SECTION: A Transformer Model for Segmentation, Classification, and Caller Identification of Marmoset Vocalization

SECTION: 1Abstract

Marmoset, a highly vocalized primate, has become a popular animal model for studying social-communicative behavior and its underlying mechanism comparing with human infant linguistic developments. In the study of vocal communication, it is vital to know the caller identities, call contents, and vocal exchanges. Previous work of a CNN has achieved a joint model for call segmentation, classification, and caller identification for marmoset vocalizations. However, the CNN has limitations in modeling long-range acoustic patterns; the Transformer architecture that has been shown to outperform CNNs, utilizes the self-attention mechanism that efficiently segregates information parallelly over long distances and captures the global structure of marmoset vocalization. We propose using the Transformer to jointly segment and classify the marmoset calls and identify the callers for each vocalization.

SECTION: 2Introduction

The common marmoset (Callithrix Jacchus) is an animal model suitable for studying social vocal communication. First, the marmosets are non-human primates genetically and neurophysiologically close to human[5]. Second, in contrast to such primates as gorillas that primarily use gestures for communication, the marmosets are highly vocal and readily to respond to other marmosets, even non-related or non-pair-bonded ones, particularly when visually hindered such as in forests when vocal contact is crucial for survival[3]. Third, marmosets exhibit human-like conversational turn-taking, exchanging calls resembling coupled oscillators[12,16]. Fourth, marmosets display prosocial behaviors: similar to human cooperative breeding, marmosets take care of offspring of nonparents[1,2], reflecting group social communications.

Researchers have been using the marmoset as an animal model to study diseases and mechanisms related to vocal communication comparing with human infant linguistic developments. Uesaka et al.[14]developed a marmoset model of autism disorder — marked by the deficits in social communication, impaired verbal interaction, and verbal perseveration — by feeding pregnant mothers with valproic acid. Using the autism model, they aimed to study the developmental vocal characteristics of autism for early diagnosis and investigate potential medicines to relieve the autism symptoms. The ability of vocal communication is shaped by innate and empirical factors. Researchers manipulate autism-related genes of marmosets[6]to study innate gene-function relationships. Researchers manipulate environments including visual or auditory sensory inputs[8]and parental interaction[11]to study the empirical modification of communicative behaviors.

Studying turn-taking vocal communication between marmosets requires extracting caller and callee information, call contents, and vocal exchanges from the recorded audios. Turesson et al. applied SVM and DNN for marmoset call classification on a small dataset of 321 marmoset calls[13]. Wisler et al. used SVM and decision tree on a larger dataset of 4 call types, each with 400 marmoset calls for classfication[17]. Uesaka et al. employed CNN to classify 3 call types (phee, twitter, and trill) to study the development and autism of the marmosets[14]. However, these studies were limited by either small datasets or focused on only a few call types.

Zhang et al. applied RNN and DNN for segmentation and classification of infant marmoset calls on a dataset that contains 10 call types, each with several thousand calls[18]. Their call types include phee, twitter, trill, trillphee, tsik, ek, pheecry, peep, and two infant-specific categories: ct-trill (twitter-connected-trill) and ct-phee (twitter-connected-phee). Sarkar et al. used the same dataset and implemented self-supervised learning on caller discrimination and classification[10]. These studies have two key limitations. First, both studies[18,10]treat segmentation as a separate task and assumed known segmentation information when classifying calls or callers.
Second, their dataset recorded individual marmosets in isolation, without communicative interaction with other marmosets, thus failing to capture the vocal characteristics of marmosets during social communication.

Oikarinen et al. used a dataset of recordings from paired of pairs marmosets housed together in single cages, enabling close-range vocal interaction[7,9]. The dataset comprises 36 sessions totally 38 hours, with 8 call types: trill, phee, trillphee, twitter, chirp, tsik, ek, and chatter, along with a noise type that indicates the silences between the calls. Applying a CNN model on the dataset, they achieved segmentation, classification, and caller identification[9].

However, while previous works as[18]and[9]used an RNN or a CNN that maps acoustic segments to call labels, the Transformer structure has been proven to outperform RNN[15]and CNN[4]in both sequential language processing and high-dimensional vision tasks. Transformer utilizes self-attention mechanism that efficiently segregates information parallelly over long distances and captures the global structure of marmoset vocalization more efficiently than RNN and more effectively than CNN. While Transformer is typically constrained by the quadratic complexity of input token length[15], making it challenging to process high-dimensional input spectra that discriminate the marmoset calls such as a phee and a trill. We can address the limitation using Vision Transformer[4]that patchizes the high-dimensional input. We propose to use the Transformer model for segmentation, classification, and caller identification of marmoset vocalizations.

We conducted our experiments on the public dataset[7]with recordings designed to study close-range vocal communication between marmoset pairs who exchange calls close together in a cage.

SECTION: 3Model

We use a two-stream Transformer model (Figure1) on the dual-audio recordings with two simultaneously recorded channels that come from two interacting animals. The model employs two stream transformer encoders that process sliding spectral segments from each channel to classify into a target label. The target labels indicate call types and caller identities (e.g., the ’tr2’ label denotes a trill call from the second of two animals in interactions). The labels also indicate segmentation information: ’noise’ denotes the non-call segment between calls.
This approach allows our two-stream Transformer model to perform three crucial tasks:

Segmentation: Identifying the start and end times of each vocalization.

Classification: Determining the type of call (e.g., trill, chirp).

Caller identification: Attributing each call to the correct animal.

By integrating these functions into a single model, we capture the complex dynamics of animal interactions through their vocalizations.

For our two-stream Transformer model, we utilize two Vision Transformer[15]modules; each (Figure1) processes the high-resolution linear spectral image by dividing it into patches that form a sequence of patch tokens. These tokens undergo a linear transformation and are augmented with positional encoding and a learnable class token. The resulting sequence passes through a typical Transformer architecture that comprises alternated self-attention and feedforward modules. The two-stream Transformer’s vision transformer modules output two encoded class tokens, which are then linear projected and concatenated, passing through a shared linear layer for final class label prediction.

SECTION: 4Evaluation

We use F-score and accuracy to evaluate classification, segmentation, and caller identification with the same evaluation methods used in[9].

For caller evaluation, we convert the predicted labels into two separate segment files for a marmoset pair (e.g., ’tr2’ labels are converted to ’tr’ and added to the segment file of the second marmoset).We evaluate the two predicted segment files against corresponding annotated ones for the marmoset pair housed together.

For segmentation evaluation, we process the annotated and predicted segment files. First, we add ’noise’ labels to fill in the intervals without any calls in the annotated segment files. Second, we reconstruct the interval for each call by merging predicted successive identical labels in the predicted segment files (e.g., the predicted ’noise, tr, tr, tr, noise’ sequence indicates a three-time-unit trill call surrounded by noisy silences where a time-unit (50 millisecond) is the window shift of sliding spectral segment inputs for prediction).

To evaluate classification on segment files, containing caller and segmentation information, we discretize the continuous segment intervals of calls and noises into 50-millisecond discrete units for predicted and annotated segment files to get two discrete label sequences of the same size.

We evaluate the hypothesized and reference label sequences (comprising 8 call types and the noise type that indicates silence between calls) by counting correctly and incorrectly classified labels. We calculate the accuracy by

whereis the summation of counts of correct noise labels and call labels,is the summation number of call and noise labels (the
sequence length);andthe counts of correct noise and call labels;andthe total number of noise and call labels.
We calculate the precision by

whereis the number of correct call labels (the same call type for the hypothesis and the reference) andis the number of error noise labels (when predicted as any call but annotated as a noise).
We calculate the recall by

whereis the number of call labels in annotated reference. And finally, we calculate the F-score by

SECTION: 5Dataset and Experimental setup

SECTION: 5.1Dataset

We experimented on the dataset[7]designed to study close-range communication between marmosets. We used dual audio recordings from the dataset when two marmosets are together in a cage (excluding recordings when two marmosets are in separate cages). Each marmoset pair, familiar with each other after at least 3 months cohabitation before experiments, is placed in a homecage (77.5cm x 77.5cm x 147cm) with visual and auditory access. The home cage is located in an animal room with other cages positioned more than 90 cm away. The marmosets in other cages have no relation as a parent, siblings, or previous cage mates to the recorded marmosets.

The researchers collected vocalizations using light recorders (Panictech and Polend digital voice recorders, 6.9g), mounted with duct tape on a leather jacket, positioned close to the mouth (5cm) of each marmoset of the recording pair (Upper subfigure (a) of Figure3). The dataset includes 10 marmoset pairs housed together. The first five pairs (4 male-female pairs and 1 male-male pair) were recorded for about 1 to 2 hours a pair, with 8 to 12 sessions where each session lasted for 5 or 10 minutes. The second five pairs (1 male-female pair and 4 male-male pairs) were recorded for about 2 to 3 hours a pair, with 4 to 6 sessions where each session lasted for 30 minutes.

When two marmosets interact each other in a cage while wearing recorders, two simultaneously recorded audios are preserved. The dual-audio recording setup is designed to address the challenging task of call identity annotation. The caller identities were annotated by comparing the spectrograms from the two simultaneous recordings. For example, when the second marmoset makes a call and the first marmoset keeps silent, the recorder worn by the second marmoset should receive a stronger and clearer signal compared to the recorder worn by the first marmoset because the sound wave of the call becomes weaker when it travels further. The close positioning of recorders to the marmosets’ mouths is crucial for this approach. Figure3demonstrates this process and shows the annotation of call type, call time, and caller identity of a dual audio recording clip.

The annotations of the dataset include the segment files for 10 marmoset pairs where each segment file of a marmoset contains a sequence of call labels with their beginning and end times. The 10 pairs include 8 pairs annotated by hand and 2 pairs annotated by a neural network and corrected by one observer. The manual annotations of the 8 pairs were split between two observers. Some sessions annotated by both observers have IOR (Inter-observer Reliability using Cohen’s Kappa) of 0.86 for call occurrence and IOR of 0.91 for the call type.

SECTION: 5.2Data division

The dataset[7]consists of the dual audio recordings of 10 pairs of marmosets, aged from 1.5 to 10 years old, where the 5 female-male pairs are unrelated and the 5 male-male pairs are siblings. The annotations include 11 call types: trill, twitter, chirp, phee, trillphee, other, ek, tsik, chatter, peep, and infant cry (Figure4). We divided the dataset with the data of pair1 as the test set, pair2 as the development set, and pair 3 to pair 10 as the training set with duration statistics (Table1) and frequency statistics (Figure4). All data come from annotated dual audio recordings of marmoset pairs housed together.

SECTION: 5.3Experimental setup for systems

We built our systems using the identical 8 call types as[9]. We converted the additional rare call types in the dataset[7](other, peep, and infant cry) into the noise type. After adding the caller identity information, our target labels become trill, phee, trillphee, twitter, chirp, tsik, ek, chatter (from the first animal), trill2, phee2, trillphee2, twitter2, chirp2, tsik2, ek2, chatter2 (from the second animal), and noise type (indicating intervals between calls for segmentation).

We implemented our backbone CNN model with the same architecture as[9]. It consists of two CNN streams whose outputs are concatenated and linear-projected for the final prediction. Each CNN stream comprises 4 CNN modules. Each module comprises two identical convolutional layers followed by max-pooling. Whenever max-pooling is applied, the number of channels in the convolutional layers doubles in the subsequent module.

We implemented our backbone Transformer model using Vision transformer that divides the original high-resolutionlinear spectral segments intopatches. These patches are linearly embedded and then passed into a Transformer module with a model dimension of 384. The Transformer module consists of 6 blocks, each with 6-head self-attention and linear modules. The linear modules have a hidden dimension of 1536. The outputs of the two Transformer streams are concatenated and passed through a shared linear layer with a dimension of 1024 for the final prediction.

We trained our two-stream system, with CNN or Transformer backbone, to map two2-dimensional linear spectral segments to the target label. The segments were generated from dual-channel audio using a sliding window of 500ms size with a 150ms shift[9]. The target labeling process was as follows: A call target label was assigned when the middle 150ms part of the segment overlapped with a human-annotated call interval; a noise target label was assigned when the middle 150ms part of the segment did not overlap with any call annotations. These call and noise label targets enable the model to classify and segment long-hour recordings.

To handle long inactive silent intervals in long-hour recordings when marmosets do not vocalize, we implemented two strategies. First, we randomly discarded 4/5 of noise segments to improve training efficiency and balance the dataset. Second, we applied data augmentation by randomly roll-shifting each spectral segment 1-5 pixels vertically and horizontally during model training. These approaches addressed the uneven distribution of vocalizations while enhancing the system’s ability to accurately classify marmoset calls.

To use the system to monitor marmoset behavior for long-hour recordings, we applied a streaming technique to enhance model prediction speed. This process involves two main steps: First, we split the test audios into non-overlapping 2500ms segments. Second, we created 50 sub-segments from each 2500ms segment, using a window size of 500ms and a window shift of 50ms: the first 41 sub-segments are complete within the current segment, while the last 9 concatenate parts from the next 2500ms segment. During prediction, after feeding a batch into the model, we obtain predicted 50ms intervals corresponding to 50 subsegments in the batch. This approach, inspired by[9], allows for efficient processing of long-duration audio recordings. We also implemented spectral feature extraction with Pytorch that uses GPU for better efficiency.

We implemented our Transformer system using a Vision Transformer model. Our two-stream Transformer system maintains the same overall architecture as our CNN system but replaces the CNN backbone with a Vision Transformer model. Following[9], our system uses Adam optimizer with a learning rate of 0.0003, decaying by a factor of 0.97 each epoch.

SECTION: 6Result and discussion

We applied our system directly to each raw long-hour audio recording (approximately 2 to 3 hours) without extra processing. We compare our PyTorch-implemented two-stream CNN and transformer system with the best sytem of[9](open-sourced) on the same dataset[7]of annotated dual audio recordings with the data division shown in the Figure4.

Our CNN system outperforms the best system from[9]in both F-score and accuracy. Furthermore, the Transformer system surpasses the CNN system across all F-score and accuracy-related metrics.

The Transformer’s superior performance can be attributed to its ability to better capture overall patterns within each segment. However, the Vision Transformer is constrained by the patch-based resolution due to its quadratic complexity in input token length, resulting in lower model resolution compared to the CNN model. In future work, we aim to improve our Transformer system for better resolution modeling.

Our systems can be applied to raw, long-hour audio recordings to segment and classify calls, as well as identify callers. This capability provides a valuable tool for recording marmoset interactions, which can support future studies on social behavior, development, and abnormalities in marmoset vocalizations. Such research could offer insights into communication, evolution, and dysfunction of the vocal language of marmoset. The research also helps facilitate comparisons between marmoset and human infant vocal development in family environments. Additionally, the Transformer update of our system offers potential for developing a unified multimodal model that integrates both video and audio inputs.

SECTION: 7Conclusion

We implemented a two-stream Transformer system that enables classification, segmentation, and caller identification for marmoset vocalization. This Transformer system outperforms previous CNN approaches. Our system can efficiently process long-hour or full-day marmoset recordings, providing rich information about vocal communication between marmosets in spontaneous interactions. This capability enhances the use of marmosets as an animal model for studying language evolution, development, and dysfunction in a communication context such as the comparative research between vocal development in marmosets and human infants in family environments.

SECTION: 8Acknowledgements

We would like to thank Yingjie WANG for early exploration in models. Part of this work was supported by RIKEN Pioneer Project 22.

SECTION: References