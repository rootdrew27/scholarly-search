SECTION: ViTGaze: Gaze Following with Interaction Features in Vision Transformers

Gaze following aims to interpret human-scene interactions by predicting the person’s focal point of gaze.
Prevailing approaches often adopt a two-stage framework, whereby multi-modality information is extracted in the initial stage for gaze target prediction.
Consequently, the efficacy of these methods highly depends on the precision of the preceding modality extraction.
Others use a single-modality approach with complex decoders, increasing network computational load.
Inspired by the remarkable success of pre-trained plain vision transformers (ViTs),
we introduce a novel single-modality gaze following framework called ViTGaze.
In contrast to previous methods, it creates a novel gaze following framework based mainly on powerful encoders (relative decoder parameters less than 1%).
Our principal insight is that the inter-token interactions within self-attention can be transferred to interactions between humans and scenes.
Leveraging this presumption, we formulate a framework consisting of a 4D interaction encoder and a 2D spatial guidance module to extract human-scene interaction information from self-attention maps.
Furthermore, our investigation reveals that ViT with self-supervised pre-training has an enhanced ability to extract correlation information.
Many experiments have been conducted to demonstrate the performance of the proposed method.
Our method achieves state-of-the-art (SOTA) performance among all single-modality methods
(3.4% improvement in the area under curve (AUC) score, 5.1% improvement in the average precision (AP))
and very comparable performance against multi-modality methods with 59% number of parameters less.

SECTION: 1Introduction

Gaze following is the task of predicting a person’s gaze target in an image.
Specifically, given an image and a bounding box of a person’s head, it aims to predict the location of the point where the person is watching.
It is widely applied in the fields of human-computer interactionapp_interactand neuroscienceapp_Neural.
Gaze following through RGB images has been a longstanding topic of research, with numerous related studies developed over time.

Previous works have taken two approaches.
One approach introduces multi-modality frameworks to improve the prediction performance.
These methods often adopt a two-stage methodology.
In the initial stage, task-specific modality predictors are employed to extract supplementary information, including depth and poses, as additional inputs to compensate for the absence of human-scene interactionsFang_DAM_2021_CVPR;Bao_ESC_2022_CVPR;Gupta_MM_2022_CVPR.
In the second stage, the visual features and the multi-modality input extracted in the initial stage are combined and utilized by the decoder to regress the gaze target heatmap.
Another approachTu_HGGTR_2022_CVPR;Tonini_GOT_2023_ICCV;tu2023jointinvolves a query-based decoder and utilizes additional object bounding boxes to improve performance by learning person-object interactions.
These methods employ a convolutional backbone for image feature extraction.
A transformer decoderdetris subsequently used to mix global information and provide gaze target predictions corresponding to the gaze queries.
However, both methods have drawbacks:

The additional information results in a multi-modal design with a two-stage framework. The accuracy depends on the performance of the prior predictions.

The query-based methods require a heavy decoder, which increases the complexity of the whole design.

We posit that these drawbacks stem from a shared design flaw: the absence of a sufficiently robust encoder for feature extraction of human-scene interactions.
Recently, the pre-trained plain vision transformers (ViTs)vithave demonstrated remarkable visual modeling capabilities.
A few works have explored the use of plain ViT on several downstream tasksvitdet;vitpose;vitmatteand achieved impressive results, highlighting the capacity of encoding rich features of the task-agnostic pre-trained ViTdino;dinov2.
Inspired by these previous works, it would be interesting to raise the following question: Can a pre-trained ViT provide an effective interactive representation between humans and the scene to describe a gazing relationship?

We propose ViTGaze, a concise single-modality and lightweight gaze following method based on pre-trained plain ViTs.
Our principal observation is that the inter-token interactions within self-attention can be transferred to interactions between humans and scenes.
The dot product operation in the self-attention mechanism inherently encodes token correlations.
Therefore, we hypothesize that the interactions between humans and scenes are also embedded in these self-attentions. This assumption of the self-attention map is consistent with the observations of previous studiesLOST;weaktr.
On this basis, we design a concise 4D interaction encoder to extract the interaction information from multi-level and multi-head attention maps and design a 2D spatial guidance module to guide it.
Owing to the strong ability of pre-trained ViTs to extract interactions between objects, ViTGaze does not introduce any decoder design.
In contrast to previous methods, ViTGaze creates a brand-new paradigm of gaze following, which is mainly based on encoders (decoder parameters account for less than 1%).
Furthermore, we also observe that self-supervised pre-trained ViTs are better at understanding token interactions. In contrast to previous ViT-based methodsvitdet;vitpose;vitmatte;cellvit, our method further investigates the importance of attention maps while using the feature map for gaze following.

Compared with existing state-of-the-art methods, our method has advantages in both performance and efficiency.
We conduct experiments on the two most widely-used benchmarks, the GazeFollowRecasens_GazeFollow_2015_NIPSand VideoAttentionTargetChong_VideoAttn_2020_CVPR.
The experimental results demonstrate the superiority of our method.
Specifically, our method achieves a 3.4% improvement in the area under curve (AUC) and a 5.1% improvement in the average precision (AP) among single-modality methods, resulting in a new state-of-the-art (SOTA) performance as shown in Fig.1.
In addition, our method achieves comparable performance to multi-modal methods (1.8% lower in distance error but 2.7% higher in AUC) with 59% fewer parameters. These results provide ample evidence of the validity of our method, demonstrating that a single-modality lightweight framework built upon pre-trained ViTs could also achieve SOTA performance in gaze following.

Our contributions can be summarized as follows:

We propose ViTGaze, a single-modality lightweight gaze following framework based on pre-trained vision transformers.
To the best of our knowledge, this is the first gaze-following method built upon the pre-training of ViTs.

We demonstrate the feasibility of extracting human-scene interaction information from inter-tokens interactions in self-attentions and design a 4D interaction module guided by a 2D spatial guidance module.

We evaluate our method on GazeFollowRecasens_GazeFollow_2015_NIPSand VideoAttentionTargetChong_VideoAttn_2020_CVPRbenchmarks.
Our method achieves SOTA performance (3.4% improvement in the AUC and 5.1% improvement in the AP) among the single-modality methods and very comparable performance with multi-modality methods with 59% fewer parameters.

SECTION: 2Related Work

SECTION: 2.1Gaze Following

Research on gaze behavior has drawn significant academic interest across multiple domains.
One prominent area is gaze estimationzhong2024uncertainty, which involves inferring gaze direction from facial cues.
Another key domain is scan path predictionzhong2024spformer;xia2020evaluation, which focuses on modeling the sequence of fixations across an image.
Unlike these works, gaze followingRecasens_GazeFollow_2015_NIPSaims to interpret human-scene interactions by locating the gaze target of a person to provide an understanding of how individuals interact with their surroundings.
Previous research on gaze following can be classified into two principal categories: single-modality methods and multi-modality methods.

The ground-breaking workRecasens_GazeFollow_2015_NIPSdesigns a single modality architecture using only RGB images as the input.
In this work, gaze following is formulated as the combination of the individual’s gaze field and the global saliency mapsaliency1;saliency2;saliency3.
The subsequent series of worksChong_Connect_2018_ECCV;Lian_ACCV_2019;zhaoLearningDrawSight2020focus on providing human-scene interactions through auxiliary tasks such as sight line estimationLian_ACCV_2019;zhaoLearningDrawSight2020.
GaTectorWang_GaTector_CVPR_2022proposes a unified framework to extract features from both head crops and scenes with a shared backbone.
HGTTRTu_HGGTR_2022_CVPRproposes a transformer-based encoder-decoder architecture, which provides implicit interaction clues through the global modeling of self-attention.
Despite the concise input of these methods, they often achieve unsatisfactory performance compared with multi-modality methods.

To further improve prediction performance, a few methods utilize additional modality information besides RGB images.
Fang et al.Fang_DAM_2021_CVPRproposed a depth-assisted architecture to estimate spatial relationships between the person and the scene through an extra depth estimator.
Furthermore, the human pose estimation sub-task is incorporated into the frameworkBao_ESC_2022_CVPR;Gupta_MM_2022_CVPR;Jian_Enhanced_ICMM_2020, offering a more precise 3D spatial interaction estimation.
The temporal information is utilized in Refs.Chong_VideoAttn_2020_CVPR;Miao_PDP_2023_WACVto improve prediction performance.
Additional object segmentation masks are incorporated in Refs.Chen_TPNet_TCSVT_2022;Hu_GazeTargetEstimation_TCSVT_2022to model object interactions.
Samy et al.sharingandirectly embeded the head position along with the coarse sight direction into a token and utilized a ViT to predict precise gaze targets.
GTRtu2023jointincorporates gaze object detection into the HGTTR framework, providing object-level supervision for accurate gaze localization.
Francesco et al.Tonini_GOT_2023_ICCVexpanded DETRdetrto detect objects and gaze points simultaneously.
However, these methods mostly adopt a two-stage design and their performance is dependent on the prediction results from the first stage.

Hence, how to design a concise and high-performance single-modality gaze following framework remains an unsolved problem.

SECTION: 2.2Pre-training of Vision Transformers

Pre-training can improve ViTvitin learning transferable visual representations.
Recently, researchersbeit;maefocus on self-supervised pre-training employing masked image modeling (MIM).
Furthermore, the integration of CLIPclipwith MIM by Refs.mvp;cae;eva;eva02results in a notable enhancement in the performance of pre-training.
Notably, approaches such as those in Refs.zhou2021ibot;dinov2leverage online tokenizers to further optimize the performance of pre-training.

The pre-trained representations of ViTs can significantly enhance the performance of downstream tasks.
ViTDetvitdetapplies pre-trained ViT to object detection by constructing a simple feature pyramid.
ViTPosevitposeinvestigates the performance of ViT in pose estimation tasks via a similar approach.
Additionally, ViT is employed in ViTMattevitmattein image matting tasks by designing a detail capture module.
In the medical field, pre-trained ViT can also further enhance the performance of cell segmentationcellvit.
In contrast, WeakTrweaktrleverages attention maps from a pre-trained ViT for weakly supervised image segmentation.

However, it remains an unexplored question whether the pre-trained representations contain enough interaction information to facilitate gaze following.

SECTION: 3Method

SECTION: 3.1Preliminary: Self-Attention

We first review the dot-product self-attention mechanism proposed in Ref.transformer.
The input token sequenceis first transformed into keys, queries, and values, whereis the length of the sequence andis the number of channels.
Then the dot product performed between queries and keys captures token correlations which are normalized and used as weights to aggregate the values.

where,, andare queries, keys, and values respectively, andrefers to the softmax operation.
The normalized dot product, which is also known as the attention map, adaptively determines the relationship between each pair of tokens.
In the vision region, an image with a size ofis first flattened to a patch sequence and then generates an attention map, which inherently follows a 4D paradigm that effectively indicates the interactions among patches.
This 4D interaction feature map has proven to be particularly effective in capturing patch correlations in an image, resulting in advanced performance in various computer vision tasks, such as weakly-supervised segmentationweaktrand unsupervised object localizationLOST.

SECTION: 3.2Overall Structure

The overall structure of our method is shown in Fig.2.
It takes an image and a bounding box of the person’s head as the inputs.
It outputs a heatmap that locates the gaze target with the highest response, and the probability that the person watches outside the scene.

Our method is composed of three components: a pre-trained vision transformer encoder that extracts a feature map with rich semantics and multi-level 4D interaction features, a 2D spatial guidance module to determine the spatial weights corresponding to each patch for person-specific interaction feature aggregation, and two prediction heads for heatmaps and in-out prediction.
These components are described in detail in the following subsections.

SECTION: 3.3Multi-level 4D Interaction Encoder

Inspired by the capacity of self-attention to capture patch-level interactions, we propose a 4D interaction encoder for efficient human-scene interaction estimation.
In Fig.3, we visualize the attention map of the token overlapped with the head of a person and the feature map output by the last block of a DINOv2dinov2pre-trained ViT.
Rich semantic information in the feature map enables effective distinction between objects.
However, it cannot represent interactions between image regions.
In contrast, a specific token’s attention map is capable of acquiring interaction information between it and other regions.
Therefore, we propose the interaction encoder to leverage attention maps, which are referred to as 4D interaction features, from pre-trained ViTs with a simple adaptation (Fig.4(a)).
Compared with the encoders used in the previous tasks (Fig.4(b)) such as object detectionvitdetand image mattingvitmattethat utilize feature maps to distinguish objects, the interaction feature-based encoder inherently facilitates the explicit capture of patch relations, which is required by gaze following.

Given an image with a resolution of. The interaction encoder extracts 4D features, which explicitly describe the interactions between image patches. The correlations among them are represented as a 4D tensor with a size of, whereand, anddenotes the patch-size of the ViT.
Different from the final feature map leveraged in other regionsvitdet;vitpose;vitmatte, this 4D representation reflects the inner-token relations which are more effective in gaze following.

Transformers pre-trained with masked image modeling capture correlations at multiple scales in multiple layersdark_secret.
To this end, we extract multi-level and multi-head 4D features to capture correlations between tokens with multiple distances.
Specifically, we extract features fromtransformer layers, and each feature is divided intosub-features based on different attention heads.
These attention maps represent patch interactions at both local and global levels and are combined to create multi-level 4D interaction features, whereandrefer to the number of semantic levels and the number of heads in the multi-head self-attention, respectively.
The extracted features are then guided by spatial information and used for heatmap prediction.

SECTION: 3.42D Spatial Guidance

We propose a 2D spatial guidance module for aggregating the 4D interaction features with the head position to obtain person-specific interaction features.
The insight between this module has two parts:
1) The attention map describes how each image patch interacts with all other patches in a 4D paradigm, while the gaze feature for following the specific person’s gaze must be a 2D feature map to predict the spatial distribution of gazing probability.
2) Due to the rich interaction features being fully extracted by the ViT encoder, it is unnecessary to design a heavy branch to extract head features and gaze cones, which is widely adopted in the previous literatureRecasens_GazeFollow_2015_NIPS;Miao_PDP_2023_WACV;Tonini_GOT_2023_ICCV;Gupta_MM_2022_CVPR.
Therefore, we formulate the aggregation as a simple weighted sum of 4D interaction features in two spatial dimensions on the basis of the head position and head features.
The module is constructed with a simple two-layer multi-layer perceptron (MLP) with softmax activation, which is used to calculate the weights of each patch.
Background patches are masked before the softmax operation to guarantee that the weights are unique to the target individual.

Built upon this 2D spatial guidance, we proceed to feature aggregation with guidance.
We obtain the person-specific interaction featuresthrough a weighted sum of 4D interaction featureswith 2D guidanceas weights.
This can be expressed in the form of matrix multiplication as Eq. (3).

The output interaction featuresrepresent multi-level person-specific interactions between the target person and each patch.
With this simple aggregation, we transfer the abundant interaction information in the whole image to the person-scene interaction corresponding to the specific person.

Unlike the interaction feature, the in-out prediction feature focuses more on the global semantics instead of the geometric comprehension of the image,
which is provided in the final ViT feature maps.
Therefore, we use image token features, guided by 2D guidance.
It is expressed by Eq. (4),
whilerepresents the image token features from the last layer of ViT.

To further engage the module in feature extraction of heads, we introduce an auxiliary head that predicts whether a patch overlaps with the heads of any people.
The auxiliary head shares the stem with the main branch and introduces an extra prediction layer to predict the patch-level probability of overlapping with the head of a person to provide more supervision of head features.
Notably, in gaze-following settings, this auxiliary task does not require any supplementary input modalities such as depth, human poses, or object segmentation.

SECTION: 3.5Prediction Heads

This component uses the person-specific interaction feature and the in-out prediction feature to predict a gaze target heatmapand a valueindicating the probability that the person watches outside the scene.
These two parts are detailed below.

Gaze heatmap head.This module employs three groups of bilinear interpolation and convolutional layers to predict the gaze location.
It converts person-specific interaction features to a logit map, where the highest activation point refers to the predicted gaze target.

In-out prediction head.We design an MLP head to predict whether the person’s gaze point is located in the image.
The head vector is fed into the two-layer MLP followed by sigmoid activation to obtain the probability that the person watches outside.

SECTION: 3.6Training objective

We train our model in an end-to-end paradigm with the training objective as a weighted sum of all tasks.
The loss consists of three parts: gaze heatmap loss, gaze in-out loss, and auxiliary head regression loss.

Gaze heatmap lossmeasures the error of gaze target prediction with a mean square error between the predicted gaze heatmap and the ground truth generated by a Gaussian blob centered at the coordinate of the gaze target.

Gaze in-out lossmeasures the prediction error of whether the person watches outside.
It is achieved by a focal lossfocal_lossbetween the predicted probability and the ground truth label.

Auxiliary head regression lossis designed to constrain the 2D spatial guidance.
It is defined as the binary cross-entropy loss between the predicted head occurrence and the ground truth heatmap which is a combination of Gaussian blobs centered at head bounding boxes in annotations.

The final loss is a linear combination of the three losses:

where,, andare weights of,, and.

SECTION: 4Experiment

SECTION: 4.1Datasets and Evaluation Metrics

Datasets.We train and test ViTGaze on the GazefollowRecasens_GazeFollow_2015_NIPSdataset and VideoAttentionTargetChong_VideoAttn_2020_CVPRdataset.

GazefollowRecasens_GazeFollow_2015_NIPScontains over 130 K annotations of people and their gaze targets in 122 K images.
The dataset is annotated with head bounding boxes, gaze points, and in-out-of-frame labels provided by Ref.Chong_Connect_2018_ECCV.
GazeFollow focuses on gaze targets inside the images, therefore only gaze heatmap regression is evaluated.

VideoAttentionTargetChong_VideoAttn_2020_CVPRcontains 1331 video clips with 165 K annotations.
We evaluate both gaze heatmap regression and watching-outside prediction.

Evaluation metrics.We adopt the following metrics to evaluate the performance of the proposed method.
AUC reflects the prediction confidence of gaze heatmaps.
Distance (Dist.) refers to the relative Euclidean distance between the ground truth and the predicted position with the highest confidence.
Since 10 annotations are provided in GazeFollow for each instance, we report both the minimum distance (Min. Dist.) and average distance (Avg. Dist.).
We use AP to evaluate the performance for watching-outside prediction in VideoAttentionTarget.

SECTION: 4.2Implementation Details

Model structure.We adopt ViT-Svitas the transformer encoder.
The encoder consists of 12 blocks with a multi-head self-attention of 6 heads.
The multi-level 4D interaction is constructed with attention maps from the 3-rd, 6-th, 9-th, and 12-th blocks.
We use an efficient implementation of multi-head attention which is available in the xFormersxFormers2022library.

Unbiased data processing.The coordinate encoding process (i.e. transforming ground-truth coordinates to heatmaps) used in current methods introduces quantization error of gaze targets and thus degrades model performance.
To address this dilemma, we follow DARKdarkposeto generate gaze heatmaps with a Gaussian kernel using the real gaze target as the center without quantization of the center coordinates.
We also adopt the post-processing method proposed in Ref.darkposeto infer the final gaze target location.

Model training.For GazeFollowRecasens_GazeFollow_2015_NIPS, we initialize the model with weights of DINOv2dinov2.
The model is trained for 15 epochs via the AdamWadamwoptimizer with a batch size of 48.
The initial learning rate is set as, and decays towith a cosine scheduler.
The weight decay is set as 0.1.
We follow DINOv2dinov2to increase the resolution of images toin the last epoch.
In addition to basic data augmentations, we follow DINOv2dinov2to apply random masks to the images.
Specifically, we replace patches in the background that occupy less than 50% of the area with a mask token with a probability of 0.5.
The loss coefficients are= 2 in the focal loss,= 100 for the heatmap regression,= 1 for the watching-outside prediction, and= 1 for auxiliary head regression.
For VideoAttentionTarget, following Refs.Fang_DAM_2021_CVPR;Gupta_MM_2022_CVPR, we finetune the model initialized with weights learned from GazeFollow.
We train for 1 epoch with the learning rate ofon VideoAttentionTarget.
The detailed training configurations are outlined in AppendixA.

SECTION: 4.3Comparison with State-of-the-Art

We present the quantitative results for the GazeFollow and VideoAttentionTarget datasets in Tab.1.
To ensure a fair comparison, we also annotate the modalities used in each model.
Additionally, we include the parameter counts and floating point operations (FLOPs) of each model to facilitate the comparison of their efficiency, where the additional modality extractors required by multi-modality methods are not taken into account.

Accuracy.As demonstrated in Tab.1, we compare ViTGaze with previous multi-modality and single-modality methods.
ViTGaze achieves new SOTA performance among single-modality methods.
Compared with the previous SOTA methodTu_HGGTR_2022_CVPR, ViTGaze outperforms 3.4% in terms of the AUC in GazeFollow and 5.1% in terms of AP in VideoAttentionTarget.
This improvement demonstrates that ViTGaze is efficient in leveraging single-modality data to achieve better performance.
Additionally, our method shows performance that is comparable to multi-modality methods without extra input used during prediction.
For instance, compared with the previous SOTA multi-modality methodTonini_GOT_2023_ICCV, our method is only 1.8% lower in terms of the distance but achieves 2.7% higher AUC on the GazeFollow benchmark.
This indicates that even with fewer modalities, ViTGaze can match and even exceed the performance of complex multi-modality methods.
This highlights the efficiency and robustness of ViTGaze to extract and utilize interaction features from single-modality data.

Efficiency.As displayed in Tab.1, our method achieves the SOTA performance with only 22 M parameters and a computational cost of only 4.62 GFLOPs.
Even the computational overhead associated with the modality extraction stage required by multi-modality methods is not considered, our approach still achieves a reduction of over 50% in the number of parameters and over 25% in the computational demand in comparison to existing methods.
This result demonstrates that 4D patch-level interaction features are capable of extracting the appropriate clues for accurate gaze target detection.
Besides, our method is a novel architecture based mainly on encoders (relative decoder parameters less than 1%).
We conduct a comprehensive comparison of the parameter distribution in ViTGaze with those of existing SOTA methods as illustrated in Tab.2.
Specifically, we list the parameter distributions of both the SOTA single-modality (RGB-only) methodTu_HGGTR_2022_CVPRand the SOTA multi-modality methodTonini_GOT_2023_ICCV.
The top-performing RGB-only method employs a hybrid architecture as the encoder for feature extraction, and it adopts a decoder constituting 26% of the total parameter count for facilitating further information processing.
The most effective multi-modality method incorporates an additional object decoder constituting 18% of the total parameter count for extra object detection.
This, in turn, leads to a two-stage decoder structure that accounts for 40% of the total parameter count.
Our method harnesses the robust representational capabilities of pre-trained ViT, requiring only a prediction head that constitutes less than 1% of the total parameter count to accomplish gaze following.
Additionally, pre-training allows us to achieve more robust interaction information extraction with a transformer-based encoder that occupies only 70% of the parameter count compared with other methods.
This demonstrates that the complex and heavy design may not be necessary for gaze following.

SECTION: 4.4Ablation Study

In this section, we conduct experiments on the GazeFollowRecasens_GazeFollow_2015_NIPSbenchmark to validate the effectiveness of our proposed method.
All the experiments are conducted with ViT-smalldinov2as the backbone.

Multi-level 4D interaction features.To demonstrate the effectiveness of multi-level 4D interaction features (M-4D features) for gaze following, we conduct a study on the features used for gaze prediction.
We build a variant via 2D feature maps i.e. the final output of the ViT for prediction.
We also compare our method to a single-level variant (S-4D features) that uses only attention maps in the last block to verify the effect of multi-level relation fusion.
The detailed results of the study on GazeFollow are presented in Tab.3.
According to the experimental results, 4D interaction features are more effective in capturing person-scene relations, whereas 2D features are incapable of capturing patch interactions and cannot directly determine the gaze targets in an end-to-end training paradigm.
Furthermore, our multi-level approach outperforms its single-level counterpart, highlighting the significance of capturing relationships at different levels.
Single-level 4D interactions fail to provide adequate local representation and cannot predict precise gaze targets.

2D spatial guidance.A comparative study is conducted to validate the significance of 2D spatial guidance, as illustrated in Tab.4.
Direct pooling of the interaction features in the head region achieves an acceptable result, indicating that it is unnecessary for the complex design of head feature extraction.
Moreover, the proposed 2D spatial guidance achieves a 0.7% improvement in the AUC, demonstrating that our design results in more informative fusion to further increase the performance.
Furthermore, the introduction of the auxiliary head prediction task effectively supervises this module and increases the effectiveness of training the 2D spatial guidance, consequently enhancing the performance of spatial guidance.

Pre-training.To validate the significance of pre-trained ViT for interaction feature extraction, we conduct experiments using DeiTdeit, DINOdino, iBOTzhou2021ibot, and DINOv2dinov2as the backbone on GazeFollowRecasens_GazeFollow_2015_NIPS, as illustrated in Tab.5.
DINO, iBOT, and DINOv2 are pre-trained on a large amount of unlabelled data to learn robust visual representations with different self-supervising algorithms.
DeiT is a supervised learning method that serves as a benchmark for comparison and contrast of the outcomes and effectiveness of self-supervised methods.
DeiT, DINO, and iBOT are trained on the ImageNet datasetimagenet, whereas DINOv2 is trained on the larger LVD-142M datasetdinov2.
Compared with DeiT pre-training, ViT with self-supervised pre-training can greatly improve the performance of the model.
The gaze following performance of these methods is highly consistent with their results on ImageNet for representation learning.
Compared with the pre-trained ViT with DeiT pre-training, the model utilizing DINOv2 pre-trained weights achieves a notable 6.8% improvement in AUC.
In AppendixB, the visualization results of ViTGaze built upon different pre-training methods are presented to illustrate the impact of pre-training methods on the granularity of predictions.
The experimental results demonstrate that the robust representational capabilities of pre-trained ViT effectively enhance gaze following performance.

Loss weights.The loss weights, i.e., and, are based on the methodology outlined in Ref.Chong_VideoAttn_2020_CVPR.
We conduct experiments onandin Tab.6.
The results demonstrate that the ratio ofkeeps each loss at the same scale to ensure that no single task dominates the training process and therefore achieves the best performance.

SECTION: 4.5Visualization Results

The visualization results on the GazeFollow datasetRecasens_GazeFollow_2015_NIPSare provided in Fig.5, where the 2D spatial guidance, multi-level person-specific interaction features, and the final predictions are displayed.
We observe that the 2D spatial guidance visualized in the second column extracts the patches overlapped with the key points of the face of a person.
The interaction features generated by the medium layer (the 6-th transformer block) reflect the local interactions of the person’s head and body and highlight the key points such as eyes.
The interaction features of the final layer in the 3-rd column show the global interactions and roughly indicate the gaze targets.
These observations reflect the importance of multi-level interaction features and 2D spatial guidance as described above.
The visualization results on the VideoAttentionTarget datasetChong_VideoAttn_2020_CVPRare provided in AppendixC.

SECTION: 5Conclusion

In this paper, we propose a new high-performance single-modality gaze following framework, ViTGaze, which is based on pre-trained plain vision transformers.
It consists of a 4D interaction encoder, 2D spatial guidance information, and two lightweight predicting heads.
Our key observation is that inter-token interactions within self-attention can be transferred to interactions between humans and scenes.
ViTGaze presents a brand-new paradigm of gaze following mainly based on encoders and its relative decoder parameters is less than 1%.
This first demonstrates that a concise single-modality framework could also achieve high gaze following performance with the help of pre-trained vision transformers.
We hope that our work can inspire more research on gaze following with pre-trained vision transformers.

The limitations and future work are as follows.
As illustrated in Fig.6, in practical situations, the predictions of ViTGaze may become ambiguous, particularly when individuals engage in complex environments with occlusions.
To address this issue, it would be beneficial to integrate the prior of gaze dynamics and global saliency.
We leave them as our future work.

SECTION: Appendix ADetailed training configurations

We summarize the detailed training configurations on GazeFollowRecasens_GazeFollow_2015_NIPSin Tab.7.
We follow DINOv2dinov2to increase the resolution of images toin the last epoch.
During our training process, we apply several data augmentation techniques to enhance the dataset.
These techniques include head bounding box jittering, color jittering, random resizing and cropping, random horizontal flips, random rotations, and random masking of the scene patches.

For the VideoAttentionTarget datasetChong_VideoAttn_2020_CVPR, we perform fine-tuning on the model pre-trained on the GazeFollow dataset.
During this fine-tuning phase, we do not adopt a learning rate scheduler and opt for a fixed learning rate of, which is sustained for a single training epoch.
All other training configurations remained unchanged from those applied during training on the GazeFollow dataset.

SECTION: Appendix BComparison of pre-training methods

To highlight the significance of pre-trained ViTs, we visualize the results of ViTGaze built upon different pre-training methods on the GazeFollow datasetRecasens_GazeFollow_2015_NIPS, as shown in Fig.7.
The supervised pre-trained methoddeit, as observed in our evaluation, tends to yield a greater number of false positive masks within the salient regions that lie outside of the individual’s field of view.
In contrast to supervised pre-training, the utilization of self-supervised pre-trained ViTsdino;zhou2021ibot;dinov2substantially enhances the gaze following performance.
Furthermore, the performance of the models consistently aligns with the outcomes of their representation learning experiments on the ImageNet benchmark.
Among all the pre-training techniques at our disposal,
the model built upon the pre-trained ViT through the DINOv2 pre-training method stands out because it achieves the highest levels of precision and granularity in predicting gaze targets.

SECTION: Appendix CVisualization Results on the VideoAttentionTarget Dataset

We visualize the results on the VideoAttentionTarget datasetChong_VideoAttn_2020_CVPR, as illustrated in Fig.8.
For each video clip, we extract one image at intervals of eight frames and subsequently select a consecutive sequence of five extracted images for presentation and analysis.

Notably, even in the absence of explicit time-series modeling, our approach remarkably excels at effective gaze following in video sequences.
This noteworthy observation demonstrates that ViTGaze constructed upon pre-trained ViT can extract abundant interaction features from RGB images without any additional input modality, facilitating robust gaze following.
Another intriguing observation is that, despite the decoupling of gaze heatmap prediction and watching-outside prediction in our method,
the responses generated on the predicted heatmap effectively reflect the confidence of the model in the direction where the person is looking.
The predicted heatmap no longer exhibits discernible activations when the gaze target is outside of the frame.

AP, average precision; AUC, area under the curve; GT, ground truth; MLP, multi-layer perception; SOTA, state of the art; ViT, vision transformer.

SECTION: Declarations

Our code is available athttps://github.com/hustvl/ViTGaze. Thedatasets analyzed during the current study are available in GazeFollowRecasens_GazeFollow_2015_NIPSand VideoAttentionTargetChong_VideoAttn_2020_CVPR.

The authors declare no competing interests.

This work was supported by the National Science and Technology Major Project (No. 2022YFB4500602).

YS conceived the initial ideas, conducted detailed experiments, and drafted the paper.
XW revised the manuscript and improved the research ideas.
JY revised the manuscript and improved the experimental design.
WL, JZ, and XX systematically refined the research framework and guided the writing of the paper.
All the authors read and approved the final manuscript.

SECTION: References