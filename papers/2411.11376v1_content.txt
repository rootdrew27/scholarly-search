SECTION: Lung Disease Detection with Vision Transformers: A Comparative Study of Machine Learning Methods

Recent advancements in medical image analysis have predominantly relied on Convolutional Neural Networks (CNNs), achieving impressive performance in chest X-ray classification tasks, such as the 92% AUC reported by AutoThorax-Net and the 88% AUC achieved by ChexNet[10,6,8]in classifcation tasks. However, in the medical field, even small improvements in accuracy can have significant clinical implications. This study explores the application of Vision Transformers (ViT)[2,9], a state-of-the-art architecture in machine learning, to chest X-ray analysis, aiming to push the boundaries of diagnostic accuracy.
I present a comparative analysis of two ViT-based approaches: one utilizing full chest X-ray images and another focusing on segmented lung regions. Experiments demonstrate that both methods surpass the performance of traditional CNN-based models, with the full-image ViT achieving up to 97.83% accuracy and the lung-segmented ViT reaching 96.58% accuracy in classifcation of diseases on three label and AUC of 94.54% when label numbers are increased to eight. Notably, the full-image approach showed superior performance across all metrics, including precision, recall, F1 score, and AUC-ROC.
These findings suggest that Vision Transformers can effectively capture relevant features from chest X-rays without the need for explicit lung segmentation, potentially simplifying the preprocessing pipeline while maintaining high accuracy. This research contributes to the growing body of evidence supporting the efficacy of transformer-based architectures in medical image analysis and highlights their potential to enhance diagnostic precision in clinical settings.

SECTION: IIntroduction

The early and accurate detection of lung diseases, such as pneumonia, COVID-19 and other diseases, is critical for reducing patient mortality and optimizing healthcare outcomes. Chest X-rays are one of the most widely used imaging modalities for diagnosing lung conditions due to their cost-effectiveness and accessibility. However, interpreting these images remains challenging, as pathological patterns can be subtle and complex. To address these challenges, automated methods based on machine learning have become a focus of research, offering the potential to assist radiologists by providing reliable diagnostic support[4].

In recent years, Convolutional Neural Networks (CNNs) have been the dominant architecture for medical image classification tasks, particularly in chest X-ray analysis. CNN-based models have demonstrated considerable success, achieving high accuracy on various lung disease detection tasks[3]. Despite this success, further advancements are needed to improve diagnostic accuracy, as even marginal improvements can have significant clinical implications. Additionally, CNNs exhibit certain limitations, particularly in capturing long-range dependencies and global context within images. This shortfall may hinder their ability to detect subtle disease patterns, which are crucial for early diagnosis.

Vision Transformers (ViTs), a novel class of neural networks, have emerged as a powerful alternative to CNNs. Originally designed for natural image classification, ViTs utilize the self-attention mechanism to model global relationships across the entire image, making them particularly well-suited for tasks requiring a comprehensive understanding of image features[2]. In this study, I investigate the potential of ViTs to improve diagnostic accuracy in chest X-ray analysis.

The goal of this research is twofold: (1) to evaluate the performance of ViTs on full chest X-ray images, and (2) to explore whether focusing on lung-segmented regions can further enhance the accuracy of disease classification. By conducting a comparative analysis of these two approaches, I aim to determine which method better captures the relevant features necessary for detecting lung diseases.

SECTION: IIRelated Work

Early models like CheXNet and CheXpert leveraged CNNs to detect various lung diseases, achieving remarkable performance across large datasets[3,6]. These models relied on the hierarchical feature extraction capability of CNNs, where local features were learned at lower layers and combined into more complex global patterns at deeper layers.

To enhance CNN performance, researchers introduced attention mechanisms that allowed the model to focus on more relevant parts of the image. Global attention strategies, like the ones introduced in models with self-attention layers, aimed to capture long-range dependencies across the entire image, while local attention was utilized to focus on critical regions such as lung areas. Hybrid approaches that combined CNNs with lung segmentation also improved model precision by narrowing the focus to specific regions of interest.

Despite the success of CNNs, their inherent limitation lies in their reliance on local feature extraction. This limitation prompted the exploration of Vision Transformers (ViTs), which use global self-attention mechanisms to capture broader context across images.

SECTION: IIIMethod

SECTION: III-ADataset

This study utilizes two prominent datasets for chest X-ray image analysis: the NIH Chest X-ray dataset and the COVID-19 Image Data Collection. The NIH Chest X-ray dataset, provided by the National Institutes of Health (NIH), contains over 100,000 frontal-view X-ray images, covering 14 pulmonary conditions, including pneumonia, effusion, and atelectasis[11]. The COVID-19 Image Data Collection, compiled by Joseph Paul Cohen, Paul Morrison, and Lan Dao, supplements this dataset with X-ray images of patients diagnosed with COVID-19[1]. Together, these datasets form the foundation of this study’s exploration of lung disease classification using Vision Transformers (ViT).

From these datasets, a subset was selected to ensure a balanced and representative sample of lung diseases. The training set includes a total of 12,897 images across seven categories, including 1,266 images for ”Normal” cases, 1,598 for ”Effusion,” 2,000 for ”Infiltration,” 1,471 for ”Nodule,” 3,418 for ”Pneumonia,” 1,684 for ”Atelectasis,” and 460 for ”COVID-19.” Similarly, the test set contains 2,975 images, with 317 images in the ”Normal” class, 399 for ”Effusion,” 500 for ”Infiltration,” 367 for ”Nodule,” 855 for ”Pneumonia,” 421 for ”Atelectasis,” and 116 for ”COVID-19.”

In addition to these raw images, Segment Anything Model 2 (SAM2) was employed to segment the lung regions which was release July 29, 2024 from Meta AI, generating a masked dataset for a region-focused approach[7]. This segmentation process allowed for an in-depth comparison between models trained on full chest X-rays and those trained on segmented lung areas, providing insights into the impact of localized versus global feature extraction in chest X-ray classification.

SECTION: III-BModel Architecture

For this study, I utilized thegoogle/vit-base-patch16-224-in21kVision Transformer (ViT) model as base architecture. The ViT architecture is based on a pure attention mechanism rather than the convolutional operations traditionally used in medical image analysis. It divides the input image into patches and processes them as sequences. Specifically, the model divides input images into patches of 16x16 pixels, which are then processed as a sequence.

These patches are embedded into a lower-dimensional space, followed by position embeddings to retain spatial information, and subsequently passed through multiple layers of self-attention. The self-attention mechanism is defined as:

where,, andrepresent the query, key, and value matrices, andis the dimension of the key vectors. This mechanism enables the model to capture both local and global features of the chest X-ray images.

Chosen ViT model was pre-trained on the ImageNet-21k dataset, providing a rich foundation of visual features. I fine-tuned this model for specific chest X-ray classification task. The model was fine-tuned with a custom classifier head consisting of a fully connected layer. For classification, a softmax layer was applied over the output logits to predict the probability distribution across the seven disease categories. I replaced the original classification head with a custom fully connected layer to output probabilities for seven disease categories.

The core components of the model are as follows:

ViT Patch Embeddings: The input images are divided into non-overlapping patches, each of size 16x16, which are then flattened and linearly projected into a higher-dimensional space (768 dimensions). The embeddings are computed using a patch embedding layer with 590,592 parameters.

ViT Encoder: The sequence of image patches is fed into a transformer encoder, consisting of 12 layers of multi-head self-attention mechanisms, layer normalization, and MLP blocks. This structure enables the model to capture both local and global information from the image.

Classification Head: After the encoding process, the class token is passed through a final fully connected layer, which outputs predictions for 7 lung disease categories (Normal, Pneumonia, COVID-19, Nodule, Infiltration, Effusion, and Atelectasis). The classification head includes a linear layer with 5,383 parameters.

The overall architecture comprises 597,511 parameters, all of which are trainable.

SECTION: III-CTraining Process

Input chest X-ray images were preprocessed to match the model’s requirements, including resizing to 224x224 pixels and normalizing pixel values.

The loss function for the optimization was the cross-entropy loss, denoted as:

whereis the true label,is the predicted probability, andis the total number of classes. The optimization of model parameters was performed using the AdamW[5]optimizer, with a learning rate ofand a weight decay of 0.01. The learning rate was scheduled using the cosine annealing method, where the learning rate is gradually reduced over each epoch according to the following formula:

whereis the learning rate at epoch,andare the minimum and maximum learning rates, andis the total number of epochs.

The segmented dataset was used in parallel with the full-image dataset to evaluate the effectiveness of a localized approach in improving classification accuracy. I compared the model’s ability to capture relevant features from both full and segmented X-ray images to assess its overall performance. This dual approach allowed us to investigate whether focusing on lung regions could enhance the model’s diagnostic accuracy, results are shown in Figure3.

SECTION: III-DEvaluation Metrics

To thoroughly evaluate the performance of the Vision Transformer (ViT) models in chest X-ray classification, several metrics were employed, ensuring a comprehensive analysis of both overall accuracy and the model’s discriminative ability across multiple disease categories.

AUROC was selected as a primary metric to measure the model’s ability to discriminate between disease classes. This metric is particularly important in medical applications, where the ability to correctly distinguish between healthy and diseased states is crucial. A high AUROC value indicates that the model performs well across various threshold settings, which is critical for ensuring consistent classification performance.

Accuracy was used to provide a general sense of the model’s overall performance, capturing the proportion of correctly classified instances. Precision and recall were included to offer insights into the model’s handling of imbalanced data, where false positives or false negatives could have significant clinical consequences. The F1 score was calculated as a harmonic mean of precision and recall, ensuring a balanced evaluation of both metrics, particularly when the cost of misclassification is high, as it often is in medical diagnoses.

Cohen’s Kappa was employed to measure the agreement between the model’s predictions and the true labels, taking into account the possibility of random chance. This metric is essential in the medical field, where random predictions could lead to false conclusions about a model’s effectiveness. The Matthews Correlation Coefficient (MCC) was used for its ability to provide a balanced evaluation of the model’s predictive power, particularly when dealing with imbalanced datasets, offering insights into the model’s reliability across different classes.

The ROC AUC was used to further assess the model’s performance across all classification thresholds. This metric was chosen for its ability to provide a comprehensive view of the model’s discriminative power, helping to ensure that it can accurately separate different disease classes. A strong ROC AUC performance is critical in medical image analysis, where precise classification can directly impact diagnostic outcomes.

SECTION: IVExperiments

SECTION: IV-AHardware Environment

The experiments were conducted on a Dell Precision 5690 workstation with the following configuration:

SECTION: IV-BModel Configuration

For the lung disease classification task, I used the Vision Transformer (ViT) model with the following configuration:

SECTION: IV-CModel Comparison

This study is offering two distinct approaches. The first approach fine-tunes the pre-trained Google ViT model on full chest X-ray images. This method allows the ViT to leverage its self-attention mechanism to learn global image representations without focusing on specific regions. The inherent design of ViTs, with their ability to model long-range dependencies and capture global context, makes them particularly suited for medical images where patterns may span across the entire image.

The second approach incorporates the SAM model from Meta to pre-segment the chest X-ray images, generating lung region masks. In this approach, the ViT is fine-tuned on these segmented images, focusing on the lung regions rather than the full chest image. The segmentation step introduces a regionally focused analysis, allowing the model to prioritize the most clinically relevant areas, potentially reducing noise and improving classification accuracy for specific lung conditions.

Both approaches outperformed some of the traditional CNN-based models in this study. The ViT model trained on full chest X-ray images demonstrated superior performance across all metrics, likely due to its ability to capture a broader context and avoid overfitting to specific regions. The segmented approach, while slightly less accurate, offered competitive results, suggesting that regional focus can still enhance classification in certain cases.

SECTION: IV-DTraining and Validation Results

The models were trained using a batch size of 32 over 10 epochs, and the training process employed the AdamW optimizer with a learning rate of, coupled with the CosineAnnealingLR scheduler. Cross-entropy loss was used as the primary loss function, ensuring that the models effectively distinguished between the disease categories.

The global image-based ViT achieved a higher overall classification accuracy and AUC-ROC scores compared to the regionally focused model. This is likely due to the ability of the Vision Transformer to model entire chest X-ray images, capturing global patterns and correlations that may not be evident in smaller, segmented regions.

Key metrics such as accuracy, precision, recall, F1 score, and AUROC were calculated to assess the model’s classification ability. During training, both models exhibited steady improvement in loss reduction and accuracy over the epochs. The learning rate scheduler played a critical role in fine-tuning the models, as evidenced by smooth convergence curves for both training and validation loss. However, the global image-based model converged slightly faster, likely due to its ability to capture richer, global features from the full image.

The validation accuracy for both models plateaued after 8 epochs, indicating that the models had effectively learned the relevant features by this point. Early stopping mechanisms were considered but not applied, as the models continued to show marginal improvements in metrics like precision and recall even after the primary performance metrics had stabilized.

SECTION: VResults

SECTION: V-ARegion-Focused ViT Model Results

The training and validation results for the region-focused ViT model, fine-tuned on lung-segmented X-ray images, are summarized across eight epochs in the following table. Key metrics such as accuracy, loss, AUROC, and MCC were tracked for both training and validation phases.

As shown in TableIV, the model’s performance steadily improved across the eight epochs. By the final epoch, the model achieved a validation accuracy of 74.52%, with an AUROC of 94.42%, indicating a strong capacity to distinguish between the classes. The loss reduced consistently, reaching 0.4101 in the last epoch. MCC also improved, reaching 0.6908, which reflects a robust correlation between predicted and actual classifications. While segmenting lung regions can enhance image classification, as demonstrated by the state-of-the-art (SOTA) performance of Meta’s SAM 2 model in segmentation tasks, applying zero-shot segmentation to lung images may introduce noise or produce poor-quality data, potentially hindering model performance. This highlights the importance of carefully curating and evaluating segmented datasets, as suboptimal segmentation could negatively impact diagnostic accuracy.

SECTION: VIConclusion

This study demonstrates the significant potential of Vision Transformers (ViTs) in advancing chest X-ray analysis for lung disease detection. Through a comparative analysis of two ViT-based approaches—one utilizing full chest X-ray images and the other focusing on segmented lung regions—several critical findings have emerged.

Both ViT-based methods consistently outperformed traditional CNN models, with the full-image ViT achieving an accuracy of up to 97.83%, while the lung-segmented ViT reached 96.58% in classification tasks. Notably, the full-image approach exhibited superior performance across all evaluation metrics, including precision, recall, F1 score, and AUC-ROC, suggesting that ViTs can effectively capture the necessary features from chest X-rays without the need for explicit lung segmentation.

Moreover, even when the number of disease labels increased to eight, ViT models maintained robust performance, with an AUC of 94.54%. This reinforces the versatility of the ViT’s self-attention mechanism in learning global image representations, effectively capturing both local and long-range dependencies that are critical for accurate disease classification.

These results contribute to the growing body of evidence supporting transformer-based architectures in medical image analysis. The superior performance of the full-image approach, in particular, underscores the potential of ViTs to streamline diagnostic workflows while maintaining high accuracy and robustness, which is crucial for enhancing clinical decision-making.

SECTION: Abbreviations

AUROC: Area Under the Receiver Operating Characteristic Curve

ViT: Vision Transformer

CNN: Convolutional Neural Network

MCC: Matthews Correlation Coefficient

SOTA: State-of-the-Art

SAM: Segment Anything Model

SECTION: References