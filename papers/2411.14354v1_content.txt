SECTION: Contrasting local and global modeling with machine learning and satellite data:
A case study estimating tree canopy height in African savannas

While advances in machine learning with satellite imagery (SatML) are facilitating environmental monitoring at a global scale, developing SatML models that are accurate and useful for local regions remains critical to understanding and acting on an ever-changing planet.
As increasing attention and resources are being devoted to training SatML models with global data, it is important to understand when improvements in global models will make it easier to train or fine-tune models that are accurate in specific regions.

To explore this question, we contrast local and global training paradigms for SatML through a case study of tree canopy height (TCH) mapping in the Karingani Game Reserve, Mozambique. 
We find that recent advances in global TCH mapping do not necessarily translate to better local modeling abilities in our study region.
Specifically, small models trained only with locally-collected data outperform published global TCH maps, and even outperform globally pretrained models that we fine-tune using local data.
Analyzing these results further, we identify specific points of conflict and synergy between local and global modeling paradigms that can inform future research toward aligning local and global performance objectives in geospatial machine learning.

SECTION: 1Introduction

Paired with a massive amount of publicly available satellite imagery, geospatial machine learning is enabling a new paradigm of environmental monitoring. Machine learning with satellite imagery (SatML) models have been deployed at a global scale to classify patterns as diverse as land cover types(Brown et al.,2022), industrial activity at sea(Paolo et al.,2024), micro-estimates of wealth(Chi et al.,2022), and forest and tree canopy height(Potapov et al.,2021; Lang et al.,2023; Tolan et al.,2024).
While efforts like these significantly extend the possibilities forglobal-scalescience and policy, understandinglocaltrends and variations in key environmental indicators remains paramount.
Many important indicators of global change in ecology and environmental sciences manifest at local or regional scales, which global-scale data do not always represent well. Moreover, data-driven policies designed to mitigate global change and its effects (e.g., policies that regulate land use, or distribute resources across populations) are
often tailored to particular localities (e.g., administrative regions or protected areas).

Prior work has exposed concerns about the accuracy of geospatial machine learning models developed for global- or national-scale prediction when evaluated in local areas.Kerner et al. (2024)found that global models of land cover maps are sub-optimal at capturing regional land cover variation of agricultural land in Sub-Saharan Africa.Aiken et al. (2023)showed that satellite-based poverty prediction maps can exhibit policy-relevant regional biases across urban and rural lines when trained on entire countries.Nyandwi et al. (2024)highlighted limitations of global building and roads datasets evaluated across diverse neighborhoods in Rwanda. Similarly,Gevaert et al. (2024)found substantial gaps between local and global performance of building segmentation maps from SatML models, with implications for disaster response. While it is tempting to expect that advances in mapping environmental phenomena across the globe will extend to improved model performance in specific local regions, these examples suggest that this may not be the case.

While past studies have exposed accuracy disparities in key local regions, they largely stop short of investigating the root causes and larger implications of these differences, and what they indicate about current methodologies in geospatial machine learning. Are drops in accuracy in some regions due to those regions being inherently difficult or unique places, or are they symptoms of a deeper discrepancy between the capacities of global and local models? When should we expect a model trained with an abundance of globally representative data will be accurate for a local region? And if local data are available for training, is fine-tuning a globally pretrained model a better strategy than training a local model from scratch? In order to answer these types of practical questions, we seek to develop a deeper understanding of when fundamental discrepancies can arise between local and global modeling objectives.

In this work, we devise a set of experiments to better understand if and when global mapping and local mapping use cases may constitute somewhat distinct goals for SatML and geospatial machine learning more generally.
We conduct a case study of specific local ecological importance, the mapping of tree canopy height (TCH), which is also representative of a class of local modeling problem settings more generally.
Through this case study, we assess the degree to which existing SatML models and maps designed for global performance can be used to map our local area, compared with a strategy that trains a model “from scratch” based only on local training data.
Our findings provide concrete evidence of a divide between local and global modeling performance in practice, and facilitate a nuanced understanding of how data and model design decisions affect this gap.
The results of our study have implications for current best practices and promising future directions to improve possible synergies between local and global monitoring efforts in geospatial machine learning across prediction domains.

Specifically, we ground our inquiry with a case study in developing locally-focused models of TCH using publicly available satellite imagery and spatially restricted UAV-LiDAR data from Karingani Game Reserve, Mozambique (Figure1).
Recent global prediction maps of TCH built on globally available satellite data from the Global Ecosystem Dynamics Investigation (GEDI) mission have achieved remarkable performance in mapping variation in tree canopy height across the globe, enabling the monitoring of large-scale change in vegetation extent and structure(Potapov et al.,2021; Lang et al.,2023; Tolan et al.,2024; Pauls et al.,2024).
At the same time, local variation in TCH is pivotal for studying vegetation dynamics across landscapes. Local variation in TCH has a strong influence on spatial heterogeneity, which directly affects biodiversity through the creation of diverse ecological niches(Davies and Asner,2014; Coverdale and Davies,2023), predator-prey interactions(Loarie et al.,2013; Davies et al.,2016)and spatial patterns of aboveground carbon storage(Aponte et al.,2020; Noulèkoun et al.,2021; Tetemke et al.,2021).
As such, there is also a separate line of work in using SatML to expand the spatial range of high-resolution TCH data that has been collected in local regions using Airborne Laser Scanning (ALS) techniques(Wilkes et al.,2015; Astola et al.,2021).

Despite the increased use of SatML to map tree canopies at both global and local scales, there is a lack of scholarship connecting and contrasting the efforts of local modeling and global modeling for TCH mapping. This is indicative of a larger gap in research aimed at understanding how to balance (or distinguish between) the goals of local and global usability in geospatial machine learning more generally. Developing such an understanding is crucial to making efficient and sustainable progress in geospatial machine learning for environmental monitoring – from developing global models that can be efficiently fine-tuned for local use cases, to understanding when expensive local data-collection efforts are most needed, and how best to leverage local data among other possible data sources in SatML models.

To better understand the common and distinct facets of local and global mapping paradigms, our first research question (RQ) is:(RQ1) How necessary are locally collected reference data for local prediction, in light of existing global models and maps?The results of our case study indicate that local data are critical for generating ecologically relevant, accurate models in the Karingani Game Reserve region. Our locally-trained models reduce mean absolute error of existing global TCH maps by 40-69%. Similar performance increases extend to measures that are stratified by ecologically-relevant gradients – including true canopy height, distance to rivers, and geology. Furthermore, using globally-pretrained models for local fine-tuning did not substantially improve model performance compared to a random initialization. Overall, the best performing model of those we compare for our task is a model trainedonlywith local data.

While RQ1 addresses the importance of obtaining local data to build SatML models that perform well locally, the question of how to build accurate local models remains. Developing a locally-focused predictive model involves many design decisions, including: which model architecture to use, which imagery sources and spectral layers are used as model input, and possibly even how much local data should be collected, and where.
In order to understand the overall impact and possible co-dependencies of these different types of decisions, our second research question is:(RQ2) Which designable factors most influence reliability of local TCH models?We design experiments to compare the relative importance of: (i) the quantity and composition of training data sampled across ecological gradients(geographic representation), (ii) spectral layers of the satellite imagery(feature representation), and (iii) different machine learning architectures(model representation), on the performance of our local models. We find that these factors all have a large influence on model performance, with each factor having roughly the same magnitude of importance, and nuanced interactions between factors.

Our case study of TCH mapping in the Karingani Game Reserve is emblematic of local mapping settings that arise across many environmental monitoring domains. Thus, our experiments have implications across domains in SatML where predictive models can be trained and deployed at many different spatial scales.
In particular, our third research question asks:(RQ3) What are potential points of conflict or synergies in local and global modeling efforts across application domains in machine learning with satellite imagery?Examining the performance of recently proposed global TCH models, we find that recent advances in global performance do not necessarily confer performance improvements in our study domain (and in fact, some methods that improve on global performance substantially degrade local performance).
While we are careful not to draw absolute conclusions from our case study from a single region, we identify specific design decisions made in past global mapping efforts that may have hindered local usability in favor of global performance, and highlight other decisions that seem to explicitly align the efforts of global and local performance.
Importantly, many of the decisions we identify as beneficial are not specific to the TCH mapping task.
Connecting our findings to recent scholarship in other SatML tasks, we find that several indicators of tensions identified in our case study have been evidenced in other SatML domains (including crop classification, population density mapping, poverty mapping, and building segmentation).
Thus, we expect that many of the trends we examine here will shed light on how we might synergize local and global mapping efforts across different prediction tasks for global monitoring with SatML.

SECTION: 2Related Work

SECTION: 2.1Local vs. global prediction in general and geospatial machine learning

The distinction between learning representations that capture local variation and those that best capture global variation has long been established, for example concerning notions of “local” and “global” learning paradigms across general manifolds(Zhou et al.,2003; Saul and Roweis,2003; Huang et al.,2008). There is a corresponding discrepancy in output (prediction) space as well. A model trained to optimize average performance across an entire population or distribution can fail to capture meaningful variation within sub-regions or sub-populations due to limited model capacity or finite training data(Rolf et al.,2022), or because the model can mainly predict relative values across sub-groups of a population, but cannot reliably distinguish variation within sub-groups(Aiken et al.,2023).

Past studies have evidenced that the local-global distinction may be especially relevant forgeospatial machine learning, where the spatial nature of predictions across the globe makes the distinction between local and global prediction particularly clear.Kerner et al. (2024)evaluate existing global and regional land-cover maps in Sub-Saharan Africa, finding that global maps perform significantly worse than maps that were developed for regional use. This phenomenon extends to national-level maps as well;Yeh et al. (2020)produced national models to predict economic well-being from satellite imagery, but found that they could resolve national variation much better than variation within urban or rural areas. This result was replicated across many countries byAiken et al. (2023).
Discrepancies between local and global accuracy of SatML predictions have also been highlighted in the environmental monitoring domains of population estimation(Kuffer et al.,2022), building mapping(Gevaert et al.,2024), road detection(Nyandwi et al.,2024)and tree canopy height mapping(Pauls et al.,2024).
With the increasing focus on global modeling efforts and foundation models in SatML, it is important to understand not just where errors in published models are distributed across the globe, but also to more fundamentally understand what aspects of geospatial modeling might differ for a global training and evaluation objective, compared to a more localized one.

SECTION: 2.2Mapping tree canopy height with satellite imagery and machine learning

Several studies use machine learning with satellite imagery inputs to extend the spatial coverage of LiDAR-based canopy height maps in different local study regions. One thread of methodology uses locally collected LiDAR data, e.g. vegetation height from airborne LiDAR scanning (ALS), as the training labels for these predictive models. For example,Wilkes et al. (2015)used random forest models and publicly available satellite imagery to extend the range of ALS maps in east Victoria, Australia, andAstola et al. (2021)used Sentinel-2 data to generate TCH maps in Finland. Our methodology for “local mapping” (described inSection4) roughly follows this type of approach.

Four recent studies produce global tree canopy height maps using satellite data and machine learning. While all four maps are aimed at global coverage, each mapping effort has distinct goals that informed their design decisions:

Potapov et al. (2021)produced a 30m resolution map of canopy height across the globe for the year 2019 using a time series of Landsat satellite images. Features derived from pixelwise time series of Landsat data and digital elevation maps are used as input to bagged regression tree models, using GEDI111While GEDI data are sparse and somewhat noisy compared to ALS or UAV-derived LiDAR data, they have the advantage of global coverage. Relevant to our study region,Li et al. (2023)assess the suitability of the GEDI RH98 data product (designed for dense forests) to African savannas around South Africa, finding that the RH98 data are reliable for canopy heights between 3-15m, with limited ability to detect shrubs below 3m.data for label supervision. To capture regional trends,Potapov et al. (2021)train separate random forest models for each Global Land Analysis and Discovery (GLAD) Analysis Ready Data (ARD) tile (1region), using satellite data from the surrounding 12∘radius (seeFigure1for reference). Following prior convention, we refer to this as the “GLAD” map.

Lang et al. (2023)produced a 10m resolution global canopy height model for the year 2020 using 12-channel Sentinel-2 images concatenated with 3 channel transformations of locations (lat, lon) as input and GEDI labels as supervision. A key goal of this effort was to improve model performance for tall-height canopy regions, compared to the previous global map.Lang et al. (2023)use a modified XceptionS2 convolutional neural network architecture. Unlike the moving window approach ofPotapov et al. (2021),Lang et al. (2023)use a single approach globally, which consists of an ensemble of several models that is used to simultaneously output estimates of TCH and estimates of uncertainty in each region. Each model in the ensemble takes as input Sentinel-2 imagery concatenated with a 3-dimensional embedding of latitude and longitude position. Following prior convention, we refer to this as the “ETH” map.

Tolan et al. (2024)produced a higher-resolution (1m) global canopy height model using 3-channel Maxar imagery spanning the years 2018-2020. To achieve their goal of a high spatial resolution TCH map, they pretrained a vision transformer model with dense ALS data from two regions (California, USA, and São Paulo, Brazil) then used global Maxar satellite data to extend this model across space, using GEDI data to calibrate the pretrained model for regions across the globe. Following prior convention, we refer to this as the “Meta” map.

Most recently,Pauls et al. (2024)produced a global TCH map at 10m resolution. Noting a gap in performance between regional and global TCH maps,Pauls et al. (2024)focused on global-scale prediction that can be competitive with local regional maps. They devised a shift-resilient loss function to account for specific structures of the GEDI measurements and included elevation data as input (in addition to Sentinel-1 and Sentinel-2 data) to improve performance in mountainous regions. We refer to this map as thePauls et al. (2024)map.

The design decisions made in producing these existing global TCH maps reflect their common goal: to produce accurateglobalmaps of tree canopy height.
In contrast, in this study, our aim is to (i) assess the degree to which these globally-focused maps and models can aid in the goal of producing accuratelocalmaps, and (ii) compare how the design decisions for global ML models and maps might differ from the design decisions for local models and maps.

Most related to the local modeling setting of our case study, two existing studies use GEDI data to train and calibrate local TCH models. Relevant to our research questions,Healey et al. (2020)studied which spatial ranges are best for calibrating local models. Using data from across the globe, they found that local models trained on data from just a 3km radius outperformed models that were trained on data from larger radii. More recently,Tsao et al. (2023)developed local (country-level) models in Nigeria, focusing on tree height mapping for smallholder palm plantations, where performance in low-height canopy cover is important. Comparing the two globally available maps at the time (GLAD and ETH),Tsao et al. (2023)found that local calibration significantly outperformed both global maps in their study region. Similarly toHealey et al. (2020), they find that a relatively small radius is best for locally calibrating models for predicting short trees in their region of study. It is worth noting that both of these studies use random forest models. It remains to be seen how these findings would extend to larger machine learning models that incorporate spatial information in imagery.

SECTION: 3Case Study Problem Setting: Mapping Tree Canopy Height in Karingani Game Reserve

Karingani Game Reserve is an ecologically diverse area on the border of Mozambique and South Africa, spanning roughly 145,000 hectares. The Karingani Game Reserve was formally established in 2008 and is comprised primarily of subtropical savanna vegetation varying widely in height and density across gradients of topography, geology, soil type and mean annual rainfall. The reserve also contains a full suite of indigenous herbivore species. In addition to being an area of ecological importance, Karingani is an ideal region for a case study examining how best to map TCH in a local environment. Since the high-resolution LiDAR-based tree canopy height data we use for our case study (described inSection3.1) has not been used to-date to train or evaluate machine learning models, our case study can simulate different possible avenues and opportunities for producing a new map of TCH in a local region.

Our case study examines how to map tree canopy height across the Karingani Game Reserve using the different resources available (a limited amount of locally collected high-resolution TCH labels (Section3.1), publicly available satellite imagery (Section3.2), and existing maps and models (Section2.2) that have been produced for mapping TCH at a global scale). While we are of course interested in generating accurate maps of this region, the primary focus of our study is to understand more generally which of these current resources are most helpful for creating accurate local maps, and to identify additional resources or techniques that might aid in local mapping efforts.

SECTION: 3.1Local LiDAR-derived tree canopy height labels

Our local label data used for model training and evaluation span a study area of 24 sites in the Karingani Game Reserve (Figure1b), collected in May and early June 2021. The 1m resolution reference tree canopy height (TCH) maps are derived by applying a canopy height model (as detailed inBoucher et al. (2023); Reiner et al. (2023)) to LiDAR sensor data acquired by flying an unoccupied aerial vehicle (UAV) across the study sites. Of the 24 sites, 22 sites span roughly 21 km2and 2 span roughly 7 km2. For context, the LiDAR data collection via UAV flight took roughly 1 day per site, including travel time.

In contrast to the globally available (but sparse) GEDI data that past studies have used to build and extend TCH models, our local LiDAR-derived TCH labels are highly accurate ground-referenced data. Thus, we consider these data to be a valuable resource for evaluating model performance in this region. Moreover, understanding the performance gain achievable when training with local TCH data will help inform the value of acquiring local LiDAR data by flying UAVs in additional targeted regions in the future.

SECTION: 3.2Extending the range of existing label data with machine learning and Sentinel-2 satellite data

While the LiDAR-derived data spans sites across the Karingani Game Reserve, there are still significant gaps in the coverage of the existing TCH data (roughly two thirds of the Karingani Game Reserve are not covered by past UAV surveying efforts). In our case study, the goal is to train machine learning models that take publicly available satellite imagery as input to extend the range of the existing TCH data.

As input to our locally-trained machine learning models, we downloaded three Sentinel-2 satellite data tiles captured at a date near to the collection of the LiDAR data (5/13/21), which span the study area and have low cloud cover percentage over the 24 sites. Since the Sentinel-2 data has roughly 10m spatial resolution, we aligned these images with coarsened 10m-resolution LiDAR-derived TCH using cubic resampling.
We chose to restrict our local modeling efforts to mainly 12-band Sentinel-2 satellite imagery for simplicity of comparison. Past global-mapping approaches have used different input imagery and layers:Potapov et al. (2021)use Landsat imagery,Lang et al. (2023)append location embeddings to Sentinel-2 imagery,Tolan et al. (2024)use proprietary Maxar imagery, andPauls et al. (2024)use Sentinel-1 and Sentinel-2 data and digital elevation maps. We investigate the effect of input layer choices on model performance inSection5.3.

SECTION: 3.3Site-stratified training and evaluation splits simulate conditions of model use outside training data sites

To reflect our case study setting of applying the trained SatML models to fill in gaps where high-fidelity TCH data are not available, we employ a spatially stratified train/validation/test split of our data. Specifically, we split the 24 training sites into three distinct sets: training (12 sites), validation (6 sites) and testing (6 sites), as depicted inFigure1a,b. For each split, no test sites are seen during the training step or the model validation step in which we pick model parameters and hyperparameters.

When we train, validate, and evaluate on spatially disjointed sets of training sites in this manner, we are essentially simulating the intended conditions of our case study(Rolf,2023). Specifically, we simulate training our model in regions where we have data, then deploying our model to a separate set of sites that were not used in training or validating the model. Using the spatially disjointed test set ensures that the performance we evaluate more faithfully represents our intended use case in the Karingani Game Reserve.
We construct four such spatial data splits, where each LiDAR collection site appears in exactly one test set and exactly one evaluation set. By analyzing performance across the four splits, we can assess the variation among the models across different data settings, characterized by the available training data and target regions for model use.

SECTION: 4Methods

SECTION: 4.1Methods overview

Our experiments inSection5will reflect a range of different possible methodologies for predicting tree canopy height (TCH) in a local region. Since several global TCH maps are available, perhaps the most straightforward way to produce a map of TCH in a local area is to take one of the existing global TCH maps, crop and align it to the extent of our study area, and use that subset of the global map as our predictions. This is our primary methodology to “evaluate existing global maps” inSection5.1.

An alternative approach would be to use the limited local labels to train a new model specifically adapted to the local region. Here, we distinguish between “local-only” models, which we train from scratch using only labels and satellite imagery from the local region (here Karingani Game Reserve), and “globally-pretrained, locally-adapated” (or locally “fine-tuned”) models, which we fine-tune using local data only, but where the model is initialized from a representation that has been pretrained on global data. We use the term “global pretraining” to encompass both supervised models trained to predict TCH across the globe as well as self-supervised models pretrained on global satellite imagery in a way that is agnostic to possible downstream tasks. InSection5.2, we use our local data (Section3.1) to train local-only models and fine-tune existing supervised and self-supervised models that have seen global satellite data, to compare the effectiveness of both approaches. Ultimately, contrasting the performance of using existing global TCH maps “out of the box,” adapting globally trained models using local data, and training models using only local data will enable us to address our first research question regarding the importance of local data for local prediction.

To address our second research question regarding the importance of different design choices for local prediction, we construct a set of experiments in which we systematically vary different data and modeling decisions and measure their effects on local performance.
Specifically, inSection5.3, we train local models using different amounts of training data to assess the importance of the abundance of local data in training or adapting models for local use.
We train models with different input layers (Sentinel-2 bands and location encodings) as a way to estimate the relative effect that different data input decisions can have on model performance.
To contextualize the relative effects of these data decisions in practice, we compare the range of performance under these different data conditions to the range of performance due to changing the machine learning model architecture.

Our third research question involves identifying potential points of cohesion or distinction between the goals of local and global modeling in geospatial machine learning. Toward this end, we leverage our experimental results in this case study and pull in context from recent scientific reports across several application domains in geospatial machine learning for environmental monitoring. Contextualizing our in-depth analysis of local modeling from our case study amidst a broader set of trends in geospatial ML, inSection6.3, we pinpoint specific design decisions as well as general trends in geospatial machine learning that we believe warrant more research attention toward understanding and bridging the divide between global and local modeling.

SECTION: 4.2Implementation details

We compare multiple neural network model architectures throughout our experiments: (1) an XceptionS2 convolutional neural network architecture used in the ETH global predictions(Lang et al.,2023), which has 8 residual blocks each with two convolutional layers with 256 filters, (2) a U-Net architecture with a ResNet-18 backbone designed for pixel segmentation tasks, which takes the entire input (here acrop of the satellite image) as context for the predictions, and (3) a small five-layer fully convolutional network (FCN) with 128 filters per layer and leaky ReLUs between layers. SeeTable3for a comparison of the number of parameters for each model.
We additionally compare to (4) non-parametric pixel-level forest models, where the input is the multiple channel input for that particular pixel. This pixelwise model does not incorporate any texture in the images, thus it constitutes a simple and informative baseline for machine learning with remotely sensed data.

Our experiments compare the strategies of training models only with local data and using local data to fine-tune models that have been initialized through pretraining with global data. When we train models only with local data (using no labels or imagery outside the study area), we randomly initialize model weights and then optimize the model using supervised learning. For the models with globally pre-trained weights available222We note that while the Meta model is published for use, fine-tuning it would require purchasing proprietary satellite imagery. Pretrained weights fromPauls et al. (2024)were not publicly available at the time we conducted our experiments.(the ETH and U-Net models), we use these weights (which were derived using global data), and then fine-tune the models using local data.
For the XceptionS2 globally pretrained weights, we use the published weights corresponding to the first model (model 0) in the ensemble fromLang et al. (2023), available at the GitHub repository accompanying their paper. Since this model was already trained in a supervised learning setting for the task of TCH prediction, we use a transfer learning strategy in which we tune only the last few layers of the network.Table2shows a small difference in training the last 2 versus the last 3 layers, so we stop there.
For the U-Net model, we use the
self-supervised learning for earth observation (SSL4EO-S12) weights fromWang et al. (2023), which have been pretrained on global Sentinel-2 imagery using self-supervised learning techniques. We experiment with freezing the pretrained encoder weights and fine-tuning just the decoder versus fine-tuning the entire network for the U-Net.

All of our models are trained to optimize a mean squared error (MSE) objective, defined pixelwise. For each model configuration and each training split, we pick the hyperparameter configuration and model weights that correspond to the best validation set loss (MSE) achieved at any point during training (up to a maximum of 200 training epochs).
For each neural network model (trained from scratch or fine-tuned), we perform a hyperparameter validation step over the initial learning rate in [0.00001, 0.0001, 0.001, 0.01] and weight decay in [0.00001, 0.0001, 0.001, 0.01]. We use the AdamW(Loshchilov and Hutter,2019)optimizer and a learning rate scheduler that reduces the learning rate on performance plateaus by a factor of 0.1 with a patience of 10 epochs. Our neural network models are trained using thetorchgeopython package(Stewart et al.,2022). For the random forest models, we use the scikit-learn implementation of random forest regression(Pedregosa et al.,2011), and tune the maximum depth of the trees in [2, 4, 8, 16], and the number of trees in the ensemble in [50, 100, 200].

To account for some anomalously high values in the LiDAR-derived TCH labels, we set any pixels greater thanm to no-data (NaN) values. Pixels with TCHm are set to. This only occurs in a minimal percentage of the overall data; 99.95% of the non-NaN 1m-TCH pixel values fall within 0-30m. Next, we coarsen the labels to 10m-resolution to be consistent with the resolution of Sentinel-2 imagery. When coarsening, we use the 90th percentile value of the 1m pixel values to maintain consistency with past work(Potapov et al.,2021; Lang et al.,2023).

We preprocess the satellite imagery input to all of our locally trained models by channelwise normalization to zero-mean, unit-variance per channel across the three Sentinel-2 tiles that span the 24 sites. When we initialize the model with weights from a previous training or pretraining procedure, we use the image preprocessing transforms used during that previous procedure, as recommended byCorley et al. (2024).

We evaluate models pixelwise, at 10m resolution unless otherwise stated. When performance is reported by site, we use the models trained with the split where this site is in the test set. When performance is reported by split, we aggregate all pixels in a split to compute the average. When a single performance metric is given, this represents the average across the four splits.

When evaluating performance of the existing global TCH maps, we exclude from evaluation any pixels that contain no-data values in the global predictions (for the GLAD map, this excludes pixels corresponding to water, snow, ice, or other no-data-valued pixels). Less than 0.01% of the non-NaN pixels in the label data have a corresponding NaN in either of the global maps for both 10m and 30m-resolution, so this has a minimal impact on the reported performance metrics.

SECTION: 5Results

SECTION: 5.1Evaluating existing global maps

We first evaluate the performance of existing global TCH maps compared to our best locally trained or fine-tuned model, using both quantitative (Section5.1.1) and qualitative (Section5.1.2) assessments. We then assess the degree to which these different maps of TCH capture relative values of aboveground biomass in each site, as a way to gauge the usability of each map for additional ecological assessments in this local area (Section5.1.3).

Figure2summarizes the performance of the existing global models and our best local model (with extended results inTable1). The existing global maps achieve average root mean squared error (RMSE) between 2.43m and 4.51m, and for all maps the variation in performance across data splits (grey dots) is generally smaller than the variation across models. The RMSE of these maps in Karingani is significantly lower than the RMSEs of each map evaluated across the globe (3.09m vs. 9.25m forPotapov et al. (2021); 4.51m vs 8.62m forLang et al. (2023); 2.43m vs. 4.73m forPauls et al. (2024)).333These global metrics are reported through a common evaluation procedure inPauls et al. (2024), which differ slightly from the original papers. For the purposes of our study, the relative rankings of global performance and local performance is most relevant.Note that for all models, the RMSE values in Karingani are lower than the global RMSEs. This indicates that the Karingani Game Reserve is not a region of especially high error for the global models, in comparison to other places around the globe.

While the global performance generally increased with each subsequent global model that has been published,Figure2shows that
some of the more recently published maps perform worse in Karingani than older maps. Until the map byPauls et al. (2024)was released, the best existing TCH map for the Karingani region (by our quantitative metrics) was the 30m resolution map produced byPotapov et al. (2021). We discuss implications of this result for understanding when and how efforts to increase global mapping performance can align with increasing local performance (our RQ3) inSection6.3.

Our locally-trained model is superior to the existing global maps by a large margin. The predictions of our best-performing model (local FCN) achieve, on average, a 64% lower RSME than the ETH map (4.51m → 1.64m), a 47% lower RMSE than the GLAD map (3.09m → 1.65m), a 53% lower RMSE than the Meta map (3.51m → 1.65m), and a 33% lower RMSE than thePauls et al. (2024)map (2.43m → 1.65m) in our study area. The gray dots inFigure2show the performance for each split individually, showing that these differences are consistent across different data splits.

Figure3disaggregates performance differences across ecologically relevant strata, including the true TCH values, geology, and distance to the nearest river corresponding to each pixel (seeAppendixBfor more details of the stratification process for these evaluations). Across these strata, our locally-trained models consistently show less bias (measured with average signed error) compared to the publicly available global models. The existing global models exhibit different biases among strata, with all models tending to under-predict pixels with higher canopy height. All models have similarly consistent prediction bias across geology and distance to rivers, with the model ofPauls et al. (2024)tending to over-predict TCH, and the remaining existing models tending to under-predict TCH. Our locally trained FCN model exhibits a median of around 0 bias, despite the out-of-sample test set (Figure1a,b) requiring some degree of spatial generalization.

Figure4plots our predictions and the data from the four publicly available maps for three representative sites in our study area. From visual inspection, our local predictions are clearly a better match to the LiDAR-derived TCH values than any of the existing global maps. In contrast, the GLAD map tends to under-predict TCH, and generally has more spatially smooth predictions than the label data (the original resolution of the GLAD map is 30m). The ETH map also tends to predict zeros more frequently, but sometimes captures higher canopy heights (reliably capturing higher canopy heights was a goal of the ETH study(Lang et al.,2023)). For example in the Massingir site, the ETH predictions tend toward more extreme values than the label data convey (Figure4, first row). The Meta predictions capture more fine-grained textural information better than the GLAD or ETH map, but tend to underpredict TCH overall. The map fromPauls et al. (2024)is the closest existing map to getting the average pixel values correct, but predictions appear to biased toward the mean value, missing some of the high and low values (note also thatPauls et al. (2024)do not estimate any canopy heights under 3m).Figure7inAppendixAplots the distribution of predictions and labels per pixel, aggregated across the 24 sites, confirming that these trends hold broadly across our study area.

To probe the degree to which these TCH maps could be used “out of the box” to assess other ecological spatial patterns related to tree canopy monitoring, we assess the degree to which each map could be locally calibrated to predict aboveground biomass (AGB). Using established allometric equations available in the ecology literature(Colgan et al.,2013), we estimate AGB (in kg) for each site using the 1m LiDAR-derived TCH values and the linear-affine formula on the average height () of pixels designated as canopy cover ().444Note that the reference AGB data are estimated from ground-truth TCH labels, not direct measurements.Canopy cover is defined as a binary variable valued atfor pixels with canopy height above a threshold of 1.5m –Colgan et al. (2013)use TCH maps with 30m resolution, while we use 1m resolution data for our reference values, and 10m resolution data for our predicted values, except for those derived from the Meta map, which has 1m resolution).
We use the predicted values to estimate the same dependent variableat each site, for each map.
We fit a least squares regression to associate these X values with the calculated AGB values for each site, as a local calibration procedure would do.

Thescores (coefficient of determination) resulting from the linear-affine fits (summarized inFigure8) are 0.11 (ETH), 0.22 (GLAD), 0.52 (Meta), 0.41 (Pauls et al. (2024)), and 0.52 (our locally trained FCN). Overall, our locally trained FCN predictions with 10m resolution provide a signal of the relative AGB across the 24 study sites that is similar to what we would get from the 1m resolution Meta map, after a linear calibration. The map fromPauls et al. (2024)captures a slightly lower degree of relative variation in AGB across sites, whereas the lowvalues of the fit from the ETH and GLAD maps suggest that these maps are not as well suited to estimating relative AGB across sites (at least not using this established calibration formula).

SECTION: 5.2Training local models from scratch vs. initializing with weights from global (pre)training

We next examine whether existing global models for machine learning with satellite imagery (SatML) can be a better starting place from which to fine-tune models with local data, versus training from scratch.Table1compares the performance of locally fine-tuned models initialized with weights from existing global models, versus initialized randomly.

Table1is organized according to two different strategies for adapting globally initialized models for local use: transferring a global supervised model for local use, and fine-tuning a task-agnostic self-supervised SatML model. We use the first strategy to adapt the XceptionS2 models – trained byLang et al. (2023)for supervised learning of global TCH with GEDI LiDAR data as the original labels – for our local use. When using this pretrained model as a starting point for local training, we deploy a transfer-learning strategy and fine-tune the last three layers of the model. We also train randomly initialized XceptionS2 models for comparison. We introduce a variant without the location embedding input layers, in case these layers hamper performance in our site-stratified setting, which includes some degree of spatial extrapolation. The results inTable1show that there are marginal differences in performance when using the pre-initialized weights versus training XceptionS2 models from scratch. Comparing within the set of XceptionS2 models we tested, the difference between the models initialized with the weights from theLang et al. (2023)model and the best randomly initialized XceptionS2 model is0.05m in RMSE and0.02m in mean absolute error (MAE).

Our second type of local adaption starts with a U-Net model with a ResNet-18 backbone, which has been pretrained byWang et al. (2023)using self-supervised learning on globally-distributed Sentinel-2 satellite imagery. We experiment with fine-tuning the entire model versus fine-tuning only the decoder layers.Table1shows that the largest difference in average performance between a pretrained model after fine-tuning and a model of the same architecture trained from scratch is0.23m in RMSE, and0.06m in MAE. We note the higher variability (across random seeds during training) in the performance of the U-Net models compared to the other model architectures.555This was in part due to model instability toward the outer edges of the image patches – we attempted to mitigate this effect during evaluation by using only the innermostpixels of eachmoving window patch when tiling our predictions as maps, though we found that the FCN and XceptionS2 models were still generally better suited to this task for our training procedure.

Overall, the most substantial performance difference over the publicly available TCH maps (last four rows ofTable1) comes from using local data in any capacity. Our best locally trained model improves performance compared to the best globally produced data product by 0.79m in average RMSE and 0.81m in average MAE. We note that even our worst locally fine-tuned models fromTable1improve performance compared to the best globally produced data products.
Under conditions where local data are used, the marginal differences in performance due to initializing models randomly or with pretrained weights are very small. In fact, the best overall performing model is still our small FCN model trained only with local data, with an improvement of 0.34m in average RMSE and 0.28m in average MAE compared to the best of our globally initialized, locally fine-tuned models.

SECTION: 5.3Relative importance of design decisions across modeling and data choices

The results detailed inSections5.1and5.2show that models tailored for local prediction can significantly improve the performance of TCH models in local regions.
In light of this, we investigate the extent to which several different design decisions a research team faces in designing local models affect the overall performance. One goal of these experiments is to investigate the relative value of future efforts ranging from innovating on the machine learning models, choosing which satellite data layers and sources to use as inputs to the model, and even deciding how much labeled data to collect, and where to collect them.

Figure5summarizes an experiment in which we vary one of the following design decisions at a time: which machine learning model architecture is used to define the model (Figure5a), which data layers are used as input to the model (Figure5b), and the amount of local labeled data available for training the supervised models (Figure5c). We find that each design decision has a substantial effect on overall performance. Interestingly, we find that the variation in model performance (RMSE) due to any of these factors is within roughly the same range; note the shared vertical axis range inFigure5.

When it comes to choice of model architecture, we found that simple convolutional models can perform well at local prediction tasks (Figure5a). Our best performing model is a small FCN model, with 128 filters per layer, and 5 layers, amounting to a total of 604,417 parameters (Table3), and a receptive field of onlypixels. This is the model we use throughout (with 12-channel Sentinel-2 inputs) as “locally-trained FCN.” The larger XceptionS2 and U-Net models had slightly worse performance, for both the randomly initialized and the globally pre-initialized variants of each model.

To understand how the availability of local data affects model performance, we take different random samples of the training sites for each of 3, 6, and 9 total sites, and compute the average model performance after training with those subsetted training sets, using our best model (locally trained FCN). To reduce the computational cost of this analysis, we fix the model hyperparameters to those used to train the local-only FCN model for each split used in the previous analyses. The total number of instances seen per epoch is kept consistent across conditions.
The model performance resulting from using a different number of training sites is shown inFigure5b. As expected, performance increases on average as the number of sites used for training increases, which is consistent across splits. The rate of returns of the average and per-split performances suggests that having even more training data would likely result in even better model performance.

Choices made about the satellite imagery also impact performance (Figure5c). For our overall best performing model architecture, performance is worst when using just the red, green, blue (RGB) spectral bands of the image, while average RMSE decreases by 10% with the addition of a fourth near-infrared (NIR) band (Figure5c). Including the remaining 8 Sentinel-2 bands has a small but positive effect on performance. Adding 3-channel location embeddings as inLang et al. (2023)also did not change performance greatly for our locally-trained FCN, and if anything decreased average performance. This was also the case for the locally fine-tuned XceptionS2 model (Table1).
The absolute differences in model performance across the panels inFigure5contextualize the impact of different design decisions that must be considered in building a SatML model for use in local areas. Overall, the spread in performance due to these different decision factors – which model architecture to use, which data layers to use, and how much training data to use or acquire – is similar in magnitude, as indicated by the similar range of performance within each panel ofFigure5. The performance gap (difference in RMSE averaged across splits) due to using our second-best locally trained model (XceptionS2) versus our best (FCN), is 0.30m, while the performance gap due to using 3-channel (instead of 12-channel) imagery with the FCN is 0.22m. Likewise, the amount of training data impacts performance significantly; the difference between using 3 sites of training data versus 12 results in an average performance gap of 0.25m.

While each of these design decisions impacts local model performance in a distinct way, it is also important to understand if and how these data and modeling choices interact.Figure6summarizes the result of ten training runs with different subsets of 3, 6, 9 or all 12 training sites across different model architectures for a single train/val/test split (the split shown corresponds toFigure1;
the overall trends we describe are consistent across all four of the data splits).Figure6shows that the average performance generally degrades with fewer sites available for training, extending the results ofFigure5b across different model architectures. The magnitude of the degradation differs across models, as seen by comparing the trends in the filled-in gray dots inFigure6across each panel.

For all models, variation in model performance across different subsets (variation in each vertical set of open gray dots) tends to increase as the number of training sites decreases (Figure6).
For the XcpetionS2 model, a substantial number of the training runs using fewer than 12 training sites actually outperformed the median performance using 12 sites.
Similarly, for the locally trained random forest model, the best of the 10 runs for each number of training sites was better than the best performing model using 12 sites.
Given the high variability in performance across each condition on models and number of training sites, it is not possible to conclude from this experiment alone whether the best-performing runs with subsetted data have better performance because of the data or because of the random instantiation of the model.
Examining the random forest models, where variation is low across random seeds for the 12-training-site condition, suggests that some of the performance improvement might be due to the change in training data sites for that model.

SECTION: 6Discussion

SECTION: 6.1The value of local data for model training, fine-tuning, and evaluation

In response to our first key research question, we find thatlocally collected reference data remain pivotal for generating the best possible local mapsof tree canopy height (TCH) in our study region.
Our first set of model comparisons (Section5.1) examined the performance differences of models trained only with locally collected labels derived from UAV-LiDAR, versus using globally-available TCH maps derived from global (but sparse and noisy) GEDI data.
While the existing global TCH maps have the benefit of being ready to use “off the shelf” without downloading any satellite imagery, training new models, or collecting local training data, their performance in the Karangani Game Reserve area was significantly worse than what we achieved with a small FCN model trained withonlya limited amount of local, high-fidelity labeled data. This finding holds up when evaluating performance across ecologically relevant strata. Considering the potential downstream task of using TCH maps to estimate aboveground biomass, the 1m resolution Meta map(Tolan et al.,2024)performed on par with our local model, while the other existing maps performed worse in this use case. This raises the importance of validating global maps for local scientific analyses, even when those maps will be calibrated for local trends.

The intermediate performance of the locally trained pixel-wise random forest (with no spatial context) inTable1suggests that some, but not all, of the performance gap between models optimized for global use and models trained or fine-tuned for local use may be due to the calibration to local label distributions.
Our second set of experiments (Section5.2) shows the value of local data for fine-tuning globally pre-initialized models for use in a specific region. While the best overall models we trained only used local data, we still find that models initialized with weights from global training procedures can be substantially improved by using local data for fine-tuning (note, for example, the improvement in performance of the locally fine-tuned XceptionS2 model over the published XceptionS2 global map fromLang et al. (2023)inTable1). This result underscores the potential for locally collected data to improve the regional performance of globally pretrained models, even as the usefulness of pretrained models for local use cases advances.

Lastly,we emphasize the importance of the locally collected data for evaluating the performance of geospatial ML models. Despite an increase in high-quality local and regional TCH maps being released recently (mostly in the Global North, e.g.,Fogel et al. (2024); National Ecological Observatory Network (2024) (NEON)), understanding and assessing the local performance of global TCH maps across Earth’s diverse continents and biomes remains difficult due to a lack of globally-distributed test sites with high quality, locally collected TCH data. We expect that careful collection of local data will remain paramount to assessing and improving progress in geospatial machine learning for global environmental monitoring.

SECTION: 6.2The effects of geographic-, feature-, and model-representation on local performance

In response to our second research question, we find thatthe choices regarding (i) what model architecture to use, (ii) what data layers to use as input, and (iii) where label data are collected can all have a large impact on local performance.
Overall, there are substantial differences in performance across the model architectures we trained and fine-tuned with local data (Table1).
Notably, all of the models we used here have been proposed and used in prior works, and are not designed specifically for our local TCH modeling task. Thus, it is possible that local performance in areas like the Karingani Game Reserve could be further improved with modeling innovations – either by designing models specifically for local training, or by designing global models to be more suitable for downstream adaptation in smaller local areas.

Data choices similarly had a large impact on local performance. The availability of local label data was crucial for success in training and fine-tuning models for local use in the Karingani Game Reserve.
The choice of which spectral bands and geographic data layers to use as input also had a significant impact on performance.
In particular, the inclusion of additional spectral bands (beyond 3-channel RGB) decreased the average error of our best model by 10%. For future researchers, decisions about which imagery to use should be based not only the spectral bands available, but also on the spatial and temporal resolution of the available imagery, which can be pivotal to model success. We expect that the difference in performance due to including all spectral bands of Sentinel-2 imagery is a conservative estimate of the performance differences that would arise from using different satellite products (e.g. Landsat or Maxar satellite imagery, which have different resolutions).

From our experiments inSection5.3, we find thatthe performance differences due to data design decisions were similar to the performance differences due to model architecture decisions.The absolute difference in local performance (measured against our best model, which used all training sites and all Sentinel-2 input layers) is strikingly similar across the three different types of variations we tested: (i) using a model architecture from recent TCH-mapping research efforts, (ii) using only RGB spectral bands from the satellite imagery input, or (iii) using only 3 randomly selected training sites instead of 12.
From these findings, we conclude that the quantity and composition of the available local training data sites affected model performance with an effect comparable to what we might expect from innovations in model architectures (as roughly reflected by the gap in performance between our best and second-best models inFigure5a).

SECTION: 6.3Identifying points of conflict and synergy in local and global mapping

Our third research question concerns identifying points of potential disparity or cohesion between the goals of local and global modeling in geospatial machine learning.
While global maps will clearly exhibit different errors in different locations (average performance in certain regions will differ from average global performance), we might hope that the existence of global models would make it easier to build good models for local use. However, the results of our case study underscore thatcurrently, globally pretrained models are not necessarily the “right” starting point for developing good local models.

After extensive experimentation, existing globally trained models did not factor in to the best local modeling solution we found in our case study. At the same time, we expect that our local models would perform terribly across the globe, due to domain shifts and lack of global diversity in our training data. This discrepancy suggests that there are at least some points of conflict between local and global mapping endeavors.
We caveat that since our case study concerns a single prediction task and study area (which allowed us to focus on the nuances of a specific local downstream use case), our discussion of RQ3 – as it pertains to geospatial modeling across general tasks – is more speculative in nature than our discussion of our first two research questions.
Interpreting the differences in performance from past global TCH mapping efforts in the context of a broader set of geospatial ML studies, we can still generate insights as to which modeling choices that influenced the differences in local performance in the Karingani Game Reserve are likely to generalize more broadly across different geospatial machine learning tasks.

Our results suggest thatthe best decisions about which data to use during training may differ for local and global use cases, both in terms of which regions to include training data from, and which input layers to use.
Regarding theregions that training data are drawn from, we hypothesize that the relatively lasting success of the GLAD map(Potapov et al.,2021)is due in part to the fact that is actually an accumulation of regional models applied across the globe. This hypothesis is consistent with results of previous work(Healey et al.,2020; Tsao et al.,2023), which showed that there may be an optimal radius of training data that is large enough to constitute enough volume and diversity of training data, yet small enough to generate specialized, accurate local models.
Regardinginput layers,Lang et al. (2023)included positional encoding layers (derivatives of latitude and longitude) to achieve good global performance. However, for our local-mapping task, including positional encoding layers did not help with local fine-tuning performance, and possibly were a detriment to local performance (Table1andFigure5).
One possible reason for this result is that while direct positional encodings can help with spatial interpolation (e.g. filling in the gaps in sparse but global GEDI data), they may not be necessary over smaller spatial extents, or may even hinder performance in spatial extrapolation settings (e.g. extending the spatial range of local training data). Similar concerns have also been raised in other geospatial machine learning domains(Meyer et al.,2019; Lilja et al.,2024).

The differences in the performance of existing global TCH maps is consistent with the hypothesis thatmodeling efforts that account for and correct local error structures in global labels may be an important step to synergizing local and global supervised learning efforts.
Only one of the three most recent global TCH maps(Pauls et al.,2024)improved upon the performance of the original 2021 GLAD global map, when evaluated in Karingani. In their study,Pauls et al. (2024)explicitly aimed to remedy gaps in local performance of global TCH models by accounting for error structures in the global GEDI data they used for training. Specifically, they accounted for localization error due to uncertainty in the position of each GEDI data point, and they removed observations from high slopes (which make the satellite-based LiDAR measurements especially noisy) during training.
The general strategy of accounting for and correcting error structures can be applied in many geospatial ML domains where some form of global labels may be available, but these labels likely come with localization noise, label noise, or statistical biases(Kuffer et al.,2022; Gevaert et al.,2024; Rolf et al.,2024).

SECTION: 7General takeaways for geospatial machine learning

In addition to addressing our three key research questions as discussed inSection6, the results from our case study provide three concrete takeaways with implications for future efforts in geospatial machine learning at different spatial scales.

Takeaway 1: The best global models and the best local models may differ in model design and training strategy.Our results provide evidence that geospatial machine learning models that capture global variation well – as all four existing global maps of tree canopy height do – might not sufficiently capture local variation in the target variable of interest without significant local calibration or fine-tuning.
Our results reinforce scholarship that found that SatML models trained in large areas often break down when evaluated in local regions, whereas locally-trained models have been evidenced to do much better. Specifically, the breakdown of global maps in local regions has been shown across domains including crop land classification(Kerner et al.,2024), population density estimation(Kuffer et al.,2022), building detection(Gevaert et al.,2024), and poverty mapping(Aiken et al.,2023).

Our best model for mapping TCH in the Karingani Game Reserve was a relatively small model trained only with local data (LiDAR-derived labels and Sentinel-2 imagery). This suggests that when training local models or pretraining global models for local use, it might be advantageous to use smaller models. At the same time, ultra-small models, such as a pixelwise random forest, obtained reasonable performance but performed worse than models that incorporate spatial context (e.g. texture) in images. Additionally, we found that design decisions aimed to enhance global modeling, such as including direct location embeddings as input to the model, may not be necessary for local models, and might actually hinder performance in a local mapping setting.

Local and global mapping efforts are both crucial to informing scientific knowledge about the Earth and its environment.
At the same, both efforts have significant costs, and it should not be assumed that global models will be sufficient for capturing local variation, without modification.
This brings us to our next major takeaway, concerning the creation of ML models for environmental monitoring for use across both global and local scales.

Takeaway 2: If geospatial machine learning models need to jointly satisfy global and local monitoring use cases, then novel research innovations may be needed.It is tempting to expect that working to improve global performance for geospatial ML models will implicitly result in models that are more useful for local modeling tasks as well.
However, results fromSection5.2suggest that at present, the tree canopy height models designed for global prediction do not necessarily provide a better model initialization for our local mapping task in the Karingani Game Reserve.
Given the substantial costs of training global models, and the additional substantial cost of collecting local data to calibrate global models (or train entirely local models), it is important to find synergy between the efforts made for global and local modeling with geospatial ML.

Our findings suggest that fundamental innovations in this general direction of research may be needed to achieve the practical goal of building global models suitable for downstream use in local settings.
Straightforward fine-tuning or calibration of global models did not garner the best local performance for tree canopy height mapping in the Karingani Game Reserve region. However, there are still several areas of opportunity for developing synergistic modeling solutions where models trained with global data may help with downstream local tasks. For example, model distillation methods(Polino et al.,2018; Vemulapalli et al.,2023)may hold promise for improved fine-tuning of large global models for local tasks. Another avenue to design models for local use cases might explicitly design the pretraining strategy for different local regions, similar to how meta-learning has been used to initialize geospatial ML models used for different prediction task domains(Rußwurm et al.,2024).
Our results have several implications for how to build geospatial ML models with global data, especially if the intent is to fine-tune these models for local contexts.

Takeaway 3: Data-centric approaches may be a promising avenue for jointly achieving local and global performance with geospatial machine learning.Several different aspects of our results indicate that data-centric innovations could be key to expanding the joint frontiers of local and global mapping efforts.
As discussed inSection6.3,
the best of the existing global models in terms of both global and local performance mostly focuses on curating and preprocessing the training data(Pauls et al.,2024), while the next best model (the GLAD model ofPotapov et al. (2021)) is actually an accumulation of regional models.
Additionally, our results inSection5.3indicate that even a very coarse and straightforward experimentation with different data conditions and processing strategies could lead to substantial increases in local performance.

Many different approaches to data-centric geospatial machine learning exist and are being developed – see the rich discussion inRoscher et al. (2024).
Adding to this discussion, our results underscore the importance of studying how best practices regarding data and model decisions in SatML might differ for local and global mapping paradigms.
Better guidelines are needed for how best to collect ground-referenced data, which data source(s) to use for which tasks, and how best to preprocess global and local data for training geospatial machine learning models – across a variety of application domains and prediction settings ranging from global to hyperlocal.

SECTION: 8Conclusion

In this paper, we studied perspectives of local and global mapping with machine learning and satellite imagery, rooted in a case study of tree canopy height mapping in the ecologically diverse Karingani Game Reserve in Mozambique.
Tree canopy height prediction is a setting in which both global and local maps are pivotal to inform environmental and ecological science and policy.
Global maps are crucial for understanding planetary ecosystem and tree composition, while local maps are crucial for scientific studies in landscape ecology and informing locally-enacted policies.
The same argument – that both global and local maps are fundamentally needed – can be made for virtually any environmental or ecological variable of interest, from land cover classification to air quality estimation.
Consequently, we need geospatial machine learning models that can resolve both large-scale global variation and fine-grained local variation in key environmental variables.

Our study focused on the task of tree canopy height mapping in Karingani Game Reserve, which has not been previously used to train or evaluate geospatial ML models. As such, these data enabled us to ground our study in considerations researchers and modelers face when applying geospatial machine learning for ecological and environmental modeling to a new area. As discussed inSection6, we expect that our main findings will generalize to different local areas and to different prediction tasks due to evidence of similar local-global map discrepancies in other geospatial prediction domains. That said, we expect that the exact numbers and nuanced insights may differ from setting to setting. It would be informative to repeat our study across many local areas and similar prediction tasks in future work (made possible with a growing number of public datasets representing mapping tasks related to trees(Weinstein et al.,2021; Reiersen et al.,2022; Ouaknine et al.,2023; Ahlswede et al.,2023; Bountos et al.,2023)).

Our results indicate that, at present, global and local mapping may constitute distinct modeling goals for geospatial machine learning. This hypothesis is supported by previous findings across tasks including cropland mapping(Kerner et al.,2024), poverty prediction(Aiken et al.,2023), population density estimation(Kuffer et al.,2022), and building detection(Gevaert et al.,2024). Even the best global models for tree canopy height mapping, which represent impressive efforts in data processing and model architecture(Potapov et al.,2021; Lang et al.,2023; Tolan et al.,2024; Pauls et al.,2024)are outperformed by simple fully convolutional networks trained on roughly 250 square kilometers of carefully collected highly accurate label data paired with publicly available Sentinel-2 satellite imagery. Moreover, we find that some of the design decisions that past work found important for global modeling were either unnecessary or potentially harmful to local performance.

Looking forward, there is a great opportunity to develop models that can flexibly interpolate between different prediction extents, ranging from global to hyperlocal.
Our study exposes a need for future research directions that prioritize the performance of geospatial machine learning models in local areas – for example models pretrained on global satellite imagery to be efficiently adapted for local use – alongside global performance. Distinguishing between the goals of local and global modeling is a first step to establishing best practices for model and design decisions for applications of geospatial machine learning for environmental monitoring. It is also an important step toward developing learning paradigms that enhance the synergy between the two efforts, for example, building models that flexibly leverage both local and global data depending on data availability and the prediction region at hand. In addition to synergistic local-global modeling efforts, an emphasis on the quality of local data for model evaluation and calibration remains critical for local, global, and synergistic model developments.

SECTION: Acknowledgements

The authors thank Nico Lang for answering questions that helped us implement the XceptionS2 models fromLang et al. (2023)and Konstantin Klemmer for feedback on an earlier version of this manuscript. Karingani Game Reserve is thanked for permission to collect LiDAR data, as well as for significant logistical support during data collection, with particular thanks to Ellery Worth for on the ground support. Pete Goodman is thanked for providing spatial data on rivers and geology. Tom Lautenbach and Michael Voysey are thanked for helping with data collection, and Jenia Singh and Peter Boucher for LiDAR data processing. Karingani Holdings are acknowledged for funding the LiDAR data collection.E.R. was supported by postdoctoral fellowships from the Harvard Data Science Initiative and the Harvard Center for Research on Computation and Society.
L.G. was supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE2140743.

SECTION: References

SECTION: Appendix AAdditional supporting results

This appendix details supporting figures and tables referenced from the main text.

SECTION: Appendix BEvaluation procedure for stratifying by feature

SECTION: B.1Ground-truth Height

To stratify performance by the ground-truth canopy height, we calculate the residuals for the pixels whose label falls within the specified range.

SECTION: B.2Distance to River

To stratify performance by distance to river, we start with a data product provided to us by researchers at the Karingani Game Reserve spanning the study area in vector format that has rivers encoded as lines. Note that these “rivers” correspond to paths through which water could flow but are not necessarily active rivers during all parts of the year. We rasterize this vector data and then for each site, we calculate the distance between each pixel in the site and every river pixel. We then take the minimum of those as the “distance to river” for that pixel. During evaluation we then calculate the residuals for the pixels whose minimum distance to any river falls within the specified range.

SECTION: B.3Geology

To stratify performance by geology type, we start with a data product spanning the study area in vector format that has geology type encoded as polygons, provided to us by researchers at the Karingani Game Reserve. We rasterize this vector data and then for each site, we record the geology type for each pixel in the site. During evaluation we then calculate the residuals for the pixels whose geology type matches the specified category.